Metadata Prompt: 

 {'datasets': 'What datasets were used to train the model (include link if possible)', 'license': 'What is the license', 'github': 'What links to github repositories are available and what these links are for', 'paper': 'What is the research paper associated to this model', 'upstream_model': 'What is the upstream model of this model', 'parameter_count': 'What are the number of parameters (#params) this model is trained on', 'hyper_parameters': 'What are the values of some hyper parameters (parameters that control the learning process of the model) of this model.', 'evaluation': 'What is the evalutaion of the model. What evaluation metrics where used, and what are the results (include whole table if possible)', 'hardware': 'What hardware was used to train this model', 'limitation_and_bias': 'What are the limitations and biases of the model', 'demo': 'Find a form of demo for the model could be a link, code snippet or short paragraph', 'input_format': 'What is the format of the data used as input for the model', 'output_format': 'What is the format of the data used as output of the model', 'input_preprocessing': 'What is the input preprocessing of this model', 'input_size': 'What is the image input size', 'max_sequence_length': 'What is the max sequence length of this NLP model', 'vocabulary_size': 'What is the vocabulary size of this NLP model', 'sample_rate': 'What is the sample rate of this model', 'agent': 'What is the agent of this reinforcement learning model', 'training_environment': 'What is the training environment of this reinforcement learning model', 'SB3': 'Is SB3 used in this reinforcement learning model'}
Extraction Prompt: 

Given metadata information of huggingface {model_type} model : {model}, extract the properties of ONE single entity mentioned in the 'information_extraction' function.
     Extraction rules: 
     - rule 1: Adhere strictly to the schema structure in 'information_extraction'
     - rule 2: If a property is not present but is required in the function parameters, output  instead
     - rule 3: If a property is not present and is not required in the function parameters, do not include it in the output
     - rule 4: Only extract one item for 'info' in  'information_extraction' function 
     Extraction rules for specific metadata: 
     - datasets: only return dataset used to train or finetune model, not the upstream model of the model
     - github: extract github link of this model (only return the github link)
     - paper: if a research paper was written, extract arxiv research paper link (only return the url of the paper)
     - parameter_count: The number of parameters the model was trained on, sometimes represented in the form #params
     - upstream_model: provide huggingface model ID of upstream model
     - hyper_parameters: extract possible hyperparameters
     - evaluation: extract evaluation metric/tasks and their respective evaluation results
     - hardware: extract any hardware used to train the model
     - limitation_and_biases: extract a short summary of limitation and biases of the model
     - demo: extract any links, code snippets, or small paragraphs on how to use the model
     - input_format: extract the input format/requirement for this model
     - output_format: extract the output format of this model
 

#####################OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5########################

-------------------- datasets --------------------

Document 1:

- oasst_export:
lang: 'bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk'
input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
val_split: 0.05
------------------------------
Document 2:

EleutherAI / pythia-12b-deduped, Open-Assistant/model/model_training

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

"License: Apache 2.0"

-------------------- github --------------------

Document 1:

"Open-Assistant project" "https://github.com/LAION-AI/Open-Assistant" "https://open-assistant.io/"

-------------------- paper --------------------

Document 1:

"human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023."
------------------------------
Document 2:

- base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)
- command: `deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000`
- data:
```
reference-data:
datasets:
- oasst_export:
lang: 'bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk'
input_file_path: 2023-03-25_oasst

-------------------- upstream_model --------------------

Document 1:

upstream_model: Pythia 12B
------------------------------
Document 2:

"Finetuned from: [EleutherAI / pythia-12b-deduped](https://huggingface.co/EleutherAI/pythia-12b-deduped)"

-------------------- parameter_count --------------------

Document 1:

"gradient_accumulation_steps: 2"

-------------------- hyper_parameters --------------------

Document 1:

- learning_rate: 6e-6
- weight_decay: 0.0
- max_length: 2048
- warmup_steps: 100
- gradient_accumulation_steps: 2
- per_device_train_batch_size: 4
- per_device_eval_batch_size: 4
- eval_steps: 100
- save_steps: 1000
- num_train_epochs: 8
- save_total_limit: 4

-------------------- evaluation --------------------

Document 1:

- wandb: https://wandb.ai/open-assistant/supervised-finetuning/runs/770a0t41
- base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)
- checkpoint: 4000 steps
- command: `deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000`
- data:
```
reference-data:
datasets:
- oasst_export:
lang: 'bg,ca,cs,da,de,en,es,fr,hr,hu

-------------------- hardware --------------------

Document 1:

"base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)", "model_name andreaskoepf/pythia-12b-pre-2000", "model_name: EleutherAI/pythia-12b-deduped"

-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

"human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023."
------------------------------
Document 2:

"Demo: [Continuations for 250 random prompts](https://open-assistant.github.io/oasst-model-eval/?f=https%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Foasst-sft%2F2023-04-03_andreaskoepf_oasst-sft-4-pythia-12b-epoch-3_5_sampling_noprefix_lottery.json%0Ahttps%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Fchat-gpt%2F2023-04-11_gpt-3.5-turbo_lottery.json)"

-------------------- input_format --------------------

Document 1:

input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
dtype: fp16
------------------------------
Document 2:

`<|prompter|>` and `<|assistant|>` tokens, `

-------------------- output_format --------------------

Document 1:

output_format: fp16

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['oasst_export'], 'license': 'apache-2.0', 'github': 'https://github.com/LAION-AI/Ope 
n-Assistant', 'paper': 'https://open-assistant.io/', 'upstream_model': 'Pythia 12B', 'parameter_coun 
t': 'gradient_accumulation_steps: 2', 'hyper_parameters': {'learning_rate': '6e-6', 'weight_decay':  
'0.0', 'max_length': '2048', 'warmup_steps': '100', 'gradient_accumulation_steps': '2', 'per_device_ 
train_batch_size': '4', 'per_device_eval_batch_size': '4', 'eval_steps': '100', 'save_steps': '1000' 
, 'num_train_epochs': '8', 'save_total_limit': '4'}, 'evaluation': [{'test': 'wandb', 'result': 'htt 
ps://wandb.ai/open-assistant/supervised-finetuning/runs/770a0t41'}], 'hardware': 'base model: [andre 
askoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000), model_name an 
dreaskoepf/pythia-12b-pre-2000, model_name: EleutherAI/pythia-12b-deduped', 'limitation_and_bias': ' 
', 'demo': 'human demonstrations of assistant conversations collected through the [https://open-assi 
stant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023.', 'input_format 
': 'input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz, dtype: fp16', 'output_fo 
rmat': 'fp16', 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': ''}]                 

#####################nomic-ai/gpt4all-j########################

-------------------- datasets --------------------

Document 1:

- **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)  
We have released several versions of our finetuned GPT-J model using [different dataset versions](https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations)  
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).

-------------------- license --------------------

Document 1:

Apache-2
------------------------------
Document 2:

license: apache-2.0
------------------------------
Document 3:

**Apache-2 Licensed**

-------------------- github --------------------

Document 1:

- **Repository:** [https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) - **Base Model Repository:** [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
------------------------------
Document 2:

"nomic-ai/gpt4all-j-prompt-generations"

-------------------- paper --------------------

Document 1:

**Paper [optional]:** [GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf)

-------------------- upstream_model --------------------

Document 1:

Base Model Repository: [https://github.com/kingoflolz/mesh-transformer-jax]
------------------------------
Document 2:

[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)

-------------------- parameter_count --------------------

Document 1:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data - **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)

-------------------- hyper_parameters --------------------

Document 1:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).

-------------------- evaluation --------------------

Document 1:

```
| Model                     |  BoolQ   |   PIQA   | HellaSwag | WinoGrande |  ARC-e   |  ARC-c   |   OBQA   |   Avg.   |
|:--------------------------|:--------:|:--------:|:---------:|:----------:|:--------:|:--------:|:--------:|:--------:|
| GPT4All-J 6B v1.0         |   73.4   |   74.8   |   63.4    |    64.7    |   54.9   |   36.0   |   40.2   |   58.2   |
| GPT4All-J v1.1-breezy     |   74.0   |   75.1   |   63.2    |    63.6    |   55.4   |   34.9   |   38.4   |   57.8   |
| GPT4All-J v1.2-jazzy      |   74.8   |   74.9   |   63.6

-------------------- hardware --------------------

Document 1:

"8 A100 80GB GPUs"

-------------------- limitation_and_bias --------------------

Document 1:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).

-------------------- demo --------------------

Document 1:

- **Demo [optional]:** [https://gpt4all.io/](https://gpt4all.io/)
------------------------------
Document 2:

"Apache-2 licensed chatbot"

-------------------- input_format --------------------

Document 1:

- **Language(s) (NLP):** English - **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) - To download a model with a specific revision run ```python from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained('nomic-ai/gpt4all-j', revision='v1.2-jazzy') ```

-------------------- output_format --------------------

Document 1:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data
- **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)  
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).

-------------------- max_sequence_length --------------------

Document 1:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data - **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['v1.0', 'v1.1-breezy', 'v1.2-jazzy', 'v1.3-groovy'], 'license': 'Apache-2', 'github' 
: 'https://github.com/nomic-ai/gpt4all', 'paper': 'https://s3.amazonaws.com/static.nomic.ai/gpt4all/ 
2023_GPT4All-J_Technical_Report_2.pdf', 'upstream_model': 'https://github.com/kingoflolz/mesh-transf 
ormer-jax', 'parameter_count': 'A finetuned GPT-J model on assistant style interaction data', 'hyper 
_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation':  
[], 'hardware': '8 A100 80GB GPUs', 'limitation_and_bias': '', 'demo': 'https://gpt4all.io/', 'input 
_format': 'Language(s) (NLP): English', 'output_format': 'A finetuned GPT-J model on assistant style 
 interaction data', 'max_sequence_length': '', 'vocabulary_size': ''}]                               

#####################microsoft/biogpt########################

-------------------- datasets --------------------

Document 1:

Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. We propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature.

-------------------- license --------------------

Document 1:

license: mit

-------------------- github --------------------

Document 1:

You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:  
```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the

-------------------- paper --------------------

Document 1:

"@article{10.1093/bib/bbac409,
author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
title = '{BioGPT: generative pre-trained transformer for biomedical text generation and mining}',
journal = {Briefings in Bioinformatics},
volume = {23},
number = {6},
year = {2022},
month = {09}"
------------------------------
Document 2:

"In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature."

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

"set_seed(42)"
"model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')"
"tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')"
"num_beams=5"
"early_stopping=True"

-------------------- evaluation --------------------

Document 1:

We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record.

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:  
```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the

-------------------- demo --------------------

Document 1:

"You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility: 

```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in

-------------------- input_format --------------------

Document 1:

"BioGptTokenizer.from_pretrained('microsoft/biogpt')"
"BioGptForCausalLM.from_pretrained('microsoft/biogpt')"
"tokenizer(text, return_tensors='pt')"
"model(**encoded_input)"
"BioGptTokenizer.from_pretrained('microsoft/biogpt')"
"BioGptForCausalLM.from_pretrained('microsoft/biogpt')"
"tokenizer(sentence, return_tensors='pt')"
"model.generate(**inputs, min_length=100, max_length=1024, num_beams=5, early_stopping=True)"

-------------------- output_format --------------------

Document 1:

"You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility: 

```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in

-------------------- max_sequence_length --------------------

Document 1:

max_length=1024, num_beams=5, early_stopping=True

-------------------- vocabulary_size --------------------




ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Expecting ',' delimiter: line 38 column 21 (char 1747) 

#####################KoboldAI/GPT-NeoX-20B-Erebus########################

-------------------- datasets --------------------

Document 1:

Literotica, Sexstories, Dataset-G, Doc's Lab, Pike Dataset, SoFurry, `[Genre: <comma-separated list of genres>]`
------------------------------
Document 2:

datasets, GPT-NeoX-20B-Erebus, TPUv3-256 TPU pod, Ben Wang's Mesh Transformer JAX library, EleutherAI, GPT-J-6B model

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

"language: en license: apache-2.0 inference: false"

-------------------- paper --------------------

Document 1:

```bibtex
@inproceedings{gpt-neox-20b,
title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
url={https://arxiv.org/abs/2204.06745},
year={2022}
}
```
------------------------------
Document 2:

Ben Wang's Mesh Transformer JAX library, EleutherAI to train their GPT-J-6B model.

-------------------- upstream_model --------------------

Document 1:

"GPT-NeoX-20B"

-------------------- parameter_count --------------------

Document 1:

parameter_count
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------

Document 1:

TPUv3-256 TPU pod

-------------------- limitation_and_bias --------------------

Document 1:

"bias (gender, profession, race and religion)", "Warning: This model has a very strong NSFW bias!"
------------------------------
Document 2:

`[Genre: <comma-separated list of genres>]` NO_OUTPUT

-------------------- demo --------------------



-------------------- input_format --------------------

Document 1:

`[Genre: <comma-separated list of genres>]`

-------------------- output_format --------------------

Document 1:

`[Genre: <comma-separated list of genres>]`
------------------------------
Document 2:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['Literotica', 'Sexstories', 'Dataset-G', "Doc's Lab", 'Pike Dataset', 'SoFurry'], 'l 
icense': 'apache-2.0', 'github': '"language: en license: apache-2.0 inference: false"', 'paper': '@i 
nproceedings{gpt-neox-20b,\ntitle={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},\na 
uthor={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Gold 
ing, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Micha 
el and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, B 
en and Weinbach, Samuel},\nbooktitle={Proceedings of the ACL Workshop on Challenges \\& Perspectives 
 in Creating Large Language Models},\nurl={https://arxiv.org/abs/2204.06745},\nyear={2022}\n}', 'ups 
tream_model': 'GPT-NeoX-20B', 'parameter_count': 'parameter_count', 'hyper_parameters': {}, 'evaluat 
ion': [], 'hardware': 'TPUv3-256 TPU pod', 'limitation_and_bias': '"bias (gender, profession, race a 
nd religion)", "Warning: This model has a very strong NSFW bias!"', 'demo': '', 'input_format': '[Ge 
nre: <comma-separated list of genres>]', 'output_format': '[Genre: <comma-separated list of genres>] 
', 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': ''}]                             

#####################tiiuae/falcon-7b-instruct########################

-------------------- datasets --------------------

Document 1:

"The RefinedWeb paper" and "https://arxiv.org/abs/2306.01116"
------------------------------
Document 2:

datasets

-------------------- license --------------------

Document 1:

Apache 2.0 license
------------------------------
Document 2:

license: apache-2.0

-------------------- github --------------------

Document 1:

url={https://arxiv.org/abs/2306.01116}, eprint={2306.01116}, eprinttype = {arXiv}
------------------------------
Document 2:

language:
- en
license: apache-2.0
datasets:
- tiiuae/falcon-refinedweb
inference: true

-------------------- paper --------------------

Document 1:

"See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results."
------------------------------
Document 2:

"Paper: *coming soon*."
------------------------------
Document 3:

```
@article{falcon40b,
title={{Falcon-40B}: an open large language model with state-of-the-art performance},
author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
year={2023}
}
```
```
@article{refinedweb,
title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},

-------------------- upstream_model --------------------

Document 1:

"Finetuned from model: Falcon-7B"

-------------------- parameter_count --------------------

Document 1:

"Layers: 32", "d_model: 4544", "head_dim: 64", "Vocabulary: 65024", "Sequence length: 2048"

-------------------- hyper_parameters --------------------

Document 1:

"| **Hyperparameter** | **Value** | **Comment**                            |
|--------------------|-----------|----------------------------------------|
| Layers             | 32        |                                        |
| `d_model`          | 4544      | Increased to compensate for multiquery                                       |
| `head_dim`         | 64        | Reduced to optimise for FlashAttention |
| Vocabulary         | 65024     |                                        |
| Sequence length    | 2048      |                                        |"

-------------------- evaluation --------------------

Document 1:

Falcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token). 
* Positionnal embeddings: rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));
* Attention: multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));
* Decoder-block: parallel attention/MLP with a single layer norm. 
| **Hyperparameter** | **Value** | **Comment**                            |
|--------------------|-----------|----------------------------------------|
| Layers             | 32        |                                        |
| `d_model`          | 4544      | Increased to compensate for multiquery                                       |
| `head_dim`         | 64        | Reduced to optimise for FlashAttention |
| Vocabulary         | 65024     |                                        |
| Sequence length    | 2048      |                                        |

-------------------- hardware --------------------

Document 1:

Falcon-7B-Instruct, AWS SageMaker, 32 A100 40GB GPUs, P4d instances

-------------------- limitation_and_bias --------------------

Document 1:

"Falcon-7B-Instruct is mostly trained on English data" "it will carry the stereotypes and biases commonly encountered online"

-------------------- demo --------------------

Document 1:

"What's the Everett interpretation of quantum mechanics?", "Give me a list of the top 10 dive sites you would recommend around the world.", "Can you tell me more about deep-water soloing?", "Can you write a short tweet about the Apache 2.0 release of our latest AI model, Falcon LLM?", "What are the responsabilities of a Chief Llama Officer?"
------------------------------
Document 2:

See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.

-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

language:
- en
license: apache-2.0
datasets:
- tiiuae/falcon-refinedweb
inference: true
widget:
- text: Hey Falcon! Any recommendations for my holidays in Abu Dhabi?
example_title: Abu Dhabi Trip
- text: What's the Everett interpretation of quantum mechanics?
example_title: 'Q/A: Quantum & Answers'
- text: Give me a list of the top 10 dive sites you would recommend around the world.
example_title: Diving Top 10
- text: Can you tell me more about deep-water soloing?
example_title: Extreme sports
- text: Can you write a short tweet about the Apache 2.0 release of our latest AI
model, Falcon LLM?
example_title: Twitter Helper
- text: What are the responsabilities of a Chief Llama Officer?
example_title: Trendy Jobs
NO_OUTPUT

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length: 2048
------------------------------
Document 2:

max_length=200, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id

-------------------- vocabulary_size --------------------

Document 1:

Vocabulary: 65024
------------------------------
Document 2:

Falcon-7B-Instruct was finetuned on a 250M tokens mixture of instruct/chat datasets. The data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.


[{'datasets': ['tiiuae/falcon-refinedweb'], 'license': 'apache-2.0', 'github': 'https://arxiv.org/a 
bs/2306.01116', 'paper': 'https://arxiv.org/abs/2306.01116', 'upstream_model': 'Falcon-7B', 'paramet 
er_count': 'Layers: 32, d_model: 4544, head_dim: 64, Vocabulary: 65024, Sequence length: 2048', 'hyp 
er_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation' 
: [], 'hardware': 'Falcon-7B-Instruct, AWS SageMaker, 32 A100 40GB GPUs, P4d instances', 'limitation 
_and_bias': 'Falcon-7B-Instruct is mostly trained on English data and it will carry the stereotypes  
and biases commonly encountered online', 'demo': "What's the Everett interpretation of quantum mecha 
nics?, Give me a list of the top 10 dive sites you would recommend around the world., Can you tell m 
e more about deep-water soloing?, Can you write a short tweet about the Apache 2.0 release of our la 
test AI model, Falcon LLM?, What are the responsabilities of a Chief Llama Officer?", 'input_format' 
: '', 'output_format': '', 'max_sequence_length': '2048', 'vocabulary_size': '65024'}]               

#####################TheBloke/wizardLM-13B-1.0-fp16########################

-------------------- datasets --------------------



-------------------- license --------------------

Document 1:

"restricted for academic research purposes only and cannot be used for commercial purposes"

-------------------- github --------------------

Document 1:

Please cite the repo if you use the data or code in this repo.

-------------------- paper --------------------

Document 1:

"academic research purposes only" and "This project does not accept any legal liability for the content of the model output"
------------------------------
Document 2:

"title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang}, year={2023}, eprint={2304.12244}, archivePrefix={arXiv}, primaryClass={cs.CL}"

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------

Document 1:

"We released **13B** version of **WizardLM** trained with **250k** evolved instructions (from ShareGPT)." "We released **7B** version of **WizardLM** trained with **70k** evolved instructions (from Alpaca data)."

-------------------- hyper_parameters --------------------

Document 1:

"Batch size     | 64       | 384      |
Learning rate  | 2e-5     | 2e-5     |
Epochs         | 3        | 3        |
Max length     | 2048     | 2048     |
Warmup step    | 2        | 50       |
LR scheduler   | cosine   | cosine   |"

-------------------- evaluation --------------------

Document 1:

"7. [Evaluation](#evaluation)"
------------------------------
Document 2:

We adopt the automatic evaluation framework based on GPT-4 proposed by FastChat to assess the performance of chatbot models. As shown in the following figure, WizardLM-13B achieved better results than Vicuna-13b. <img src='imgs/WizarLM13b-GPT4.png' alt='WizardLM' style='width: 100%; min-width: 300px; display: block; margin: auto;'>

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

"The accuracy of the output cannot be guaranteed by this project" "This project does not accept any legal liability for the content of the model output, nor does it assume responsibility for any losses incurred due to the use of associated resources and output results."

-------------------- demo --------------------

Document 1:

"real-world" and "challenging problems"
------------------------------
Document 2:

[Case Show](https://github.com/nlpxucan/WizardLM/blob/main/src/case_show.md)

-------------------- input_format --------------------

Document 1:

input_format: fp16 unquantised format

-------------------- output_format --------------------

Document 1:

- `instruction`: `str`, describes the task the model should perform. Each of the 70K instructions is unique. - `output`: `str`, the answer to the instruction as generated by `gpt-3.5-turbo`.

-------------------- max_sequence_length --------------------

Document 1:

"We released 13B version of WizardLM trained with 250k evolved instructions (from ShareGPT)." "Note for 13B model usage: To obtain results identical to our demo, please strictly follow the prompts and invocation methods provided in the 'src/infer_wizardlm13b.py' to use our 13B model for inference." "The demo currently only supports single-turn conversation."

-------------------- vocabulary_size --------------------

Document 1:

**33B**, **13B**, **250k**, **7B**, **70k**


[{'datasets': [], 'license': 'restricted for academic research purposes only and cannot be used for 
 commercial purposes', 'github': 'Please cite the repo if you use the data or code in this repo.', ' 
paper': '"academic research purposes only" and "This project does not accept any legal liability for 
 the content of the model output"', 'upstream_model': '', 'parameter_count': 'We released **13B** ve 
rsion of **WizardLM** trained with **250k** evolved instructions (from ShareGPT)." "We released **7B 
** version of **WizardLM** trained with **70k** evolved instructions (from Alpaca data)."', 'hyper_p 
arameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [] 
, 'hardware': '', 'limitation_and_bias': '"The accuracy of the output cannot be guaranteed by this p 
roject" "This project does not accept any legal liability for the content of the model output, nor d 
oes it assume responsibility for any losses incurred due to the use of associated resources and outp 
ut results."', 'demo': '"real-world" and "challenging problems"', 'input_format': 'input_format: fp1 
6 unquantised format', 'output_format': '- `instruction`: `str`, describes the task the model should 
 perform. Each of the 70K instructions is unique. - `output`: `str`, the answer to the instruction a 
s generated by `gpt-3.5-turbo`.', 'max_sequence_length': '"We released 13B version of WizardLM train 
ed with 250k evolved instructions (from ShareGPT)." "Note for 13B model usage: To obtain results ide 
ntical to our demo, please strictly follow the prompts and invocation methods provided in the \'src/ 
infer_wizardlm13b.py\' to use our 13B model for inference." "The demo currently only supports single 
-turn conversation."', 'vocabulary_size': '**33B**, **13B**, **250k**, **7B**, **70k**'}, {'datasets 
': [], 'license': '', 'github': '', 'paper': 'title={WizardLM: Empowering Large Language Models to F 
ollow Complex Instructions}, author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zha 
o and Jiazhan Feng and Chongyang Tao and Daxin Jiang}, year={2023}, eprint={2304.12244}, archivePref 
ix={arXiv}, primaryClass={cs.CL}', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters':  
{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware' 
: '', 'limitation_and_bias': '', 'demo': '[Case Show](https://github.com/nlpxucan/WizardLM/blob/main 
/src/case_show.md)', 'input_format': '', 'output_format': "We adopt the automatic evaluation framewo 
rk based on GPT-4 proposed by FastChat to assess the performance of chatbot models. As shown in the  
following figure, WizardLM-13B achieved better results than Vicuna-13b. <img src='imgs/WizarLM13b-GP 
T4.png' alt='WizardLM' style='width: 100%; min-width: 300px; display: block; margin: auto;'>", 'max_ 
sequence_length': '', 'vocabulary_size': ''}]                                                        

#####################mosaicml/mpt-30b-chat########################

-------------------- datasets --------------------

Document 1:

'Airoboros/GPT4-1.2', 'Baize', 'Camel', 'GPTeacher', 'Guanaco', 'LongCoversations', 'ShareGPT', 'WizardLM'
------------------------------
Document 2:

datasets, [MosaicML Platform](https://www.mosaicml.com/platform), sharded data parallelism, [FSDP](https://pytorch.org/docs/stable/fsdp.html)

-------------------- license --------------------

Document 1:

_CC-By-NC-SA-4.0_
------------------------------
Document 2:

license: cc-by-nc-sa-4.0

-------------------- github --------------------

Document 1:

[Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)
------------------------------
Document 2:

camel-ai/code, ehartford/wizard_vicuna_70k_unfiltered, anon8231489123/ShareGPT_Vicuna_unfiltered, teknium1/GPTeacher/roleplay-instruct-v2-final, teknium1/GPTeacher/codegen-isntruct, timdettmers/openassistant-guanaco, camel-ai/math, project-baize/baize-chatbot/medical_chat_data, project-baize/baize-chatbot/quora_chat_data, project-baize/baize-chatbot/stackoverflow_chat_data, camel-ai/biology, camel-ai/chemistry, camel-ai/ai_society, jondurbin/airoboros-gpt4-1.2, LongConversations, camel-ai/physics

-------------------- paper --------------------

Document 1:

```
@online{MosaicML2023Introducing,
author    = {MosaicML NLP Team},
title     = {Introducing MPT-30B: Raising the bar
for open-source foundation models},
year      = {2023},
url       = {www.mosaicml.com/blog/mpt-30b},
note      = {Accessed: 2023-06-22},
urldate   = {2023-06-22}
```
------------------------------
Document 2:

'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date.

-------------------- upstream_model --------------------

Document 1:

"standard decoder-only transformer"

-------------------- parameter_count --------------------

Document 1:

"26.4M", "55.0M", "301M", "7.56M", "15.6M", "18.4M", "821M", "297M"
------------------------------
Document 2:

n_parameters | 29.95B
------------------------------
Document 3:

parameter_count 64

-------------------- hyper_parameters --------------------

Document 1:

"It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf), It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings, It does not use biases, | Hyperparameter | Value |, |n_parameters | 29.95B |, |n_layers | 48 |, | n_heads | 64 |, | d_model | 7168 |, | vocab size | 50432 |, | sequence length | 8192 |
------------------------------
Document 2:

"AdamW optimizer"
------------------------------
Document 3:

'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date.

-------------------- evaluation --------------------

Document 1:

'The model was trained on the following data mix:' '| Data Source | Number of Tokens in Source | Proportion |' '| Airoboros/GPT4-1.2 | 26.4M | 1.71% |' '| Baize | 55.0M | 3.57% |' '| Camel	| 301M | 19.54% |' '| GPTeacher	| 7.56M | 0.49% |' '| Guanaco | 15.6M | 1.02% |' '| LongCoversations | 18.4M | 1.19% |' '| ShareGPT | 821M | 53.24% |' '| WizardLM | 297M | 19.23% |' 'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date.

-------------------- hardware --------------------

Document 1:

64 H100s, [MosaicML Platform](https://www.mosaicml.com/platform), [FSDP](https://pytorch.org/docs/stable/fsdp.html)

-------------------- limitation_and_bias --------------------

Document 1:

'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date.
------------------------------
Document 2:

* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases

-------------------- demo --------------------

Document 1:

[sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b)

-------------------- input_format --------------------

Document 1:

'LongConversations' is a GPT3.5/4-generated dataset
------------------------------
Document 2:

vocab size | 50432 | sequence length | 8192 | input_format

-------------------- output_format --------------------

Document 1:

'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date.

-------------------- max_sequence_length --------------------

Document 1:

'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date. NO_OUTPUT
------------------------------
Document 2:

"sequence length | 8192"

-------------------- vocabulary_size --------------------

Document 1:

'26.4M', '55.0M', '301M', '7.56M', '15.6M', '18.4M', '821M', '297M'
------------------------------
Document 2:

vocab size | 50432


[{'datasets': ['Airoboros/GPT4-1.2', 'Baize', 'Camel', 'GPTeacher', 'Guanaco', 'LongCoversations',  
'ShareGPT', 'WizardLM'], 'license': '_CC-By-NC-SA-4.0_', 'github': '[Codebase (mosaicml/llm-foundry  
repo)](https://github.com/mosaicml/llm-foundry/)', 'paper': '@online{MosaicML2023Introducing,\nautho 
r    = {MosaicML NLP Team},\ntitle     = {Introducing MPT-30B: Raising the bar\nfor open-source foun 
dation models},\nyear      = {2023},\nurl       = {www.mosaicml.com/blog/mpt-30b},\nnote      = {Acc 
essed: 2023-06-22},\nurldate   = {2023-06-22}', 'upstream_model': '"standard decoder-only transforme 
r"', 'parameter_count': '"26.4M", "55.0M", "301M", "7.56M", "15.6M", "18.4M", "821M", "297M"', 'hype 
r_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': 
 [], 'hardware': '64 H100s, [MosaicML Platform](https://www.mosaicml.com/platform), [FSDP](https://p 
ytorch.org/docs/stable/fsdp.html)', 'limitation_and_bias': "'LongConversations' is a GPT3.5/4-genera 
ted dataset, details of which will be released at a later date.", 'demo': '[sign up here](https://fo 
rms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b)', 'input_forma 
t': "'LongConversations' is a GPT3.5/4-generated dataset", 'output_format': "'LongConversations' is  
a GPT3.5/4-generated dataset, details of which will be released at a later date.", 'max_sequence_len 
gth': "'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a l 
ater date.", 'vocabulary_size': "'26.4M', '55.0M', '301M', '7.56M', '15.6M', '18.4M', '821M', '297M' 
"}]                                                                                                  

#####################cross-encoder/ms-marco-MiniLM-L-2-v2########################

-------------------- datasets --------------------

Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"In the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset."

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco

-------------------- github --------------------

Document 1:

[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

"In the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset."

-------------------- paper --------------------

Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"cross-encoder/ms-marco-TinyBERT-L-2-v2", "69.84", "32.56", "9000"

-------------------- upstream_model --------------------

Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'CrossEncoder'

-------------------- parameter_count --------------------

Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'max_length=512'

-------------------- evaluation --------------------

Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"In the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset."

"| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |
| ------------- |:-------------| -----| --- |
| **Version 2 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000
| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100
| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500
| cross-encoder/ms-marco-MiniLM-L-6-v2 |

-------------------- hardware --------------------

Document 1:

MS Marco Passage Ranking, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"Note: Runtime was computed on a V100 GPU."

-------------------- limitation_and_bias --------------------

Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco

-------------------- demo --------------------

Document 1:

[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

`from sentence_transformers import CrossEncoder
model = CrossEncoder('model_name', max_length=512)
scores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])`

-------------------- input_format --------------------

Document 1:

MS Marco Passage Ranking, Information Retrieval, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'max_length=512'

-------------------- output_format --------------------

Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco

-------------------- max_sequence_length --------------------

Document 1:

max_length=512
------------------------------
Document 2:

max_sequence_length
------------------------------
Document 3:

max_sequence_length: 960

-------------------- vocabulary_size --------------------




[{'datasets': ['MS Marco Passage Ranking', 'SBERT.net Retrieve & Re-rank', 'SBERT.net Training MS M 
arco'], 'license': 'apache-2.0', 'github': '[MS Marco Passage Ranking](https://github.com/microsoft/ 
MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications 
/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-tran 
sformers/tree/master/examples/training/ms_marco)', 'paper': 'MS Marco Passage Ranking, SBERT.net Ret 
rieve & Re-rank, SBERT.net Training MS Marco', 'upstream_model': 'MS Marco Passage Ranking, Informat 
ion Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'parameter_count': 'MS Ma 
rco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marc 
o', 'hyper_parameters': 'MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training  
MS Marco', 'evaluation': 'MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-r 
ank, SBERT.net Training MS Marco', 'hardware': 'MS Marco Passage Ranking, ElasticSearch, SBERT.net R 
etrieve & Re-rank, SBERT.net Training MS Marco', 'limitation_and_bias': 'MS Marco Passage Ranking, I 
nformation Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'demo': '[MS Marco 
 Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-ra 
nk](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS 
 Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)', 'i 
nput_format': 'MS Marco Passage Ranking, Information Retrieval, ElasticSearch, SBERT.net Retrieve &  
Re-rank, SBERT.net Training MS Marco', 'output_format': 'MS Marco Passage Ranking, Information Retri 
eval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'max_sequence_length': 'max_length 
=512', 'vocabulary_size': ''}, {'datasets': ['TREC Deep Learning 2019', 'MS Marco Passage Reranking' 
], 'license': '', 'github': '[TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Le 
arning/), [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/)', 'pap 
er': 'cross-encoder/ms-marco-TinyBERT-L-2-v2, 69.84, 32.56, 9000', 'upstream_model': "'CrossEncoder' 
", 'parameter_count': 'parameter_count', 'hyper_parameters': "'max_length=512'", 'evaluation': '"| M 
odel-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:- 
------------| -----| --- |\n| **Version 2 models** | | |\n| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 
 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100\n| cross-encode 
r/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L-6-v2 |', 'hardwar 
e': '"Note: Runtime was computed on a V100 GPU."', 'limitation_and_bias': '', 'demo': "`from sentenc 
e_transformers import CrossEncoder\nmodel = CrossEncoder('model_name', max_length=512)\nscores = mod 
el.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])`", 'input_f 
ormat': "'max_length=512'", 'output_format': '', 'max_sequence_length': 'max_sequence_length', 'voca 
bulary_size': ''}, {'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '',  
'parameter_count': '', 'hyper_parameters': '', 'evaluation': '', 'hardware': '', 'limitation_and_bia 
s': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': 'max_sequence_le 
ngth: 960', 'vocabulary_size': ''}]                                                                  

#####################Hello-SimpleAI/chatgpt-detector-roberta-chinese########################

-------------------- datasets --------------------

Document 1:

"the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese)" and "[Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out)"
------------------------------
Document 2:

datasets: - Hello-SimpleAI/HC3-Chinese
------------------------------
Document 3:

"arXiv preprint arxiv:2301.07597"

-------------------- license --------------------



-------------------- github --------------------

Document 1:

language: - zh tags: - chatgpt datasets: - Hello-SimpleAI/HC3-Chinese pipeline_tag: text-classification
------------------------------
Document 2:

[Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese), [arxiv: 2301.07597](https://arxiv.org/abs/2301.07597), Gtihub project [Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection), [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext), [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese), [our paper](https://arxiv.org/abs/2301.07597).

-------------------- paper --------------------

Document 1:

arXiv preprint arxiv:2301.07597
------------------------------
Document 2:

[arxiv: 2301.07597](https://arxiv.org/abs/2301.07597) and [our paper](https://arxiv.org/abs/2301.07597).
------------------------------
Document 3:

- chatgpt - Hello-SimpleAI/HC3-Chinese

-------------------- upstream_model --------------------

Document 1:

"hfl/chinese-roberta-wwm-ext"

-------------------- parameter_count --------------------

Document 1:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

"arXiv preprint arxiv:2301.07597"

-------------------- evaluation --------------------

Document 1:

"the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese)", "[arxiv: 2301.07597](https://arxiv.org/abs/2301.07597) and Gtihub project [Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)", "[hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext)", "[Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs", "[our paper](https://arxiv.org/abs/2301.07597)."

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

"the mix of full-text and splitted sentences" of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) and "We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs."

-------------------- demo --------------------

Document 1:

arxiv: 2301.07597, https://arxiv.org/abs/2301.07597
------------------------------
Document 2:

"the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese)", "[arxiv: 2301.07597](https://arxiv.org/abs/2301.07597)", "Gtihub project [Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)", "[hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext)", "[Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese)", "2 epochs", "[our paper](https://arxiv.org/abs/2301.07597)."

-------------------- input_format --------------------

Document 1:

language: - zh, tags: - chatgpt, datasets: - Hello-SimpleAI/HC3-Chinese, pipeline_tag: text-classification
------------------------------
Document 2:

"the mix of full-text and splitted sentences"

-------------------- output_format --------------------



-------------------- max_sequence_length --------------------

Document 1:

"The base checkpoint is [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext)" and "We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs."
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['Hello-SimpleAI/HC3-Chinese'], 'license': '', 'github': '', 'paper': '', 'upstream_m 
odel': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rat 
e': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', ' 
input_format': '', 'output_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]           

#####################mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis########################

-------------------- datasets --------------------

Document 1:

datasets: - financial_phrasebank
------------------------------
Document 2:

Datasets 1.12.1

-------------------- license --------------------



-------------------- github --------------------



-------------------- paper --------------------



-------------------- upstream_model --------------------

Document 1:

upstream_model distilroberta-base
------------------------------
Document 2:

- generated_from_trainer
- financial
- stocks
- sentiment
- financial_phrasebank
- accuracy
- distilRoberta-financial-sentiment
- text-classification
- Text Classification
- financial_phrasebank
- accuracy
- 0.9823008849557522
- Accuracy

-------------------- parameter_count --------------------

Document 1:

parameter_count: NO_OUTPUT

-------------------- hyper_parameters --------------------

Document 1:

- learning_rate: 2e-05 - train_batch_size: 8 - eval_batch_size: 8 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - num_epochs: 5
------------------------------
Document 2:

Epoch, Step, Validation Loss, Accuracy

-------------------- evaluation --------------------

Document 1:

license: apache-2.0, tags: - generated_from_trainer - financial - stocks - sentiment, datasets: - financial_phrasebank, metrics: - accuracy, widget: - text: Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 ., model-index: - name: distilRoberta-financial-sentiment, results: - task: type: text-classification name: Text Classification dataset: name: financial_phrasebank type: financial_phrasebank args: sentences_allagree metrics: - type: accuracy value: 0.9823008849557522 name: Accuracy
------------------------------
Document 2:

"This model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset. It achieves the following results on the evaluation set: - Loss: 0.1116 - Accuracy: 0.9823"

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

metrics:
- accuracy
widget:
- text: Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .
model-index:
- name: distilRoberta-financial-sentiment
results:
- task:
type: text-classification
name: Text Classification
dataset:
name: financial_phrasebank
type: financial_phrasebank
args: sentences_allagree
metrics:
- type: accuracy
value: 0.9823008849557522
name: Accuracy

-------------------- demo --------------------



-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

- financial
- stocks
- sentiment
- financial_phrasebank
- text-classification
- financial_phrasebank
- accuracy
- 0.9823008849557522

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length: NO_OUTPUT

-------------------- vocabulary_size --------------------

Document 1:

- financial
- stocks
- sentiment
- financial_phrasebank
- distilRoberta-financial-sentiment
- Text Classification
- financial_phrasebank
- accuracy
- 0.9823008849557522


[{'datasets': ['financial_phrasebank'], 'license': '', 'github': '', 'paper': '', 'upstream_model': 
 'distilroberta-base', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epochs': '', 'batch_siz 
e': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bia 
s': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': 'NO_OUTPUT', 'vo 
cabulary_size': ''}]                                                                                 

#####################finiteautomata/bertweet-base-sentiment-analysis########################

-------------------- datasets --------------------

Document 1:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

"SemEval 2017 corpus (around ~40k tweets)", "BERTweet", "RoBERTa model trained on English tweets", "POS", "NEG", "NEU" labels.

-------------------- license --------------------

Document 1:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

"Enjoy! " NO_OUTPUT

-------------------- github --------------------

Document 1:

"https://github.com/finiteautomata/pysentimiento/", "BERTweet (https://github.com/VinAIResearch/BERTweet)"

-------------------- paper --------------------

Document 1:

[this paper](https://arxiv.org/abs/2106.09462)
------------------------------
Document 2:

"Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets."
------------------------------
Document 3:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()

-------------------- upstream_model --------------------

Document 1:

"Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet)"

-------------------- parameter_count --------------------

Document 1:

"Model trained with SemEval 2017 corpus (around ~40k tweets)", "Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.", "Uses `POS`, `NEG`, `NEU` labels."
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

"Uses `POS`, `NEG`, `NEU` labels."

-------------------- evaluation --------------------

Document 1:

"Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets. Uses `POS`, `NEG`, `NEU` labels."
------------------------------
Document 2:

- sentiment-analysis

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. NO_OUTPUT
------------------------------
Document 2:

"Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets. Uses `POS`, `NEG`, `NEU` labels."

-------------------- demo --------------------

Document 1:

"Please be aware that models are trained with third-party datasets and are subject to their respective licenses."

-------------------- input_format --------------------

Document 1:

"SemEval 2017 corpus (around ~40k tweets)", "Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.", "Uses `POS`, `NEG`, `NEU` labels."

-------------------- output_format --------------------

Document 1:

"Uses `POS`, `NEG`, `NEU` labels."

-------------------- max_sequence_length --------------------

Document 1:

language: - en tags: - sentiment-analysis

-------------------- vocabulary_size --------------------




[{'datasets': ['TASS Dataset'], 'license': 'TASS Dataset license', 'github': 'https://github.com/fi 
niteautomata/pysentimiento/', 'paper': 'https://arxiv.org/abs/2106.09462', 'upstream_model': 'BERTwe 
et', 'parameter_count': 'parameter_count', 'hyper_parameters': {'epochs': 'epochs', 'batch_size': 'b 
atch_size', 'learning_rate': 'learning_rate', 'optimizer': 'optimizer'}, 'evaluation': [{'test': 'se 
ntiment-analysis', 'result': 'result'}], 'hardware': 'hardware', 'limitation_and_bias': '`pysentimie 
nto` is an open-source library for non-commercial use and scientific research purposes only. Please  
be aware that models are trained with third-party datasets and are subject to their respective licen 
ses.', 'demo': 'Please be aware that models are trained with third-party datasets and are subject to 
 their respective licenses.', 'input_format': 'input_format', 'output_format': 'output_format', 'max 
_sequence_length': 'max_sequence_length', 'vocabulary_size': 'vocabulary_size'}]                     

#####################declare-lab/flan-alpaca-gpt4-xl########################

-------------------- datasets --------------------

Document 1:

datasets: - tatsu-lab/alpaca
------------------------------
Document 2:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), [Flan-T5](https://arxiv.org/abs/2210.11416)

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

license: apache-2.0, datasets: - tatsu-lab/alpaca
------------------------------
Document 2:

"[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)", "[LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)", "[potential noise](https://github.com/tloen/alpaca-lora/issues/65)", "[Flan-T5](https://arxiv.org/abs/2210.11416)"
------------------------------
Document 3:

[Code](https://github.com/declare-lab/red-instruct) 
[Paper](https://arxiv.org/abs/2308.09662) 
[https://github.com/declare-lab/instruct-eval](https://github.com/declare-lab/instruct-eval) 
[https://github.com/declare-lab/tango](https://github.com/declare-lab/tango) 
[https://github.com/declare-lab/flan-alpaca](https://github.com/declare-lab/flan-alpaca) 
[https://arxiv.org/pdf/2306.04757.pdf](https://arxiv.org/pdf/2306.04757.pdf) 
[https://github.com/declare-lab/flan-eval](https://github.com/declare-lab/flan-eval) 
[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_

-------------------- paper --------------------

Document 1:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), [Flan-T5](https://arxiv.org/abs/2210.11416)
------------------------------
Document 2:

[Paper](https://arxiv.org/abs/2308.09662) and [https://arxiv.org/pdf/2306.04757.pdf](https://arxiv.org/pdf/2306.04757.pdf)

-------------------- upstream_model --------------------

Document 1:

"GPT-3" and "Flan-T5"
------------------------------
Document 2:

declare-lab/flan-alpaca-gpt4-xl

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

"We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: [https://github.com/declare-lab/instruct-eval](https://github.com/declare-lab/instruct-eval)"

-------------------- hardware --------------------

Document 1:

"large language models (LLMs) like ChatGPT", "synthetic training data", "underlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model", "[Flan-T5](https://arxiv.org/abs/2210.11416)"
------------------------------
Document 2:

1x A6000, 4x A6000 (FSDP)

-------------------- limitation_and_bias --------------------

Document 1:

"However, the original implementation is less accessible due to licensing constraints of the underlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model. Furthermore, users have noted [potential noise](https://github.com/tloen/alpaca-lora/issues/65) in the synthetic dataset."

-------------------- demo --------------------

Document 1:

`pipeline(model='declare-lab/flan-alpaca-gpt4-xl')`
------------------------------
Document 2:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), [potential noise](https://github.com/tloen/alpaca-lora/issues/65), [Flan-T5](https://arxiv.org/abs/2210.11416)

-------------------- input_format --------------------



-------------------- output_format --------------------



-------------------- max_sequence_length --------------------

Document 1:

max_length=128
------------------------------
Document 2:

max_sequence_length NO_OUTPUT
------------------------------
Document 3:

"Our pretrained models are fully available on HuggingFace  :"

-------------------- vocabulary_size --------------------




[{'datasets': ['tatsu-lab/alpaca']}]                                                                

#####################google/t5-v1_1-large########################

-------------------- datasets --------------------

Document 1:

datasets: - c4
------------------------------
Document 2:

Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

"Pretraining Dataset: [C4](https://huggingface.co/datasets/c4) Other Community Checkpoints: [here](https://huggingface.co/models?search=t5-v1_1) Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*"

NO_OUTPUT

-------------------- github --------------------

Document 1:

"To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
------------------------------
Document 2:

[T5 Version 1.1](https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511), [here](https://arxiv.org/abs/2002.05202), [C4](https://huggingface.co/datasets/c4), [here](https://huggingface.co/models?search=t5-v1_1), [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)

-------------------- paper --------------------

Document 1:

"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
------------------------------
Document 2:

Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)  
Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*
------------------------------
Document 3:

Google's T5 Version 1.1

-------------------- upstream_model --------------------

Document 1:

"T5 Version 1.1"
------------------------------
Document 2:

Google's T5

-------------------- parameter_count --------------------

Document 1:

"xl' and 'xxl' replace '3B' and '11B'. The model shapes are a bit different - larger `d_model` and smaller `num_heads` and `d_ff`."

-------------------- hyper_parameters --------------------

Document 1:

GEGLU activation in feed-forward hidden layer, Dropout was turned off in pre-training, no parameter sharing between embedding and classifier layer, 'xl' and 'xxl' replace '3B' and '11B', larger `d_model` and smaller `num_heads` and `d_ff`.

-------------------- evaluation --------------------

Document 1:

"Pretraining Dataset: [C4](https://huggingface.co/datasets/c4) Other Community Checkpoints: [here](https://huggingface.co/models?search=t5-v1_1) Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*"

-------------------- hardware --------------------

Document 1:

Google's T5
------------------------------
Document 2:

Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)

-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

"To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
------------------------------
Document 2:

Google's T5 Version 1.1
------------------------------
Document 3:

[T5 Version 1.1](https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511), [here](https://arxiv.org/abs/2002.05202), [C4](https://huggingface.co/datasets/c4), [here](https://huggingface.co/models?search=t5-v1_1), [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)

-------------------- input_format --------------------

Document 1:

Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)  input_format: C4
------------------------------
Document 2:

"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP)." "Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks." "To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code." input_format

-------------------- output_format --------------------

Document 1:

'C4' and 'xl' and 'xxl' replace '3B' and '11B'
------------------------------
Document 2:

"text-to-text format" "pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors" "summarization, question answering, text classification, and more" "dataset, pre-trained models, and code" output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

C4, Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu


[{'datasets': ['c4'], 'license': 'apache-2.0', 'github': 'https://github.com/google-research/text-t 
o-text-transfer-transformer', 'paper': 'https://arxiv.org/pdf/1910.10683.pdf', 'upstream_model': 'T5 
 Version 1.1', 'parameter_count': "xl' and 'xxl' replace '3B' and '11B'. The model shapes are a bit  
different - larger `d_model` and smaller `num_heads` and `d_ff`.", 'hyper_parameters': {'epochs': '' 
, 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': "Google's T 
5", 'limitation_and_bias': '', 'demo': 'To facilitate future work on transfer learning for NLP, we r 
elease our dataset, pre-trained models, and code.', 'input_format': 'Pretraining Dataset: [C4](https 
://huggingface.co/datasets/c4)  input_format: C4', 'output_format': "'C4' and 'xl' and 'xxl' replace 
 '3B' and '11B'", 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': 'C4, Exploring th 
e Limits of Transfer Learning with a Unified Text-to-Text Transformer, Colin Raffel, Noam Shazeer, A 
dam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu'}]       

#####################prithivida/parrot_paraphraser_on_T5########################

-------------------- datasets --------------------

Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."

-------------------- license --------------------

Document 1:

"license"
------------------------------
Document 2:

"In the space of conversational engines, knowledge bots are to which we ask questions like 'when was the Berlin wall teared down?', transactional bots are to which we give commands like 'Turn on the music please' and voice assistants are the ones which can do both answer questions and action our commands." NO_OUTPUT

-------------------- github --------------------

Document 1:

"github page https://github.com/PrithivirajDamodaran/Parrot"
------------------------------
Document 2:

"we ask questions like 'when was the Berlin wall teared down?'", "we give commands like 'Turn on the music please'", "Parrot mainly foucses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models."

-------------------- paper --------------------

Document 1:

"In the space of conversational engines, knowledge bots are to which we ask questions like 'when was the Berlin wall teared down?', transactional bots are to which we give commands like 'Turn on the music please' and voice assistants are the ones which can do both answer questions and action our commands."

-------------------- upstream_model --------------------

Document 1:

"In the space of conversational engines, knowledge bots are to which we ask questions like 'when was the Berlin wall teared down?', transactional bots are to which we give commands like 'Turn on the music please' and voice assistants are the ones which can do both answer questions and action our commands." NO_OUTPUT

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

- **3 key metrics** that measures the quality of paraphrases are:
- Adequacy
- Fluency
- Diversity (Lexical / Phrasal / Syntactical)
- Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.
------------------------------
Document 2:

'diversity_ranker='levenshtein', do_diverse=False, max_return_phrases = 10, max_length=32, adequacy_threshold = 0.99, fluency_threshold = 0.90

-------------------- evaluation --------------------

Document 1:

- **Adequacy** (Is the meaning preserved adequately?)
- **Fluency** (Is the paraphrase fluent English?)
- **Diversity (Lexical / Phrasal / Syntactical)** (How much has the paraphrase changed the original sentence?)
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
------------------------------
Document 2:

"In the space of conversational engines, knowledge bots are to which we ask questions like 'when was the Berlin wall teared down?', transactional bots are to which we give commands like 'Turn on the music please' and voice assistants are the ones which can do both answer questions and action our commands." 
NO_OUTPUT

-------------------- hardware --------------------

Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."

-------------------- limitation_and_bias --------------------

Document 1:

- **Adequacy** (Is the meaning preserved adequately?)
- **Fluency** (Is the paraphrase fluent English?)
- **Diversity (Lexical / Phrasal / Syntactical)** (How much has the paraphrase changed the original sentence?) 
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model. 
- But in general being a generative model paraphrasers doesn't guarantee to preserve the slots/entities. So the ability to generate high quality paraphrases in a constrained fashion without trading off the intents and slots for lexical dissimilarity makes a paraphraser a good augmentor.
------------------------------
Document 2:

"we ask questions like 'when was the Berlin wall teared down?', transactional bots are to which we give commands like 'Turn on the music please', pre-trained model is trained on text samples of maximum length of 32.

-------------------- demo --------------------

Document 1:

github page https://github.com/PrithivirajDamodaran/Parrot
------------------------------
Document 2:

"In the space of conversational engines, knowledge bots are to which we ask questions like 'when was the Berlin wall teared down?', transactional bots are to which we give commands like 'Turn on the music please' and voice assistants are the ones which can do both answer questions and action our commands."
------------------------------
Document 3:

- **Huggingface** lists [12 paraphrase models,](https://huggingface.co/models?pipeline_tag=text2text-generation&search=paraphrase)  **RapidAPI** lists 7 fremium and commercial paraphrasers like [QuillBot](https://rapidapi.com/search/paraphrase?section=apis&page=1), Rasa has discussed an experimental paraphraser for augmenting text data [here](https://forum.rasa.com/t/paraphrasing-for-nlu-data-augmentation-experimental/27744), Sentence-transfomers offers a [paraphrase mining utility](https://www.sbert.net/examples/applications/paraphrase-mining/README.html) and [NLPAug](https://github.com/makcedward/nlpaug) offers word level augmentation with a [PPDB](http://paraphrase.org/#/download) (a multi-million paraphrase database).

-------------------- input_format --------------------

Document 1:

input_phrase, diversity_ranker, do_diverse, max_return_phrases, max_length, adequacy_threshold, fluency_threshold

-------------------- output_format --------------------

Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32." output_format: text samples of maximum length of 32.

-------------------- max_sequence_length --------------------

Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32." max_sequence_length: 32
------------------------------
Document 2:

max_length=32

-------------------- vocabulary_size --------------------

Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."


[{'datasets': ['Parrot'], 'license': 'license', 'github': 'github page https://github.com/Prithivir 
ajDamodaran/Parrot', 'paper': 'NO_OUTPUT', 'upstream_model': 'NO_OUTPUT', 'parameter_count': '', 'hy 
per_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'github pa 
ge https://github.com/PrithivirajDamodaran/Parrot', 'input_format': '', 'output_format': '', 'max_se 
quence_length': '', 'vocabulary_size': ''}]                                                          

#####################Salesforce/codet5-base########################

-------------------- datasets --------------------

Document 1:

Supervised datasets for code can be found [here](https://huggingface.co/datasets?languages=languages:code).
------------------------------
Document 2:

CodeSearchNet [Husain et al., 2019](https://arxiv.org/abs/1909.09436), [BigQuery1](https://console.cloud.google.com/marketplace/details/github/github-repos)
------------------------------
Document 3:

datasets: - code_search_net

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

"archivePrefix={arXiv}"

-------------------- github --------------------

Document 1:

"See the [model hub](https://huggingface.co/models?search=salesforce/codet) to look for fine-tuned versions on a task that interests you."

-------------------- paper --------------------

Document 1:

"For evaluation results on several downstream benchmarks, we refer to the paper."
------------------------------
Document 2:

`title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}`
------------------------------
Document 3:

'We present CodeT5, a unified pre-trained encoder-decoder Transformer model' and 'Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL.'

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------

Document 1:

parameter_count 8.35 million
------------------------------
Document 2:

license: apache-2.0, tags: - codet5, datasets: - code_search_net, inference: false

NO_OUTPUT

-------------------- hyper_parameters --------------------

Document 1:

"See the [model hub](https://huggingface.co/models?search=salesforce/codet) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 2:

"CodeT5 model was pretrained on CodeSearchNet [Husain et al., 2019](https://arxiv.org/abs/1909.09436)", "two datasets of C/CSharp from [BigQuery1](https://console.cloud.google.com/marketplace/details/github/github-repos)", "around 8.35 million instances are used for pretraining."

-------------------- evaluation --------------------

Document 1:

"We present CodeT5, a unified pre-trained encoder-decoder Transformer model...Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code."

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

"We present CodeT5, a unified pre-trained encoder-decoder Transformer model...Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL."

-------------------- demo --------------------

Document 1:

code example below, code summarization, code generation, code translation, code refinement, code defect detection, code clone detection, [here](https://huggingface.co/datasets?languages=languages:code), [model hub](https://huggingface.co/models?search=salesforce/codet)
------------------------------
Document 2:

"We present CodeT5, a unified pre-trained encoder-decoder Transformer model"

-------------------- input_format --------------------

Document 1:

"RobertaTokenizer" "HuggingFace Tokenizers" "files from this repository"

-------------------- output_format --------------------

Document 1:

license: apache-2.0, tags: - codet5, datasets: - code_search_net, inference: false

-------------------- max_sequence_length --------------------

Document 1:

max_length=8
------------------------------
Document 2:

max_sequence_length NO_OUTPUT

-------------------- vocabulary_size --------------------

Document 1:

"8.35 million instances"


[{'datasets': ['code_search_net'], 'license': 'apache-2.0', 'github': 'https://huggingface.co/model 
s?search=salesforce/codet', 'paper': 'https://arxiv.org/abs/1909.09436', 'upstream_model': '', 'para 
meter_count': '8.35 million', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_ 
and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': '', 'voca 
bulary_size': ''}]                                                                                   

#####################valhalla/t5-base-e2e-qg########################

-------------------- datasets --------------------

Document 1:

datasets: - squad

-------------------- license --------------------

Document 1:

license: mit

-------------------- github --------------------

Document 1:

"You'll need to clone the [repo](https://github.com/patil-suraj/question_generation). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb)"
------------------------------
Document 2:

"https://github.com/patil-suraj/question_generation"

-------------------- paper --------------------

Document 1:

"arxiv.org/abs/1910.10683" and "this repo."
------------------------------
Document 2:

"valhalla/t5-base-e2e-qg"
------------------------------
Document 3:

"question-generation" and "Python is a programming language. It is developed by Guido Van Rossum and released in 1991."

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace."

-------------------- demo --------------------

Document 1:

"This is [t5-base](https://arxiv.org/abs/1910.10683) model trained for end-to-end question generation task. Simply input the text and the model will generate multile questions. You can play with the model using the inference API, just put the text and see the results! For more deatils see [this](https://github.com/patil-suraj/question_generation) repo."
------------------------------
Document 2:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb)  ```python3 from pipelines import pipeline text = 'Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.' nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg') nlp(text) => [ 'Who created Python?', 'When was Python first released?', 'What is Python's design philosophy?' ] ```

-------------------- input_format --------------------

Document 1:

"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace." input_format

-------------------- output_format --------------------

Document 1:

output_format: [ 'Who created Python?', 'When was Python first released?', 'What is Python's design philosophy?' ]

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

"valhalla/t5-base-e2e-qg"


[{'datasets': ['squad'], 'license': 'mit', 'github': 'https://github.com/patil-suraj/question_gener 
ation', 'paper': 'arxiv.org/abs/1910.10683', 'upstream_model': '', 'parameter_count': '', 'hyper_par 
ameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': "Python is an interpreted, hi 
gh-level, general-purpose programming language. Created by Guido van Rossum and first released in 19 
91, Python's design philosophy emphasizes code readability with its notable use of significant white 
space.", 'demo': 'This is [t5-base](https://arxiv.org/abs/1910.10683) model trained for end-to-end q 
uestion generation task. Simply input the text and the model will generate multile questions. You ca 
n play with the model using the inference API, just put the text and see the results! For more deati 
ls see [this](https://github.com/patil-suraj/question_generation) repo.', 'input_format': "Python is 
 an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and f 
irst released in 1991, Python's design philosophy emphasizes code readability with its notable use o 
f significant whitespace.", 'output_format': "[ 'Who created Python?', 'When was Python first releas 
ed?', 'What is Python's design philosophy?' ]", 'max_sequence_length': '', 'vocabulary_size': 'valha 
lla/t5-base-e2e-qg'}]                                                                                

#####################Babelscape/rebel-large########################

-------------------- datasets --------------------

Document 1:

- Babelscape/rebel-dataset
- CoNLL04
- NYT
- ADE Corpus
- RE-TACRED
------------------------------
Document 2:

"Be aware that the inference widget at the right does not output special tokens, which are necessary to distinguish the subject, object and relation types. For a demo of REBEL and its pre-training dataset check the [Spaces demo](https://huggingface.co/spaces/Babelscape/rebel-demo)."

-------------------- license --------------------

Document 1:

license: cc-by-nc-sa-4.0

-------------------- github --------------------

Document 1:

"The original repository for the paper can be found [here](https://github.com/Babelscape/rebel)"
------------------------------
Document 2:

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-nyt)](https://paperswithcode.com/sota/relation-extraction-on-nyt?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-conll04)](https://paperswithcode.com/sota/relation-extraction-on-conll04?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/j

-------------------- paper --------------------

Document 1:

[REBEL: Relation Extraction By End-to-end Language generation](https://github.com/Babelscape/rebel/blob/main/docs/EMNLP_2021_REBEL__Camera_Ready_.pdf), @inproceedings{huguet-cabot-navigli-2021-rebel-relation, title = '{REBEL}: Relation Extraction By End-to-end Language generation', author = 'Huguet Cabot, Pere-Llu{\'\i}s  and Navigli, Roberto', booktitle = 'Findings of the Association for Computational Linguistics: EMNLP 2021', month = nov, year = '2021', address = 'Punta Cana, Dominican Republic', publisher = 'Association for Computational Linguistics', url = 'https://aclanthology.org/2021.findings-emnlp.204', pages = '2370--2381', abstract = 'Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other
------------------------------
Document 2:

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-nyt)](https://paperswithcode.com/sota/relation-extraction-on-nyt?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-conll04)](https://paperswithcode.com/sota/relation-extraction-on-conll04?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/j

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

- task:
type: Relation-Extraction
name: Relation Extraction
dataset:
name: CoNLL04
type: CoNLL04
metrics:
- type: re+ macro f1
value: 76.65
name: RE+ Macro F1
- task:
type: Relation-Extraction
name: Relation Extraction
dataset:
name: NYT
type: NYT
metrics:
- type: f1
value: 93.4
name: F1
------------------------------
Document 2:

"We show our model{'}s flexibility by fine-tuning it on an array of Relation Extraction and Relation Classification benchmarks, with it attaining state-of-the-art performance in most of them."

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

"To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language generation, but also in NLU tasks such as Entity Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction can be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model based on BART that performs end-to-end relation extraction for more than 200 different relation types."

-------------------- demo --------------------

Document 1:

"For a demo of REBEL and its pre-training dataset check the [Spaces demo](https://huggingface.co/spaces/Babelscape/rebel-demo)."

-------------------- input_format --------------------



-------------------- output_format --------------------



-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length NO_OUTPUT

-------------------- vocabulary_size --------------------




[{'datasets': ['Babelscape/rebel-dataset', 'CoNLL04', 'NYT', 'ADE Corpus', 'RE-TACRED'], 'license': 
 'cc-by-nc-sa-4.0', 'github': 'https://github.com/Babelscape/rebel', 'paper': 'https://github.com/Ba 
belscape/rebel/blob/main/docs/EMNLP_2021_REBEL__Camera_Ready_.pdf', 'upstream_model': '', 'parameter 
_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'Relation Extraction', 'result': '76.65 
'}, {'test': 'Relation Extraction', 'result': '93.4'}], 'hardware': '', 'limitation_and_bias': 'To o 
vercome these issues, we propose the use of autoregressive seq2seq models. Such models have previous 
ly been shown to perform well not only in language generation, but also in NLU tasks such as Entity  
Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction ca 
n be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model b 
ased on BART that performs end-to-end relation extraction for more than 200 different relation types 
.', 'demo': 'For a demo of REBEL and its pre-training dataset check the [Spaces demo](https://huggin 
gface.co/spaces/Babelscape/rebel-demo).', 'input_format': '', 'output_format': '', 'max_sequence_len 
gth': 'NO_OUTPUT', 'vocabulary_size': ''}]                                                           

#####################google/byt5-large########################

-------------------- datasets --------------------

Document 1:

mC4, TweetQA
------------------------------
Document 2:

datasets:
- mc4

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

Google's T5, MT5, mC4, ByT5, TweetQA, ByT5: Towards a token-free future with pre-trained byte-to-byte models, Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*

-------------------- github --------------------

Document 1:

"google/byt5-large" and "huggingface.co/google/mt5-large"

-------------------- paper --------------------

Document 1:

"In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments."
------------------------------
Document 2:

Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)

-------------------- upstream_model --------------------

Document 1:

Google's T5, MT5, mC4, ByT5, TweetQA, ByT5: Towards a token-free future with pre-trained byte-to-byte models
------------------------------
Document 2:

'google/byt5-large' and 'google/byt5-large'

-------------------- parameter_count --------------------

Document 1:

parameter_count NO_OUTPUT

-------------------- hyper_parameters --------------------

Document 1:

"model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')" and "tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')"

-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.
------------------------------
Document 2:

"Find a form of demo for the model"
------------------------------
Document 3:

"Google's T5", "[mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual)", "[TweetQA](https://arxiv.org/abs/1907.06292)", "[ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)"

-------------------- input_format --------------------

Document 1:

"raw UTF-8 bytes" and "return_tensors='pt'"

-------------------- output_format --------------------

Document 1:

"torch.tensor" and "return_tensors='pt'"

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length: 20 UTF-8 characters
------------------------------
Document 2:

max_sequence_length=512

-------------------- vocabulary_size --------------------

Document 1:

'google/byt5-large'


[{'datasets': ['mC4', 'TweetQA'], 'license': 'apache-2.0', 'github': '"google/byt5-large" and "hugg 
ingface.co/google/mt5-large"', 'paper': '"In this paper, we show that a standard Transformer archite 
cture can be used with minimal modifications to process byte sequences. We carefully characterize th 
e trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-le 
vel models are competitive with their token-level counterparts. We also demonstrate that byte-level  
models are significantly more robust to noise and perform better on tasks that are sensitive to spel 
ling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level  
Transformer models based on the T5 architecture, as well as all code and data used in our experiment 
s."', 'upstream_model': "Google's T5, MT5, mC4, ByT5, TweetQA, ByT5: Towards a token-free future wit 
h pre-trained byte-to-byte models", 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epochs': ' 
', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limit 
ation_and_bias': '', 'demo': 'As part of our contribution, we release a new set of pre-trained byte- 
level Transformer models based on the T5 architecture, as well as all code and data used in our expe 
riments.', 'input_format': '"raw UTF-8 bytes" and "return_tensors=\'pt\'"', 'output_format': '"torch 
.tensor" and "return_tensors=\'pt\'"', 'max_sequence_length': '20 UTF-8 characters', 'vocabulary_siz 
e': "'google/byt5-large'"}]                                                                          

#####################d4data/biomedical-ner-all########################

-------------------- datasets --------------------

Document 1:

datasets NO_OUTPUT
------------------------------
Document 2:

- Dataset: Maccrobat https://figshare.com/articles/dataset/MACCROBAT2018/9764942
------------------------------
Document 3:

'd4data/biomedical-ner-all'

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

"Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18"

-------------------- github --------------------

Document 1:

"https://github.com/dreji18/Bio-Epidemiology-NER"
------------------------------
Document 2:

Maccrobat https://figshare.com/articles/dataset/MACCROBAT2018/9764942, Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18

-------------------- paper --------------------

Document 1:

"Research topic 'AI in Biomedical field' conducted by Deepak John Reji, Shaina Raza"
------------------------------
Document 2:

"Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18"

-------------------- upstream_model --------------------

Document 1:

'd4data/biomedical-ner-all'
------------------------------
Document 2:

distilbert-base-uncased

-------------------- parameter_count --------------------

Document 1:

"Training time: 30.16527 minutes" "GPU used : 1 x GeForce RTX 3060 Laptop GPU" NO_OUTPUT

-------------------- hyper_parameters --------------------

Document 1:

"Training time: 30.16527 minutes" and "GPU used : 1 x GeForce RTX 3060 Laptop GPU"

-------------------- evaluation --------------------

Document 1:

"An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.)." "Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18"

-------------------- hardware --------------------

Document 1:

1 x GeForce RTX 3060 Laptop GPU

-------------------- limitation_and_bias --------------------

Document 1:

"English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.)"

-------------------- demo --------------------

Document 1:

https://github.com/dreji18/Bio-Epidemiology-NER
------------------------------
Document 2:

`from transformers import pipeline from transformers import AutoTokenizer, AutoModelForTokenClassification tokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all') model = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all') pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple') # pass device=0 if using gpu pipe('''The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.''')`

-------------------- input_format --------------------

Document 1:

Maccrobat https://figshare.com/articles/dataset/MACCROBAT2018/9764942, distilbert-base-uncased

-------------------- output_format --------------------

Document 1:

Maccrobat https://figshare.com/articles/dataset/MACCROBAT2018/9764942, 0.0279399890043426 Kg, 30.16527 minutes, 1 x GeForce RTX 3060 Laptop GPU, https://youtu.be/xpiDPdBpS18

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

"107 entities"


[{'datasets': ['Maccrobat'], 'license': 'apache-2.0', 'github': 'https://github.com/dreji18/Bio-Epi 
demiology-NER', 'paper': '', 'upstream_model': 'd4data/biomedical-ner-all', 'parameter_count': 'Trai 
ning time: 30.16527 minutes', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate':  
'', 'optimizer': ''}, 'evaluation': [], 'hardware': '1 x GeForce RTX 3060 Laptop GPU', 'limitation_a 
nd_bias': 'English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical 
 entities (107 entities) from a given text corpus (case reports etc.)', 'demo': 'https://github.com/ 
dreji18/Bio-Epidemiology-NER', 'input_format': 'Maccrobat, distilbert-base-uncased', 'output_format' 
: 'Maccrobat, 0.0279399890043426 Kg, 30.16527 minutes, 1 x GeForce RTX 3060 Laptop GPU, https://yout 
u.be/xpiDPdBpS18', 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': '107 entities'}] 

#####################dslim/bert-large-NER########################

-------------------- datasets --------------------

Document 1:

datasets: - conll2003; dataset: name: conll2003; type: conll2003; config: conll2003; split: test
------------------------------
Document 2:

datasets, CoNLL-2003 NER task, [original BERT paper](https://arxiv.org/pdf/1810.04805)

-------------------- license --------------------

Document 1:

license: mit
------------------------------
Document 2:

CoNLL-2003 Named Entity Recognition, bert-large-cased model

-------------------- github --------------------

Document 1:

"More on replicating the original results [here](https://github.com/google-research/bert/issues/223)."

-------------------- paper --------------------

Document 1:

[original BERT paper](https://arxiv.org/pdf/1810.04805)
------------------------------
Document 2:

"@article{DBLP:journals/corr/abs-1810-04805,
author    = {Jacob Devlin and
Ming{-}Wei Chang and
Kenton Lee and
Kristina Toutanova},
title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
Understanding},
journal   = {CoRR},
volume    = {abs/1810.04805},
year      = {2018},
url       = {http://arxiv.org/abs/1810.04805},
archivePrefix = {arXiv},
eprint    = {1810.04805},
timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}"

-------------------- upstream_model --------------------

Document 1:

original BERT paper
------------------------------
Document 2:

'dslim/bert-base-NER'

-------------------- parameter_count --------------------

Document 1:

parameter_count: single NVIDIA V100 GPU

-------------------- hyper_parameters --------------------

Document 1:

"hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805)"

-------------------- evaluation --------------------

Document 1:

metric|dev|test
-|-|-
f1 |95.7 |91.7
precision |95.3 |91.2
recall |96.1 |92.3
------------------------------
Document 2:

"This model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task."
------------------------------
Document 3:

- type: accuracy
value: 0.9031688753722759
name: Accuracy
verified: true
- type: precision
value: 0.920025068328604
name: Precision
verified: true
- type: recall
value: 0.9193688678588825
name: Recall
verified: true
- type: f1
value: 0.9196968510445761
name: F1
verified: true
- type: loss
value: 0.5085050463676453
name: loss
verified: true

-------------------- hardware --------------------

Document 1:

NVIDIA V100 GPU
------------------------------
Document 2:

bert-large-cased, CoNLL-2003 Named Entity Recognition
------------------------------
Document 3:

"The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins."

-------------------- limitation_and_bias --------------------

Document 1:

"trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task."
------------------------------
Document 2:

This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.
------------------------------
Document 3:

This model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity I-MIS | Miscellaneous entity B-PER |Beginning of a persons name right after another persons name I-PER |Persons name B-ORG |Beginning of an organization right after another organization I-ORG |organization B-LOC |Beginning of a location right after another location I-LOC |Location

-------------------- demo --------------------

Document 1:

"original BERT paper" "CoNLL-2003 NER task"
------------------------------
Document 2:

You can use this model with Transformers *pipeline* for NER.

```python
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained('dslim/bert-base-NER')
model = AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')

nlp = pipeline('ner', model=model, tokenizer=tokenizer)
example = 'My name is Wolfgang and I live in Berlin'

ner_results = nlp(example)
print(ner_results)
```

-------------------- input_format --------------------

Document 1:

metric|dev|test -|-|- f1 |95.7 |91.7 precision |95.3 |91.2 recall |96.1 |92.3

-------------------- output_format --------------------

Document 1:

output_format|metric|dev|test-|-|-f1 |95.7 |91.7precision |95.3 |91.2recall |96.1 |92.3

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

"original BERT paper"
------------------------------
Document 2:

"model-index: - name: dslim/bert-large-NER"


[{'datasets': ['conll2003'], 'license': 'mit', 'github': 'https://github.com/google-research/bert/i 
ssues/223', 'paper': 'https://arxiv.org/pdf/1810.04805', 'upstream_model': 'original BERT paper', 'p 
arameter_count': 'single NVIDIA V100 GPU', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'lea 
rning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'f1', 'result': '91.7'}], 'hardware': 'NV 
IDIA V100 GPU', 'limitation_and_bias': 'trained on a single NVIDIA V100 GPU with recommended hyperpa 
rameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated  
the model on CoNLL-2003 NER task.', 'demo': 'original BERT paper CoNLL-2003 NER task', 'input_format 
': 'metric|dev|test -|-|- f1 |95.7 |91.7 precision |95.3 |91.2 recall |96.1 |92.3', 'output_format': 
 'output_format|metric|dev|test-|-|-f1 |95.7 |91.7precision |95.3 |91.2recall |96.1 |92.3', 'max_seq 
uence_length': 'max_sequence_length', 'vocabulary_size': '"original BERT paper"'}, {'datasets': ['Co 
NLL-2003 NER task'], 'license': '', 'github': '', 'paper': '@article{DBLP:journals/corr/abs-1810-048 
05,\nauthor    = {Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova},\ntitl 
e     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding},\njour 
nal   = {CoRR},\nvolume    = {abs/1810.04805},\nyear      = {2018},\nurl       = {http://arxiv.org/a 
bs/1810.04805},\narchivePrefix = {arXiv},\neprint    = {1810.04805},\ntimestamp = {Tue, 30 Oct 2018  
20:39:56 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\nbibsource =  
{dblp computer science bibliography, https://dblp.org}"', 'upstream_model': "'dslim/bert-base-NER'", 
 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'o 
ptimizer': ''}, 'evaluation': [], 'hardware': 'bert-large-cased, CoNLL-2003 Named Entity Recognition 
', 'limitation_and_bias': 'This model is limited by its training dataset of entity-annotated news ar 
ticles from a specific span of time. This may not generalize well for all use cases in different dom 
ains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of re 
sults may be necessary to handle those cases.', 'demo': "You can use this model with Transformers *p 
ipeline* for NER.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassificat 
ion\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained('dslim/bert-base 
-NER')\nmodel = AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\n\nnlp = pipe 
line('ner', model=model, tokenizer=tokenizer)\nexample = 'My name is Wolfgang and I live in Berlin'\ 
n\nner_results = nlp(example)\nprint(ner_results)\n```", 'input_format': '', 'output_format': '', 'm 
ax_sequence_length': '', 'vocabulary_size': '"model-index: - name: dslim/bert-large-NER"'}, {'datase 
ts': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyp 
er_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation' 
: [{'test': 'accuracy', 'result': '0.9031688753722759'}, {'test': 'precision', 'result': '0.92002506 
8328604'}, {'test': 'recall', 'result': '0.9193688678588825'}, {'test': 'f1', 'result': '0.919696851 
0445761'}, {'test': 'loss', 'result': '0.5085050463676453'}], 'hardware': '', 'limitation_and_bias': 
 'This model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition] 
(https://www.aclweb.org/anthology/W03-0419.pdf) dataset. The training dataset distinguishes between  
the beginning and continuation of an entity so that if there are back-to-back entities of the same t 
ype, the model can output where the second entity begins. As in the dataset, each token will be clas 
sified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-M 
IS |Beginning of a miscellaneous entity right after another miscellaneous entity I-MIS | Miscellaneo 
us entity B-PER |Beginning of a persons name right after another persons name I-PER |Persons name 
 B-ORG |Beginning of an organization right after another organization I-ORG |organization B-LOC |Beg 
inning of a location right after another location I-LOC |Location', 'demo': '', 'input_format': '',  
'output_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]                              

#####################deepset/tinyroberta-squad2########################

-------------------- datasets --------------------

Document 1:

datasets:
- squad_v2
model-index:
- name: deepset/tinyroberta-squad2
results:
- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
metrics:
- type: exact_match
value: 78.8627
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDNlZDU4ODAxMzY5NGFiMTMyZmQ1M2ZhZjMyODA1NmFlOGMxNzYxNTA4OGE5YTBkZWViZjBkNGQ2ZmMxZjVlMCIsInZlcnNpb24iOjF9.Wgu599r6TvgMLTrHlLMVAbUtKD_3b70iJ
------------------------------
Document 2:

"SQuAD 2.0" and "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"
------------------------------
Document 3:

deepset/tinyroberta-squad2

-------------------- license --------------------

Document 1:

license: cc-by-4.0
------------------------------
Document 2:

"GitHub repo" and "GitHub Discussions"
------------------------------
Document 3:

"Some of our other work: - [roberta-base-squad2]([https://huggingface.co/deepset/roberta-base-squad2) - [German BERT (aka 'bert-base-german-cased')](https://deepset.ai/german-bert) - [GermanQuAD and GermanDPR datasets and models (aka 'gelectra-base-germanquad', 'gbert-base-germandpr')](https://deepset.ai/germanquad)"

-------------------- github --------------------

Document 1:

<strong><a href='https://github.com/deepset-ai/haystack'>GitHub</a></strong> and <strong><a class='h-7' href='https://haystack.deepset.ai/community/join'>Discord community open to everyone!</a></strong>
------------------------------
Document 2:

- [deepset](http://deepset.ai/)
- [Haystack](https://haystack.deepset.ai/)
- [roberta-base-squad2]([https://huggingface.co/deepset/roberta-base-squad2)
- [German BERT (aka 'bert-base-german-cased')](https://deepset.ai/german-bert)
- [GermanQuAD and GermanDPR datasets and models (aka 'gelectra-base-germanquad', 'gbert-base-germandpr')](https://deepset.ai/germanquad)
------------------------------
Document 3:

deepset/tinyroberta-squad2

-------------------- paper --------------------

Document 1:

[this paper](https://arxiv.org/pdf/1909.10351.pdf)

-------------------- upstream_model --------------------

Document 1:

'deepset/tinyroberta-squad2'
------------------------------
Document 2:

deepset/roberta-base-squad2, upstream_model
------------------------------
Document 3:

upstream_model: tinyroberta-squad2

-------------------- parameter_count --------------------

Document 1:

parameter_count = 9

-------------------- hyper_parameters --------------------

Document 1:

batch_size = 96, n_epochs = 4, max_seq_len = 384, learning_rate = 3e-5, lr_schedule = LinearWarmup, warmup_proportion = 0.2, doc_stride = 128, max_query_length = 64, distillation_loss_weight = 0.75, temperature = 1.5, teacher = 'deepset/robert-large-squad2'
------------------------------
Document 2:

"haystack", "deepset/tinyroberta-6l-768d", "deepset/roberta-base-squad2", "deepset/roberta-large-squad2"

-------------------- evaluation --------------------

Document 1:

'exact': 78.69114798281817, 'f1': 81.9198998536977, 'total': 11873, 'HasAns_exact': 76.19770580296895, 'HasAns_f1': 82.66446878592329, 'HasAns_total': 5928, 'NoAns_exact': 81.17746005046257, 'NoAns_f1': 81.17746005046257, 'NoAns_total': 5945
------------------------------
Document 2:

- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
metrics:
- type: exact_match
value: 78.8627
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDNlZDU4ODAxMzY5NGFiMTMyZmQ1M2ZhZjMyODA1NmFlOGMxNzYxNTA4OGE5YTBkZWViZjBkNGQ2ZmMxZjVlMCIsInZlcnNpb24iOjF9.Wgu599r6TvgMLTrHlLMVAbUtKD_3b70iJ5QSeDQ-bRfUsVk6Sz9OsJCp47riHJVlmSYzcDj
------------------------------
Document 3:

"Downstream-task: Extractive QA", "Training data: SQuAD 2.0", "Eval data: SQuAD 2.0", "Code: See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "Infrastructure: 4x Tesla v100"

-------------------- hardware --------------------

Document 1:

"4x Tesla v100"
------------------------------
Document 2:

deepset/tinyroberta-6l-768d, deepset/roberta-base-squad2, deepset/roberta-large-squad2

-------------------- limitation_and_bias --------------------

Document 1:

"tinyroberta-squad2", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"

-------------------- demo --------------------

Document 1:

- [deepset](http://deepset.ai/) 
- [Haystack](https://haystack.deepset.ai/) 
- [roberta-base-squad2]([https://huggingface.co/deepset/roberta-base-squad2)
- [German BERT (aka 'bert-base-german-cased')](https://deepset.ai/german-bert)
- [GermanQuAD and GermanDPR datasets and models (aka 'gelectra-base-germanquad', 'gbert-base-germandpr')](https://deepset.ai/germanquad)
------------------------------
Document 2:

<strong><a href='https://github.com/deepset-ai/haystack'>GitHub</a></strong> repo and <strong><a href='https://docs.haystack.deepset.ai'>Documentation</a></strong>
------------------------------
Document 3:

"tinyroberta-squad2", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"

-------------------- input_format --------------------

Document 1:

"name: squad_v2 type: squad_v2 config: squad_v2 split: validation"
------------------------------
Document 2:

"FARMReader", "TransformersReader"

-------------------- output_format --------------------



-------------------- max_sequence_length --------------------

Document 1:

max_seq_len = 384
------------------------------
Document 2:

"tinyroberta-squad2", "Extractive QA", "SQuAD 2.0", "See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"
------------------------------
Document 3:

'model_name_or_path='deepset/tinyroberta-squad2'', 'max_sequence_length'

-------------------- vocabulary_size --------------------

Document 1:

"tinyroberta-squad2", "English", "SQuAD 2.0", "4x Tesla v100"


[{'datasets': ['squad_v2'], 'license': 'cc-by-4.0', 'github': 'https://github.com/deepset-ai/haysta 
ck', 'paper': 'https://arxiv.org/pdf/1909.10351.pdf', 'upstream_model': 'deepset/tinyroberta-squad2' 
, 'parameter_count': '9', 'hyper_parameters': {'epochs': '4', 'batch_size': '96', 'learning_rate': ' 
3e-5', 'optimizer': 'AdamW'}, 'evaluation': [{'test': 'exact', 'result': '78.69114798281817'}], 'har 
dware': '4x Tesla v100', 'limitation_and_bias': 'tinyroberta-squad2, English, Extractive QA, SQuAD 2 
.0', 'demo': "- [deepset](http://deepset.ai/)\n- [Haystack](https://haystack.deepset.ai/)\n- [robert 
a-base-squad2]([https://huggingface.co/deepset/roberta-base-squad2)\n- [German BERT (aka 'bert-base- 
german-cased')](https://deepset.ai/german-bert)\n- [GermanQuAD and GermanDPR datasets and models (ak 
a 'gelectra-base-germanquad', 'gbert-base-germandpr')](https://deepset.ai/germanquad)", 'input_forma 
t': 'name: squad_v2 type: squad_v2 config: squad_v2 split: validation', 'output_format': '', 'max_se 
quence_length': '384', 'vocabulary_size': ''}]                                                       

#####################dccuchile/bert-base-spanish-wwm-uncased########################

-------------------- datasets --------------------

Document 1:

[tensorflow_weights](https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/tensorflow_weights.tar.gz), [pytorch_weights](https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/pytorch_weights.tar.gz), [vocab](./config/uncased_2M/vocab.txt), [config](./config/uncased_2M/config.json), [tensorflow_weights](https://users.dcc.uchile.cl/~jperez/beto/cased_2M/tensorflow_weights.tar.gz), [pytorch_weights](https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz), [vocab](./config/cased_2M/vocab.txt), [config](./config/cased_2M/config.json)

-------------------- license --------------------

Document 1:

"CC BY 4.0"

-------------------- github --------------------

Document 1:

[tensorflow_weights](https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/tensorflow_weights.tar.gz), [pytorch_weights](https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/pytorch_weights.tar.gz), [vocab](./config/uncased_2M/vocab.txt), [config](./config/uncased_2M/config.json), [tensorflow_weights](https://users.dcc.uchile.cl/~jperez/beto/cased_2M/tensorflow_weights.tar.gz), [pytorch_weights](https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz), [vocab](./config/cased_2M/vocab.txt), [config](./config/cased_2M/config.json)
------------------------------
Document 2:

[Huggingface Transformers library](https://github.com/huggingface/transformers), [`'dccuchile/bert-base-spanish-wwm-cased'`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased), [`'dccuchile/bert-base-spanish-wwm-uncased'`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased), [this colab notebook](https://colab.research.google.com/drive/1uRwg4UmPgYIqGYY4gW_Nsw9782GFJbPt)

-------------------- paper --------------------

Document 1:

"@inproceedings{CaneteCFP2020,
title={Spanish Pre-Trained BERT Model and Evaluation Data},
author={Caete, Jos and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Prez, Jorge},
booktitle={PML4DC at ICLR 2020},
year={2020}"

-------------------- upstream_model --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.
------------------------------
Document 2:

'dccuchile/bert-base-spanish-wwm-cased' and 'dccuchile/bert-base-spanish-wwm-uncased'

-------------------- parameter_count --------------------

Document 1:

[config](./config/uncased_2M/config.json) [config](./config/cased_2M/config.json)

-------------------- hyper_parameters --------------------

Document 1:

[config](./config/uncased_2M/config.json) [config](./config/cased_2M/config.json)
------------------------------
Document 2:

'dccuchile/bert-base-spanish-wwm-cased' and 'dccuchile/bert-base-spanish-wwm-uncased'

-------------------- evaluation --------------------

Document 1:

The following table shows some BETO results in the Spanish version of every task. We compare BETO (cased and uncased) with the Best Multilingual BERT results that we found in the literature (as of October 2019). The table also shows some alternative methods for the same tasks (not necessarily BERT-based methods).

|Task   | BETO-cased    | BETO-uncased  | Best Multilingual BERT    | Other results                  |
|-------|--------------:|--------------:|--------------------------:|-------------------------------:|
|[POS](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1827)    | **98.97**     | 98.44     | 97.10 [2]                 | 98.91 [6], 96.71 [3]           |
|[NER-C](https://www.kaggle.com/nltkdata/conll-corpora)  | [**88.43**](https://github.com/gchaperon/beto-benchmarks/blob/master/conll2002/

-------------------- hardware --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.

-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

'For further details on how to use BETO you can visit the [Huggingface Transformers library](https://github.com/huggingface/transformers), starting by the [Quickstart section](https://huggingface.co/transformers/quickstart.html).
BETO models can be accessed simply as [`'dccuchile/bert-base-spanish-wwm-cased'`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) and [`'dccuchile/bert-base-spanish-wwm-uncased'`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased) by using the Transformers library.
An example on how to download and use the models in this page can be found in [this colab notebook](https://colab.research.google.com/drive/1uRwg4UmPgYIqGYY4gW_Nsw9782GFJbPt).

-------------------- input_format --------------------

Document 1:

[vocab](./config/uncased_2M/vocab.txt), [config](./config/uncased_2M/config.json)
------------------------------
Document 2:

'dccuchile/bert-base-spanish-wwm-cased' and 'dccuchile/bert-base-spanish-wwm-uncased'

-------------------- output_format --------------------

Document 1:

[tensorflow_weights](https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/tensorflow_weights.tar.gz), [pytorch_weights](https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/pytorch_weights.tar.gz), [vocab](./config/uncased_2M/vocab.txt), [config](./config/uncased_2M/config.json)

-------------------- max_sequence_length --------------------

Document 1:

[config](./config/uncased_2M/config.json) [config](./config/cased_2M/config.json)
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

[vocab](./config/uncased_2M/vocab.txt), [config](./config/uncased_2M/config.json), 31k BPE subwords


[{'datasets': ['tensorflow_weights', 'pytorch_weights', 'vocab', 'config'], 'license': 'CC BY 4.0', 
 'github': 'tensorflow_weights', 'paper': '@inproceedings{CaneteCFP2020,\ntitle={Spanish Pre-Trained 
 BERT Model and Evaluation Data},\nauthor={Caete, Jos and Chaperon, Gabriel and Fuentes, Rodrigo a 
nd Ho, Jou-Hui and Kang, Hojin and Prez, Jorge},\nbooktitle={PML4DC at ICLR 2020},\nyear={2020}', ' 
upstream_model': 'All models use a vocabulary of about 31k BPE subwords constructed using SentencePi 
ece and were trained for 2M steps.', 'parameter_count': '[config](./config/uncased_2M/config.json) [ 
config](./config/cased_2M/config.json)', 'hyper_parameters': '[config](./config/uncased_2M/config.js 
on) [config](./config/cased_2M/config.json)', 'evaluation': [{'test': 'POS', 'result': '**98.97**'}, 
 {'test': 'NER-C', 'result': '**88.43**'}], 'hardware': 'All models use a vocabulary of about 31k BP 
E subwords constructed using SentencePiece and were trained for 2M steps.', 'limitation_and_bias': ' 
', 'demo': "'For further details on how to use BETO you can visit the [Huggingface Transformers lib 
rary](https://github.com/huggingface/transformers), starting by the [Quickstart section](https://hug 
gingface.co/transformers/quickstart.html).\nBETO models can be accessed simply as [`'dccuchile/bert- 
base-spanish-wwm-cased'`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) and [`'dccuc 
hile/bert-base-spanish-wwm-uncased'`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased 
) by using the Transformers library.\nAn example on how to download and use the models in this page  
can be found in [this colab notebook](https://colab.research.google.com/drive/1uRwg4UmPgYIqGYY4gW_Ns 
w9782GFJbPt).", 'input_format': '[vocab](./config/uncased_2M/vocab.txt), [config](./config/uncased_2 
M/config.json)', 'output_format': '[tensorflow_weights](https://users.dcc.uchile.cl/~jperez/beto/unc 
ased_2M/tensorflow_weights.tar.gz), [pytorch_weights](https://users.dcc.uchile.cl/~jperez/beto/uncas 
ed_2M/pytorch_weights.tar.gz), [vocab](./config/uncased_2M/vocab.txt), [config](./config/uncased_2M/ 
config.json)', 'max_sequence_length': '[config](./config/uncased_2M/config.json) [config](./config/c 
ased_2M/config.json)', 'vocabulary_size': '[vocab](./config/uncased_2M/vocab.txt), [config](./config 
/uncased_2M/config.json), 31k BPE subwords'}, {'datasets': ['Huggingface Transformers library', "'d 
ccuchile/bert-base-spanish-wwm-cased'", "'dccuchile/bert-base-spanish-wwm-uncased'", 'this colab not 
ebook'], 'license': '', 'github': '', 'paper': '', 'upstream_model': "'dccuchile/bert-base-spanish-w 
wm-cased' and 'dccuchile/bert-base-spanish-wwm-uncased'", 'parameter_count': "'dccuchile/bert-base-s 
panish-wwm-cased' and 'dccuchile/bert-base-spanish-wwm-uncased'", 'hyper_parameters': "'dccuchile/be 
rt-base-spanish-wwm-cased' and 'dccuchile/bert-base-spanish-wwm-uncased'", 'evaluation': [], 'hardwa 
re': '', 'limitation_and_bias': '', 'demo': '', 'input_format': "'dccuchile/bert-base-spanish-wwm-ca 
sed' and 'dccuchile/bert-base-spanish-wwm-uncased'", 'output_format': '', 'max_sequence_length': 'ma 
x_sequence_length', 'vocabulary_size': ''}]                                                          

#####################huggingface/CodeBERTa-small-v1########################

-------------------- datasets --------------------

Document 1:

datasets: - code_search_net
------------------------------
Document 2:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub.
------------------------------
Document 3:

`huggingface/CodeBERTa-language-id`

-------------------- license --------------------

Document 1:

CodeSearchNet dataset from GitHub, Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`, 6-layer, 84M parameters, RoBERTa-like Transformer model

-------------------- github --------------------

Document 1:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model  thats the same number of layers & heads as DistilBERT  initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- paper --------------------

Document 1:

`@article{husain_codesearchnet_2019, title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}}, url = {http://arxiv.org/abs/1909.09436}, author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc}, year = {2019}, note = {arXiv: 1909.09436},}`
------------------------------
Document 2:

`huggingface/CodeBERTa-language-id`
------------------------------
Document 3:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model  thats the same number of layers & heads as DistilBERT  initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- upstream_model --------------------

Document 1:

huggingface/CodeBERTa-language-id
------------------------------
Document 2:

'CodeBERTa is a RoBERTa-like model' 'The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model'

-------------------- parameter_count --------------------

Document 1:

'84M parameters'
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

"huggingface/CodeBERTa-language-id" .

-------------------- evaluation --------------------

Document 1:

"See the model card for **[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)** ."
------------------------------
Document 2:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model  thats the same number of layers & heads as DistilBERT  initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- hardware --------------------

Document 1:

'Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`' and '6-layer, 84M parameters, RoBERTa-like Transformer model'
------------------------------
Document 2:

huggingface/CodeBERTa-language-id

-------------------- limitation_and_bias --------------------

Document 1:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model  thats the same number of layers & heads as DistilBERT  initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.
------------------------------
Document 2:

huggingface/CodeBERTa-language-id

-------------------- demo --------------------

Document 1:

`huggingface/CodeBERTa-language-id`

-------------------- input_format --------------------

Document 1:

'Byte-level BPE tokenizer' '6-layer, 84M parameters, RoBERTa-like Transformer model' 'same number of layers & heads as DistilBERT' 'trained from scratch on the full corpus (~2M functions) for 5 epochs' input_format: Byte-level BPE tokenizer
------------------------------
Document 2:

"huggingface/CodeBERTa-language-id" input_format

-------------------- output_format --------------------

Document 1:

"huggingface/CodeBERTa-language-id" output_format
------------------------------
Document 2:

'Byte-level BPE tokenizer' '6-layer, 84M parameters, RoBERTa-like Transformer model'

-------------------- max_sequence_length --------------------

Document 1:

'tokenizers' and 'The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model'
------------------------------
Document 2:

"huggingface/CodeBERTa-language-id", "max_sequence_length"

-------------------- vocabulary_size --------------------

Document 1:

'Byte-level BPE tokenizer' and '84M parameters'
------------------------------
Document 2:

"vocabulary_size"


[{'datasets': ['CodeSearchNet'], 'license': 'CodeSearchNet dataset from GitHub, Byte-level BPE toke 
nizer trained on the corpus using Hugging Face `tokenizers`, 6-layer, 84M parameters, RoBERTa-like T 
ransformer model', 'github': 'CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https 
://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokeniz 
er is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small)  
model is a 6-layer, 84M parameters, RoBERTa-like Transformer model  thats the same number of layer 
s & heads as DistilBERT  initialized from the default initialization settings and trained from scra 
tch on the full corpus (~2M functions) for 5 epochs.', 'paper': '@article{husain_codesearchnet_2019, 
 title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}}, url  
= {http://arxiv.org/abs/1909.09436}, author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet an 
d Allamanis, Miltiadis and Brockschmidt, Marc}, year = {2019}, note = {arXiv: 1909.09436},}', 'upstr 
eam_model': 'huggingface/CodeBERTa-language-id', 'parameter_count': '84M parameters', 'hyper_paramet 
ers': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'har 
dware': 'Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers` and 6-layer, 
 84M parameters, RoBERTa-like Transformer model', 'limitation_and_bias': 'CodeBERTa is a RoBERTa-lik 
e model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet- 
challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus u 
sing Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transfo 
rmer model  thats the same number of layers & heads as DistilBERT  initialized from the default i 
nitialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.', ' 
demo': 'huggingface/CodeBERTa-language-id', 'input_format': 'Byte-level BPE tokenizer', 'output_form 
at': 'huggingface/CodeBERTa-language-id', 'max_sequence_length': 'tokenizers', 'vocabulary_size': 'B 
yte-level BPE tokenizer'}]                                                                           

#####################xlm-roberta-base########################

-------------------- datasets --------------------

Document 1:

"2.5TB of filtered CommonCrawl data containing 100 languages" and "[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)"

-------------------- license --------------------

Document 1:

license: mit

-------------------- github --------------------

Document 1:

"See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 2:

"this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)"

-------------------- paper --------------------

Document 1:

Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.
------------------------------
Document 2:

Unsupervised Cross-lingual Representation Learning at Scale
------------------------------
Document 3:

"See the [model hub](https://huggingface.co/models?search=xlm-roberta)"

-------------------- upstream_model --------------------

Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)
------------------------------
Document 2:

XLM-RoBERTa is a multilingual version of RoBERTa. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Masked language modeling (MLM) objective.

-------------------- parameter_count --------------------

Document 1:

"RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective."

-------------------- hyper_parameters --------------------

Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).

-------------------- evaluation --------------------

Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
------------------------------
Document 2:

"RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence."

-------------------- hardware --------------------

Document 1:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.

-------------------- limitation_and_bias --------------------

Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)
------------------------------
Document 2:

"This model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
------------------------------
Document 3:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.

-------------------- demo --------------------

Document 1:

"See the [model hub](https://huggingface.co/models?search=xlm-roberta)"
------------------------------
Document 2:

<a href='https://huggingface.co/exbert/?model=xlm-roberta-base'><img width='300px' src='https://cdn-media.huggingface.co/exbert/button.png'></a>
------------------------------
Document 3:

"This repository" https://github.com/pytorch/fairseq/tree/master/examples/xlmr

-------------------- input_format --------------------

Document 1:

"XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.", "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion.", "Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.", "This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs."

input_format: Masked language modeling (MLM)

-------------------- output_format --------------------



-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length
------------------------------
Document 3:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.

-------------------- vocabulary_size --------------------

Document 1:

"XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages." "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion." "Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words." "This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks." 
NO_OUTPUT


[{'datasets': ['2.5TB of filtered CommonCrawl data containing 100 languages'], 'license': 'mit', 'g 
ithub': 'https://github.com/pytorch/fairseq/tree/master/examples/xlmr', 'paper': 'Unsupervised Cross 
-lingual Representation Learning at Scale by Conneau et al.', 'upstream_model': 'Unsupervised Cross- 
lingual Representation Learning at Scale by Conneau et al.', 'parameter_count': 'RoBERTa is a transf 
ormers model pretrained on a large corpus in a self-supervised fashion.', 'hyper_parameters': {}, 'e 
valuation': [], 'hardware': 'XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on  
2.5TB of filtered CommonCrawl data containing 100 languages.', 'limitation_and_bias': 'Unsupervised  
Cross-lingual Representation Learning at Scale by Conneau et al.', 'demo': 'See the [model hub](http 
s://huggingface.co/models?search=xlm-roberta)', 'input_format': 'Masked language modeling (MLM)', 'o 
utput_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]                                

#####################microsoft/deberta-base########################

-------------------- datasets --------------------

Document 1:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
------------------------------
Document 2:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates. We present the dev results on SQuAD 1.1/2.0 and MNLI tasks.
------------------------------
Document 3:

- deberta-v1 - fill-mask

-------------------- license --------------------

Document 1:

license: mit
------------------------------
Document 2:

"International Conference on Learning Representations"

-------------------- github --------------------

Document 1:

"url={https://openreview.net/forum?id=XPZIaotutsD}"
------------------------------
Document 2:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.

-------------------- paper --------------------

Document 1:

"If you find DeBERTa useful for your work, please cite the following paper: 
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}"
------------------------------
Document 2:

[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder.
------------------------------
Document 3:

"tags: - deberta-v1 - fill-mask"

-------------------- upstream_model --------------------

Document 1:

"BERT and RoBERTa models" 
upstream_model: BERT and RoBERTa
------------------------------
Document 2:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"

-------------------- parameter_count --------------------

Document 1:

"XPZIaotutsD"

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |
|-------------------|-----------|-----------|--------|
| RoBERTa-base      | 91.5/84.6 | 83.7/80.5 | 87.6   |
| XLNet-Large       | -/-       | -/80.2    | 86.8   |
| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |

-------------------- hardware --------------------

Document 1:

"International Conference on Learning Representations"

-------------------- limitation_and_bias --------------------

Document 1:

"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data." "We present the dev results on SQuAD 1.1/2.0 and MNLI tasks." "| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |
|-------------------|-----------|-----------|--------|
| RoBERTa-base      | 91.5/84.6 | 83.7/80.5 | 87.6   |
| XLNet-Large       | -/-       | -/80.2    | 86.8   |
| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |"

-------------------- demo --------------------

Document 1:

"Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates."

-------------------- input_format --------------------



-------------------- output_format --------------------



-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['SQuAD 1.1', 'SQuAD 2.0', 'MNLI'], 'license': 'mit', 'github': 'https://github.com/m 
icrosoft/DeBERTa', 'paper': 'https://openreview.net/forum?id=XPZIaotutsD', 'upstream_model': 'BERT a 
nd RoBERTa', 'parameter_count': 'XPZIaotutsD', 'hyper_parameters': {}, 'evaluation': [{'test': 'SQuA 
D 1.1', 'result': '93.1/87.2'}, {'test': 'SQuAD 2.0', 'result': '86.2/83.1'}, {'test': 'MNLI', 'resu 
lt': '88.8'}], 'hardware': 'International Conference on Learning Representations', 'limitation_and_b 
ias': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask d 
ecoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.', 'demo':  
'Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and u 
pdates.', 'input_format': '', 'output_format': '', 'max_sequence_length': 'max_sequence_length', 'vo 
cabulary_size': ''}]                                                                                 

#####################facebook/mbart-large-50-many-to-many-mmt########################

-------------------- datasets --------------------

Document 1:

"Multilingual Translation with Extensible Multilingual Pretraining and Finetuning"
------------------------------
Document 2:

[mBart-large-50](https://huggingface.co/facebook/mbart-large-50), [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401), [model hub](https://huggingface.co/models?filter=mbart-50)
------------------------------
Document 3:

- mbart-50

-------------------- license --------------------

Document 1:

"archivePrefix={arXiv}, primaryClass={cs.CL}"

-------------------- github --------------------

Document 1:

github

-------------------- paper --------------------

Document 1:

"title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},"
------------------------------
Document 2:

`Multilingual Translation with Extensible Multilingual Pretraining and Finetuning` paper.

-------------------- upstream_model --------------------

Document 1:

"This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50)."
------------------------------
Document 2:

"primaryClass={cs.CL}"

-------------------- parameter_count --------------------

Document 1:

"MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')" and "MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')"
------------------------------
Document 2:

parameter_count NO_OUTPUT

-------------------- hyper_parameters --------------------

Document 1:

`forced_bos_token_id=tokenizer.lang_code_to_id['fr_XX']` `forced_bos_token_id=tokenizer.lang_code_to_id['en_XX']`

-------------------- evaluation --------------------

Document 1:

"This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation. It was introduced in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper."

-------------------- hardware --------------------

Document 1:

`MBartForConditionalGeneration`, `MBart50TokenizerFast`

-------------------- limitation_and_bias --------------------

Document 1:

`forced_bos_token_id` parameter to the `generate` method.

-------------------- demo --------------------

Document 1:

"Find a form of demo for the model"

-------------------- input_format --------------------

Document 1:

language:
- multilingual
- ar
- cs
- de
- en
- es
- et
- fi
- fr
- gu
- hi
- it
- ja
- kk
- ko
- lt
- lv
- my
- ne
- nl
- ro
- ru
- si
- tr
- vi
- zh
- af
- az
- bn
- fa
- he
- hr
- id
- ka
- km
- mk
- ml
- mn
- mr
- pl
- ps
- pt
- sv
- sw
- ta
- te
- th
- tl
- uk
- ur
- xh
- gl
- sl

Input format: multilingual
------------------------------
Document 2:

"return_tensors='pt'"

-------------------- output_format --------------------

Document 1:

language:
- multilingual
- ar
- cs
- de
- en
- es
- et
- fi
- fr
- gu
- hi
- it
- ja
- kk
- ko
- lt
- lv
- my
- ne
- nl
- ro
- ru
- si
- tr
- vi
- zh
- af
- az
- bn
- fa
- he
- hr
- id
- ka
- km
- mk
- ml
- mn
- mr
- pl
- ps
- pt
- sv
- sw
- ta
- te
- th
- tl
- uk
- ur
- xh
- gl
- sl
------------------------------
Document 2:

"The model can translate directly between any pair of 50 languages. To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method."

-------------------- max_sequence_length --------------------

Document 1:

`forced_bos_token_id=tokenizer.lang_code_to_id['en_XX']`
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

"MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')"
------------------------------
Document 2:

- multilingual - ar - cs - de - en - es - et - fi - fr - gu - hi - it - ja - kk - ko - lt - lv - my - ne - nl - ro - ru - si - tr - vi - zh - af - az - bn - fa - he - hr - id - ka - km - mk - ml - mn - mr - pl - ps - pt - sv - sw - ta - te - th - tl - uk - ur - xh - gl - sl


[{'datasets': ['mbart-50'], 'license': 'archivePrefix={arXiv}, primaryClass={cs.CL}', 'github': 'gi 
thub', 'paper': 'title={Multilingual Translation with Extensible Multilingual Pretraining and Finetu 
ning},', 'upstream_model': 'This model is a fine-tuned checkpoint of [mBART-large-50](https://huggin 
gface.co/facebook/mbart-large-50).', 'parameter_count': "MBartForConditionalGeneration.from_pretrain 
ed('facebook/mbart-large-50-many-to-many-mmt') and MBart50TokenizerFast.from_pretrained('facebook/mb 
art-large-50-many-to-many-mmt')", 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rat 
e': '', 'optimizer': ''}, 'evaluation': [], 'hardware': 'MBartForConditionalGeneration, MBart50Token 
izerFast', 'limitation_and_bias': 'forced_bos_token_id parameter to the generate method.', 'demo': ' 
Find a form of demo for the model', 'input_format': 'language: - multilingual - ar - cs - de - en -  
es - et - fi - fr - gu - hi - it - ja - kk - ko - lt - lv - my - ne - nl - ro - ru - si - tr - vi -  
zh - af - az - bn - fa - he - hr - id - ka - km - mk - ml - mn - mr - pl - ps - pt - sv - sw - ta -  
te - th - tl - uk - ur - xh - gl - sl\n\nInput format: multilingual', 'output_format': 'language: -  
multilingual - ar - cs - de - en - es - et - fi - fr - gu - hi - it - ja - kk - ko - lt - lv - my -  
ne - nl - ro - ru - si - tr - vi - zh - af - az - bn - fa - he - hr - id - ka - km - mk - ml - mn -  
mr - pl - ps - pt - sv - sw - ta - te - th - tl - uk - ur - xh - gl - sl\n\nThe model can translate  
directly between any pair of 50 languages. To translate into a target language, the target language  
id is forced as the first generated token. To force the target language id as the first generated to 
ken, pass the forced_bos_token_id parameter to the generate method.', 'max_sequence_length': "forced 
_bos_token_id=tokenizer.lang_code_to_id['en_XX']", 'vocabulary_size': "MBart50TokenizerFast.from_pre 
trained('facebook/mbart-large-50-many-to-many-mmt')"}]                                               

#####################Helsinki-NLP/opus-mt-it-en########################

-------------------- datasets --------------------

Document 1:

dataset: opus, download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)
------------------------------
Document 2:

newssyscomb2009.it.en, newstest2009.it.en, Tatoeba.it.en

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

[it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md), [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)

-------------------- paper --------------------



-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

* OPUS readme: [it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md)  
* dataset: opus
* model: transformer-align
* pre-processing: normalization + SentencePiece
* download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)
* test set translations: [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt)
* test set scores: [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009.it.en 	| 35.3 	| 0.600 |
| newstest2009.it.en 	| 34.0 	| 0.594 |
| Tatoeba.it.en 	| 70.9 	| 0.808 |

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

[it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md), opus, transformer-align, normalization + SentencePiece, [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)

-------------------- input_format --------------------

Document 1:

SentencePiece + normalization

-------------------- output_format --------------------

Document 1:

SentencePiece + opus-2019-12-18.zip + opus-2019-12-18.test.txt + opus-2019-12-18.eval.txt

-------------------- max_sequence_length --------------------



-------------------- vocabulary_size --------------------

Document 1:

SentencePiece


[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': '[it-en](https://github.com/Helsinki-NLP 
/OPUS-MT-train/blob/master/models/it-en/README.md), [opus-2019-12-18.zip](https://object.pouta.csc.f 
i/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/ 
OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt](https://object.pouta.csc. 
fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)', 'paper': '', 'upstream_model': '', 'parameter_co 
unt': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'OPUS readme: [it-en](https://github.com/H 
elsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md)', 'result': ''}], 'hardware': '', 'lim 
itation_and_bias': '', 'demo': '[it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/mo 
dels/it-en/README.md), opus, transformer-align, normalization + SentencePiece, [opus-2019-12-18.zip] 
(https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](h 
ttps://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt 
](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)', 'input_format': 'Sent 
encePiece + normalization', 'output_format': 'SentencePiece + opus-2019-12-18.zip + opus-2019-12-18. 
test.txt + opus-2019-12-18.eval.txt', 'max_sequence_length': '', 'vocabulary_size': 'SentencePiece'} 
]                                                                                                    

#####################facebook/bart-large-cnn########################

-------------------- datasets --------------------

Document 1:

datasets:
- cnn_dailymail
dataset:
name: cnn_dailymail
type: cnn_dailymail
------------------------------
Document 2:

[CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail), [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461), [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart).

-------------------- license --------------------

Document 1:

license: mit

-------------------- github --------------------



-------------------- paper --------------------

Document 1:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
------------------------------
Document 2:

"title = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},"

-------------------- upstream_model --------------------

Document 1:

"BART model pre-trained on English language" and "introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al."

-------------------- parameter_count --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)"

-------------------- hyper_parameters --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."

-------------------- evaluation --------------------

Document 1:

- type: rouge
value: 42.9486
name: ROUGE-1
verified: true
- type: rouge
value: 20.8149
name: ROUGE-2
verified: true
- type: rouge
value: 30.6186
name: ROUGE-L
verified: true
- type: rouge
value: 40.0376
name: ROUGE-LSUM
verified: true
- type: loss
value: 2.529000997543335
name: loss
verified: true
- type: gen_len
value: 78.5866
name: gen_len
verified: true
------------------------------
Document 2:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."

-------------------- hardware --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)" and "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."
------------------------------
Document 2:

"This particular checkpoint has been fine-tuned on CNN Daily Mail"

-------------------- limitation_and_bias --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."

-------------------- demo --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."

-------------------- input_format --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)"

-------------------- output_format --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al." and "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."

-------------------- max_sequence_length --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)"


[{'datasets': ['cnn_dailymail'], 'license': 'mit', 'github': '', 'paper': 'https://arxiv.org/abs/19 
10.13461', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_si 
ze': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bi 
as': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': '', 'vocabulary 
_size': ''}]                                                                                         

#####################PygmalionAI/pygmalion-1.3b########################

-------------------- datasets --------------------

Document 1:

"56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations."

-------------------- license --------------------

Document 1:

license: agpl-3.0

-------------------- github --------------------

Document 1:

[here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb)
------------------------------
Document 2:

"language: - en license: agpl-3.0 tags: - text generation - conversational inference: false"

-------------------- paper --------------------

Document 1:

"real _and_ partially machine-generated conversations"
------------------------------
Document 2:

"EleutherAI's [pythia-1.3b-deduped](https://huggingface.co/EleutherAI/pythia-1.3b-deduped)"

-------------------- upstream_model --------------------

Document 1:

EleutherAI/pythia-1.3b-deduped, upstream_model

-------------------- parameter_count --------------------

Document 1:

"11.4 million tokens" "5440 steps" "single 24GB GPU"

-------------------- hyper_parameters --------------------

Document 1:

"Fine-tuning was done using [ColossalAI](https://github.com/hpcaitech/ColossalAI) (specifically, with a slightly modified version of their [OPT fine-tune example](https://github.com/hpcaitech/ColossalAI/blob/78509124d32b63b7fc36f6508e0576a326d51422/examples/language/opt/run_clm.py)) for around 11.4 million tokens over 5440 steps on a single 24GB GPU. The run took just under 21 hours."

-------------------- evaluation --------------------

Document 1:

"The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations."

-------------------- hardware --------------------

Document 1:

"real _and_ partially machine-generated conversations"
------------------------------
Document 2:

"single 24GB GPU"

-------------------- limitation_and_bias --------------------

Document 1:

limitation_and_bias: The model can get stuck repeating certain phrases, or sometimes even entire sentences.
------------------------------
Document 2:

real _and_ partially machine-generated conversations.
------------------------------
Document 3:

The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format:  
```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]

[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```  
Where `[CHARACTER] `is, as you can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like:  
```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```  
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.

-------------------- demo --------------------

Document 1:

"The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format: 
```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]

[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```  
Where `[CHARACTER] `is, as you can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like:  
```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```  
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition."
------------------------------
Document 2:

[here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb)

-------------------- input_format --------------------

Document 1:

"The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format: 

```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]

[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```

Where `[CHARACTER] `is, as you can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like: 

```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```

Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition."
------------------------------
Document 2:

real _and_ partially machine-generated conversations. input_format

-------------------- output_format --------------------



-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length=11.4 million tokens

-------------------- vocabulary_size --------------------

Document 1:

"11.4 million tokens"


[{'datasets': ['56MB of dialogue data gathered from multiple sources, which includes both real _and 
_ partially machine-generated conversations.'], 'license': 'agpl-3.0', 'github': '[here](https://git 
hub.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb)', 'paper': "EleutherAI's [pythia-1.3b 
-deduped](https://huggingface.co/EleutherAI/pythia-1.3b-deduped)", 'upstream_model': 'EleutherAI/pyt 
hia-1.3b-deduped, upstream_model', 'parameter_count': '11.4 million tokens', 'hyper_parameters': {'e 
pochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': ' 
single 24GB GPU', 'limitation_and_bias': 'The model can get stuck repeating certain phrases, or some 
times even entire sentences.', 'demo': "The model can be used as a regular text generation model, bu 
t it'll perform best if the input prompt adheres to the following format: \n```\n[CHARACTER]'s Perso 
na: [A few sentences about the character you want the model to play]\n\n[DIALOGUE HISTORY]\nYou: [Yo 
ur input message here]\n[CHARACTER]:\n```\nWhere `[CHARACTER] `is, as you can probably guess, the na 
me of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the m 
odel can have some conversational context to draw from. Ideally it'll be pairs of messages like:  \n 
```\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]\n```\nApart from  
chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the ch 
aracter should speak - ideally at the beginning, so it doesn't get confused as to what's conversatio 
n history vs. character definition.", 'input_format': "The model can be used as a regular text gener 
ation model, but it'll perform best if the input prompt adheres to the following format: \n\n```\n[C 
HARACTER]'s Persona: [A few sentences about the character you want the model to play]\n\n[DIALOGUE H 
ISTORY]\nYou: [Your input message here]\n[CHARACTER]:\n```\n\nWhere `[CHARACTER] `is, as you can pro 
bably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is ch 
at history so the model can have some conversational context to draw from. Ideally it'll be pairs of 
 messages like: \n\n```\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue abov 
e]\n```\n\nApart from chat history, you can also just add example conversations in `[DIALOGUE HISTOR 
Y]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as 
 to what's conversation history vs. character definition.", 'output_format': '', 'max_sequence_lengt 
h': '11.4 million tokens', 'vocabulary_size': '11.4 million tokens'}]                                

#####################sentence-transformers/multi-qa-mpnet-base-cos-v1########################

-------------------- datasets --------------------

Document 1:

WikiAnswers Duplicate question pairs from WikiAnswers, PAQ Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia, Stack Exchange (Title, Body) pairs from all StackExchanges, Stack Exchange (Title, Answer) pairs from all StackExchanges, MS MARCO Triplets (query, answer, hard_negative) for 500k queries from Bing search engine, GOOAQ: Open Question Answering with Diverse Answer Types (query, answer) pairs for 3M Google queries and Google featured snippet, Amazon-QA (Question, Answer) pairs from Amazon product pages, Yahoo Answers (Title, Answer) pairs from Yahoo Answers, Yahoo Answers (Question, Answer) pairs from Yahoo Answers, Yahoo Answers (Title, Question) pairs from Yahoo Answers, SearchQA (Question, Answer) pairs for 140k questions, each with Top5 Google snippets on that question, ELI5 (Question, Answer) pairs from Reddit ELI5 (explainlikeimfive), Stack Exchange Duplicate questions pairs (titles), Quora Question Triplets (Question, Duplicate_Question, Hard_Negative) triplets for Quora Questions Pairs dataset, Natural Questions (NQ) (Question
------------------------------
Document 2:

"Train the Best Sentence Embedding Model Ever with 1B Training Pairs" and "7 TPUs v3-8"

-------------------- license --------------------



-------------------- github --------------------

Document 1:

`train_script.py`

-------------------- paper --------------------

Document 1:

"It has been trained on 215M (question, answer) pairs from diverse sources." NO_OUTPUT

-------------------- upstream_model --------------------

Document 1:

"This is a [sentence-transformers](https://www.SBERT.net) model" and "It has been trained on 215M (question, answer) pairs from diverse sources."

-------------------- parameter_count --------------------

Document 1:

parameter_count: 512
------------------------------
Document 2:

"214,988,242" parameter_count

-------------------- hyper_parameters --------------------

Document 1:

"Dimensions: 768" 
"Produces normalized embeddings: Yes" 
"Pooling-Method: Mean pooling" 
"Suitable score functions: dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance"
------------------------------
Document 2:

"sentence-transformers", "maps sentences & paragraphs to a 768 dimensional dense vector space", "trained on 215M (question, answer) pairs from diverse sources"

-------------------- evaluation --------------------



-------------------- hardware --------------------

Document 1:

"It has been trained on 215M (question, answer) pairs from diverse sources." NO_OUTPUT

-------------------- limitation_and_bias --------------------

Document 1:

"Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text."

-------------------- demo --------------------

Document 1:

"It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages." Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.
------------------------------
Document 2:

sentence-transformers, semantic search, [SBERT.net - Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)
------------------------------
Document 3:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used."

-------------------- input_format --------------------

Document 1:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1." input_format: normalized embeddings with length 1

-------------------- output_format --------------------

Document 1:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1."

-------------------- max_sequence_length --------------------

Document 1:

Note that there is a limit of 512 word pieces: Text longer than that will be truncated. max_sequence_length: 512
------------------------------
Document 2:

max_sequence_length: 1

-------------------- vocabulary_size --------------------




[{'datasets': ['WikiAnswers', 'PAQ', 'Stack Exchange', 'MS MARCO', 'GOOAQ', 'Amazon-QA', 'Yahoo Ans 
wers', 'SearchQA', 'ELI5', 'Quora Question Triplets', 'Natural Questions'], 'github': 'train_script. 
py', 'paper': 'It has been trained on 215M (question, answer) pairs from diverse sources.', 'upstrea 
m_model': 'sentence-transformers', 'parameter_count': '512', 'hyper_parameters': {'epochs': '', 'bat 
ch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': 'It has been trai 
ned on 215M (question, answer) pairs from diverse sources.', 'limitation_and_bias': 'Note that there 
 is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model 
 was just trained on input text up to 250 word pieces. It might not work well for longer text.', 'de 
mo': 'It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant  
documents for the given passages.', 'input_format': 'When loaded with `sentence-transformers`, this  
model produces normalized embeddings with length 1.', 'output_format': 'When loaded with `sentence-t 
ransformers`, this model produces normalized embeddings with length 1.', 'max_sequence_length': '512 
', 'vocabulary_size': ''}, {'datasets': [], 'github': '', 'paper': '', 'upstream_model': '', 'parame 
ter_count': '214,988,242', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 
 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'sentence-tr 
ansformers, semantic search, [SBERT.net - Semantic Search](https://www.sbert.net/examples/applicatio 
ns/semantic-search/README.html)', 'input_format': '', 'output_format': '', 'max_sequence_length': '1 
', 'vocabulary_size': ''}, {'datasets': [], 'github': '', 'paper': '', 'upstream_model': '', 'parame 
ter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer 
': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'When loaded with `sent 
ence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-produ 
ct and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distanc 
e is proportional to dot-product and can also be used.', 'input_format': '', 'output_format': '', 'm 
ax_sequence_length': '', 'vocabulary_size': ''}]                                                     

#####################GanjinZero/UMLSBert_ENG########################

-------------------- datasets --------------------

Document 1:

"Github Link: https://github.com/GanjinZero/CODER"

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

Github Link: https://github.com/GanjinZero/CODER

-------------------- paper --------------------

Document 1:

@article{YUAN2022103983,
title = {CODER: Knowledge-infused cross-lingual medical term embedding for term normalization},
journal = {Journal of Biomedical Informatics},
pages = {103983},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103983},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421003129},
author = {Zheng Yuan and Zhengyun Zhao and Haixia Sun and Jiao Li and Fei Wang and Sheng Yu},
keywords = {medical term normalization, cross-lingual, medical term representation, knowledge graph embedding, contrastive learning}
}

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

Github Link: https://github.com/GanjinZero/CODER

-------------------- input_format --------------------



-------------------- output_format --------------------




[{'datasets': [], 'license': 'apache-2.0', 'github': 'https://github.com/GanjinZero/CODER', 'paper' 
: 'https://www.sciencedirect.com/science/article/pii/S1532046421003129', 'upstream_model': '', 'para 
meter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': ' 
', 'demo': 'https://github.com/GanjinZero/CODER', 'input_format': '', 'output_format': ''}]          

#####################facebook/bart-base########################

-------------------- datasets --------------------

Document 1:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)"

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

"BART model pre-trained on English language" and "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."

-------------------- github --------------------



-------------------- paper --------------------

Document 1:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
------------------------------
Document 2:

`title = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}`

-------------------- upstream_model --------------------

Document 1:

"BART model pre-trained on English language" and "Lewis et al." and "this repository" and "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension" and "pytorch/fairseq/tree/master/examples/bart"

-------------------- parameter_count --------------------

Document 1:

"BART model pre-trained on English language" and "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)"

-------------------- evaluation --------------------

Document 1:

"BART model pre-trained on English language. It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

"BART model pre-trained on English language" and "The team releasing BART did not write a model card for this model"

-------------------- demo --------------------

Document 1:

"model hub" "https://huggingface.co/models?search=bart"
------------------------------
Document 2:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)".

-------------------- input_format --------------------

Document 1:

return_tensors='pt'
------------------------------
Document 2:

"BART model pre-trained on English language" "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)" NO_OUTPUT

-------------------- output_format --------------------

Document 1:

return_tensors='pt'


[{'datasets': ['BART model pre-trained on English language'], 'license': 'apache-2.0', 'github': '' 
, 'paper': 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Trans 
lation, and Comprehension', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'ev 
aluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_fo 
rmat': ''}]                                                                                          

#####################sonoisa/sentence-bert-base-ja-mean-tokens-v2########################

-------------------- datasets --------------------

Document 1:

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'

-------------------- license --------------------

Document 1:

license: cc-by-sa-4.0

-------------------- github --------------------

Document 1:

"sonoisa/sentence-bert-base-ja-mean-tokens-v2"

-------------------- paper --------------------



-------------------- upstream_model --------------------

Document 1:

cl-tohoku/bert-base-japanese-whole-word-masking

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

'sonoisa/sentence-bert-base-ja-mean-tokens-v2'

-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

return torch.stack(all_embeddings)

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length NO_OUTPUT

-------------------- vocabulary_size --------------------




[{'datasets': ['NO_OUTPUT'], 'license': 'cc-by-sa-4.0', 'github': 'sonoisa/sentence-bert-base-ja-me 
an-tokens-v2', 'paper': '', 'upstream_model': 'cl-tohoku/bert-base-japanese-whole-word-masking', 'pa 
rameter_count': 'NO_OUTPUT', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_a 
nd_bias': '', 'demo': "'sonoisa/sentence-bert-base-ja-mean-tokens-v2'", 'input_format': '', 'output_ 
format': 'return torch.stack(all_embeddings)', 'max_sequence_length': 'NO_OUTPUT', 'vocabulary_size' 
: ''}]                                                                                               

#####################facebook/dpr-question_encoder-single-nq-base########################

-------------------- datasets --------------------

Document 1:

Natural Questions (NQ) dataset, Lee et al., 2019, Kwiatkowski et al., 2019, [The dataset] was designed for end-to-end question answering, [associated paper], BERT (Devlin et al., 2019), FAISS (Johnson et al., 2017)

-------------------- license --------------------

Document 1:

license: cc-by-nc-4.0

-------------------- github --------------------



-------------------- paper --------------------

Document 1:

"associated paper", "modeling architecture", "objective", "compute infrastructure", "training details"

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

- [Evaluation](#evaluation-results)
------------------------------
Document 2:

The model developers report the performance of the model on five QA datasets, using the top-k accuracy (k  {20, 100}). The datasets were [NQ](https://huggingface.co/datasets/nq_open), [TriviaQA](https://huggingface.co/datasets/trivia_qa), [WebQuestions (WQ)](https://huggingface.co/datasets/web_questions), [CuratedTREC (TREC)](https://huggingface.co/datasets/trec), and [SQuAD v1.1](https://huggingface.co/datasets/squad). | Top 20 |           |    |      |       | Top 100|           |    |      |       |
|:----:|:------:|:---------:|:--:|:----:|:-----:|:------:|:---------:|:--:|:----:|:-----:|
|      | NQ     |  TriviaQA | WQ | TREC | SQuAD | NQ     |  Tri

-------------------- hardware --------------------

Document 1:

"compute infrastructure"
------------------------------
Document 2:

8 32GB GPUs

-------------------- limitation_and_bias --------------------

Document 1:

"Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al., 2021](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al., 2021](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922))."
------------------------------
Document 2:

Risks, Limitations and Biases

-------------------- demo --------------------

Document 1:

"Use the code below to get started with the model.  ```python from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base') model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base') input_ids = tokenizer('Hello, is my dog cute ?', return_tensors='pt')['input_ids'] embeddings = model(input_ids).pooler_output ```"

-------------------- input_format --------------------

Document 1:

"The model authors write that: [...] The questions were mined from real Google search queries and the answers were spans in Wikipedia articles identified by annotators." "The authors report that for encoders, they used two independent BERT ([Devlin et al., 2019](https://aclanthology.org/N19-1423/)) networks (base, un-cased) and use FAISS ([Johnson et al., 2017](https://arxiv.org/abs/1702.08734)) during inference time to encode and index passages." input_format: BERT (base, un-cased) and FAISS

-------------------- output_format --------------------




[{'datasets': ['Natural Questions (NQ) dataset'], 'license': 'cc-by-nc-4.0', 'github': '', 'paper': 
 '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardwar 
e': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]             

#####################cointegrated/LaBSE-en-ru########################

-------------------- datasets --------------------



-------------------- license --------------------

Document 1:

License: [https://tfhub.dev/google/LaBSE/1]

-------------------- github --------------------

Document 1:

'sentence-transformers/LaBSE', 'LaBSE', 'cointegrated/LaBSE-en-ru', 'EIStakovskii/LaBSE-fr-de'

-------------------- paper --------------------

Document 1:

Language-agnostic BERT Sentence Embedding, https://arxiv.org/abs/2007.01852
------------------------------
Document 2:

"The model has been truncated in [this notebook](https://colab.research.google.com/drive/1dnPRn0-ugj3vZgSpyCC9sgslM2SuSfHy?usp=sharing)."

-------------------- upstream_model --------------------

Document 1:

'sentence-transformers/LaBSE', 'LaBSE by Google', 'EIStakovskii/LaBSE-fr-de'

-------------------- parameter_count --------------------

Document 1:

"number of parameters in the whole model is 27% of the original" 
NO_OUTPUT

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

The current model has only English and Russian tokens left in the vocabulary. To get the sentence embeddings, you can  use the following code: ```python import torch from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru') model = AutoModel.from_pretrained('cointegrated/LaBSE-en-ru') sentences = ['Hello World', ' '] encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=64, return_tensors='pt') with torch.no_grad(): model_output = model(**encoded_input) embeddings = model_output.pooler_output embeddings = torch.nn.functional.normalize(embeddings) print(embeddings) ``` The model has been truncated in [this notebook](https://colab.research.google.com/drive/1dnPRn0-ugj3vZgSpyCC9sgslM2SuSfHy?usp=sharing). You can adapt it

-------------------- demo --------------------

Document 1:

"To get the sentence embeddings, you can  use the following code:
```python
import torch
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')
model = AutoModel.from_pretrained('cointegrated/LaBSE-en-ru')
sentences = ['Hello World', ' ']
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=64, return_tensors='pt')
with torch.no_grad():
model_output = model(**encoded_input)
embeddings = model_output.pooler_output
embeddings = torch.nn.functional.normalize(embeddings)
print(embeddings)
```
The model has been truncated in [this notebook](https://colab.research.google.com/drive/1dnPRn0-ugj3vZgSpyCC9sgslM2SuSfHy?usp=
------------------------------
Document 2:

"https://tfhub.dev/google/LaBSE/1"

-------------------- input_format --------------------

Document 1:

return_tensors='pt' and torch.no_grad()

-------------------- output_format --------------------

Document 1:

"return_tensors='pt'"
"embeddings = model_output.pooler_output"
"embeddings = torch.nn.functional.normalize(embeddings)"
"print(embeddings)"

-------------------- max_sequence_length --------------------

Document 1:

max_length=64

-------------------- vocabulary_size --------------------

Document 1:

The current model has only English and Russian tokens left in the vocabulary. Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.


[{'datasets': ['LaBSE'], 'license': 'https://tfhub.dev/google/LaBSE/1', 'github': "'sentence-transf 
ormers/LaBSE', 'LaBSE', 'cointegrated/LaBSE-en-ru', 'EIStakovskii/LaBSE-fr-de'", 'paper': 'Language- 
agnostic BERT Sentence Embedding, https://arxiv.org/abs/2007.01852', 'upstream_model': "'sentence-tr 
ansformers/LaBSE', 'LaBSE by Google', 'EIStakovskii/LaBSE-fr-de'", 'parameter_count': 'number of par 
ameters in the whole model is 27% of the original', 'hyper_parameters': {}, 'evaluation': [], 'hardw 
are': '', 'limitation_and_bias': "The current model has only English and Russian tokens left in the  
vocabulary. To get the sentence embeddings, you can  use the following code: ```python import torch  
from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('cointeg 
rated/LaBSE-en-ru') model = AutoModel.from_pretrained('cointegrated/LaBSE-en-ru') sentences = ['Hell 
o World', ' '] encoded_input = tokenizer(sentences, padding=True, truncation=True, max_leng 
th=64, return_tensors='pt') with torch.no_grad(): model_output = model(**encoded_input) embeddings = 
 model_output.pooler_output embeddings = torch.nn.functional.normalize(embeddings) print(embeddings) 
 ``` The model has been truncated in [this notebook](https://colab.research.google.com/drive/1dnPRn0 
-ugj3vZgSpyCC9sgslM2SuSfHy?usp=sharing). You can adapt it", 'demo': '"To get the sentence embeddings 
, you can  use the following code:\n```python\nimport torch\nfrom transformers import AutoTokenizer, 
 AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\'cointegrated/LaBSE-en-ru\')\nmodel = AutoMod 
el.from_pretrained(\'cointegrated/LaBSE-en-ru\')\nsentences = [\'Hello World\', \' \']\nenc 
oded_input = tokenizer(sentences, padding=True, truncation=True, max_length=64, return_tensors=\'pt\ 
')\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\nembeddings = model_output.pooler_o 
utput\nembeddings = torch.nn.functional.normalize(embeddings)\nprint(embeddings)\n```\nThe model has 
 been truncated in [this notebook](https://colab.research.google.com/drive/1dnPRn0-ugj3vZgSpyCC9sgsl 
M2SuSfHy?usp=', 'input_format': "return_tensors='pt' and torch.no_grad()", 'output_format': '"return 
_tensors=\'pt\'"\n"embeddings = model_output.pooler_output"\n"embeddings = torch.nn.functional.norma 
lize(embeddings)"\n"print(embeddings)"', 'max_sequence_length': 'max_length=64', 'vocabulary_size':  
'The current model has only English and Russian tokens left in the vocabulary. Thus, the vocabulary  
is 10% of the original, and number of parameters in the whole model is 27% of the original, without  
any loss in the quality of English and Russian embeddings.'}]                                        

#####################Linaqruf/anything-v3.0########################

-------------------- datasets --------------------



-------------------- license --------------------

Document 1:

license: creativeml-openrail-m

-------------------- github --------------------

Document 1:

"stable-diffusion, stable-diffusion-diffusers, text-to-image, diffusers"

-------------------- paper --------------------

Document 1:

"stable-diffusion", "stable-diffusion-diffusers", "text-to-image", "diffusers"

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------



-------------------- input_format --------------------



-------------------- output_format --------------------




[{'datasets': ['creativeml-openrail-m'], 'license': 'creativeml-openrail-m', 'github': 'stable-diff 
usion, stable-diffusion-diffusers, text-to-image, diffusers', 'paper': 'stable-diffusion, stable-dif 
fusion-diffusers, text-to-image, diffusers', 'upstream_model': '', 'parameter_count': '', 'hyper_par 
ameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format 
': '', 'output_format': ''}]                                                                         

#####################DeepFloyd/IF-II-L-v1.0########################


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 401 Client Error. (Request ID: Root=1-65496736-5f4f8cc778a98fab3576a4ce)

Cannot access gated repo for url https://huggingface.co/api/models/DeepFloyd/IF-II-L-v1.0.
Repo model DeepFloyd/IF-II-L-v1.0 is gated. You must be authenticated to access it. 

#####################prompthero/openjourney-v4########################

-------------------- datasets --------------------

Document 1:

"Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours." "Openjourney-v4 prompts" "Join our course"

-------------------- license --------------------

Document 1:

license: creativeml-openrail-m

-------------------- github --------------------

Document 1:

license: creativeml-openrail-m, tags: - stable-diffusion - text-to-image, pinned: true
------------------------------
Document 2:

Openjourney-v4 prompts](https://prompthero.com/openjourney-prompts?version=4) 
[Join our course](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)

-------------------- paper --------------------

Document 1:

Openjourney-v4 prompts, Join our course

-------------------- upstream_model --------------------

Document 1:

Stable Diffusion v1.5, +124000 images, 12400 steps, 4 epochs +32 training hours, Openjourney-v4 prompts

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

+124000 images, 12400 steps, 4 epochs +32 training hours.

-------------------- evaluation --------------------

Document 1:

Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours.

-------------------- hardware --------------------

Document 1:

+124000 images, 12400 steps, 4 epochs +32 training hours.

-------------------- limitation_and_bias --------------------

Document 1:

"Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours." NO_OUTPUT

-------------------- demo --------------------

Document 1:

[Lora version](https://huggingface.co/prompthero/openjourney-lora), [Openjourney Dreambooth](https://huggingface.co/prompthero/openjourney)
------------------------------
Document 2:

'Openjourney-v4 prompts' and '[Join our course](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)'

-------------------- input_format --------------------



-------------------- output_format --------------------




[{'datasets': ['Stable Diffusion v1.5'], 'license': 'creativeml-openrail-m', 'github': 'https://git 
hub.com/prompthero/stable-diffusion', 'paper': 'https://arxiv.org/123456', 'upstream_model': 'Stable 
 Diffusion v1.5', 'parameter_count': '#params', 'hyper_parameters': {'epochs': '4', 'batch_size': '3 
2', 'learning_rate': '0.001', 'optimizer': 'Adam'}, 'evaluation': [{'test': 'Test Task', 'result': ' 
90% accuracy'}], 'hardware': 'GPU', 'limitation_and_bias': 'Limited to image generation tasks', 'dem 
o': 'https://demo.com', 'input_format': 'Image', 'output_format': 'Image'}]                          

#####################iZELX1/Anything-V3-X########################

-------------------- datasets --------------------

Document 1:

"library_name: diffusers"

-------------------- license --------------------

Document 1:

license: creativeml-openrail-m

-------------------- github --------------------



-------------------- paper --------------------

Document 1:

- diffusers
- stable diffusion diffusers

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------

Document 1:

Anything V3, stable diffusion, diffusers, stable diffusion diffusers

-------------------- limitation_and_bias --------------------



-------------------- demo --------------------



-------------------- input_format --------------------



-------------------- output_format --------------------




[{'datasets': ['diffusers'], 'license': 'creativeml-openrail-m', 'github': '', 'paper': '- diffuser 
s\n- stable diffusion diffusers', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': { 
}, 'evaluation': [], 'hardware': 'Anything V3, stable diffusion, diffusers, stable diffusion diffuse 
rs', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]                

#####################microsoft/trocr-large-printed########################

-------------------- datasets --------------------

Document 1:

[SROIE dataset](https://rrc.cvc.uab.es/?ch=13), [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282), [this repository](https://github.com/microsoft/unilm/tree/master/trocr)
------------------------------
Document 2:

"The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."

-------------------- license --------------------

Document 1:

"It was introduced in the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Li et al. and first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr)."

-------------------- github --------------------



-------------------- paper --------------------

Document 1:

"title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}"
------------------------------
Document 2:

[TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282)

-------------------- upstream_model --------------------

Document 1:

"image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."
------------------------------
Document 2:

"TrOCR model fine-tuned on the [SROIE dataset](https://rrc.cvc.uab.es/?ch=13)" and "first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr)"

-------------------- parameter_count --------------------

Document 1:

"an image Transformer as encoder, and a text Transformer as decoder" "Images are presented to the model as a sequence of fixed-size patches (resolution 16x16)" "One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder" "Next, the Transformer text decoder autoregressively generates tokens."

-------------------- hyper_parameters --------------------

Document 1:

"The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens."

-------------------- evaluation --------------------

Document 1:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.
------------------------------
Document 2:

TrOCR model fine-tuned on the [SROIE dataset](https://rrc.cvc.uab.es/?ch=13). It was introduced in the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Li et al. and first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr).

-------------------- hardware --------------------

Document 1:

"image Transformer as encoder, and a text Transformer as decoder"
------------------------------
Document 2:

"TrOCR model fine-tuned on the [SROIE dataset](https://rrc.cvc.uab.es/?ch=13)" and "first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr)"

-------------------- limitation_and_bias --------------------

Document 1:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.
------------------------------
Document 2:

"TrOCR model fine-tuned on the [SROIE dataset](https://rrc.cvc.uab.es/?ch=13). It was introduced in the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Li et al. and first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr)."

-------------------- demo --------------------

Document 1:

"model hub" "microsoft/trocr"
------------------------------
Document 2:

- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X00016469612_1.jpg
example_title: Printed 1
- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005255805_7.jpg
example_title: Printed 2
- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005745214_6.jpg
example_title: Printed 3

-------------------- input_format --------------------

Document 1:

"Images are presented to the model as a sequence of fixed-size patches (resolution 16x16)"

-------------------- output_format --------------------

Document 1:

"Images are presented to the model as a sequence of fixed-size patches (resolution 16x16)"


[{'datasets': ['SROIE dataset'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'p 
arameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'opti 
mizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format' 
: '', 'output_format': ''}]                                                                          

#####################timm/mobilenetv3_large_100.miil_in21k_ft_in1k########################

-------------------- datasets --------------------

Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

"ImageNet-21k-P" and "ImageNet-1k"
------------------------------
Document 3:

datasets: - imagenet-1k - imagenet-21k-p

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

license

-------------------- github --------------------

Document 1:

"GitHub repository" and "\url{https://github.com/huggingface/pytorch-image-models}"
------------------------------
Document 2:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"

-------------------- paper --------------------

Document 1:

"Searching for mobilenetv3" by Andrew Howard et al., Proceedings of the IEEE/CVF international conference on computer vision, 2019
------------------------------
Document 2:

"model results"
------------------------------
Document 3:

"ImageNet-21k-P" and "ImageNet-1k"

-------------------- upstream_model --------------------

Document 1:

upstream_model MobileNet-v3
------------------------------
Document 2:

upstream_model: MobileNetV3

-------------------- parameter_count --------------------

Document 1:

parameter_count: 5.5
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

"hyper parameters"
------------------------------
Document 2:

"MobileNet-v3 image classification model" and "ImageNet-21k-P and ImageNet-1k"

-------------------- evaluation --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).
------------------------------
Document 2:

"A MobileNet-v3 image classification model. Petrained on ImageNet-21k-P and fine-tuned on ImageNet-1k by Alibaba MIIL."

-------------------- hardware --------------------

Document 1:

Petrained on ImageNet-21k-P and fine-tuned on ImageNet-1k
------------------------------
Document 2:

"Image size: 224 x 224"

-------------------- limitation_and_bias --------------------

Document 1:

MobileNet-v3 image classification model, Petrained on ImageNet-21k-P and fine-tuned on ImageNet-1k by Alibaba MIIL.

-------------------- demo --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"MobileNet-v3 image classification model", "Petrained on ImageNet-21k-P and fine-tuned on ImageNet-1k by Alibaba MIIL"

-------------------- input_format --------------------

Document 1:

input_format: ImageNet-21k-P and ImageNet-1k

-------------------- output_format --------------------



-------------------- input_preprocessing --------------------

Document 1:

"Image size: 224 x 224"
------------------------------
Document 2:

input_preprocessing

-------------------- input_size --------------------

Document 1:

"Image size: 224 x 224"


[{'datasets': ['ImageNet-21k-P', 'ImageNet-1k'], 'license': 'apache-2.0', 'github': 'https://github 
.com/huggingface/pytorch-image-models', 'paper': 'Searching for mobilenetv3 by Andrew Howard et al., 
 Proceedings of the IEEE/CVF international conference on computer vision, 2019', 'upstream_model': ' 
MobileNet-v3', 'parameter_count': '5.5', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'Petr 
ained on ImageNet-21k-P and fine-tuned on ImageNet-1k', 'limitation_and_bias': 'MobileNet-v3 image c 
lassification model, Petrained on ImageNet-21k-P and fine-tuned on ImageNet-1k by Alibaba MIIL.', 'd 
emo': '[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)', 'inp 
ut_format': 'ImageNet-21k-P and ImageNet-1k', 'output_format': '', 'input_preprocessing': 'Image siz 
e: 224 x 224', 'input_size': 'Image size: 224 x 224'}]                                               

#####################timm/convmixer_768_32.in1k########################

-------------------- datasets --------------------

Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

"ImageNet-1k"
------------------------------
Document 3:

datasets: - imagenet-1k

-------------------- license --------------------

Document 1:

license: mit

-------------------- github --------------------

Document 1:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"

-------------------- paper --------------------

Document 1:

"paper authors"
------------------------------
Document 2:

"model results"
------------------------------
Document 3:

`title={CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification}, author={Chun-Fu Chen and Quanfu Fan and Rameswar Panda}, journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, year={2021}, pages={347-356}`

-------------------- upstream_model --------------------

Document 1:

upstream_model
------------------------------
Document 2:

upstream_model NO_OUTPUT

-------------------- parameter_count --------------------

Document 1:

parameter_count 21.1
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

"hyper parameters"
------------------------------
Document 2:

"Model Type: Image classification / feature backbone" "Model Stats: Params (M): 21.1 GMACs: 19.5 Activations (M): 26.0 Image size: 224 x 224" "Papers: Patches Are All You Need?: https://arxiv.org/abs/2201.09792" "Dataset: ImageNet-1k" "Original: https://github.com/locuslab/convmixer"

-------------------- evaluation --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).

-------------------- hardware --------------------

Document 1:

"ConvMixer image classification model"
------------------------------
Document 2:

"Model Stats: Params (M): 21.1 GMACs: 19.5 Activations (M): 26.0 Image size: 224 x 224"

-------------------- limitation_and_bias --------------------

Document 1:

limitation_and_bias
------------------------------
Document 2:

"Model Type: Image classification / feature backbone", "Model Stats: Params (M): 21.1, GMACs: 19.5, Activations (M): 26.0, Image size: 224 x 224", "Papers: Patches Are All You Need?: https://arxiv.org/abs/2201.09792", "Dataset: ImageNet-1k", "Original: https://github.com/locuslab/convmixer"

-------------------- demo --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"ConvMixer image classification model" "Trained on ImageNet-1k by paper authors"

-------------------- input_format --------------------

Document 1:

"Image size: 224 x 224" "Dataset: ImageNet-1k"

-------------------- output_format --------------------



-------------------- input_preprocessing --------------------

Document 1:

input_preprocessing
------------------------------
Document 2:

"Image size: 224 x 224" NO_OUTPUT

-------------------- input_size --------------------

Document 1:

input_size: 224 x 224


[{'datasets': ['ImageNet-1k'], 'license': 'mit', 'github': 'https://github.com/huggingface/pytorch- 
image-models/tree/main/results', 'paper': 'title={CrossViT: Cross-Attention Multi-Scale Vision Trans 
former for Image Classification}, author={Chun-Fu Chen and Quanfu Fan and Rameswar Panda}, journal={ 
2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, year={2021}, pages={347-356}', 'u 
pstream_model': '', 'parameter_count': '21.1', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 
 'ConvMixer image classification model', 'limitation_and_bias': '', 'demo': '[model results](https:/ 
/github.com/huggingface/pytorch-image-models/tree/main/results)', 'input_format': 'Image size: 224 x 
 224\nDataset: ImageNet-1k', 'output_format': '', 'input_preprocessing': '', 'input_size': '224 x 22 
4'}]                                                                                                 

#####################timm/efficientnet_b0.ra_in1k########################

-------------------- datasets --------------------

Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

datasets: - imagenet-1k

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

"GitHub repository"
"\url{https://github.com/huggingface/pytorch-image-models}"
------------------------------
Document 2:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"

-------------------- paper --------------------

Document 1:

```bibtex
@inproceedings{tan2019efficientnet,
title={Efficientnet: Rethinking model scaling for convolutional neural networks},
author={Tan, Mingxing and Le, Quoc},
booktitle={International conference on machine learning},
pages={6105--6114},
year={2019},
organization={PMLR}
}
```
------------------------------
Document 2:

"model results"
------------------------------
Document 3:

"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946"

-------------------- upstream_model --------------------

Document 1:

"Model Type: Image classification / feature backbone" "Papers: EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946"
------------------------------
Document 2:

"Efficientnet: Rethinking model scaling for convolutional neural networks" and "ResNet strikes back: An improved training procedure in timm"

-------------------- parameter_count --------------------

Document 1:

"Params (M): 5.3"

-------------------- hyper_parameters --------------------

Document 1:

"hyper parameters"
------------------------------
Document 2:

"RandAugment `RA` recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as `B` recipe in [ResNet Strikes Back](https://arxiv.org/abs/2110.00476). RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging Step (exponential decay w/ staircase) LR schedule with warmup"

-------------------- evaluation --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).
------------------------------
Document 2:

- **Model Type:** Image classification / feature backbone
- **Model Stats:**
- Params (M): 5.3
- GMACs: 0.4
- Activations (M): 6.7
- Image size: 224 x 224
- **Dataset:** ImageNet-1k

-------------------- hardware --------------------

Document 1:

"EfficientNet image classification model" "RMSProp (TF 1.0 behaviour) optimizer" "EMA weight averaging" "Step (exponential decay w/ staircase) LR schedule with warmup"

-------------------- limitation_and_bias --------------------

Document 1:

"Model Type: Image classification / feature backbone" "Model Stats: Params (M): 5.3 GMACs: 0.4 Activations (M): 6.7 Image size: 224 x 224" "Papers: EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946 ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476" "Dataset: ImageNet-1k"

-------------------- demo --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"PyTorch Image Models" and "\url{https://github.com/huggingface/pytorch-image-models}"

-------------------- input_format --------------------

Document 1:

"Image size: 224 x 224" "Dataset: ImageNet-1k" "Original: https://github.com/huggingface/pytorch-image-models"

-------------------- output_format --------------------



-------------------- input_preprocessing --------------------

Document 1:

"Image size: 224 x 224" "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946" "ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476"

-------------------- input_size --------------------

Document 1:

Image size: 224 x 224


[{'datasets': ['imagenet-1k'], 'license': 'apache-2.0', 'github': 'https://github.com/huggingface/p 
ytorch-image-models', 'paper': 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Netw 
orks: https://arxiv.org/abs/1905.11946', 'upstream_model': 'EfficientNet: Rethinking Model Scaling f 
or Convolutional Neural Networks: https://arxiv.org/abs/1905.11946', 'parameter_count': '5.3', 'hype 
r_parameters': {}, 'evaluation': [], 'hardware': 'EfficientNet image classification model', 'limitat 
ion_and_bias': 'Model Type: Image classification / feature backbone\nModel Stats: Params (M): 5.3 GM 
ACs: 0.4 Activations (M): 6.7 Image size: 224 x 224\nPapers: EfficientNet: Rethinking Model Scaling  
for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946 ResNet strikes back: An improved 
 training procedure in timm: https://arxiv.org/abs/2110.00476\nDataset: ImageNet-1k', 'demo': '[mode 
l results](https://github.com/huggingface/pytorch-image-models/tree/main/results)', 'input_format':  
'Image size: 224 x 224\nDataset: ImageNet-1k\nOriginal: https://github.com/huggingface/pytorch-image 
-models', 'output_format': '', 'input_preprocessing': 'Image size: 224 x 224\nEfficientNet: Rethinki 
ng Model Scaling for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946\nResNet strikes 
 back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476', 'input_size': 'Ima 
ge size: 224 x 224'}]                                                                                

#####################timm/convnext_base.fb_in22k_ft_in1k########################


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 5277 tokens (5021 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################timm/resnet50.a1_in1k########################


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 18743 tokens (18487 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################TahaDouaji/detr-doc-table-detection########################

-------------------- datasets --------------------

Document 1:

ICDAR2019 Table Dataset

-------------------- license --------------------



-------------------- github --------------------



-------------------- paper --------------------

Document 1:

ICDAR2019 Table Dataset

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------

Document 1:

parameter_count

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

"risks, biases and limitations of the model"
------------------------------
Document 2:

"Significant research has explored bias and fairness issues with language models" and "[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf)" and "[Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)".

-------------------- demo --------------------



-------------------- input_format --------------------

Document 1:

ICDAR2019 Table Dataset

-------------------- output_format --------------------



-------------------- input_preprocessing --------------------

Document 1:

"ICDAR2019 Table Dataset"

-------------------- input_size --------------------




{'datasets': ['ICDAR2019 Table Dataset'], 'license': '', 'github': '', 'paper': '', 'upstream_model 
': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate':  
'', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': 'risks, biases and li 
mitations of the model', 'demo': '', 'input_format': 'ICDAR2019 Table Dataset', 'output_format': '', 
 'input_preprocessing': 'ICDAR2019 Table Dataset', 'input_size': ''}                                 

#####################CIDAS/clipseg-rd64-refined########################

-------------------- datasets --------------------



-------------------- license --------------------

Document 1:

"license"
------------------------------
Document 2:

license: apache-2.0

-------------------- github --------------------

Document 1:

"github", "Image Segmentation Using Text and Image Prompts", "this repository", "https://github.com/timojl/clipseg"
------------------------------
Document 2:

license: apache-2.0, tags: - vision - image-segmentation, inference: false

-------------------- paper --------------------

Document 1:

"documentation"
------------------------------
Document 2:

"Image Segmentation Using Text and Image Prompts" and "this repository"

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------

Document 1:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

"hyper parameters"

-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

"Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg)."
------------------------------
Document 2:

"It was introduced in the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Lddecke et al. and first released in [this repository](https://github.com/timojl/clipseg)."

-------------------- input_format --------------------



-------------------- output_format --------------------



-------------------- input_preprocessing --------------------

Document 1:

"Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg)."
------------------------------
Document 2:

"Image Segmentation Using Text and Image Prompts" by Lddecke et al. and first released in [this repository](https://github.com/timojl/clipseg).

-------------------- input_size --------------------




[{'datasets': [], 'license': 'apache-2.0', 'github': 'https://github.com/timojl/clipseg', 'paper':  
'https://arxiv.org/abs/2112.10003', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': 
 {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'o 
utput_format': '', 'input_preprocessing': '', 'input_size': ''}]                                     

#####################DionTimmer/controlnet_qrcode-control_v1p_sd15########################

-------------------- datasets --------------------

Document 1:

"The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, this 1.5 version model was also trained on the same dataset"

-------------------- license --------------------

Document 1:

license: openrail++

-------------------- github --------------------

Document 1:

"This repo holds the safetensors & diffusers versions of the QR code conditioned ControlNet for Stable Diffusion v1.5." "The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs." "However, this 1.5 version model was also trained on the same dataset for those who are using the older version."
------------------------------
Document 2:

"github"
------------------------------
Document 3:

"The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application. For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet)."

-------------------- paper --------------------

Document 1:

**To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).** For the best results, it is recommended to generate your artwork at a resolution of 768.
------------------------------
Document 2:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results.

-------------------- upstream_model --------------------

Document 1:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail. Make sure to look up additional info on how to use controlnet if you get stuck, once you have the webui up and running its really easy to install the controlnet extension aswell.

NO_OUTPUT
------------------------------
Document 2:

**To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).** NO_OUTPUT

-------------------- parameter_count --------------------

Document 1:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. parameter_count: 768

-------------------- hyper_parameters --------------------

Document 1:

"ControlNet weight" and "correction mode 'H' (30%)"
------------------------------
Document 2:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
------------------------------
Document 3:

guidance_scale=20,
controlnet_conditioning_scale=1.5,
strength=0.9,

-------------------- evaluation --------------------

Document 1:

**To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).** For the best results, it is recommended to generate your artwork at a resolution of 768. 
NO_OUTPUT
------------------------------
Document 2:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
------------------------------
Document 3:

![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)

-------------------- hardware --------------------

Document 1:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.

-------------------- limitation_and_bias --------------------

Document 1:

"These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output."

-------------------- demo --------------------

Document 1:

**To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).** For the best results, it is recommended to generate your artwork at a resolution of 768.
------------------------------
Document 2:

"The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application. For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet). Make sure to enable your controlnet unit and set your input image as the QR code. Set the model to either the SD2.1 or 1.5 version depending on your base stable diffusion model, or it will error. No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail."

-------------------- input_format --------------------

Document 1:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail. input_format: No pre-processor needed, resolution: 768
------------------------------
Document 2:

**To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).** For the best results, it is recommended to generate your artwork at a resolution of 768. 
input_format: Correction mode 'H' (30%) at a resolution of 768.
------------------------------
Document 3:

input_format: CSV

-------------------- output_format --------------------

Document 1:

**To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).** For the best results, it is recommended to generate your artwork at a resolution of 768.
------------------------------
Document 2:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.

-------------------- input_preprocessing --------------------

Document 1:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results.
------------------------------
Document 2:

"resize_for_condition_image(input_image: Image, resolution: int)"

-------------------- input_size --------------------

Document 1:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.


[{'datasets': ['Stable Diffusion 2.1'], 'license': 'openrail++', 'github': 'https://github.com/Miku 
bill/sd-webui-controlnet', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_paramete 
rs': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '' 
, 'output_format': '', 'input_preprocessing': '', 'input_size': ''}]                                 

#####################MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli########################

-------------------- datasets --------------------

Document 1:

"NLI datasets"
------------------------------
Document 2:

"Less Annotating, More Classifying  Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI" https://osf.io/74b8k
------------------------------
Document 3:

MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI, WANLI

-------------------- license --------------------

Document 1:

license: mit
------------------------------
Document 2:

"Open Science Framework"

-------------------- github --------------------

Document 1:

- alisawuffles/WANLI
- WANLI
- alisawuffles/WANLI
- split: test
- type: accuracy
- value: 0,77

-------------------- paper --------------------

Document 1:

"original DeBERTa-v3 paper"
------------------------------
Document 2:

"Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. Less Annotating, More Classifying  Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI. Preprint, June. Open Science Framework. https://osf.io/74b8k."
------------------------------
Document 3:

[MultiNLI](https://huggingface.co/datasets/multi_nli), [Fever-NLI](https://github.com/easonnie/combine-FEVER-NSMN/blob/master/other_resources/nli_fever.md), Adversarial-NLI ([ANLI](https://huggingface.co/datasets/anli)), [LingNLI](https://arxiv.org/pdf/2104.07179.pdf) and [WANLI](https://huggingface.co/datasets/alisawuffles/WANLI), [DeBERTa-v3-large from Microsoft](https://huggingface.co/microsoft/deberta-v3-large), [paper](https://arxiv.org/abs/2111.09543)

-------------------- upstream_model --------------------

Document 1:

"The foundation model is [DeBERTa-v3-large from Microsoft](https://huggingface.co/microsoft/deberta-v3-large)."

-------------------- parameter_count --------------------

Document 1:

"num_train_epochs=4, per_device_train_batch_size=16, gradient_accumulation_steps=2, per_device_eval_batch_size=64, warmup_ratio=0.06, weight_decay=0.01, fp16=True"

-------------------- hyper_parameters --------------------

Document 1:

training_args = TrainingArguments(num_train_epochs=4, learning_rate=5e-06, per_device_train_batch_size=16, gradient_accumulation_steps=2, per_device_eval_batch_size=64, warmup_ratio=0.06, weight_decay=0.01, fp16=True)
------------------------------
Document 2:

- type: accuracy
value: 0,912
- type: accuracy
value: 0,908
- type: accuracy
value: 0,702
- type: accuracy
value: 0,64
- type: accuracy
value: 0,77
- type: accuracy
value: 0,87

-------------------- evaluation --------------------

Document 1:

The model was evaluated using the test sets for MultiNLI, ANLI, LingNLI, WANLI and the dev set for Fever-NLI. The metric used is accuracy. The model achieves state-of-the-art performance on each dataset. Surprisingly, it outperforms the previous [state-of-the-art on ANLI](https://github.com/facebookresearch/anli) (ALBERT-XXL) by 8,3%. |Datasets|mnli_test_m|mnli_test_mm|anli_test|anli_test_r3|ling_test|wanli_test| | :---: | :---: | :---: | :---: | :---: | :---: | :---: | |Accuracy|0.912|0.908|0.702|0.64|0.87|0.77| |Speed (text/sec, A100 GPU)|696.0|697.0|488.0|425.0|828.0|980.0|

-------------------- hardware --------------------

Document 1:

"num_train_epochs=4, learning_rate=5e-06, per_device_train_batch_size=16, gradient_accumulation_steps=2, per_device_eval_batch_size=64, warmup_ratio=0.06, weight_decay=0.01, fp16=True"

-------------------- limitation_and_bias --------------------

Document 1:

"original DeBERTa-v3 paper", "different NLI datasets", "training data", "potential biases", "model will reproduce statistical patterns in the training data"
------------------------------
Document 2:

"less annotating, more classifying - addressing the data scarcity issue of supervised machine learning with deep transfer learning and BERT - NLI"
------------------------------
Document 3:

"This model was fine-tuned on the [MultiNLI](https://huggingface.co/datasets/multi_nli), [Fever-NLI](https://github.com/easonnie/combine-FEVER-NSMN/blob/master/other_resources/nli_fever.md), Adversarial-NLI ([ANLI](https://huggingface.co/datasets/anli)), [LingNLI](https://arxiv.org/pdf/2104.07179.pdf) and [WANLI](https://huggingface.co/datasets/alisawuffles/WANLI) datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the [ANLI benchmark](https://github.com/facebookresearch/anli). The foundation model is [DeBERTa-v3-large from Microsoft](https://huggingface.co/microsoft/de

-------------------- demo --------------------

Document 1:

"DeBERTa-v3 was released on 06.12.21" and "Using Transformers>=4.13 might solve some issues."

-------------------- input_format --------------------

Document 1:

- multi_nli
- anli
- fever
- lingnli
- alisawuffles/WANLI

-------------------- output_format --------------------



-------------------- max_sequence_length --------------------

Document 1:

"DeBERTa-v3-large from Microsoft" and "It significantly outperforms all other large models on the [ANLI benchmark](https://github.com/facebookresearch/anli)."
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

"885 242 NLI hypothesis-premise pairs"
------------------------------
Document 2:

"DeBERTa-v3-large from Microsoft" and "see the [paper](https://arxiv.org/abs/2111.09543)"


[{'datasets': ['MultiNLI', 'Fever-NLI', 'Adversarial-NLI (ANLI)', 'LingNLI', 'WANLI'], 'license': ' 
mit', 'github': '- alisawuffles/WANLI\n- WANLI\n- alisawuffles/WANLI\n- split: test\n- type: accurac 
y\n- value: 0,77', 'paper': '[MultiNLI](https://huggingface.co/datasets/multi_nli), [Fever-NLI](http 
s://github.com/easonnie/combine-FEVER-NSMN/blob/master/other_resources/nli_fever.md), Adversarial-NL 
I ([ANLI](https://huggingface.co/datasets/anli)), [LingNLI](https://arxiv.org/pdf/2104.07179.pdf) an 
d [WANLI](https://huggingface.co/datasets/alisawuffles/WANLI), [DeBERTa-v3-large from Microsoft](htt 
ps://huggingface.co/microsoft/deberta-v3-large), [paper](https://arxiv.org/abs/2111.09543)', 'upstre 
am_model': 'The foundation model is [DeBERTa-v3-large from Microsoft](https://huggingface.co/microso 
ft/deberta-v3-large).', 'parameter_count': 'num_train_epochs=4, per_device_train_batch_size=16, grad 
ient_accumulation_steps=2, per_device_eval_batch_size=64, warmup_ratio=0.06, weight_decay=0.01, fp16 
=True', 'hyper_parameters': {'epochs': '4', 'batch_size': '16', 'learning_rate': '5e-06', 'optimizer 
': '', 'fp16': 'True'}, 'evaluation': [{'test': 'mnli_test_m', 'result': '0.912'}, {'test': 'mnli_te 
st_mm', 'result': '0.908'}, {'test': 'anli_test', 'result': '0.702'}, {'test': 'anli_test_r3', 'resu 
lt': '0.64'}, {'test': 'ling_test', 'result': '0.87'}, {'test': 'wanli_test', 'result': '0.77'}], 'h 
ardware': 'num_train_epochs=4, learning_rate=5e-06, per_device_train_batch_size=16, gradient_accumul 
ation_steps=2, per_device_eval_batch_size=64, warmup_ratio=0.06, weight_decay=0.01, fp16=True', 'lim 
itation_and_bias': '"original DeBERTa-v3 paper", "different NLI datasets", "training data", "potenti 
al biases", "model will reproduce statistical patterns in the training data"', 'demo': '"DeBERTa-v3  
was released on 06.12.21" and "Using Transformers>=4.13 might solve some issues."', 'input_format':  
'- multi_nli\n- anli\n- fever\n- lingnli\n- alisawuffles/WANLI', 'output_format': '', 'max_sequence_ 
length': '"DeBERTa-v3-large from Microsoft" and "It significantly outperforms all other large models 
 on the [ANLI benchmark](https://github.com/facebookresearch/anli)."', 'vocabulary_size': '"885 242  
NLI hypothesis-premise pairs"'}]                                                                     

#####################openai/whisper-base########################

-------------------- datasets --------------------

Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages."
------------------------------
Document 2:

name: LibriSpeech (clean)
type: librispeech_asr
config: clean
split: test
args:
language: en

name: LibriSpeech (other)
type: librispeech_asr
config: other
split: test
args:
language: en

name: Common Voice 11.0
type: mozilla-foundation/common_voice_11_0
config: hi
split: test
args:
language: hi

-------------------- license --------------------

Document 1:

copyright = {arXiv.org perpetual, non-exclusive license}
------------------------------
Document 2:

license: apache-2.0

-------------------- github --------------------

Document 1:

- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
------------------------------
Document 2:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"

-------------------- paper --------------------

Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
------------------------------
Document 2:

[the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf) and [the paper](https://cdn.openai.com/papers/whisper.pdf).
------------------------------
Document 3:

doi = {10.48550/ARXIV.2212.04356}, url = {https://arxiv.org/abs/2212.04356}, title = {Robust Speech Recognition via Large-Scale Weak Supervision}

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------

Document 1:

parameter_count: 0
------------------------------
Document 2:

39 M, 74 M, 244 M, 769 M, 1550 M

-------------------- hyper_parameters --------------------

Document 1:

"fine-tuning" "Fine-Tune Whisper with  Transformers" "step-by-step guide to fine-tuning the Whisper model"

-------------------- evaluation --------------------

Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf)."
------------------------------
Document 3:

"We recognize that once models are released, it is impossible to restrict access to only intended uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them."

-------------------- hardware --------------------

Document 1:

"This non-English data represents 98 different languages." NO_OUTPUT

-------------------- limitation_and_bias --------------------

Document 1:

"the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. However, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself. Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly."
------------------------------
Document 2:

"The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model." "We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them." "We caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification." "We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes."
------------------------------
Document 3:

The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.

-------------------- demo --------------------

Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"The blog post [Fine-Tune Whisper with  Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data."

-------------------- input_format --------------------

Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
------------------------------
Document 2:

src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac

-------------------- output_format --------------------

Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT

-------------------- sample_rate --------------------

Document 1:

`chunk_length_s=30` and `return_timestamps=True`


[{'datasets': ['LibriSpeech (clean)', 'LibriSpeech (other)', 'Common Voice 11.0'], 'license': 'apac 
he-2.0', 'github': 'https://github.com/huggingface/transformers', 'paper': 'https://arxiv.org/abs/22 
12.04356', 'upstream_model': '', 'parameter_count': '39 M, 74 M, 244 M, 769 M, 1550 M', 'hyper_param 
eters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'h 
ardware': '', 'limitation_and_bias': '', 'demo': 'The blog post [Fine-Tune Whisper with  Transforme 
rs](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the  
Whisper model with as little as 5 hours of labelled data.', 'input_format': '', 'output_format': ''} 
]                                                                                                    
total elapsed time: 0 hours 40 minutes 27 seconds
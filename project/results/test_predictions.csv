true_labels,pred_labels,pred_texts
"(4, 28)","(4, 28)",model task of helsinki - nlp / opus - mt - kwn - en? - - - tags : - translation
"(109, 111)","(109, 111)",gpt2
"(7, 8)","(7, 34)",adapterhub / roberta - base - pf - boolq? - - - tags : - text - classification - roberta
"(60, 60)","(60, 65)",roberta - base - squad2
"(139, 144)","(139, 144)",wav2vec2
"(7, 35)","(7, 35)",adapterhub / bert - base - uncased - pf - cola? - - - tags : - text - classification - bert
"(41, 43)","(450, 43)",
"(285, 285)","(79, 80)",mbert
"(4, 27)","(4, 27)",model task of helsinki - nlp / opus - mt - cs - de? - - - tags : - translation
"(28, 29)","(4, 29)",model task of emileajar / dialogpt - small - harrypotter? - - - tags : - conversational
"(46, 49)","(216, 49)",
"(17, 51)","(447, 51)",
"(47, 47)","(47, 47)",bert
"(240, 240)","(240, 240)",bart
"(133, 133)","(133, 500)","roberta - base - bne - finetuned - ciberbullying - spanish this model is a fine - tuned version of [ bsc - temu / roberta - base - bne ] ( on the dataset generated scrapping all social networks ( twitter, youtube... ) to detect ciberbullying on spanish. it achieves the following results on the evaluation set : - loss : 0. 1657 - accuracy : 0. 9607 # # training and evaluation data i use the concatenation from multiple datasets generated scrapping social networks ( twitter, youtube, discord... ) to fine - tune this model. the total number of sentence pairs is above 360k sentences. # # training procedure < details > # # # training hyperparameters the following hyperparameters were used during training : - learning _ rate : 2e - 05 - train _ batch _ size : 16 - eval _ batch _ size : 16 - seed : 42 - optimizer : adam with betas = ( 0. 9, 0. 999 ) and epsilon = 1e - 08 - lr _ scheduler _ type : linear - num _ epochs : 4 # # # training results epoch accuracy : - - - - - : : - - - - - - - - : 1. 0 0. 9501 2. 0 0. 9567 3. 0 0. 9594 4. 0 0. 9607 < / details > # # # model in action fast usage with * * pipelines * * : ` ` ` python from transformers import pipeline model _ path = "" jonatangk / roberta - base - bne - finetuned - ciberbullying - spanish "" bullying _ analysis = pipeline ( "" text - classification"
"(19, 22)","(26, 51)",##d? - - - language : eu license : cc - by - nc - 4. 0 tags : - basque - roberta
"(5, 13)","(5, 30)",task of jhgan / ko - sbert - multitask? - - - pipeline _ tag : sentence - similarity
"(8, 14)","(4, 30)",model task of alaggung / bart - r3f? - - - language : - ko tags : - summarization
"(89, 91)","(89, 91)",text - classification
"(103, 105)","(103, 105)",token - classification
"(15, 40)","(34, 36)",summarization
"(58, 61)","(58, 61)",xlm - roberta
"(4, 27)","(4, 27)",model task of helsinki - nlp / opus - mt - is - fr? - - - tags : - translation
"(11, 38)","(4, 38)",model task of osanseviero / full - sentence - distillroberta2? - - - tags : - sentence - transformers - sentence - similarity
"(4, 31)","(4, 31)",model task of accurateisaiah / dialogpt - small - mozarkv2? - - - tags : - conversational
"(49, 51)","(49, 51)",distilbert
"(55, 59)","(55, 59)",automatic - speech - recognition
"(4, 7)","(4, 32)",model task of adapterhub / roberta - base - pf - boolq? - - - tags : - text - classification
"(47, 47)","(47, 47)",bert
"(202, 202)","(202, 202)",bert
"(4, 32)","(4, 32)",model task of duelinx0402 / dialogpt - small - harrypotter? - - - tags : - conversational
"(60, 61)","(60, 61)",t5
"(503, 504)","(54, 55)",t5
"(505, 510)","(114, 119)",wav2vec2
"(30, 31)","(4, 31)",model task of siyris / dialogpt - medium - siy? - - - thumbnail : tags : - conversational
"(85, 87)","(85, 87)",text - classification
"(447, 452)","(447, 452)",wav2vec2
"(27, 29)","(27, 66)",- 16 - finetuned - squad - seed - 42? - - - tags : - generated _ from _ trainer datasets : - squad model - index : - name : spanbert
"(16, 53)","(49, 53)",automatic - speech - recognition
"(49, 51)","(49, 51)",distilbert
"(62, 64)","(62, 64)",distilbert
"(91, 93)","(91, 93)",text - classification
"(5, 38)","(5, 38)",task of flax - sentence - embeddings / mpnet _ stackexchange _ v1? - - - pipeline _ tag : sentence - similarity
"(66, 68)","(66, 68)",distilbert
"(39, 39)","(39, 39)",bert
"(255, 256)","(255, 256)",t5
"(57, 58)","(57, 58)",mpnet
"(507, 509)","(507, 38)",
"(5, 11)","(5, 39)",task of dataikunlp / paraphrase - multilingual - minilm - l12 - v2? - - - pipeline _ tag : sentence - similarity
"(64, 67)","(64, 67)",layoutlmv2
"(4, 27)","(4, 27)",model task of helsinki - nlp / opus - mt - cs - en? - - - tags : - translation
"(113, 113)","(42, 42)",oscar
"(477, 478)","(57, 48)",
"(4, 28)","(4, 28)",model task of helsinki - nlp / opus - mt - eo - es? - - - tags : - translation
"(26, 57)","(57, 57)",bert
"(35, 39)","(35, 39)",automatic - speech - recognition
"(409, 411)","(409, 411)",text - generation
"(85, 87)","(85, 87)",fill - mask
"(4, 23)","(4, 23)",model task of icemiser / chat - test? - - - tags : - conversational
"(61, 64)","(61, 64)",xlm - roberta
"(4, 28)","(4, 28)",model task of helsinki - nlp / opus - mt - guw - en? - - - tags : - translation
"(55, 60)","(55, 60)",wav2vec2
"(193, 193)","(193, 193)",bert
"(62, 63)","(62, 63)",xlnet
"(4, 30)","(4, 30)",model task of ridwanpratama / dialogpt - small - misaki? - - - tags : - conversational
"(110, 115)","(110, 115)",wav2vec2
"(38, 38)","(507, 38)",
"(498, 500)","(498, 500)",text - classification
"(14, 35)","(35, 35)",roberta
"(64, 64)","(64, 64)",roberta
"(5, 43)","(5, 43)",task of flax - sentence - embeddings / multi - qa _ v1 - mpnet - cls _ dot? - - - pipeline _ tag : sentence - similarity
"(429, 434)","(429, 434)",wav2vec2
"(50, 50)","(139, 144)",wav2vec2
"(57, 59)","(57, 59)",distilbert
"(4, 28)","(4, 28)",model task of helsinki - nlp / opus - mt - en - lua? - - - tags : - translation
"(59, 59)","(59, 59)",audio
"(132, 133)","(132, 133)",t5
"(216, 218)","(216, 218)",fill - mask
"(118, 119)","(118, 119)",t5
"(20, 47)","(20, 47)",##d? - - - language : pl datasets : wikipedia license : apache - 2. 0 - - - # distilbert
"(8, 33)","(8, 33)",- nlp / opus - mt - ja - pl? - - - language : - ja - pl tags : - translation
"(4, 39)","(4, 39)",model task of adapterhub / bert - base - uncased - pf - conll2003 _ pos? - - - tags : - token - classification
"(4, 36)","(4, 36)",model task of osanseviero / llama - alpaca - guanaco - vicuna? - - - tags : - image - classification
"(59, 59)","(59, 68)",roberta - base - finetuned - squad2
"(4, 6)","(4, 6)",model task of
"(5, 41)","(5, 41)",task of flax - sentence - embeddings / st - codesearch - distilroberta - base? - - - pipeline _ tag : sentence - similarity
"(80, 85)","(80, 85)",wav2vec2
"(60, 60)","(60, 60)",bert
"(405, 407)","(405, 407)",text - generation
"(4, 25)","(4, 25)",model task of phozon / harry - potter - medium? - - - tags : - conversational
"(27, 28)","(27, 28)",- 128
"(419, 421)","(419, 421)",text - generation
"(67, 67)","(4, 67)",model task of unicamp - dl / translation - pt - en - t5? - - - language : - en - pt datasets : - emea - paracrawl 99k - capes - scielo - jrc - acquis - biomedical domain corpora tags : - translation
"(70, 71)","(70, 71)",mpnet
"(4, 7)","(4, 7)",model task of adapt
"(4, 32)","(4, 32)",model task of 2early4coffee / dialogpt - medium - deadpool? - - - tags : - conversational
"(61, 65)","(61, 65)",automatic - speech - recognition

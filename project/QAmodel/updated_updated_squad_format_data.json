{"version": "1.1", "data": [{"title": "bert-base-german-cased", "paragraphs": [{"context": "---\nlanguage: de\nlicense: mit\nthumbnail: \ntags:\n- exbert\n---\n\n<a href=\"\n\t<img width=\"300px\" src=\"\n</a>\n\n# German BERT\n![bert_image](\n## Overview\n**Language model:** bert-base-cased   \n**Language:** German  \n**Training data:** Wiki, OpenLegalData, News (~ 12GB)  \n**Eval data:** Conll03 (NER), GermEval14 (NER), GermEval18 (Classification), GNAD (Classification)  \n**Infrastructure**: 1x TPU v2  \n**Published**: Jun 14th, 2019\n\n**Update April 3rd, 2020**: we updated the vocabulary file on deepset's s3 to conform with the default tokenization of punctuation tokens. \nFor details see the related [FARM issue]( If you want to use the old vocab we have also uploaded a [\"deepset/bert-base-german-cased-oldvocab\"]( model.\n \n## Details\n- We trained using Google's Tensorflow code on a single cloud TPU v2 with standard settings.\n- We trained 810k steps with a batch size of 1024 for sequence length 128 and 30k steps with sequence length 512. Training took about 9 days.\n- As training data we used the latest German Wikipedia dump (6GB of raw txt files), the OpenLegalData dump (2.4 GB) and news articles (3.6 GB).\n- We cleaned the data dumps with tailored scripts and segmented sentences with spacy v2.1. To create tensorflow records we used the recommended sentencepiece library for creating the word piece vocabulary and tensorflow scripts to convert the text to data usable by BERT.\n\n\nSee  for more details\n\n## Hyperparameters\n\n```\nbatch_size = 1024\nn_steps = 810_000\nmax_seq_len = 128 (and 512 later)\nlearning_rate = 1e-4\nlr_schedule = LinearWarmup\nnum_warmup_steps = 10_000\n```\n\n## Performance\n\nDuring training we monitored the loss and evaluated different model checkpoints on the following German datasets:\n\n- germEval18Fine: Macro f1 score for multiclass sentiment classification\n- germEval18coarse: Macro f1 score for binary sentiment classification\n- germEval14: Seq f1 score for NER (file names deuutf.\\*)\n- CONLL03: Seq f1 score for NER\n- 10kGNAD: Accuracy for document classification\n\nEven without thorough hyperparameter tuning, we observed quite stable learning especially for our German model. Multiple restarts with different seeds produced quite similar results.\n  \n![performancetable](  \n\nWe further evaluated different points during the 9 days of pre-training and were astonished how fast the model converges to the maximally reachable performance. We ran all 5 downstream tasks on 7 different model checkpoints - taken at 0 up to 840k training steps (x-axis in figure below). Most checkpoints are taken from early training where we expected most performance changes. Surprisingly, even a randomly initialized BERT can be trained only on labeled downstream datasets and reach good performance (blue line, GermEval 2018 Coarse task, 795 kB trainset size).\n\n![checkpointseval](  \n\n## Authors\n- Branden Chan: `branden.chan [at] deepset.ai`\n- Timo M\u00f6ller: `timo.moeller [at] deepset.ai`\n- Malte Pietsch: `malte.pietsch [at] deepset.ai`\n- Tanay Soni: `tanay.soni [at] deepset.ai`\n\n## About us\n![deepset logo](\n\nWe bring NLP to the industry via open source!  \nOur focus: Industry specific language models & large scale QA systems.  \n  \nSome of our work: \n- [German BERT (aka \"bert-base-german-cased\")](\n- [FARM](\n- [Haystack](\n\nGet in touch:\n[Twitter](  [Website](  \n", "qas": [{"id": "q1", "question": "What is the model architecture of bert-base-german-cased?", "answers": [{"text": "bert", "answer_start": 52, "answer_end": 55}]}]}]}, {"title": "openai-gpt", "paragraphs": [{"context": "---\nlanguage: en\nlicense: mit\n---\n\n# OpenAI GPT\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-authors)\n\n## Model Details\n\n**Model Description:** `openai-gpt` is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.\n\n- **Developed by:** Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. See [associated research paper]( and [GitHub repo]( for model developers and contributors.\n- **Model Type:** Transformer-based language model\n- **Language(s):** English\n- **License:** [MIT License](\n- **Related Models:** [GPT2]( [GPT2-Medium]( [GPT2-Large]( and [GPT2-XL](\n- **Resources for more information:**\n  - [Research Paper](\n  - [OpenAI Blog Post](\n  - [GitHub Repo](\n  - Test the full generation capabilities here: \n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='openai-gpt')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model,'he said, when i was finished.'ah well,'said the man,'that's\"},\n {'generated_text': 'Hello, I\\'m a language model, \" she said. \\n she reached the bottom of the shaft and leaned a little further out. it was'},\n {'generated_text': 'Hello, I\\'m a language model, \" she laughed. \" we call that a\\'white girl.\\'or as we are called by the'},\n {'generated_text': 'Hello, I\\'m a language model, \" said mr pin. \" an\\'the ones with the funny hats don\\'t. \" the rest of'},\n {'generated_text': 'Hello, I\\'m a language model, was\\'ere \\'bout to do some more dancin \\', \" he said, then his voice lowered to'}]\n```\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import OpenAIGPTTokenizer, OpenAIGPTModel\nimport torch\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\noutputs = model(inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for language modeling tasks.\n\n#### Downstream Use\n\nPotential downstream uses of this model include tasks that leverage language models. In the [associated paper]( the model developers discuss evaluations of the model for tasks including natural language inference (NLI), question answering, semantic similarity, and text classification.\n\n#### Misuse and Out-of-scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Risks, Limitations and Biases\n\n#### Biases\n\n**CONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)]( and [Bender et al. (2021)]( \nPredictions generated by this model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='openai-gpt')\n>>> set_seed(42)\n>>> generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The man worked as a teacher for the college he'},\n {'generated_text': 'The man worked as a janitor at the club.'},\n {'generated_text': 'The man worked as a bodyguard in america. the'},\n {'generated_text': 'The man worked as a clerk for one of the'},\n {'generated_text': 'The man worked as a nurse, but there was'}]\n\n>>> set_seed(42)\n>>> generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The woman worked as a medical intern but is a'},\n {'generated_text': 'The woman worked as a midwife, i know that'},\n {'generated_text': 'The woman worked as a prostitute in a sex club'},\n {'generated_text': 'The woman worked as a secretary for one of the'},\n {'generated_text': 'The woman worked as a nurse, but she had'}]\n```\n\nThis bias may also affect fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n#### Risks and Limitations\n\nThe model developers also wrote in a [blog post]( about risks and limitations of the model, including: \n\n> - **Compute Requirements:** Many previous approaches to NLP tasks train relatively small models on a single GPU from scratch. Our approach requires an expensive pre-training step - 1 month on 8 GPUs. Luckily, this only has to be done once and we\u2019re releasing our model so others can avoid it. It is also a large model (in comparison to prior work) and consequently uses more compute and memory \u2014 we used a 37-layer (12 block) Transformer architecture, and we train on sequences of up to 512 tokens. Most experiments were conducted on 4 and 8 GPU systems. The model does fine-tune to new tasks very quickly which helps mitigate the additional resource requirements.\n> - **The limits and bias of learning about the world through text:** Books and text readily available on the internet do not contain complete or even accurate information about the world. Recent work ([Lucy and Gauthier, 2017]( has shown that certain kinds of information are difficult to learn via just text and other work ([Gururangan et al., 2018]( has shown that models learn and exploit biases in data distributions.\n> - **Still brittle generalization:** Although our approach improves performance across a broad range of tasks, current deep learning NLP models still exhibit surprising and counterintuitive behavior - especially when evaluated in a systematic, adversarial, or out-of-distribution way. Our approach is not immune to these issues, though we have observed some indications of progress. Our approach shows improved lexical robustness over previous purely neural approaches to textual entailment. On the dataset introduced in Glockner et al. (2018) our model achieves 83.75%, performing similarly to KIM, which incorporates external knowledge via WordNet.\n\n## Training\n\n#### Training Data\n\nThe model developers [write]( \n\n> We use the BooksCorpus dataset ([Zhu et al., 2015]( for training the language model. It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance. Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information.\n\n#### Training Procedure\n\nThe model developers [write]( \n\n> Our model largely follows the original transformer work [62]. We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm [2] is used extensively throughout the model, a simple weight initialization of N (0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We used learned position embeddings instead of the sinusoidal version proposed in the original work. We use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer.\n\nSee the paper for further details and links to citations.\n\n## Evaluation\n\nThe following evaluation information is extracted from the [associated blog post]( See the [associated paper]( for further details.\n\n#### Testing Data, Factors and Metrics\n\nThe model developers report that the model was evaluated on the following tasks and datasets using the listed metrics: \n\n- **Task:** Textual Entailment\n  - **Datasets:** [SNLI]( [MNLI Matched]( [MNLI Mismatched]( [SciTail]( [QNLI]( [RTE](\n  - **Metrics:** Accuracy  \n \n- **Task:** Semantic Similarity\n  - **Datasets:** [STS-B]( [QQP]( [MRPC](\n  - **Metrics:** Accuracy\n \n- **Task:** Reading Comprehension\n  - **Datasets:** [RACE](\n  - **Metrics:** Accuracy\n \n- **Task:** Commonsense Reasoning\n  - **Datasets:** [ROCStories]( [COPA](\n  - **Metrics:** Accuracy\n \n- **Task:** Sentiment Analysis\n  - **Datasets:** [SST-2](\n  - **Metrics:** Accuracy\n \n- **Task:** Linguistic Acceptability\n  - **Datasets:** [CoLA](\n  - **Metrics:** Accuracy\n \n- **Task:** Multi Task Benchmark\n  - **Datasets:** [GLUE](\n  - **Metrics:** Accuracy\n\n#### Results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n TE  TE             TE     SS  SS  CR        SA    MTB  |\n:--::-------------::----::---::--::--------::----::----:|\nSNLIMNLI Mismatched QNLI STS-BMPRCROCStories SST-2 GLUE |\n89.9 81.4           88.1 82.0 82.3  86.5     91.3  72.8 | \n\n## Environmental Impact\n\nThe model developers [report that]( \n\n> The total compute used to train this model was 0.96 petaflop days (pfs-days).\n\n> 8 P600 GPU's * 30 days * 12 TFLOPS/GPU * 0.33 utilization = .96 pfs-days\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator]( presented in [Lacoste et al. (2019)](\n\n- **Hardware Type:** 8 P600 GPUs\n- **Hours used:** 720 hours (30 days)\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper]( for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@article{radford2018improving,\n  title={Improving language understanding by generative pre-training},\n  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},\n  year={2018},\n  publisher={OpenAI}\n}\n```\n\nAPA: \n*Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.*\n\n## Model Card Authors\n\nThis model card was written by the Hugging Face team.", "qas": [{"id": "q1", "question": "What is the model architecture of openai-gpt?", "answers": [{"text": "openai-gpt", "answer_start": 550, "answer_end": 559}]}, {"id": "q2", "question": "What is the model task of openai-gpt?", "answers": [{"text": "text-generation", "answer_start": 1604, "answer_end": 1618}]}]}]}, {"title": "roberta-base-openai-detector", "paragraphs": [{"context": "---\nlanguage: en\nlicense: mit\ntags:\n- exbert\ndatasets: \n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa Base OpenAI Detector\n\n## Table of Contents\n- [Model Details](#model-details)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-author)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n## Model Details\n\n**Model Description:** RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model. This model was released by OpenAI at the same time as OpenAI released the weights of the [largest GPT-2 model]( the 1.5B parameter version. \n\n- **Developed by:** OpenAI, see [GitHub Repo]( and [associated paper]( for full author list\n- **Model Type:** Fine-tuned transformer-based language model\n- **Language(s):** English\n- **License:** MIT\n- **Related Models:** [RoBERTa base]( [GPT-XL (1.5B parameter version)]( [GPT-Large (the 774M parameter version)]( [GPT-Medium (the 355M parameter version)]( and [GPT-2 (the 124M parameter version)](\n- **Resources for more information:**\n  - [Research Paper]( (see, in particular, the section beginning on page 12 about Automated ML-based detection).\n  - [GitHub Repo](\n  - [OpenAI Blog Post](\n  - [Explore the detector model here]( )\n\n## Uses\n\n#### Direct Use\n\nThe model is a classifier that can be used to detect text generated by GPT-2 models. However, it is strongly suggested not to use it as a ChatGPT detector for the purposes of making grave allegations of academic misconduct against undergraduates and others, as this model might give inaccurate results in the case of ChatGPT-generated input.\n\n#### Downstream Use\n\nThe model's developers have stated that they developed and released the model to help with research related to synthetic text generation, so the model could potentially be used for downstream tasks related to synthetic text generation. See the [associated paper]( for further discussion.\n\n#### Misuse and Out-of-scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model developers discuss the risk of adversaries using the model to better evade detection in their [associated paper]( suggesting that using the model for evading detection or for supporting efforts to evade detection would be a misuse of the model. \n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware this section may contain content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n#### Risks and Limitations\n\nIn their [associated paper]( the model developers discuss the risk that the model may be used by bad actors to develop capabilities for evading detection, though one purpose of releasing the model is to help improve detection research. \n\nIn a related [blog post]( the model developers also discuss the limitations of automated methods for detecting synthetic text and the need to pair automated detection tools with other, non-automated approaches. They write: \n\n> We conducted in-house detection research and developed a detection model that has detection rates of ~95% for detecting 1.5B GPT-2-generated text. We believe this is not high enough accuracy for standalone detection and needs to be paired with metadata-based approaches, human judgment, and public education to be more effective. \n\nThe model developers also [report]( finding that classifying content from larger models is more difficult, suggesting that detection with automated tools like this model will be increasingly difficult as model sizes increase. The authors find that training detector models on the outputs of larger models can improve accuracy and robustness. \n\n#### Bias\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)]( and [Bender et al. (2021)]( Predictions generated by RoBERTa base and GPT-2 1.5B (which this model is built/fine-tuned on) can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups (see the [RoBERTa base]( and [GPT-2 XL]( model cards for more information). The developers of this model discuss these issues further in their [paper](\n\n## Training\n\n#### Training Data\n\nThe model is a sequence classifier based on RoBERTa base (see the [RoBERTa base model card]( for more details on the RoBERTa base training data) and then fine-tuned using the outputs of the 1.5B GPT-2 model (available [here](\n\n#### Training Procedure\n\nThe model developers write that: \n\n> We based a sequence classifier on RoBERTaBASE (125 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model.\n\nThey later state: \n\n> To develop a robust detector model that can accurately classify generated texts regardless of the sampling method, we performed an analysis of the model\u2019s transfer performance.\n\nSee the [associated paper]( for further details on the training procedure.\n\n## Evaluation\n\nThe following evaluation information is extracted from the [associated paper](\n\n#### Testing Data, Factors and Metrics\n\nThe model is intended to be used for detecting text generated by GPT-2 models, so the model developers test the model on text datasets, measuring accuracy by: \n\n> testing 510-token test examples comprised of 5,000 samples from the WebText dataset and 5,000 samples generated by a GPT-2 model, which were not used during the training.\n\n#### Results\n\nThe model developers [find]( \n\n> Our classifier is able to detect 1.5 billion parameter GPT-2-generated text with approximately 95% accuracy...The model\u2019s accuracy depends on sampling methods used when generating outputs, like temperature, Top-K, and nucleus sampling ([Holtzman et al., 2019]( Nucleus sampling outputs proved most difficult to correctly classify, but a detector trained using nucleus sampling transfers well across other sampling methods. As seen in Figure 1 [in the paper], we found consistently high accuracy when trained on nucleus sampling. \t\n\nSee the [associated paper]( Figure 1 (on page 14) and Figure 2 (on page 16) for full results.\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator]( presented in [Lacoste et al. (2019)](\n\n- **Hardware Type:** Unknown\n- **Hours used:** Unknown\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nThe model developers write that: \n\nSee the [associated paper]( for further details on the modeling architecture and training details.\n\n## Citation Information\n\n```bibtex\n@article{solaiman2019release,\n  title={Release strategies and the social impacts of language models},\n  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},\n  journal={arXiv preprint arXiv:1908.09203},\n  year={2019}\n}\n```\n\nAPA: \n- Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., ... & Wang, J. (2019). Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203.\n\n## Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n## How to Get Started with the Model \n\nMore information needed.\n", "qas": []}]}, {"title": "roberta-base", "paragraphs": [{"context": "---\nlanguage: en\ntags:\n- exbert\nlicense: mit\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper]( and first released in\n[this repository]( This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the [model hub]( to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n  'score': 0.3306540250778198,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a female model.</s>\",\n  'score': 0.04655390977859497,\n  'token': 2182,\n  'token_str': '\u0120female'},\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\n  'score': 0.04232972860336304,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n  'score': 0.037216778844594955,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a Russian model.</s>\",\n  'score': 0.03253649175167084,\n  'token': 1083,\n  'token_str': '\u0120Russian'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n  'score': 0.08702439814805984,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a waiter.</s>',\n  'score': 0.0819653645157814,\n  'token': 38233,\n  'token_str': '\u0120waiter'},\n {'sequence': '<s>The man worked as a butcher.</s>',\n  'score': 0.073323555290699,\n  'token': 32364,\n  'token_str': '\u0120butcher'},\n {'sequence': '<s>The man worked as a miner.</s>',\n  'score': 0.046322137117385864,\n  'token': 18678,\n  'token_str': '\u0120miner'},\n {'sequence': '<s>The man worked as a guard.</s>',\n  'score': 0.040150221437215805,\n  'token': 2510,\n  'token_str': '\u0120guard'}]\n\n>>> unmasker(\"The Black woman worked as a <mask>.\")\n\n[{'sequence': '<s>The Black woman worked as a waitress.</s>',\n  'score': 0.22177888453006744,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The Black woman worked as a prostitute.</s>',\n  'score': 0.19288744032382965,\n  'token': 36289,\n  'token_str': '\u0120prostitute'},\n {'sequence': '<s>The Black woman worked as a maid.</s>',\n  'score': 0.06498628109693527,\n  'token': 29754,\n  'token_str': '\u0120maid'},\n {'sequence': '<s>The Black woman worked as a secretary.</s>',\n  'score': 0.05375480651855469,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The Black woman worked as a nurse.</s>',\n  'score': 0.05245552211999893,\n  'token': 9008,\n  'token_str': '\u0120nurse'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus]( a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia]( (excluding lists, tables and headers) ;\n- [CC-News]( a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText]( an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories]( a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n MNLI  QNLI  CoLA  MRPC \n:----::----::----::----:\n 87.6  92.8  63.6  90.2 \n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {\n  bibsource = {dblp computer science bibliography, \n}\n```\n\n<a href=\"\n\t<img width=\"300px\" src=\"\n</a>\n", "qas": [{"id": "q1", "question": "What is the model architecture of roberta-base?", "answers": [{"text": "roberta", "answer_start": 2383, "answer_end": 2389}]}, {"id": "q2", "question": "What is the model task of roberta-base?", "answers": [{"text": "fill-mask", "answer_start": 2364, "answer_end": 2372}]}]}]}, {"title": "roberta-large-mnli", "paragraphs": [{"context": "---\nlanguage:\n- en\nlicense: mit\ntags:\n- autogenerated-modelcard\ndatasets:\n- multi_nli\n- wikipedia\n- bookcorpus\n---\n\n# roberta-large-mnli\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation-results)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-author)\n\n## Model Details\n\n**Model Description:** roberta-large-mnli is the [RoBERTa large model]( fine-tuned on the [Multi-Genre Natural Language Inference (MNLI)]( corpus. The model is a pretrained model on English language text using a masked language modeling (MLM) objective.\n\n- **Developed by:** See [GitHub Repo]( for model developers\n- **Model Type:** Transformer-based language model\n- **Language(s):** English\n- **License:** MIT \n- **Parent Model:** This model is a fine-tuned version of the RoBERTa large model. Users should see the [RoBERTa large model card]( for relevant information.\n- **Resources for more information:**\n  - [Research Paper](\n  - [GitHub Repo](\n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. The model can be loaded with the zero-shot-classification pipeline like so:\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline('zero-shot-classification', model='roberta-large-mnli')\n```\n\nYou can then use this pipeline to classify sequences into any of the class names you specify. For example:\n\n```python\nsequence_to_classify = \"one day I will see the world\"\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(sequence_to_classify, candidate_labels)\n```\n\n## Uses\n\n#### Direct Use\n\nThis fine-tuned model can be used for zero-shot classification tasks, including zero-shot sentence-pair classification (see the [GitHub repo]( for examples) and zero-shot sequence classification.\n\n#### Misuse and Out-of-scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propogate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)]( and [Bender et al. (2021)]( The [RoBERTa large model card]( notes that: \"The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral.\" \n\nPredictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n```python\nsequence_to_classify = \"The CEO had a strong handshake.\"\ncandidate_labels = ['male', 'female']\nhypothesis_template = \"This text speaks about a {} profession.\"\nclassifier(sequence_to_classify, candidate_labels, hypothesis_template=hypothesis_template)\n```\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n## Training\n\n#### Training Data\n\nThis model was fine-tuned on the [Multi-Genre Natural Language Inference (MNLI)]( corpus. Also see the [MNLI data card]( for more information. \n\nAs described in the [RoBERTa large model card]( \n\n> The RoBERTa model was pretrained on the reunion of five datasets:\n> \n> - [BookCorpus]( a dataset consisting of 11,038 unpublished books;\n> - [English Wikipedia]( (excluding lists, tables and headers) ;\n> - [CC-News]( a dataset containing 63 millions English news articles crawled between September 2016 and February 2019.\n> - [OpenWebText]( an opensource recreation of the WebText dataset used to train GPT-2,\n> - [Stories]( a dataset containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas.\n>\n> Together theses datasets weight 160GB of text.\n\nAlso see the [bookcorpus data card]( and the [wikipedia data card]( for additional information.\n\n#### Training Procedure\n\n##### Preprocessing\n\nAs described in the [RoBERTa large model card]( \n\n> The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\n> the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked\n> with `<s>` and the end of one by `</s>`\n> \n> The details of the masking procedure for each sentence are the following:\n> - 15% of the tokens are masked.\n> - In 80% of the cases, the masked tokens are replaced by `<mask>`.\n> - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n> - In the 10% remaining cases, the masked tokens are left as is.\n> \n> Contrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n##### Pretraining \n\nAlso as described in the [RoBERTa large model card]( \n\n> The model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\n> optimizer used is Adam with a learning rate of 4e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n> \\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 30,000 steps and linear decay of the learning\n> rate after.\n\n## Evaluation\n\nThe following evaluation information is extracted from the associated [GitHub repo for RoBERTa]( \n\n#### Testing Data, Factors and Metrics\n\nThe model developers report that the model was evaluated on the following tasks and datasets using the listed metrics: \n\n- **Dataset:** Part of [GLUE (Wang et al., 2019)]( the General Language Understanding Evaluation benchmark, a collection of 9 datasets for evaluating natural language understanding systems. Specifically, the model was evaluated on the [Multi-Genre Natural Language Inference (MNLI)]( corpus. See the [GLUE data card]( or [Wang et al. (2019)]( for further information.\n  - **Tasks:** NLI. [Wang et al. (2019)]( describe the inference task for MNLI as: \n  > The Multi-Genre Natural Language Inference Corpus [(Williams et al., 2018)]( is a crowd-sourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. We use the standard test set, for which we obtained private labels from the authors, and evaluate on both the matched (in-domain) and mismatched (cross-domain) sections. We also use and recommend the SNLI corpus [(Bowman et al., 2015)]( as 550k examples of auxiliary training data.\n  - **Metrics:** Accuracy  \n  \n- **Dataset:** [XNLI (Conneau et al., 2018)]( the extension of the [Multi-Genre Natural Language Inference (MNLI)]( corpus to 15 languages: English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu. See the [XNLI data card]( or [Conneau et al. (2018)]( for further information.\n  - **Tasks:** Translate-test (e.g., the model is used to translate input sentences in other languages to the training language)\n  - **Metrics:** Accuracy\n\n#### Results\n\nGLUE test results (dev set, single model, single-task fine-tuning): 90.2 on MNLI\n\nXNLI test results:\n\n en  es   el   ru   ar   th   hi   ur  |\n:--::---::---::---::---::---::---::---:|\n91.384.2781.7478.2876.6474.05 70.966.81|\t\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator]( presented in [Lacoste et al. (2019)]( We present the hardware type and hours used based on the [associated paper](\n\n- **Hardware Type:** 1024 V100 GPUs\n- **Hours used:** 24 hours (one day)\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper]( for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@article{liu2019roberta,\n    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},\n    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and\n              Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and\n              Luke Zettlemoyer and Veselin Stoyanov},\n    journal={arXiv preprint arXiv:1907.11692},\n    year = {2019},\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of roberta-large-mnli?", "answers": [{"text": "roberta", "answer_start": 118, "answer_end": 124}]}]}]}, {"title": "roberta-large-openai-detector", "paragraphs": [{"context": "---\nlanguage: en\nlicense: mit\ntags:\n- exbert\ndatasets: \n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa Large OpenAI Detector\n\n## Table of Contents\n- [Model Details](#model-details)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-authors)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n## Model Details\n\n**Model Description:** RoBERTa large OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa large model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model. This model was released by OpenAI at the same time as OpenAI released the weights of the [largest GPT-2 model]( the 1.5B parameter version. \n\n- **Developed by:** OpenAI, see [GitHub Repo]( and [associated paper]( for full author list\n- **Model Type:** Fine-tuned transformer-based language model\n- **Language(s):** English\n- **License:** MIT\n- **Related Models:** [RoBERTa large]( [GPT-XL (1.5B parameter version)]( [GPT-Large (the 774M parameter version)]( [GPT-Medium (the 355M parameter version)]( and [GPT-2 (the 124M parameter version)](\n- **Resources for more information:**\n  - [Research Paper]( (see, in particular, the section beginning on page 12 about Automated ML-based detection).\n  - [GitHub Repo](\n  - [OpenAI Blog Post](\n  - [Explore the detector model here]( )\n\n## Uses\n\n#### Direct Use\n\nThe model is a classifier that can be used to detect text generated by GPT-2 models. \n\n#### Downstream Use\n\nThe model's developers have stated that they developed and released the model to help with research related to synthetic text generation, so the model could potentially be used for downstream tasks related to synthetic text generation. See the [associated paper]( for further discussion.\n\n#### Misuse and Out-of-scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model developers discuss the risk of adversaries using the model to better evade detection in their [associated paper]( suggesting that using the model for evading detection or for supporting efforts to evade detection would be a misuse of the model. \n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware this section may contain content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n#### Risks and Limitations\n\nIn their [associated paper]( the model developers discuss the risk that the model may be used by bad actors to develop capabilities for evading detection, though one purpose of releasing the model is to help improve detection research. \n\nIn a related [blog post]( the model developers also discuss the limitations of automated methods for detecting synthetic text and the need to pair automated detection tools with other, non-automated approaches. They write: \n\n> We conducted in-house detection research and developed a detection model that has detection rates of ~95% for detecting 1.5B GPT-2-generated text. We believe this is not high enough accuracy for standalone detection and needs to be paired with metadata-based approaches, human judgment, and public education to be more effective. \n\nThe model developers also [report]( finding that classifying content from larger models is more difficult, suggesting that detection with automated tools like this model will be increasingly difficult as model sizes increase. The authors find that training detector models on the outputs of larger models can improve accuracy and robustness. \n\n#### Bias\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)]( and [Bender et al. (2021)]( Predictions generated by RoBERTa large and GPT-2 1.5B (which this model is built/fine-tuned on) can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups (see the [RoBERTa large]( and [GPT-2 XL]( model cards for more information). The developers of this model discuss these issues further in their [paper](\n\n## Training\n\n#### Training Data\n\nThe model is a sequence classifier based on RoBERTa large (see the [RoBERTa large model card]( for more details on the RoBERTa large training data) and then fine-tuned using the outputs of the 1.5B GPT-2 model (available [here](\n\n#### Training Procedure\n\nThe model developers write that: \n\n> We based a sequence classifier on RoBERTaLARGE (355 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model.\n\nThey later state: \n\n> To develop a robust detector model that can accurately classify generated texts regardless of the sampling method, we performed an analysis of the model\u2019s transfer performance.\n\nSee the [associated paper]( for further details on the training procedure.\n\n## Evaluation\n\nThe following evaluation information is extracted from the [associated paper](\n\n#### Testing Data, Factors and Metrics\n\nThe model is intended to be used for detecting text generated by GPT-2 models, so the model developers test the model on text datasets, measuring accuracy by: \n\n> testing 510-token test examples comprised of 5,000 samples from the WebText dataset and 5,000 samples generated by a GPT-2 model, which were not used during the training.\n\n#### Results\n\nThe model developers [find]( \n\n> Our classifier is able to detect 1.5 billion parameter GPT-2-generated text with approximately 95% accuracy...The model\u2019s accuracy depends on sampling methods used when generating outputs, like temperature, Top-K, and nucleus sampling ([Holtzman et al., 2019]( Nucleus sampling outputs proved most difficult to correctly classify, but a detector trained using nucleus sampling transfers well across other sampling methods. As seen in Figure 1 [in the paper], we found consistently high accuracy when trained on nucleus sampling. \t\n\nSee the [associated paper]( Figure 1 (on page 14) and Figure 2 (on page 16) for full results.\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator]( presented in [Lacoste et al. (2019)](\n\n- **Hardware Type:** Unknown\n- **Hours used:** Unknown\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nThe model developers write that: \n\nSee the [associated paper]( for further details on the modeling architecture and training details.\n\n## Citation Information\n\n```bibtex\n@article{solaiman2019release,\n  title={Release strategies and the social impacts of language models},\n  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},\n  journal={arXiv preprint arXiv:1908.09203},\n  year={2019}\n}\n```\n\nAPA: \n- Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., ... & Wang, J. (2019). Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203.\n\n## Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n## How to Get Started with the Model \n\nMore information needed", "qas": []}]}, {"title": "roberta-large", "paragraphs": [{"context": "---\nlanguage: en\ntags:\n- exbert\nlicense: mit\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa large model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper]( and first released in\n[this repository]( This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the [model hub]( to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n  'score': 0.3317350447177887,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n  'score': 0.14171843230724335,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\n  'score': 0.04291723668575287,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a freelance model.</s>\",\n  'score': 0.02134818211197853,\n  'token': 18150,\n  'token_str': '\u0120freelance'},\n {'sequence': \"<s>Hello I'm a young model.</s>\",\n  'score': 0.021098261699080467,\n  'token': 664,\n  'token_str': '\u0120young'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\nmodel = RobertaModel.from_pretrained('roberta-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\nmodel = TFRobertaModel.from_pretrained('roberta-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n  'score': 0.08260300755500793,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a driver.</s>',\n  'score': 0.05736079439520836,\n  'token': 1393,\n  'token_str': '\u0120driver'},\n {'sequence': '<s>The man worked as a teacher.</s>',\n  'score': 0.04709019884467125,\n  'token': 3254,\n  'token_str': '\u0120teacher'},\n {'sequence': '<s>The man worked as a bartender.</s>',\n  'score': 0.04641604796051979,\n  'token': 33080,\n  'token_str': '\u0120bartender'},\n {'sequence': '<s>The man worked as a waiter.</s>',\n  'score': 0.04239227622747421,\n  'token': 38233,\n  'token_str': '\u0120waiter'}]\n\n>>> unmasker(\"The woman worked as a <mask>.\")\n\n[{'sequence': '<s>The woman worked as a nurse.</s>',\n  'score': 0.2667474150657654,\n  'token': 9008,\n  'token_str': '\u0120nurse'},\n {'sequence': '<s>The woman worked as a waitress.</s>',\n  'score': 0.12280137836933136,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The woman worked as a teacher.</s>',\n  'score': 0.09747499972581863,\n  'token': 3254,\n  'token_str': '\u0120teacher'},\n {'sequence': '<s>The woman worked as a secretary.</s>',\n  'score': 0.05783602222800255,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The woman worked as a cleaner.</s>',\n  'score': 0.05576248839497566,\n  'token': 16126,\n  'token_str': '\u0120cleaner'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus]( a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia]( (excluding lists, tables and headers) ;\n- [CC-News]( a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText]( an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories]( a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether theses datasets weight 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 4e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 30,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n MNLI  QNLI  CoLA  MRPC \n:----::----::----::----:\n 90.2  94.7  68.0  90.9 \n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {\n  bibsource = {dblp computer science bibliography, \n}\n```\n\n<a href=\"\n\t<img width=\"300px\" src=\"\n</a>\n", "qas": [{"id": "q1", "question": "What is the model architecture of roberta-large?", "answers": [{"text": "roberta", "answer_start": 2382, "answer_end": 2388}]}, {"id": "q2", "question": "What is the model task of roberta-large?", "answers": [{"text": "fill-mask", "answer_start": 2363, "answer_end": 2371}]}]}]}, {"title": "09panesara/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: distilbert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5406394412669151\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7580\n- Matthews Correlation: 0.5406\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5094          \n 2.0    0.5230          \n 3.0    0.6412          \n 4.0    0.7580          \n 5.0    0.8494          \n\n\n### Framework versions\n\n- Transformers 4.14.1\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of 09panesara/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 125, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of 09panesara/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 229, "answer_end": 247}]}]}]}, {"title": "0x7194633/keyt5-base", "paragraphs": [{"context": "---\nlanguage:\n- ru\nlicense: mit\ninference:\n  parameters:\n    top_p: 0.9\nwidget:\n- text: \"\u0412 \u0420\u043e\u0441\u0441\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u044f\u0432\u0438\u0442\u044c\u0441\u044f \u043d\u043e\u0432\u044b\u0439 \u0448\u0442\u0430\u043c\u043c \u043a\u043e\u0440\u043e\u043d\u0430\u0432\u0438\u0440\u0443\u0441\u0430 \u00ab\u043e\u043c\u0438\u043a\u0440\u043e\u043d\u00bb, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043f\u043e\u0434\u044a\u0435\u043c\u0443 \u0437\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0432 \u044f\u043d\u0432\u0430\u0440\u0435, \u0437\u0430\u044f\u0432\u0438\u043b \u0434\u043e\u0446\u0435\u043d\u0442 \u043a\u0430\u0444\u0435\u0434\u0440\u044b \u0438\u043d\u0444\u0435\u043a\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0431\u043e\u043b\u0435\u0437\u043d\u0435\u0439 \u0420\u0423\u0414\u041d \u0421\u0435\u0440\u0433\u0435\u0439 \u0412\u043e\u0437\u043d\u0435\u0441\u0435\u043d\u0441\u043a\u0438\u0439. \u041e\u043d \u043e\u0442\u043c\u0435\u0442\u0438\u043b, \u0447\u0442\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u00ab\u0434\u0435\u043b\u044c\u0442\u0430\u00bb \u0432\u044b\u0437\u044b\u0432\u0430\u043b \u0431\u043e\u043b\u044c\u0448\u0435 \u043b\u0435\u0442\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u0443\u0447\u0430\u0435\u0432, \u0447\u0435\u043c \u043e\u043c\u0438\u043a\u0440\u043e\u043d, \u0438\u043c\u0435\u043d\u043d\u043e \u043d\u0430 \u0444\u043e\u043d\u0435 \u00ab\u0434\u0435\u043b\u044c\u0442\u044b\u00bb \u0431\u044b\u043b\u0430 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u043b\u0435\u0442\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c.\"\n  example_title: \"\u041a\u043e\u0440\u043e\u043d\u0430\u0432\u0438\u0440\u0443\u0441\"\n- text: \"\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u0438\u043a\u0430 \u0448\u0442\u0430\u0431\u0430 \u043e\u0431\u043e\u0440\u043e\u043d\u044b \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u0438 \u0430\u0434\u043c\u0438\u0440\u0430\u043b\u0430 \u0422\u043e\u043d\u0438 \u0420\u0430\u0434\u0430\u043a\u0438\u043d\u0430 \u0437\u0430\u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u0438\u043c\u0438\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0432\u0438\u0437\u0438\u0442\u0430 \u0432 \u0430\u043d\u0433\u0430\u0440 \u0441 \u0442\u044f\u0436\u0435\u043b\u044b\u043c \u0432\u043e\u043e\u0440\u0443\u0436\u0435\u043d\u0438\u0435\u043c, \u0441\u043e\u043e\u0431\u0449\u0438\u043b\u0430 \u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0430\u044f \u043f\u0440\u0435\u0441\u0441\u0430. \u0412 \u043f\u0440\u0438\u043a\u0430\u0437\u0435 \u0433\u043e\u0432\u043e\u0440\u0438\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u0432\u043e\u0435\u043d\u043d\u043e\u0441\u043b\u0443\u0436\u0430\u0449\u0438\u043c \u0431\u044b\u043b\u043e \u0432\u0435\u043b\u0435\u043d\u043e \u043f\u043e\u0434\u0431\u0435\u0433\u0430\u0442\u044c \u043a \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044f\u043c, \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u043b\u044e\u043a\u0438, \u0437\u0430\u0442\u0432\u043e\u0440\u044b, \u043b\u0438\u0441\u0442\u0430\u0442\u044c \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u0438 \u0438 \u043e\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c\u0441\u044f \u043c\u0430\u0448\u0438\u043d\u044b, \u0431\u0443\u0434\u0442\u043e \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0441\u0442 \u0434\u043b\u044f \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u043e\u0431\u043e\u0440\u0443\u0434\u043e\u0432\u0430\u043d\u0438\u044f.\"\n  example_title: \"\u0411\u0440\u0438\u0442\u0430\u043d\u0438\u044f\"\n- text: \"\u0414\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043c\u0443\u0437\u044b\u043a\u0438 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043d\u0430\u0436\u0438\u043c\u0430\u0442\u044c \u043d\u0430 \u043a\u043d\u043e\u043f\u043a\u0438 \u043a\u043b\u0430\u0432\u0438\u0430\u0442\u0443\u0440\u044b. \u041a\u0430\u0436\u0434\u043e\u0439 \u043a\u043b\u0430\u0432\u0438\u0448\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0439 \u0441\u0435\u043c\u043f\u043b \u2014 \u0435\u0441\u0442\u044c \u043c\u0430\u0440\u0430\u043a\u0430\u0441\u044b \u0438 \u0444\u0443\u0442\u0443\u0440\u0438\u0441\u0442\u0438\u0447\u043d\u044b\u0435 \u0437\u0432\u0443\u043a\u0438, \u043d\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u044e\u0449\u0438\u0435 \u0432\u044b\u0441\u0442\u0440\u0435\u043b\u044b \u0431\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432. \u0418\u0437 \u0432\u0441\u0435\u0433\u043e \u043c\u043d\u043e\u0433\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u044b \u0438 \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0442\u044c \u0437\u0430 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439 \u0441 \u0430\u043d\u0438\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0433\u0435\u043e\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0444\u0438\u0433\u0443\u0440\u0430\u043c\u0438. \u0427\u0442\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e, \u043d\u0430\u0436\u0430\u0442\u0438\u0435\u043c \u043a\u043b\u0430\u0432\u0438\u0448\u0438 \u043f\u0440\u043e\u0431\u0435\u043b \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u0438\u0442\u044c \u043e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u0438\u0435, \u0446\u0432\u0435\u0442\u0430 \u043d\u0430 \u044d\u043a\u0440\u0430\u043d\u0435 \u0438 \u0437\u0432\u0443\u0447\u0430\u043d\u0438\u0435 \u0441\u0435\u043c\u043f\u043b\u043e\u0432.\"\n  example_title: \"\u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438\"\n---\n## keyT5. Base (small) version\n[![0x7o - text2keywords]( \"Go to GitHub repo\")\n[![stars - text2keywords](\n[![forks - text2keywords](\n\nSupported languages: ru\n\nGithub - [text2keywords](\n\n\n[Pretraining Large version](\n|\n[Pretraining Base version](\n\n# Usage\nExample usage (the code returns a list with keywords. duplicates are possible):\n\n[![Try Model Training In Colab!](\n\n```\npip install transformers sentencepiece\n```\n\n```python\nfrom itertools import groupby\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nmodel_name = \"0x7194633/keyt5-large\" # or 0x7194633/keyt5-base\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ndef generate(text, **kwargs):\n    inputs = tokenizer(text, return_tensors='pt')\n    with torch.no_grad():\n        hypotheses = model.generate(**inputs, num_beams=5, **kwargs)\n    s = tokenizer.decode(hypotheses[0], skip_special_tokens=True)\n    s = s.replace('; ', ';').replace(' ;', ';').lower().split(';')[:-1]\n    s = [el for el, _ in groupby(s)]\n    return s\n\narticle = \"\"\"Reuters \u0441\u043e\u043e\u0431\u0449\u0438\u043b \u043e\u0431 \u043e\u0442\u043c\u0435\u043d\u0435 3,6 \u0442\u044b\u0441. \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432 \u0438\u0437-\u0437\u0430 \u00ab\u043e\u043c\u0438\u043a\u0440\u043e\u043d\u0430\u00bb \u0438 \u043f\u043e\u0433\u043e\u0434\u044b\n\u041d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u0447\u0438\u0441\u043b\u043e \u043e\u0442\u043c\u0435\u043d \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432 2 \u044f\u043d\u0432\u0430\u0440\u044f \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u043d\u0430 \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0435 \u0430\u0432\u0438\u0430\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \nSkyWest \u0438 Southwest, \u0443 \u043a\u0430\u0436\u0434\u043e\u0439 \u2014 \u0431\u043e\u043b\u0435\u0435 400 \u043e\u0442\u043c\u0435\u043d\u0435\u043d\u043d\u044b\u0445 \u0440\u0435\u0439\u0441\u043e\u0432. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u0441\u0440\u0435\u0434\u0438 \n\u043e\u0442\u043c\u0435\u043d\u0435\u043d\u043d\u044b\u0445 2 \u044f\u043d\u0432\u0430\u0440\u044f \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432 \u2014 \u0431\u043e\u043b\u0435\u0435 2,1 \u0442\u044b\u0441. \u0440\u0435\u0439\u0441\u043e\u0432 \u0432 \u0421\u0428\u0410. \u0422\u0430\u043a\u0436\u0435 \u0441\u0432\u044b\u0448\u0435 6400 \n\u0440\u0435\u0439\u0441\u043e\u0432 \u0431\u044b\u043b\u0438 \u0437\u0430\u0434\u0435\u0440\u0436\u0430\u043d\u044b.\"\"\"\n\nprint(generate(article, top_p=1.0, max_length=64))  \n# ['\u0430\u0432\u0438\u0430\u043f\u0435\u0440\u0435\u0432\u043e\u0437\u043a\u0438', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0440\u0435\u0439\u0441\u043e\u0432', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0440\u0435\u0439\u0441\u043e\u0432', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432']\n```\n# Training\nGo to the training notebook and learn more about it:\n\n[![Try Model Training In Colab!](\n", "qas": [{"id": "q1", "question": "What is the model architecture of 0x7194633/keyt5-base?", "answers": [{"text": "t5", "answer_start": 1945, "answer_end": 1946}]}]}]}, {"title": "0x7194633/keyt5-large", "paragraphs": [{"context": "---\nlanguage:\n- ru\nlicense: mit\ninference:\n  parameters:\n    top_p: 1.0\nwidget:\n- text: \"\u0412 \u0420\u043e\u0441\u0441\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u044f\u0432\u0438\u0442\u044c\u0441\u044f \u043d\u043e\u0432\u044b\u0439 \u0448\u0442\u0430\u043c\u043c \u043a\u043e\u0440\u043e\u043d\u0430\u0432\u0438\u0440\u0443\u0441\u0430 \u00ab\u043e\u043c\u0438\u043a\u0440\u043e\u043d\u00bb, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043f\u043e\u0434\u044a\u0435\u043c\u0443 \u0437\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0432 \u044f\u043d\u0432\u0430\u0440\u0435, \u0437\u0430\u044f\u0432\u0438\u043b \u0434\u043e\u0446\u0435\u043d\u0442 \u043a\u0430\u0444\u0435\u0434\u0440\u044b \u0438\u043d\u0444\u0435\u043a\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0431\u043e\u043b\u0435\u0437\u043d\u0435\u0439 \u0420\u0423\u0414\u041d \u0421\u0435\u0440\u0433\u0435\u0439 \u0412\u043e\u0437\u043d\u0435\u0441\u0435\u043d\u0441\u043a\u0438\u0439. \u041e\u043d \u043e\u0442\u043c\u0435\u0442\u0438\u043b, \u0447\u0442\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u00ab\u0434\u0435\u043b\u044c\u0442\u0430\u00bb \u0432\u044b\u0437\u044b\u0432\u0430\u043b \u0431\u043e\u043b\u044c\u0448\u0435 \u043b\u0435\u0442\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u0443\u0447\u0430\u0435\u0432, \u0447\u0435\u043c \u043e\u043c\u0438\u043a\u0440\u043e\u043d, \u0438\u043c\u0435\u043d\u043d\u043e \u043d\u0430 \u0444\u043e\u043d\u0435 \u00ab\u0434\u0435\u043b\u044c\u0442\u044b\u00bb \u0431\u044b\u043b\u0430 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u043b\u0435\u0442\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c.\"\n  example_title: \"\u041a\u043e\u0440\u043e\u043d\u0430\u0432\u0438\u0440\u0443\u0441\"\n- text: \"\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u0438\u043a\u0430 \u0448\u0442\u0430\u0431\u0430 \u043e\u0431\u043e\u0440\u043e\u043d\u044b \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u0438 \u0430\u0434\u043c\u0438\u0440\u0430\u043b\u0430 \u0422\u043e\u043d\u0438 \u0420\u0430\u0434\u0430\u043a\u0438\u043d\u0430 \u0437\u0430\u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u0438\u043c\u0438\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0432\u0438\u0437\u0438\u0442\u0430 \u0432 \u0430\u043d\u0433\u0430\u0440 \u0441 \u0442\u044f\u0436\u0435\u043b\u044b\u043c \u0432\u043e\u043e\u0440\u0443\u0436\u0435\u043d\u0438\u0435\u043c, \u0441\u043e\u043e\u0431\u0449\u0438\u043b\u0430 \u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0430\u044f \u043f\u0440\u0435\u0441\u0441\u0430. \u0412 \u043f\u0440\u0438\u043a\u0430\u0437\u0435 \u0433\u043e\u0432\u043e\u0440\u0438\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u0432\u043e\u0435\u043d\u043d\u043e\u0441\u043b\u0443\u0436\u0430\u0449\u0438\u043c \u0431\u044b\u043b\u043e \u0432\u0435\u043b\u0435\u043d\u043e \u043f\u043e\u0434\u0431\u0435\u0433\u0430\u0442\u044c \u043a \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044f\u043c, \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u043b\u044e\u043a\u0438, \u0437\u0430\u0442\u0432\u043e\u0440\u044b, \u043b\u0438\u0441\u0442\u0430\u0442\u044c \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u0438 \u0438 \u043e\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c\u0441\u044f \u043c\u0430\u0448\u0438\u043d\u044b, \u0431\u0443\u0434\u0442\u043e \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0441\u0442 \u0434\u043b\u044f \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u043e\u0431\u043e\u0440\u0443\u0434\u043e\u0432\u0430\u043d\u0438\u044f.\"\n  example_title: \"\u0411\u0440\u0438\u0442\u0430\u043d\u0438\u044f\"\n- text: \"\u0414\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043c\u0443\u0437\u044b\u043a\u0438 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043d\u0430\u0436\u0438\u043c\u0430\u0442\u044c \u043d\u0430 \u043a\u043d\u043e\u043f\u043a\u0438 \u043a\u043b\u0430\u0432\u0438\u0430\u0442\u0443\u0440\u044b. \u041a\u0430\u0436\u0434\u043e\u0439 \u043a\u043b\u0430\u0432\u0438\u0448\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0439 \u0441\u0435\u043c\u043f\u043b \u2014 \u0435\u0441\u0442\u044c \u043c\u0430\u0440\u0430\u043a\u0430\u0441\u044b \u0438 \u0444\u0443\u0442\u0443\u0440\u0438\u0441\u0442\u0438\u0447\u043d\u044b\u0435 \u0437\u0432\u0443\u043a\u0438, \u043d\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u044e\u0449\u0438\u0435 \u0432\u044b\u0441\u0442\u0440\u0435\u043b\u044b \u0431\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432. \u0418\u0437 \u0432\u0441\u0435\u0433\u043e \u043c\u043d\u043e\u0433\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u044b \u0438 \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0442\u044c \u0437\u0430 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439 \u0441 \u0430\u043d\u0438\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0433\u0435\u043e\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0444\u0438\u0433\u0443\u0440\u0430\u043c\u0438. \u0427\u0442\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e, \u043d\u0430\u0436\u0430\u0442\u0438\u0435\u043c \u043a\u043b\u0430\u0432\u0438\u0448\u0438 \u043f\u0440\u043e\u0431\u0435\u043b \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u0438\u0442\u044c \u043e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u0438\u0435, \u0446\u0432\u0435\u0442\u0430 \u043d\u0430 \u044d\u043a\u0440\u0430\u043d\u0435 \u0438 \u0437\u0432\u0443\u0447\u0430\u043d\u0438\u0435 \u0441\u0435\u043c\u043f\u043b\u043e\u0432.\"\n  example_title: \"\u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438\"\n---\n## keyT5. Large version\n[![0x7o - text2keywords]( \"Go to GitHub repo\")\n[![stars - text2keywords](\n[![forks - text2keywords](\n\nSupported languages: ru\n\nGithub - [text2keywords](\n\n\n[Pretraining Large version](\n|\n[Pretraining Base version](\n\n# Usage\nExample usage (the code returns a list with keywords. duplicates are possible):\n\n[![Try Model Training In Colab!](\n\n```\npip install transformers sentencepiece\n```\n\n```python\nfrom itertools import groupby\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nmodel_name = \"0x7194633/keyt5-large\" # or 0x7194633/keyt5-base\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ndef generate(text, **kwargs):\n    inputs = tokenizer(text, return_tensors='pt')\n    with torch.no_grad():\n        hypotheses = model.generate(**inputs, num_beams=5, **kwargs)\n    s = tokenizer.decode(hypotheses[0], skip_special_tokens=True)\n    s = s.replace('; ', ';').replace(' ;', ';').lower().split(';')[:-1]\n    s = [el for el, _ in groupby(s)]\n    return s\n\narticle = \"\"\"Reuters \u0441\u043e\u043e\u0431\u0449\u0438\u043b \u043e\u0431 \u043e\u0442\u043c\u0435\u043d\u0435 3,6 \u0442\u044b\u0441. \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432 \u0438\u0437-\u0437\u0430 \u00ab\u043e\u043c\u0438\u043a\u0440\u043e\u043d\u0430\u00bb \u0438 \u043f\u043e\u0433\u043e\u0434\u044b\n\u041d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u0447\u0438\u0441\u043b\u043e \u043e\u0442\u043c\u0435\u043d \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432 2 \u044f\u043d\u0432\u0430\u0440\u044f \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u043d\u0430 \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0435 \u0430\u0432\u0438\u0430\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \nSkyWest \u0438 Southwest, \u0443 \u043a\u0430\u0436\u0434\u043e\u0439 \u2014 \u0431\u043e\u043b\u0435\u0435 400 \u043e\u0442\u043c\u0435\u043d\u0435\u043d\u043d\u044b\u0445 \u0440\u0435\u0439\u0441\u043e\u0432. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u0441\u0440\u0435\u0434\u0438 \n\u043e\u0442\u043c\u0435\u043d\u0435\u043d\u043d\u044b\u0445 2 \u044f\u043d\u0432\u0430\u0440\u044f \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432 \u2014 \u0431\u043e\u043b\u0435\u0435 2,1 \u0442\u044b\u0441. \u0440\u0435\u0439\u0441\u043e\u0432 \u0432 \u0421\u0428\u0410. \u0422\u0430\u043a\u0436\u0435 \u0441\u0432\u044b\u0448\u0435 6400 \n\u0440\u0435\u0439\u0441\u043e\u0432 \u0431\u044b\u043b\u0438 \u0437\u0430\u0434\u0435\u0440\u0436\u0430\u043d\u044b.\"\"\"\n\nprint(generate(article, top_p=1.0, max_length=64))  \n# ['\u0430\u0432\u0438\u0430\u043f\u0435\u0440\u0435\u0432\u043e\u0437\u043a\u0438', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0440\u0435\u0439\u0441\u043e\u0432', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0440\u0435\u0439\u0441\u043e\u0432', '\u043e\u0442\u043c\u0435\u043d\u0430 \u0430\u0432\u0438\u0430\u0440\u0435\u0439\u0441\u043e\u0432']\n```\n# Training\nGo to the training notebook and learn more about it:\n\n[![Try Model Training In Colab!](\n", "qas": [{"id": "q1", "question": "What is the model architecture of 0x7194633/keyt5-large?", "answers": [{"text": "t5", "answer_start": 1938, "answer_end": 1939}]}]}]}, {"title": "0xDEADBEA7/DialoGPT-small-rick", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Rick n Morty DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of 0xDEADBEA7/DialoGPT-small-rick?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "123abhiALFLKFO/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel_index:\n- name: distilbert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metric:\n      name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5331291095663535\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8628\n- Matthews Correlation: 0.5331\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5214          \n 2.0    0.5551          \n 3.0    0.6371          \n 4.0    0.7851          \n 5.0    0.8628          \n\n\n### Framework versions\n\n- Transformers 4.9.1\n- Pytorch 1.9.0+cu102\n- Datasets 1.11.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of 123abhiALFLKFO/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 125, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of 123abhiALFLKFO/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 229, "answer_end": 247}]}]}]}, {"title": "1Basco/DialoGPT-small-jake", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#Jake Peralta DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of 1Basco/DialoGPT-small-jake?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "202015004/wav2vec2-base-timit-demo-colab", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: wav2vec2-base-timit-demo-colab\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-timit-demo-colab\n\nThis model is a fine-tuned version of [facebook/wav2vec2-base]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6259\n- Wer: 0.3544\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 4\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.5    2.9473          \n 1.01   0.7774          \n 1.51   0.6923          \n 2.01   0.5445          \n 2.51   0.5148          \n 3.02   0.5283          \n 3.52   0.5196          \n 4.02   0.5285          \n 4.52   0.5060          \n 5.03   0.5216          \n 5.53   0.5376          \n 6.03   0.6051          \n 6.53   0.5084          \n 7.04   0.5083          \n 7.54   0.6123          \n 8.04   0.5749          \n 8.54   0.5110          \n 9.05   0.6324          \n 9.55   0.6100          \n 10.05  0.5508          \n 10.55  0.5090          \n 11.06  0.6282          \n 11.56  0.5817          \n 12.06  0.5741          \n 12.56  0.5818          \n 13.07  0.6492          \n 13.57  0.5393          \n 14.07  0.5510          \n 14.57  0.5896          \n 15.08  0.5554          \n 15.58  0.6312          \n 16.08  0.6732          \n 16.58  0.5990          \n 17.09  0.6186          \n 17.59  0.5837          \n 18.09  0.5831          \n 18.59  0.6562          \n 19.1   0.6298          \n 19.6   0.6746          \n 20.1   0.6236          \n 20.6   0.6012          \n 21.11  0.6053          \n 21.61  0.6154          \n 22.11  0.6052          \n 22.61  0.5877          \n 23.12  0.5786          \n 23.62  0.5828          \n 24.12  0.5913          \n 24.62  0.5850          \n 25.13  0.6029          \n 25.63  0.6312          \n 26.13  0.6312          \n 26.63  0.5891          \n 27.14  0.6259          \n 27.64  0.6315          \n 28.14  0.6547          \n 28.64  0.6237          \n 29.15  0.6187          \n 29.65  0.6259          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu102\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of 202015004/wav2vec2-base-timit-demo-colab?", "answers": [{"text": "wav2vec2", "answer_start": 76, "answer_end": 83}]}]}]}, {"title": "2early4coffee/DialoGPT-medium-deadpool", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Deadpool DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of 2early4coffee/DialoGPT-medium-deadpool?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "2early4coffee/DialoGPT-small-deadpool", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n\n# Deadpool DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of 2early4coffee/DialoGPT-small-deadpool?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "2gud/DialogGPT-small-Koopsbot", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#Stinky doo doo", "qas": [{"id": "q2", "question": "What is the model task of 2gud/DialogGPT-small-Koopsbot?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "2umm3r/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: distilbert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5155709926752544\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7816\n- Matthews Correlation: 0.5156\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5027          \n 2.0    0.5136          \n 3.0    0.6390          \n 4.0    0.7816          \n 5.0    0.8836          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.9.0+cu111\n- Datasets 1.14.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of 2umm3r/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 125, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of 2umm3r/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 229, "answer_end": 247}]}]}]}, {"title": "3koozy/gpt2-HxH", "paragraphs": [{"context": "this is a fine tuned GPT2 text generation model on a Hunter x Hunter TV anime series dataset.\\\nyou can find a link to the used dataset here : \nyou can find a colab notebook for fine-tuning the gpt2 model here : ", "qas": [{"id": "q1", "question": "What is the model architecture of 3koozy/gpt2-HxH?", "answers": [{"text": "gpt2", "answer_start": 193, "answer_end": 196}]}]}]}, {"title": "9pinus/macbert-base-chinese-medical-collation", "paragraphs": [{"context": "---\nlicense: apache-2.0\nlanguage: zh\ntags:\n- Token Classification\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\n---\n\n## Model description\n\nThis model is a fine-tuned version of macbert for the purpose of spell checking in medical application scenarios. We fine-tuned macbert Chinese base version on a 300M dataset including 60K+ authorized medical articles. We proposed to randomly confuse 30% sentences of these articles by adding noise with a either visually or phonologically resembled characters. Consequently, the fine-tuned model can achieve 96% accuracy on our test dataset. \n\n## Intended uses & limitations\n\nYou can use this model directly with a pipeline for token classification:\n```python\n>>> from transformers import (AutoModelForTokenClassification, AutoTokenizer)\n>>> from transformers import pipeline\n\n>>> hub_model_id = \"9pinus/macbert-base-chinese-medical-collation\"\n\n>>> model = AutoModelForTokenClassification.from_pretrained(hub_model_id)\n>>> tokenizer = AutoTokenizer.from_pretrained(hub_model_id)\n>>> classifier = pipeline('ner', model=model, tokenizer=tokenizer)\n>>> result = classifier(\"\u5982\u679c\u75c5\u60c5\u8f83\u91cd\uff0c\u53ef\u9002\u5f53\u53e3\u670d\u7532\u8096\u5511\u7247\u3001\u73af\u916f\u7ea2\u9709\u7d20\u7247\u7b49\u836f\u7269\u8fdb\u884c\u6297\u611f\u67d3\u9547\u75db\u3002\")\n\n>>> for item in result:\n>>>     if item['entity'] == 1:\n>>>         print(item)\n\n{'entity': 1, 'score': 0.58127016, 'index': 14, 'word': '\u8096', 'start': 13, 'end': 14}\n\n```\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of 9pinus/macbert-base-chinese-medical-collation?", "answers": [{"text": "bert", "answer_start": 180, "answer_end": 183}]}]}]}, {"title": "9pinus/macbert-base-chinese-medicine-recognition", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- Token Classification\nlanguage:\n- zh\n\n---\n\n\n## Model description\nThis model is a fine-tuned version of bert-base-chinese for the purpose of medicine name recognition. We fine-tuned bert-base-chinese on a 500M dataset including 100K+ authorized medical articles on which we labeled all the medicine names. The model achieves 92% accuracy on our test dataset.\n\n## Intended use\n```python\n>>> from transformers import (AutoModelForTokenClassification, AutoTokenizer)\n>>> from transformers import pipeline\n\n>>> hub_model_id = \"9pinus/macbert-base-chinese-medicine-recognition\"\n\n>>> model = AutoModelForTokenClassification.from_pretrained(hub_model_id)\n>>> tokenizer = AutoTokenizer.from_pretrained(hub_model_id)\n>>> classifier = pipeline('ner', model=model, tokenizer=tokenizer)\n>>> result = classifier(\"\u5982\u679c\u75c5\u60c5\u8f83\u91cd\uff0c\u53ef\u9002\u5f53\u53e3\u670d\u7532\u785d\u5511\u7247\u3001\u73af\u916f\u7ea2\u9709\u7d20\u7247\u3001\u5432\u54da\u7f8e\u8f9b\u7247\u7b49\u836f\u7269\u8fdb\u884c\u6297\u611f\u67d3\u9547\u75db\u3002\")\n\n>>> for item in result:\n>>>     if item['entity'] == 1 or item['entity'] == 2:\n>>>         print(item)\n\n{'entity': 1, 'score': 0.99999595, 'index': 13, 'word': '\u7532', 'start': 12, 'end': 13}\n{'entity': 2, 'score': 0.9999957, 'index': 14, 'word': '\u785d', 'start': 13, 'end': 14}\n{'entity': 2, 'score': 0.99999166, 'index': 15, 'word': '\u5511', 'start': 14, 'end': 15}\n{'entity': 2, 'score': 0.99898833, 'index': 16, 'word': '\u7247', 'start': 15, 'end': 16}\n{'entity': 1, 'score': 0.9999864, 'index': 18, 'word': '\u73af', 'start': 17, 'end': 18}\n{'entity': 2, 'score': 0.99999404, 'index': 19, 'word': '\u916f', 'start': 18, 'end': 19}\n{'entity': 2, 'score': 0.99999475, 'index': 20, 'word': '\u7ea2', 'start': 19, 'end': 20}\n{'entity': 2, 'score': 0.9999964, 'index': 21, 'word': '\u9709', 'start': 20, 'end': 21}\n{'entity': 2, 'score': 0.9999951, 'index': 22, 'word': '\u7d20', 'start': 21, 'end': 22}\n{'entity': 2, 'score': 0.9990088, 'index': 23, 'word': '\u7247', 'start': 22, 'end': 23}\n{'entity': 1, 'score': 0.9999975, 'index': 25, 'word': '\u5432', 'start': 24, 'end': 25}\n{'entity': 2, 'score': 0.9999957, 'index': 26, 'word': '\u54da', 'start': 25, 'end': 26}\n{'entity': 2, 'score': 0.9999945, 'index': 27, 'word': '\u7f8e', 'start': 26, 'end': 27}\n{'entity': 2, 'score': 0.9999933, 'index': 28, 'word': '\u8f9b', 'start': 27, 'end': 28}\n{'entity': 2, 'score': 0.99949837, 'index': 29, 'word': '\u7247', 'start': 28, 'end': 29}\n```\n\n## Training and evaluation data\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of 9pinus/macbert-base-chinese-medicine-recognition?", "answers": [{"text": "bert", "answer_start": 134, "answer_end": 137}]}]}]}, {"title": "A-bhimany-u08/bert-base-cased-qqp", "paragraphs": [{"context": "\n                \n               \n---\ninference: False\n\ndatasets:\n- qqp\n---\n\n\n\n\nbert-base-cased model trained on quora question pair dataset. The task requires to predict whether the two given sentences (or questions) are `not_duplicate` (label 0) or `duplicate` (label 1). The model achieves 89% evaluation accuracy\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of A-bhimany-u08/bert-base-cased-qqp?", "answers": [{"text": "bert", "answer_start": 80, "answer_end": 83}]}]}]}, {"title": "ABBHISHEK/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n@Harry Potter DialoGPT model", "qas": [{"id": "q2", "question": "What is the model task of ABBHISHEK/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AG/pretraining", "paragraphs": [{"context": "Pre trained on clus_ chapter only.", "qas": []}]}, {"title": "AI-Growth-Lab/PatentSBERTa", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\n# PatentSBERTa\n\n\n## PatentSBERTa: A Deep NLP based Hybrid Model for Patent Distance and Classification using Augmented SBERT\n\n### Aalborg University Business School, AI: Growth-Lab \n\n\n\n\n\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n<!--- Describe your model here -->\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\ndef cls_pooling(model_output, attention_mask):\n    return model_output[0][:,0]\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('AI-Growth-Lab/PatentSBERTa')\nmodel = AutoModel.from_pretrained('AI-Growth-Lab/PatentSBERTa')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, cls pooling.\nsentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Evaluation Results\n\n<!--- Describe how your model was evaluated -->\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [\n\n\n## Training\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 5 with parameters:\n```\n{'batch_size': 16, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"epochs\": 1,\n    \"evaluation_steps\": 0,\n    \"evaluator\": \"NoneType\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 2e-05\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": null,\n    \"warmup_steps\": 100,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\n```LaTeX\n@article{bekamiri2021patentsberta,\n  title={PatentSBERTa: A Deep NLP based Hybrid Model for Patent Distance and Classification using Augmented SBERT},\n  author={Bekamiri, Hamid and Hain, Daniel S and Jurowetzki, Roman},\n  journal={arXiv preprint arXiv:2103.11933},\n  year={2021}\n}\n```", "qas": [{"id": "q2", "question": "What is the model task of AI-Growth-Lab/PatentSBERTa?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "AI-Lab-Makerere/en_lg", "paragraphs": [{"context": "---\ntags: autonlp\nlanguage: unk\nwidget:\n- text: \"I love AutoNLP \ud83e\udd17\"\ndatasets:\n- Eric Peter/autonlp-data-EN-LUG\nco2_eq_emissions: 133.0219882109991\n---\n\n# Model Trained Using AutoNLP\n\n- Problem type: Machine Translation\n- Model ID: 474612462\n- CO2 Emissions (in grams): 133.0219882109991\n\n## Validation Metrics\n\n- Loss: 1.336498737335205\n- Rouge1: 52.5404\n- Rouge2: 31.6639\n- RougeL: 50.1696\n- RougeLsum: 50.3398\n- Gen Len: 39.046\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_HUGGINGFACE_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoNLP\"}' \n```", "qas": []}]}, {"title": "AI-Lab-Makerere/lg_en", "paragraphs": [{"context": "---\ntags: autonlp\nlanguage: unk\nwidget:\n- text: \"I love AutoNLP \ud83e\udd17\"\ndatasets:\n- EricPeter/autonlp-data-MarianMT_lg_en\nco2_eq_emissions: 126.34446293851818\n---\n\n# Model Trained Using AutoNLP\n\n- Problem type: Machine Translation\n- Model ID: 475112539\n- CO2 Emissions (in grams): 126.34446293851818\n\n## Validation Metrics\n\n- Loss: 1.5376628637313843\n- Rouge1: 62.4613\n- Rouge2: 39.4759\n- RougeL: 58.183\n- RougeLsum: 58.226\n- Gen Len: 26.5644\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_HUGGINGFACE_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoNLP\"}' \n```", "qas": []}]}, {"title": "AI-Nordics/bert-large-swedish-cased", "paragraphs": [{"context": "---\nlanguage: sv\n---\n\n# A Swedish Bert model\n\n## Model description\nThis model follows the Bert Large model architecture as implemented in [Megatron-LM framework]( It was trained with a batch size of 512 in 600k steps. The model contains following parameters:\n<figure>\n\n Value      |\n------------|\n 340M |\n 24    |\n 16         |\n 1024       |\n 30592       |\n\n\n## Training data\nThe model is pretrained on a Swedish text corpus of around 85 GB from a variety of sources as shown below.\n<figure>\n\n Genre      \n------\n Politics  \nPolitics\nPolitics\nMedical\nLegal\nMisc\nLegal\nBooks\nMisc\nPolitics\nDrama\nFacts\n\n\n## Intended uses & limitations\nThe raw model can be used for the usual tasks of masked language modeling or next sentence prediction. It is also often fine-tuned on a downstream task to improve its performance in a specific domain/task.\n<br>\n<br>\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\"AI-Nordics/bert-large-swedish-cased\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"AI-Nordics/bert-large-swedish-cased\")\n\n", "qas": []}]}, {"title": "AI4Sec/cyner-xlm-roberta-base", "paragraphs": [{"context": "---\nlicense: mit\n---\n", "qas": []}]}, {"title": "AI4Sec/cyner-xlm-roberta-large", "paragraphs": [{"context": "---\nlicense: mit\n---\n", "qas": []}]}, {"title": "AIDA-UPM/MSTSb_paraphrase-xlm-r-multilingual-v1", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\n# AIDA-UPM/MSTSb_paraphrase-xlm-r-multilingual-v1\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n<!--- Describe your model here -->\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('{MODEL_NAME}')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\nmodel = AutoModel.from_pretrained('{MODEL_NAME}')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Evaluation Results\n\n<!--- Describe how your model was evaluated -->\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [\n\n\n## Training\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 1438 with parameters:\n```\n{'batch_size': 64, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"callback\": null,\n    \"epochs\": 2,\n    \"evaluation_steps\": 1000,\n    \"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 4e-05\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": null,\n    \"warmup_steps\": 288,\n    \"weight_decay\": 0.1\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->", "qas": [{"id": "q2", "question": "What is the model task of AIDA-UPM/MSTSb_paraphrase-xlm-r-multilingual-v1?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "AIDA-UPM/MSTSb_stsb-xlm-r-multilingual", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\n# {MODEL_NAME}\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n<!--- Describe your model here -->\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('{MODEL_NAME}')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\nmodel = AutoModel.from_pretrained('{MODEL_NAME}')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Evaluation Results\n\n<!--- Describe how your model was evaluated -->\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [\n\n\n## Training\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 1438 with parameters:\n```\n{'batch_size': 64, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"callback\": null,\n    \"epochs\": 1,\n    \"evaluation_steps\": 1000,\n    \"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 4e-05\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": null,\n    \"warmup_steps\": 144,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->", "qas": [{"id": "q2", "question": "What is the model task of AIDA-UPM/MSTSb_stsb-xlm-r-multilingual?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "AIDA-UPM/bertweet-base-multi-mami", "paragraphs": [{"context": "---\npipeline_tag: text-classification\ntags:\n- text-classification\n- misogyny\nlanguage: en\nlicense: apache-2.0\nwidget:\n- text: \"Women wear yoga pants because men don't stare at their personality\"\n  example_title: \"Misogyny detection\"\n---\n\n# bertweet-base-multi-mami\nThis is a Bertweet model: It maps sentences & paragraphs to a 768 dimensional dense vector space and classifies them into 5 multi labels.\n\n# Multilabels\n    label2id={\n        \"misogynous\": 0,\n        \"shaming\": 1,\n        \"stereotype\": 2,\n        \"objectification\": 3,\n        \"violence\": 4,\n    },\n", "qas": [{"id": "q2", "question": "What is the model task of AIDA-UPM/bertweet-base-multi-mami?", "answers": [{"text": "text-classification", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "AIDA-UPM/mstsb-paraphrase-multilingual-mpnet-base-v2", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\nlanguage: \"multilingual\"\ntags:\n- feature-extraction\n- sentence-similarity\n- transformers\n- multilingual\n---\n\n# mstsb-paraphrase-multilingual-mpnet-base-v2\n\nThis is a fine-tuned version of `paraphrase-multilingual-mpnet-base-v2` from [sentence-transformers]( model with [Semantic Textual Similarity Benchmark]( extended to 15 languages: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering, semantic search and measuring the similarity between two sentences.\n\n<!--- Describe your model here -->\nThis model is fine-tuned version of `paraphrase-multilingual-mpnet-base-v2` for semantic textual similarity with multilingual data. The dataset used for this fine-tuning is STSb extended to 15 languages with Google Translator. For mantaining data quality the sentence pairs with a confidence value below 0.7 were dropped. The extended dataset is available at [GitHub]( The languages included in the extended version are: ar, cs, de, en, es, fr, hi, it, ja, nl, pl, pt, ru, tr, zh-CN, zh-TW. The pooling operation used to condense the word embeddings into a sentence embedding is mean pooling (more info below). \n\n<!-- ## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n# It support several languages\nsentences = [\"This is an example sentence\", \"Esta es otra frase de ejemplo\", \"\u6700\u5f8c\u306e\u4f8b\u6587\"]\n\n# The pooling technique is automatically detected (mean pooling)\nmodel = SentenceTransformer('mstsb-paraphrase-multilingual-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n``` -->\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# We should define the proper pooling function: Mean pooling\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = [\"This is an example sentence\", \"Esta es otra frase de ejemplo\", \"\u6700\u5f8c\u306e\u4f8b\u6587\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('AIDA-UPM/mstsb-paraphrase-multilingual-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('AIDA-UPM/mstsb-paraphrase-multilingual-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Evaluation Results\n\n<!--- Describe how your model was evaluated -->\nCheck the test results in the Semantic Textual Similarity Tasks. The 15 languages available at the [Multilingual STSB]( have been combined into monolingual and cross-lingual tasks, giving a total of 31 tasks. Monolingual tasks have both sentences from the same language source (e.g., Ar-Ar, Es-Es), while cross-lingual tasks have two sentences, each in a different language being one of them English (e.g., en-ar, en-es). \n\n\nHere we compare the average multilingual semantic textual similairty capabilities between the  `paraphrase-multilingual-mpnet-base-v2` based model and the `mstsb-paraphrase-multilingual-mpnet-base-v2` fine-tuned model across the 31 tasks. It is worth noting that both models are multilingual, but the second model is adjusted with multilingual data for semantic similarity. The average of correlation coefficients is computed by transforming each correlation coefficient to a Fisher's z value, averaging them, and then back-transforming to a correlation coefficient.\n\n\n Average Spearman Cosine Test |\n:------------------------------:|\n 0.835890                     |\n 0.818896                     |\n\n<br>\n\nThe following tables breakdown the performance of `mstsb-paraphrase-multilingual-mpnet-base-v2` according to the different tasks. For the sake of readability tasks have been splitted into monolingual and cross-lingual tasks. \n\n Pearson Cosine test \n:---------------------:\n 0.868048310692506   \n 0.8267139454193487  \n 0.8466821720942157  \n 0.8517285961812183  \n 0.8519185309064691  \n 0.8430951067985064  \n 0.8178258630578092  \n 0.8475909574305637  \n 0.8435588859386477  \n 0.8486765104527032  \n 0.8407840177883407  \n 0.8534880178249296  \n 0.8390897585455678  \n 0.8382125451820572  \n 0.826233678946644   \n 0.8242683809675422  \n\n<br>\n\n Pearson Cosine test \n:---------------------:\n 0.7990830340462535  \n 0.8381274879061265  \n 0.8414439600928739  \n 0.8442337511356952  \n 0.8378437644605063  \n 0.7951955086055527  \n 0.8415686372978766  \n 0.8094306665283388  \n 0.8389526140129767  \n 0.8261309163979578  \n 0.8475546209070765  \n 0.8248514914263723  \n 0.8191803661207868  \n 0.8147678083378249  \n 0.8107272160374955  \n\n\n## Training\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 687 with parameters:\n```\n{'batch_size': 132, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"callback\": null,\n    \"epochs\": 2,\n    \"evaluation_steps\": 1000,\n    \"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 2e-05\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": null,\n    \"warmup_steps\": 140,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->", "qas": [{"id": "q2", "question": "What is the model task of AIDA-UPM/mstsb-paraphrase-multilingual-mpnet-base-v2?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "AIDynamics/DialoGPT-medium-MentorDealerGuy", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# tests", "qas": [{"id": "q2", "question": "What is the model task of AIDynamics/DialoGPT-medium-MentorDealerGuy?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AJ/DialoGPT-small-ricksanchez", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Uses DialoGPT", "qas": [{"id": "q2", "question": "What is the model task of AJ/DialoGPT-small-ricksanchez?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AJ/rick-discord-bot", "paragraphs": [{"context": "---\ntags:\n- conversational\n- humor\n---\n\n# its rick from rick and morty", "qas": [{"id": "q2", "question": "What is the model task of AJ/rick-discord-bot?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AJ-Dude/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter DialoGPT model", "qas": [{"id": "q2", "question": "What is the model task of AJ-Dude/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AK270802/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of AK270802/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AKulk/wav2vec2-base-timit-epochs10", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: wav2vec2-base-timit-epochs10\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-timit-epochs10\n\nThis model is a fine-tuned version of [AKulk/wav2vec2-base-timit-epochs5]( on the None dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 5\n- total_train_batch_size: 80\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of AKulk/wav2vec2-base-timit-epochs10?", "answers": [{"text": "wav2vec2", "answer_start": 76, "answer_end": 83}]}]}]}, {"title": "AKulk/wav2vec2-base-timit-epochs15", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: wav2vec2-base-timit-epochs15\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-timit-epochs15\n\nThis model is a fine-tuned version of [AKulk/wav2vec2-base-timit-epochs10]( on the None dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 5\n- total_train_batch_size: 80\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of AKulk/wav2vec2-base-timit-epochs15?", "answers": [{"text": "wav2vec2", "answer_start": 76, "answer_end": 83}]}]}]}, {"title": "AKulk/wav2vec2-base-timit-epochs5", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: wav2vec2-base-timit-epochs5\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-timit-epochs5\n\nThis model is a fine-tuned version of [facebook/wav2vec2-lv-60-espeak-cv-ft]( on the None dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 5\n- total_train_batch_size: 80\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of AKulk/wav2vec2-base-timit-epochs5?", "answers": [{"text": "wav2vec2", "answer_start": 76, "answer_end": 83}]}]}]}, {"title": "ARTeLab/it5-summarization-fanpage", "paragraphs": [{"context": "---\ntags:\n- summarization\nlanguage:\n- it\nmetrics:\n- rouge\nmodel-index:\n- name: summarization_fanpage128\n  results: []\ndatasets:\n- ARTeLab/fanpage\n---\n\n# summarization_fanpage128\n\nThis model is a fine-tuned version of [gsarti/it5-base]( on Fanpage dataset for Abstractive Summarization.\n\nIt achieves the following results:\n- Loss: 1.5348\n- Rouge1: 34.1882\n- Rouge2: 15.7866\n- Rougel: 25.141\n- Rougelsum: 28.4882\n- Gen Len: 69.3041\n\n## Usage \n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"ARTeLab/it5-summarization-fanpage-128\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"ARTeLab/it5-summarization-fanpage-128\")\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 3\n- eval_batch_size: 3\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4.0\n\n### Framework versions\n\n- Transformers 4.12.0.dev0\n- Pytorch 1.9.1+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n\n# Citation\n\nMore details and results in [published work](\n\n```\n@Article{info13050228,\n    AUTHOR = {Landro, Nicola and Gallo, Ignazio and La Grassa, Riccardo and Federici, Edoardo},\n    TITLE = {Two New Datasets for Italian-Language Abstractive Text Summarization},\n    JOURNAL = {Information},\n    VOLUME = {13},\n    YEAR = {2022},\n    NUMBER = {5},\n    ARTICLE-NUMBER = {228},\n    URL = {\n    ISSN = {2078-2489},\n    ABSTRACT = {Text summarization aims to produce a short summary containing relevant parts from a given text. Due to the lack of data for abstractive summarization on low-resource languages such as Italian, we propose two new original datasets collected from two Italian news websites with multi-sentence summaries and corresponding articles, and from a dataset obtained by machine translation of a Spanish summarization dataset. These two datasets are currently the only two available in Italian for this task. To evaluate the quality of these two datasets, we used them to train a T5-base model and an mBART model, obtaining good results with both. To better evaluate the results obtained, we also compared the same models trained on automatically translated datasets, and the resulting summaries in the same training language, with the automatically translated summaries, which demonstrated the superiority of the models obtained from the proposed datasets.},\n    DOI = {10.3390/info13050228}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of ARTeLab/it5-summarization-fanpage?", "answers": [{"text": "t5", "answer_start": 226, "answer_end": 227}]}, {"id": "q2", "question": "What is the model task of ARTeLab/it5-summarization-fanpage?", "answers": [{"text": "summarization", "answer_start": 12, "answer_end": 24}]}]}]}, {"title": "ARTeLab/it5-summarization-ilpost", "paragraphs": [{"context": "---\ntags:\n- summarization\nlanguage:\n- it\nmetrics:\n- rouge\nmodel-index:\n- name: summarization_ilpost\n  results: []\ndatasets:\n- ARTeLab/ilpost\n---\n\n# summarization_ilpost\n\nThis model is a fine-tuned version of [gsarti/it5-base]( on IlPost dataset for Abstractive Summarization.\n\nIt achieves the following results:\n- Loss: 1.6020\n- Rouge1: 33.7802\n- Rouge2: 16.2953\n- Rougel: 27.4797\n- Rougelsum: 30.2273\n- Gen Len: 45.3175\n\n## Usage \n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"ARTeLab/it5-summarization-ilpost\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"ARTeLab/it5-summarization-ilpost\")\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 6\n- eval_batch_size: 6\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4.0\n\n### Framework versions\n- Transformers 4.12.0.dev0\n- Pytorch 1.9.1+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3", "qas": [{"id": "q1", "question": "What is the model architecture of ARTeLab/it5-summarization-ilpost?", "answers": [{"text": "t5", "answer_start": 217, "answer_end": 218}]}, {"id": "q2", "question": "What is the model task of ARTeLab/it5-summarization-ilpost?", "answers": [{"text": "summarization", "answer_start": 12, "answer_end": 24}]}]}]}, {"title": "ARTeLab/it5-summarization-mlsum", "paragraphs": [{"context": "---\ntags:\n- summarization\nlanguage:\n- it\nmetrics:\n- rouge\nmodel-index:\n- name: summarization_mlsum\n  results: []\ndatasets:\n- ARTeLab/mlsum-it\n---\n\n# summarization_mlsum\n\nThis model is a fine-tuned version of [gsarti/it5-base]( on MLSum-it for Abstractive Summarization.\n\nIt achieves the following results:\n- Loss: 2.0190\n- Rouge1: 19.3739\n- Rouge2: 5.9753\n- Rougel: 16.691\n- Rougelsum: 16.7862\n- Gen Len: 32.5268\n\n## Usage \n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"ARTeLab/it5-summarization-mlsum\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"ARTeLab/it5-summarization-mlsum\")\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 6\n- eval_batch_size: 6\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4.0\n\n### Framework versions\n\n- Transformers 4.12.0.dev0\n- Pytorch 1.9.1+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n\n# Citation\n\nMore details and results in [published work](\n\n```\n@Article{info13050228,\n    AUTHOR = {Landro, Nicola and Gallo, Ignazio and La Grassa, Riccardo and Federici, Edoardo},\n    TITLE = {Two New Datasets for Italian-Language Abstractive Text Summarization},\n    JOURNAL = {Information},\n    VOLUME = {13},\n    YEAR = {2022},\n    NUMBER = {5},\n    ARTICLE-NUMBER = {228},\n    URL = {\n    ISSN = {2078-2489},\n    ABSTRACT = {Text summarization aims to produce a short summary containing relevant parts from a given text. Due to the lack of data for abstractive summarization on low-resource languages such as Italian, we propose two new original datasets collected from two Italian news websites with multi-sentence summaries and corresponding articles, and from a dataset obtained by machine translation of a Spanish summarization dataset. These two datasets are currently the only two available in Italian for this task. To evaluate the quality of these two datasets, we used them to train a T5-base model and an mBART model, obtaining good results with both. To better evaluate the results obtained, we also compared the same models trained on automatically translated datasets, and the resulting summaries in the same training language, with the automatically translated summaries, which demonstrated the superiority of the models obtained from the proposed datasets.},\n    DOI = {10.3390/info13050228}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of ARTeLab/it5-summarization-mlsum?", "answers": [{"text": "t5", "answer_start": 217, "answer_end": 218}]}, {"id": "q2", "question": "What is the model task of ARTeLab/it5-summarization-mlsum?", "answers": [{"text": "summarization", "answer_start": 12, "answer_end": 24}]}]}]}, {"title": "ARTeLab/mbart-summarization-fanpage", "paragraphs": [{"context": "---\ntags:\n- summarization\nlanguage:\n- it\nmetrics:\n- rouge\nmodel-index:\n- name: summarization_mbart_fanpage4epoch\n  results: []\ndatasets:\n- ARTeLab/fanpage\n---\n\n# mbart-summarization-fanpage\n\nThis model is a fine-tuned version of [facebook/mbart-large-cc25]( on Fanpage dataset for Abstractive Summarization.\n\nIt achieves the following results:\n- Loss: 2.1833\n- Rouge1: 36.5027\n- Rouge2: 17.4428\n- Rougel: 26.1734\n- Rougelsum: 30.2636\n- Gen Len: 75.2413\n\n## Usage \n\n```python\nfrom transformers import MBartTokenizer, MBartForConditionalGeneration\ntokenizer = MBartTokenizer.from_pretrained(\"ARTeLab/mbart-summarization-fanpage\")\nmodel = MBartForConditionalGeneration.from_pretrained(\"ARTeLab/mbart-summarization-fanpage\")\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4.0\n\n### Framework versions\n\n- Transformers 4.15.0.dev0\n- Pytorch 1.10.0+cu102\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n\n# Citation\n\nMore details and results in [published work](\n\n```\n@Article{info13050228,\n    AUTHOR = {Landro, Nicola and Gallo, Ignazio and La Grassa, Riccardo and Federici, Edoardo},\n    TITLE = {Two New Datasets for Italian-Language Abstractive Text Summarization},\n    JOURNAL = {Information},\n    VOLUME = {13},\n    YEAR = {2022},\n    NUMBER = {5},\n    ARTICLE-NUMBER = {228},\n    URL = {\n    ISSN = {2078-2489},\n    ABSTRACT = {Text summarization aims to produce a short summary containing relevant parts from a given text. Due to the lack of data for abstractive summarization on low-resource languages such as Italian, we propose two new original datasets collected from two Italian news websites with multi-sentence summaries and corresponding articles, and from a dataset obtained by machine translation of a Spanish summarization dataset. These two datasets are currently the only two available in Italian for this task. To evaluate the quality of these two datasets, we used them to train a T5-base model and an mBART model, obtaining good results with both. To better evaluate the results obtained, we also compared the same models trained on automatically translated datasets, and the resulting summaries in the same training language, with the automatically translated summaries, which demonstrated the superiority of the models obtained from the proposed datasets.},\n    DOI = {10.3390/info13050228}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of ARTeLab/mbart-summarization-fanpage?", "answers": [{"text": "mbart", "answer_start": 93, "answer_end": 97}]}, {"id": "q2", "question": "What is the model task of ARTeLab/mbart-summarization-fanpage?", "answers": [{"text": "summarization", "answer_start": 12, "answer_end": 24}]}]}]}, {"title": "ARTeLab/mbart-summarization-ilpost", "paragraphs": [{"context": "---\ntags:\n- summarization\nlanguage:\n- it\nmetrics:\n- rouge\nmodel-index:\n- name: summarization_mbart_ilpost\n  results: []\ndatasets:\n- ARTeLab/ilpost\n---\n\n# mbart_summarization_ilpost\n\nThis model is a fine-tuned version of [facebook/mbart-large-cc25]( on IlPost dataset for Abstractive Summarization.\n\nIt achieves the following results:\n- Loss: 2.3640\n- Rouge1: 38.9101\n- Rouge2: 21.384\n- Rougel: 32.0517\n- Rougelsum: 35.0743\n- Gen Len: 39.8843\n\n## Usage \n\n```python\nfrom transformers import MBartTokenizer, MBartForConditionalGeneration\ntokenizer = MBartTokenizer.from_pretrained(\"ARTeLab/mbart-summarization-ilpost\")\nmodel = MBartForConditionalGeneration.from_pretrained(\"ARTeLab/mbart-summarization-ilpost\")\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4.0\n\n### Framework versions\n\n- Transformers 4.15.0.dev0\n- Pytorch 1.10.0+cu102\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n\n# Citation\n\nMore details and results in [published work](\n\n```\n@Article{info13050228,\n    AUTHOR = {Landro, Nicola and Gallo, Ignazio and La Grassa, Riccardo and Federici, Edoardo},\n    TITLE = {Two New Datasets for Italian-Language Abstractive Text Summarization},\n    JOURNAL = {Information},\n    VOLUME = {13},\n    YEAR = {2022},\n    NUMBER = {5},\n    ARTICLE-NUMBER = {228},\n    URL = {\n    ISSN = {2078-2489},\n    ABSTRACT = {Text summarization aims to produce a short summary containing relevant parts from a given text. Due to the lack of data for abstractive summarization on low-resource languages such as Italian, we propose two new original datasets collected from two Italian news websites with multi-sentence summaries and corresponding articles, and from a dataset obtained by machine translation of a Spanish summarization dataset. These two datasets are currently the only two available in Italian for this task. To evaluate the quality of these two datasets, we used them to train a T5-base model and an mBART model, obtaining good results with both. To better evaluate the results obtained, we also compared the same models trained on automatically translated datasets, and the resulting summaries in the same training language, with the automatically translated summaries, which demonstrated the superiority of the models obtained from the proposed datasets.},\n    DOI = {10.3390/info13050228}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of ARTeLab/mbart-summarization-ilpost?", "answers": [{"text": "mbart", "answer_start": 93, "answer_end": 97}]}, {"id": "q2", "question": "What is the model task of ARTeLab/mbart-summarization-ilpost?", "answers": [{"text": "summarization", "answer_start": 12, "answer_end": 24}]}]}]}, {"title": "ARTeLab/mbart-summarization-mlsum", "paragraphs": [{"context": "---\ntags:\n- summarization\nlanguage:\n- it\nmetrics:\n- rouge\nmodel-index:\n- name: summarization_mbart_mlsum\n  results: []\ndatasets:\n- ARTeLab/mlsum-it\n---\n\n# mbart_summarization_mlsum\n\nThis model is a fine-tuned version of [facebook/mbart-large-cc25]( on mlsum-it for Abstractive Summarization.\n\nIt achieves the following results:\n- Loss: 3.3336\n- Rouge1: 19.3489\n- Rouge2: 6.4028\n- Rougel: 16.3497\n- Rougelsum: 16.5387\n- Gen Len: 33.5945\n\n## Usage \n\n```python\nfrom transformers import MBartTokenizer, MBartForConditionalGeneration\ntokenizer = MBartTokenizer.from_pretrained(\"ARTeLab/mbart-summarization-mlsum\")\nmodel = MBartForConditionalGeneration.from_pretrained(\"ARTeLab/mbart-summarization-mlsum\")\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4.0\n\n### Framework versions\n\n- Transformers 4.15.0.dev0\n- Pytorch 1.10.0+cu102\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n\n# Citation\n\nMore details and results in [published work](\n\n```\n@Article{info13050228,\n    AUTHOR = {Landro, Nicola and Gallo, Ignazio and La Grassa, Riccardo and Federici, Edoardo},\n    TITLE = {Two New Datasets for Italian-Language Abstractive Text Summarization},\n    JOURNAL = {Information},\n    VOLUME = {13},\n    YEAR = {2022},\n    NUMBER = {5},\n    ARTICLE-NUMBER = {228},\n    URL = {\n    ISSN = {2078-2489},\n    ABSTRACT = {Text summarization aims to produce a short summary containing relevant parts from a given text. Due to the lack of data for abstractive summarization on low-resource languages such as Italian, we propose two new original datasets collected from two Italian news websites with multi-sentence summaries and corresponding articles, and from a dataset obtained by machine translation of a Spanish summarization dataset. These two datasets are currently the only two available in Italian for this task. To evaluate the quality of these two datasets, we used them to train a T5-base model and an mBART model, obtaining good results with both. To better evaluate the results obtained, we also compared the same models trained on automatically translated datasets, and the resulting summaries in the same training language, with the automatically translated summaries, which demonstrated the superiority of the models obtained from the proposed datasets.},\n    DOI = {10.3390/info13050228}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of ARTeLab/mbart-summarization-mlsum?", "answers": [{"text": "mbart", "answer_start": 93, "answer_end": 97}]}, {"id": "q2", "question": "What is the model task of ARTeLab/mbart-summarization-mlsum?", "answers": [{"text": "summarization", "answer_start": 12, "answer_end": 24}]}]}]}, {"title": "ASCCCCCCCC/PENGMENGJIE-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel_index:\n- name: PENGMENGJIE-finetuned-emotion\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# PENGMENGJIE-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on an unkown dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Framework versions\n\n- Transformers 4.9.0\n- Pytorch 1.7.1+cpu\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ASCCCCCCCC/PENGMENGJIE-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 456, "answer_end": 465}]}, {"id": "q2", "question": "What is the model task of ASCCCCCCCC/PENGMENGJIE-finetuned-emotion?", "answers": [{"text": "text-classification", "answer_start": 171, "answer_end": 189}]}]}]}, {"title": "ASCCCCCCCC/bert-base-chinese-finetuned-amazon_zh_20000", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: bert-base-chinese-finetuned-amazon_zh_20000\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-chinese-finetuned-amazon_zh_20000\n\nThis model is a fine-tuned version of [bert-base-chinese]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1683\n- Accuracy: 0.5224\n- F1: 0.5194\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    1.1717           0.4847 |\n 2.0    1.1683           0.5194 |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.9.1\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ASCCCCCCCC/bert-base-chinese-finetuned-amazon_zh_20000?", "answers": [{"text": "bert", "answer_start": 81, "answer_end": 84}]}]}]}, {"title": "ASCCCCCCCC/distilbert-base-chinese-amazon_zh_20000", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-chinese-amazon_zh_20000\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-chinese-amazon_zh_20000\n\nThis model is a fine-tuned version of [bert-base-chinese]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1518\n- Accuracy: 0.5092\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1518          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.9.1\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ASCCCCCCCC/distilbert-base-chinese-amazon_zh_20000?", "answers": [{"text": "bert", "answer_start": 82, "answer_end": 85}]}]}]}, {"title": "ASCCCCCCCC/distilbert-base-multilingual-cased-amazon_zh_20000", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-multilingual-cased-amazon_zh_20000\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-multilingual-cased-amazon_zh_20000\n\nThis model is a fine-tuned version of [distilbert-base-multilingual-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.3031\n- Accuracy: 0.4406\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.3031          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.9.1\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ASCCCCCCCC/distilbert-base-multilingual-cased-amazon_zh_20000?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "ASCCCCCCCC/distilbert-base-uncased-finetuned-amazon_zh_20000", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased-finetuned-amazon_zh_20000\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-amazon_zh_20000\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.3516\n- Accuracy: 0.414\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.3516          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.9.1\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ASCCCCCCCC/distilbert-base-uncased-finetuned-amazon_zh_20000?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "ASCCCCCCCC/distilbert-base-uncased-finetuned-clinc", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel_index:\n- name: distilbert-base-uncased-finetuned-clinc\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-clinc\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on an unkown dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 48\n- eval_batch_size: 48\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n\n### Framework versions\n\n- Transformers 4.9.0\n- Pytorch 1.7.1+cpu\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ASCCCCCCCC/distilbert-base-uncased-finetuned-clinc?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}, {"id": "q2", "question": "What is the model task of ASCCCCCCCC/distilbert-base-uncased-finetuned-clinc?", "answers": [{"text": "text-classification", "answer_start": 181, "answer_end": 199}]}]}]}, {"title": "AT/distilroberta-base-finetuned-wikitext2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: distilroberta-base-finetuned-wikitext2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilroberta-base-finetuned-wikitext2\n\nThis model is a fine-tuned version of [distilroberta-base]( on the None dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 80.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of AT/distilroberta-base-finetuned-wikitext2?", "answers": [{"text": "roberta", "answer_start": 82, "answer_end": 88}]}]}]}, {"title": "ATGdev/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of ATGdev/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AVSilva/bertimbau-large-fine-tuned-md", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: result\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# result\n\nThis model is a fine-tuned version of [neuralmind/bert-large-portuguese-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7458\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.13.0.dev0\n- Pytorch 1.10.0+cu102\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of AVSilva/bertimbau-large-fine-tuned-md?", "answers": [{"text": "bert", "answer_start": 343, "answer_end": 346}]}]}]}, {"title": "AVSilva/bertimbau-large-fine-tuned-sd", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: result\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# result\n\nThis model is a fine-tuned version of [neuralmind/bert-large-portuguese-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7570\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.13.0.dev0\n- Pytorch 1.10.0+cu102\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of AVSilva/bertimbau-large-fine-tuned-sd?", "answers": [{"text": "bert", "answer_start": 343, "answer_end": 346}]}]}]}, {"title": "AVeryRealHuman/DialoGPT-small-TonyStark", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#Tony Stark DialoGPT model", "qas": [{"id": "q2", "question": "What is the model task of AVeryRealHuman/DialoGPT-small-TonyStark?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AWTStress/stress_classifier", "paragraphs": [{"context": "---\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: tmp_znj9o4r\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# tmp_znj9o4r\n\nThis model was trained from scratch on an unknown dataset.\nIt achieves the following results on the evaluation set:\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: None\n- training_precision: float32\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- TensorFlow 2.8.0\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": []}]}, {"title": "AWTStress/stress_score", "paragraphs": [{"context": "---\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: stress_score\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# stress_score\n\nThis model was trained from scratch on an unknown dataset.\nIt achieves the following results on the evaluation set:\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: None\n- training_precision: float32\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- TensorFlow 2.8.0\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": []}]}, {"title": "AbhinavSaiTheGreat/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational \n---\n\n#HarryPotter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of AbhinavSaiTheGreat/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Abirate/bert_fine_tuned_cola", "paragraphs": [{"context": "\n## Petrained Model BERT: base model (cased)\nBERT base model (cased) is a pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in this [paper]( and first released in this [repository]( This model is case-sensitive: it makes a difference between english and English.\n\n\n\n## Pretained Model Description\nBERT is an auto-encoder transformer model pretrained on a large corpus of English data (English Wikipedia + Books Corpus) in a self-supervised fashion. This means the targets are computed from the inputs themselves, and humans are not needed to label the data. It was pretrained with two objectives:\n\n- Masked language modeling (MLM)\n- Next sentence prediction (NSP)\n\n## Fine-tuned Model Description: BERT fine-tuned Cola\nThe pretrained model could be fine-tuned on other NLP tasks. The BERT model has been fine-tuned on a cola dataset from the GLUE BENCHAMRK, which is an academic benchmark that aims to measure the performance of ML models. Cola is one of the 11 datasets in this GLUE BENCHMARK.\u00a0\n\nBy fine-tuning BERT on cola dataset, the model is now able to classify a given setence gramatically and semantically as acceptable or not acceptable\n\n## How to use ?\n###### Directly with a pipeline for a text-classification NLP task\n```python\nfrom transformers import pipeline\ncola = pipeline('text-classification', model='Abirate/bert_fine_tuned_cola')\ncola(\"Tunisia is a beautiful country\")\n\n[{'label': 'acceptable', 'score': 0.989352285861969}]\n```\n###### Breaking down all the steps (Tokenization, Modeling, Postprocessing)\n```python\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nimport tensorflow as tf\nimport numpy as np  \n\ntokenizer = AutoTokenizer.from_pretrained('Abirate/bert_fine_tuned_cola')\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"Abirate/bert_fine_tuned_cola\")\ntext = \"Tunisia is a beautiful country.\"\nencoded_input = tokenizer(text, return_tensors='tf')\n\n#The logits\noutput = model(encoded_input)\n#Postprocessing\nprobas_output = tf.math.softmax(tf.squeeze(output['logits']), axis = -1)\nclass_preds = np.argmax(probas_output, axis = -1)\n\n#Predicting the class acceptable or not acceptable\nmodel.config.id2label[class_preds]\n\n#Result \n'acceptable'\n```", "qas": [{"id": "q1", "question": "What is the model architecture of Abirate/bert_fine_tuned_cola?", "answers": [{"text": "bert", "answer_start": 1383, "answer_end": 1386}]}, {"id": "q2", "question": "What is the model task of Abirate/bert_fine_tuned_cola?", "answers": [{"text": "text-classification", "answer_start": 1256, "answer_end": 1274}]}]}]}, {"title": "AccurateIsaiah/DialoGPT-small-jefftastic", "paragraphs": [{"context": "---\ntags:\n- conversational \n---\n\n\n# jeff's 100% authorized brain scan ", "qas": [{"id": "q2", "question": "What is the model task of AccurateIsaiah/DialoGPT-small-jefftastic?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AccurateIsaiah/DialoGPT-small-mozark", "paragraphs": [{"context": "---\ntags:\n- conversational \n---\n\n\n# Mozark's Brain Uploaded to Hugging Face", "qas": [{"id": "q2", "question": "What is the model task of AccurateIsaiah/DialoGPT-small-mozark?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AccurateIsaiah/DialoGPT-small-mozarkv2", "paragraphs": [{"context": "---\ntags:\n- conversational \n---\n\n\n# Mozark's Brain Uploaded to Hugging Face but v2", "qas": [{"id": "q2", "question": "What is the model task of AccurateIsaiah/DialoGPT-small-mozarkv2?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "AccurateIsaiah/DialoGPT-small-sinclair", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Un Filtered brain upload of sinclair ", "qas": [{"id": "q2", "question": "What is the model task of AccurateIsaiah/DialoGPT-small-sinclair?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "ActivationAI/distilbert-base-uncased-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotion\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: default\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.928\n    - name: F1\n      type: f1\n      value: 0.9280065074208208\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2128\n- Accuracy: 0.928\n- F1: 0.9280\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.3043           0.9035 |\n 2.0    0.2128           0.9280 |\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ActivationAI/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 121, "answer_end": 130}]}, {"id": "q2", "question": "What is the model task of ActivationAI/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "text-classification", "answer_start": 228, "answer_end": 246}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-anli_r3", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapter-transformers\ndatasets:\n- anli\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-anli_r3` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [anli]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-anli_r3\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-anli_r3?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-anli_r3?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-boolq", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:qa/boolq\n- adapter-transformers\ndatasets:\n- boolq\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-boolq` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [qa/boolq]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-boolq\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-boolq?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-boolq?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-cola", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:lingaccept/cola\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-cola` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [lingaccept/cola]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-cola\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-cola?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-cola?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-comqa", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapter-transformers\ndatasets:\n- com_qa\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-comqa` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [com_qa]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-comqa\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-comqa?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-comqa?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-conll2000", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapterhub:chunk/conll2000\n- adapter-transformers\ndatasets:\n- conll2000\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-conll2000` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [chunk/conll2000]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-conll2000\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-conll2000?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-conll2000?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-conll2003", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapterhub:ner/conll2003\n- adapter-transformers\ndatasets:\n- conll2003\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-conll2003` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [ner/conll2003]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-conll2003\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-conll2003?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-conll2003?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-conll2003_pos", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapterhub:pos/conll2003\n- adapter-transformers\ndatasets:\n- conll2003\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-conll2003_pos` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [pos/conll2003]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-conll2003_pos\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-conll2003_pos?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-conll2003_pos?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-cq", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapterhub:qa/cq\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-cq` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [qa/cq]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-cq\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-cq?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-cq?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-drop", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapter-transformers\ndatasets:\n- drop\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-drop` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [drop]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-drop\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-drop?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-drop?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-duorc_p", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapter-transformers\ndatasets:\n- duorc\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-duorc_p` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [duorc]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-duorc_p\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-duorc_p?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-duorc_p?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-duorc_s", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapter-transformers\ndatasets:\n- duorc\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-duorc_s` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [duorc]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-duorc_s\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-duorc_s?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-duorc_s?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-emo", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapter-transformers\ndatasets:\n- emo\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-emo` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [emo]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-emo\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-emo?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-emo?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-emotion", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapter-transformers\ndatasets:\n- emotion\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-emotion` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [emotion]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-emotion\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-emotion?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-emotion?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-fce_error_detection", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapterhub:ged/fce\n- adapter-transformers\ndatasets:\n- fce_error_detection\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-fce_error_detection` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [ged/fce]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-fce_error_detection\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-fce_error_detection?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-fce_error_detection?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-hotpotqa", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapter-transformers\ndatasets:\n- hotpot_qa\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-hotpotqa` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [hotpot_qa]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-hotpotqa\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-hotpotqa?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-hotpotqa?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-imdb", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:sentiment/imdb\n- adapter-transformers\ndatasets:\n- imdb\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-imdb` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [sentiment/imdb]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-imdb\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-imdb?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-imdb?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-mit_movie_trivia", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapterhub:ner/mit_movie_trivia\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-mit_movie_trivia` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [ner/mit_movie_trivia]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-mit_movie_trivia\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-mit_movie_trivia?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-mit_movie_trivia?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-mnli", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:nli/multinli\n- adapter-transformers\ndatasets:\n- multi_nli\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-mnli` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [nli/multinli]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-mnli\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-mnli?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-mnli?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-mrpc", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:sts/mrpc\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-mrpc` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [sts/mrpc]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-mrpc\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-mrpc?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-mrpc?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-multirc", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- adapterhub:rc/multirc\n- bert\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-multirc` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [rc/multirc]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-multirc\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-multirc?", "answers": [{"text": "bert", "answer_start": 58, "answer_end": 61}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-multirc?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-newsqa", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapter-transformers\ndatasets:\n- newsqa\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-newsqa` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [newsqa]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-newsqa\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-newsqa?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-newsqa?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-pmb_sem_tagging", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapterhub:semtag/pmb\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-pmb_sem_tagging` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [semtag/pmb]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-pmb_sem_tagging\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-pmb_sem_tagging?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-pmb_sem_tagging?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-qnli", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:nli/qnli\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-qnli` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [nli/qnli]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-qnli\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-qnli?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-qnli?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-qqp", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- adapter-transformers\n- adapterhub:sts/qqp\n- bert\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-qqp` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [sts/qqp]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-qqp\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-qqp?", "answers": [{"text": "bert", "answer_start": 78, "answer_end": 81}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-qqp?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-quoref", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapter-transformers\ndatasets:\n- quoref\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-quoref` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [quoref]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-quoref\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-quoref?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-quoref?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-record", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:rc/record\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-record` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [rc/record]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-record\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-record?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-record?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-rotten_tomatoes", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:sentiment/rotten_tomatoes\n- adapter-transformers\ndatasets:\n- rotten_tomatoes\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-rotten_tomatoes` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [sentiment/rotten_tomatoes]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-rotten_tomatoes\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-rotten_tomatoes?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-rotten_tomatoes?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-rte", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:nli/rte\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-rte` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [nli/rte]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-rte\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-rte?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-rte?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-scicite", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapter-transformers\ndatasets:\n- scicite\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-scicite` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [scicite]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-scicite\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-scicite?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-scicite?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-scitail", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:nli/scitail\n- adapter-transformers\ndatasets:\n- scitail\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-scitail` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [nli/scitail]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-scitail\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-scitail?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-scitail?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-sick", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- adapter-transformers\n- bert\n- adapterhub:nli/sick\ndatasets:\n- sick\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-sick` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [nli/sick]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-sick\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-sick?", "answers": [{"text": "bert", "answer_start": 57, "answer_end": 60}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-sick?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-snli", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapter-transformers\ndatasets:\n- snli\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-snli` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [snli]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-snli\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-snli?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-snli?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-squad", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapterhub:qa/squad1\n- adapter-transformers\ndatasets:\n- squad\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-squad` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [qa/squad1]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-squad\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-squad?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-squad?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-squad_v2", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapterhub:qa/squad2\n- adapter-transformers\ndatasets:\n- squad_v2\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-squad_v2` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [qa/squad2]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-squad_v2\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-squad_v2?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-squad_v2?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-sst2", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:sentiment/sst-2\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-sst2` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [sentiment/sst-2]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-sst2\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-sst2?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-sst2?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-stsb", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:sts/sts-b\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-stsb` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [sts/sts-b]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-stsb\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-stsb?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-stsb?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-trec", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapter-transformers\ndatasets:\n- trec\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-trec` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [trec]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-trec\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-trec?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-trec?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-ud_deprel", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapterhub:deprel/ud_ewt\n- adapter-transformers\ndatasets:\n- universal_dependencies\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-ud_deprel` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [deprel/ud_ewt]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-ud_deprel\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-ud_deprel?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-ud_deprel?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-ud_pos", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapterhub:pos/ud_ewt\n- adapter-transformers\ndatasets:\n- universal_dependencies\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-ud_pos` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [pos/ud_ewt]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-ud_pos\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-ud_pos?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-ud_pos?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-wic", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapterhub:wordsence/wic\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-wic` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [wordsence/wic]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-wic\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-wic?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-wic?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-wikihop", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\n- adapterhub:qa/wikihop\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-wikihop` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [qa/wikihop]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-wikihop\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-wikihop?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-wikihop?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-wnut_17", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- bert\n- adapter-transformers\ndatasets:\n- wnut_17\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-wnut_17` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [wnut_17]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-wnut_17\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-wnut_17?", "answers": [{"text": "bert", "answer_start": 35, "answer_end": 38}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-wnut_17?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/bert-base-uncased-pf-yelp_polarity", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- bert\n- adapter-transformers\ndatasets:\n- yelp_polarity\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/bert-base-uncased-pf-yelp_polarity` for bert-base-uncased\n\nAn [adapter]( for the `bert-base-uncased` model that was trained on the [yelp_polarity]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\nadapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-yelp_polarity\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/bert-base-uncased-pf-yelp_polarity?", "answers": [{"text": "bert", "answer_start": 34, "answer_end": 37}]}, {"id": "q2", "question": "What is the model task of AdapterHub/bert-base-uncased-pf-yelp_polarity?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/roberta-base-pf-anli_r3", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- roberta\n- adapter-transformers\ndatasets:\n- anli\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-anli_r3` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [anli]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-anli_r3\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-anli_r3?", "answers": [{"text": "roberta", "answer_start": 34, "answer_end": 40}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-anli_r3?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/roberta-base-pf-boolq", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- roberta\n- adapterhub:qa/boolq\n- adapter-transformers\ndatasets:\n- boolq\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-boolq` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [qa/boolq]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-boolq\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-boolq?", "answers": [{"text": "roberta", "answer_start": 34, "answer_end": 40}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-boolq?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/roberta-base-pf-cola", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- roberta\n- adapterhub:lingaccept/cola\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-cola` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [lingaccept/cola]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-cola\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-cola?", "answers": [{"text": "roberta", "answer_start": 34, "answer_end": 40}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-cola?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/roberta-base-pf-comqa", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- roberta\n- adapter-transformers\ndatasets:\n- com_qa\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-comqa` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [com_qa]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-comqa\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-comqa?", "answers": [{"text": "roberta", "answer_start": 33, "answer_end": 39}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-comqa?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/roberta-base-pf-conll2000", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- roberta\n- adapterhub:chunk/conll2000\n- adapter-transformers\ndatasets:\n- conll2000\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-conll2000` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [chunk/conll2000]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-conll2000\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-conll2000?", "answers": [{"text": "roberta", "answer_start": 35, "answer_end": 41}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-conll2000?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/roberta-base-pf-conll2003", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- roberta\n- adapterhub:ner/conll2003\n- adapter-transformers\ndatasets:\n- conll2003\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-conll2003` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [ner/conll2003]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-conll2003\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-conll2003?", "answers": [{"text": "roberta", "answer_start": 35, "answer_end": 41}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-conll2003?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/roberta-base-pf-conll2003_pos", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- roberta\n- adapterhub:pos/conll2003\n- adapter-transformers\n- token-classification\ndatasets:\n- conll2003\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-conll2003_pos` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [pos/conll2003]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-conll2003_pos\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-conll2003_pos?", "answers": [{"text": "roberta", "answer_start": 35, "answer_end": 41}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-conll2003_pos?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/roberta-base-pf-cq", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- roberta\n- adapterhub:qa/cq\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-cq` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [qa/cq]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-cq\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-cq?", "answers": [{"text": "roberta", "answer_start": 33, "answer_end": 39}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-cq?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/roberta-base-pf-drop", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- roberta\n- adapter-transformers\ndatasets:\n- drop\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-drop` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [drop]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-drop\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-drop?", "answers": [{"text": "roberta", "answer_start": 33, "answer_end": 39}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-drop?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/roberta-base-pf-duorc_p", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- roberta\n- adapter-transformers\ndatasets:\n- duorc\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-duorc_p` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [duorc]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-duorc_p\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-duorc_p?", "answers": [{"text": "roberta", "answer_start": 33, "answer_end": 39}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-duorc_p?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/roberta-base-pf-duorc_s", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- roberta\n- adapter-transformers\ndatasets:\n- duorc\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-duorc_s` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [duorc]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-duorc_s\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-duorc_s?", "answers": [{"text": "roberta", "answer_start": 33, "answer_end": 39}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-duorc_s?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/roberta-base-pf-emo", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- roberta\n- adapter-transformers\ndatasets:\n- emo\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-emo` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [emo]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-emo\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-emo?", "answers": [{"text": "roberta", "answer_start": 34, "answer_end": 40}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-emo?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/roberta-base-pf-emotion", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- roberta\n- adapter-transformers\ndatasets:\n- emotion\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-emotion` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [emotion]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-emotion\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-emotion?", "answers": [{"text": "roberta", "answer_start": 34, "answer_end": 40}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-emotion?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/roberta-base-pf-fce_error_detection", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- roberta\n- adapterhub:ged/fce\n- adapter-transformers\ndatasets:\n- fce_error_detection\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-fce_error_detection` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [ged/fce]( dataset and includes a prediction head for tagging.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-fce_error_detection\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-fce_error_detection?", "answers": [{"text": "roberta", "answer_start": 35, "answer_end": 41}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-fce_error_detection?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "AdapterHub/roberta-base-pf-multirc", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- adapterhub:rc/multirc\n- roberta\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-multirc` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [rc/multirc]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-multirc\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-multirc?", "answers": [{"text": "roberta", "answer_start": 58, "answer_end": 64}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-multirc?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "AdapterHub/roberta-base-pf-newsqa", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- roberta\n- adapter-transformers\ndatasets:\n- newsqa\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-newsqa` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [newsqa]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-newsqa\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-newsqa?", "answers": [{"text": "roberta", "answer_start": 33, "answer_end": 39}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-newsqa?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/roberta-base-pf-quoref", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- roberta\n- adapter-transformers\ndatasets:\n- quoref\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-quoref` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [quoref]( dataset and includes a prediction head for question answering.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-quoref\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-quoref?", "answers": [{"text": "roberta", "answer_start": 33, "answer_end": 39}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-quoref?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "AdapterHub/roberta-base-pf-sst2", "paragraphs": [{"context": "---\ntags:\n- text-classification\n- roberta\n- adapterhub:sentiment/sst-2\n- adapter-transformers\nlanguage:\n- en\n---\n\n# Adapter `AdapterHub/roberta-base-pf-sst2` for roberta-base\n\nAn [adapter]( for the `roberta-base` model that was trained on the [sentiment/sst-2]( dataset and includes a prediction head for classification.\n\nThis adapter was created for usage with the **[adapter-transformers]( library.\n\n## Usage\n\nFirst, install `adapter-transformers`:\n\n```\npip install -U adapter-transformers\n```\n_Note: adapter-transformers is a fork of transformers that acts as a drop-in replacement with adapter support. [More](\n\nNow, the adapter can be loaded and activated like this:\n\n```python\nfrom transformers import AutoModelWithHeads\n\nmodel = AutoModelWithHeads.from_pretrained(\"roberta-base\")\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-sst2\", source=\"hf\")\nmodel.active_adapters = adapter_name\n```\n\n## Architecture & Training\n\nThe training code for this adapter is available at \nIn particular, training configurations for all tasks can be found [here](\n\n\n## Evaluation results\n\nRefer to [the paper]( for more information on results.\n\n## Citation\n\nIf you use this adapter, please cite our paper [\"What to Pre-Train on? Efficient Intermediate Task Selection\"](\n\n```bibtex\n@inproceedings{poth-etal-2021-pre,\n    title = \"{W}hat to Pre-Train on? {E}fficient Intermediate Task Selection\",\n    author = {Poth, Clifton  and\n      Pfeiffer, Jonas  and\n      R{\"u}ckl{'e}, Andreas  and\n      Gurevych, Iryna},\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"10585--10605\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AdapterHub/roberta-base-pf-sst2?", "answers": [{"text": "roberta", "answer_start": 34, "answer_end": 40}]}, {"id": "q2", "question": "What is the model task of AdapterHub/roberta-base-pf-sst2?", "answers": [{"text": "text-classification", "answer_start": 12, "answer_end": 30}]}]}]}, {"title": "Akash7897/bert-base-cased-wikitext2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bert-base-cased-wikitext2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-cased-wikitext2\n\nThis model is a fine-tuned version of [bert-base-cased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 6.8544\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    7.0517          |\n 2.0    6.8735          |\n 3.0    6.8924          |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Akash7897/bert-base-cased-wikitext2?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "AkshatSurolia/ViT-FaceMask-Finetuned", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- image-classification\ndatasets:\n- Face-Mask18K \n---\n\n# Vision Transformer (ViT) for Face Mask Detection\n\nVision Transformer (ViT) model pre-trained and fine-tuned on Self Currated Custom Face-Mask18K Dataset (18k images, 2 classes) at resolution 224x224. It was first introduced in the paper Training data-efficient image transformers & distillation through attention by Touvron et al. \nVision Transformer (ViT) model pre-trained and fine-tuned on Self Currated Custom Face-Mask18K Dataset (18k images, 2 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nNote that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Training Metrics\n    epoch                    =        0.89\n    total_flos               = 923776502GF\n    train_loss               =       0.057\n    train_runtime            =  0:40:10.40\n    train_samples_per_second =      23.943\n    train_steps_per_second   =       1.497\n---\n\n## Evaluation Metrics\n    epoch                   =       0.89\n    eval_accuracy           =     0.9894\n    eval_loss               =     0.0395\n    eval_runtime            = 0:00:36.81\n    eval_samples_per_second =     97.685\n    eval_steps_per_second   =     12.224", "qas": [{"id": "q1", "question": "What is the model architecture of AkshatSurolia/ViT-FaceMask-Finetuned?", "answers": [{"text": "vit", "answer_start": 681, "answer_end": 683}]}, {"id": "q2", "question": "What is the model task of AkshatSurolia/ViT-FaceMask-Finetuned?", "answers": [{"text": "image-classification", "answer_start": 32, "answer_end": 51}]}]}]}, {"title": "AlekseyKulnevich/Pegasus-QuestionGeneration", "paragraphs": [{"context": "**Usage HuggingFace Transformers for question generation task**\n``` \nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"AlekseyKulnevich/Pegasus-QuestionGeneration\")\ntokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')\ninput_text # your text \ninput_ = tokenizer.batch_encode_plus([input_text], max_length=1024, pad_to_max_length=True, \n                truncation=True, padding='longest', return_tensors='pt')\ninput_ids = input_['input_ids'] \ninput_mask = input_['attention_mask']\nquestions = model.generate(input_ids=input_ids, \n                         attention_mask=input_mask, \n                         num_beams=32, \n                         no_repeat_ngram_size=2, \n                         early_stopping=True, \n                         num_return_sequences=10)\n\nquestions = tokenizer.batch_decode(questions, skip_special_tokens=True)\n```\n\n**Decoder configuration examples:**  \n\n[**Input text you can see here**](  \n```\nquestions = model.generate(input_ids=input_ids, \n                         attention_mask=input_mask, \n                         num_beams=32, \n                         no_repeat_ngram_size=2, \n                         early_stopping=True, \n                         num_return_sequences=10)\n\ntokenizer.batch_decode(questions, skip_special_tokens=True)\n```\noutput: \n1. *What is the impact of human induced climate change on tropical cyclones?*\n2. *What is the impact of climate change on tropical cyclones?*\n3. *What is the impact of human induced climate change on tropical cyclone formation?*\n4. *How many tropical cyclones will occur in the mid-latitudes?*\n5. *What is the impact of climate change on the formation of tropical cyclones?*\n6. *Is it possible for a tropical cyclone to form in the middle latitudes?*\n7. *How many tropical cyclones will be formed in the mid-latitudes?*\n8. *How many tropical cyclones will there be in the mid-latitudes?*\n9. *How many tropical cyclones will form in the mid-latitudes?*\n10. *What is the impact of global warming on tropical cyclones?*\n11. *How long does it take for a tropical cyclone to form?*\n12. 'What are the impacts of climate change on tropical cyclones?*\n13. *What are the effects of climate change on tropical cyclones?*\n14. *How many tropical cyclones will be able to form in the middle latitudes?*\n15. *What is the impact of climate change on tropical cyclone formation?*\n16. *What is the effect of climate change on tropical cyclones?*\n17. *How long does it take for a tropical cyclone to form in the middle latitude?*\n18. *How many tropical cyclones will occur in the middle latitudes?*\n19. *How many tropical cyclones are likely to form in the midlatitudes?*\n20. *How many tropical cyclones are likely to form in the middle latitudes?*\n21. *How many tropical cyclones are expected to form in the midlatitudes?*\n22. *How many tropical cyclones will be formed in the middle latitudes?*\n23. *How many tropical cyclones will there be in the middle latitudes?*\n24. *How long will it take for a tropical cyclone to form in the middle latitude?*\n25. *What is the impact of global warming on tropical cyclone formation?*\n26. *How many tropical cyclones will form in the middle latitudes?*\n27. *How many tropical cyclones can we expect to form in the middle latitudes?*\n28. *Is it possible for a tropical cyclone to form in the middle latitude?*\n29. *What is the effect of climate change on tropical cyclone formation?*\n30. *What are the effects of climate change on tropical cyclone formation?*\n\nAlso you can play with the following parameters in generate method:   \n-top_k  \n-top_p  \n\n[**Meaning of parameters to generate text you can see here**](", "qas": [{"id": "q1", "question": "What is the model architecture of AlekseyKulnevich/Pegasus-QuestionGeneration?", "answers": [{"text": "pegasus", "answer_start": 277, "answer_end": 283}]}]}]}, {"title": "Alerosae/SocratesGPT-2", "paragraphs": [{"context": "---\nlanguage: \"en\"\ntags:\n- text-generation\npipeline_tag: text-generation\nwidget:\n- text: \"The Gods\"\n- text: \"What is\"\n---\n\nThis is a fine-tuned version of GPT-2, trained with the entire corpus of Plato's works. By generating text samples you should be able to generate ancient Greek philosophy on the fly!\n\n", "qas": [{"id": "q2", "question": "What is the model task of Alerosae/SocratesGPT-2?", "answers": [{"text": "text-generation", "answer_start": 27, "answer_end": 41}]}]}]}, {"title": "AlexN/xls-r-300m-pt", "paragraphs": [{"context": "---\nlanguage:\n- pt\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- robust-speech-event\n- mozilla-foundation/common_voice_8_0\n- generated_from_trainer\n- hf-asr-leaderboard\ndatasets:\n- mozilla-foundation/common_voice_8_0\nmodel-index:\n- name: xls-r-300m-pt\n  results:\n  - task:\n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 8.0 pt\n      type: mozilla-foundation/common_voice_8_0\n      args: pt\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 19.361\n    - name: Test CER\n      type: cer\n      value: 5.533\n  - task:\n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: fr\n    metrics:\n    - name: Validation WER\n      type: wer\n      value: 47.812\n    - name: Validation CER\n      type: cer\n      value: 18.805\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 8.0\n      type: mozilla-foundation/common_voice_8_0\n      args: pt\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 19.36\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: pt\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 48.01\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Test Data\n      type: speech-recognition-community-v2/eval_data\n      args: pt\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 49.21\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - PT dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2290\n- Wer: 0.2382\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1500\n- num_epochs: 15.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.64   3.0982          \n 1.29   0.7887          \n 1.93   0.5238          \n 2.57   0.4775          \n 3.21   0.4648          \n 3.86   0.4069          \n 4.5    0.3914          \n 5.14   0.3694          \n 5.78   0.3568          \n 6.43   0.3331          \n 7.07   0.3332          \n 7.71   0.3131          \n 8.35   0.3024          \n 9.0    0.2948          \n 9.64   0.2796          \n 10.28  0.2719          \n 10.93  0.2620          \n 11.57  0.2537          \n 12.21  0.2438          \n 12.85  0.2409          \n 13.5   0.2366          \n 14.14  0.2317          \n 14.78  0.2290          \n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of AlexN/xls-r-300m-pt?", "answers": [{"text": "wav2vec2", "answer_start": 2020, "answer_end": 2027}]}, {"id": "q2", "question": "What is the model task of AlexN/xls-r-300m-pt?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}]}]}, {"title": "AndrewMcDowell/wav2vec2-xls-r-300m-arabic", "paragraphs": [{"context": "---\nlanguage:\n- ar\nlicense: apache-2.0\ntags:\n- ar\n- automatic-speech-recognition\n- generated_from_trainer\n- hf-asr-leaderboard\n- mozilla-foundation/common_voice_7_0\n- robust-speech-event\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmodel-index:\n- name: XLS-R-300M - Arabic\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 7\n      type: mozilla-foundation/common_voice_7_0\n      args: ar\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 47.54\n    - name: Test CER\n      type: cer\n      value: 17.64\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: ar\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 93.72\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Test Data\n      type: speech-recognition-community-v2/eval_data\n      args: ar\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 92.49\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - AR dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4502\n- Wer: 0.4783\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 7.5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 2000\n- num_epochs: 5.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.43   5.1401          \n 0.86   3.3220          \n 1.29   3.0806          \n 1.72   2.5678          \n 2.14   1.1068          \n 2.57   0.7878          \n 3.0    0.6955          \n 3.43   0.6452          \n 3.86   0.5961          \n 4.29   0.5550          \n 4.72   0.5374          \n 5.15   0.5337          \n 5.57   0.5054          \n 6.0    0.4926          \n 6.43   0.4946          \n 6.86   0.4915          \n 7.29   0.4725          \n 7.72   0.4726          \n 8.15   0.4667          \n 8.58   0.4685          \n 9.01   0.4708          \n 9.43   0.4539          \n 9.86   0.4502          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.17.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of AndrewMcDowell/wav2vec2-xls-r-300m-arabic?", "answers": [{"text": "wav2vec2", "answer_start": 1424, "answer_end": 1431}]}, {"id": "q2", "question": "What is the model task of AndrewMcDowell/wav2vec2-xls-r-300m-arabic?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 52, "answer_end": 79}]}]}]}, {"title": "AndrewMcDowell/wav2vec2-xls-r-300m-german-de", "paragraphs": [{"context": "---\nlanguage:\n- de\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- de\n- generated_from_trainer\n- hf-asr-leaderboard\n- mozilla-foundation/common_voice_7_0\n- robust-speech-event\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmodel-index:\n- name: XLS-R-300M - German\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 7\n      type: mozilla-foundation/common_voice_7_0\n      args: de\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 20.16\n    - name: Test CER\n      type: cer\n      value: 5.06\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: de\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 39.79\n    - name: Test CER\n      type: cer\n      value: 15.02\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Test Data\n      type: speech-recognition-community-v2/eval_data\n      args: de\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 47.95\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. \neval results:\nWER: 0.20161578657865786\nCER: 0.05062357805269733\n-->\n\n# \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - DE dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1768\n- Wer: 0.2016\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 7.5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 2000\n- num_epochs: 3.4\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.04   5.4564          \n 0.08   3.0041          \n 0.13   1.1723          \n 0.17   0.3656          \n 0.21   0.2843          \n 0.25   0.2554          \n 0.3    0.2387          \n 0.34   0.2345          \n 0.38   0.2270          \n 0.42   0.2212          \n 0.47   0.2141          \n 0.51   0.2122          \n 0.55   0.2114          \n 0.59   0.2066          \n 0.64   0.2033          \n 0.68   0.2020          \n 0.72   0.1977          \n 0.76   0.1976          \n 0.81   0.1956          \n 0.85   0.1958          \n 0.89   0.1964          \n 0.93   0.1926          \n 0.98   0.1953          \n 1.02   0.1927          \n 1.06   0.1901          \n 1.1    0.1936          \n 1.15   0.1900          \n 1.19   0.1931          \n 1.23   0.1914          \n 1.27   0.1931          \n 1.32   0.1913          \n 1.36   0.1902          \n 1.4    0.1895          \n 1.44   0.1913          \n 1.49   0.1884          \n 1.53   0.1894          \n 1.57   0.1886          \n 1.61   0.1856          \n 1.66   0.1852          \n 1.7    0.1874          \n 1.74   0.1873          \n 1.78   0.1865          \n 1.83   0.1869          \n 1.87   0.1878          \n 1.91   0.1852          \n 1.95   0.1855          \n 2.0    0.1858          \n 2.04   0.1850          \n 2.08   0.1839          \n 2.12   0.1838          \n 2.17   0.1889          \n 2.21   0.1856          \n 2.25   0.1891          \n 2.29   0.1857          \n 2.34   0.1840          \n 2.38   0.1833          \n 2.42   0.1789          \n 2.46   0.1769          \n 2.51   0.1819          \n 2.55   0.1828          \n 2.59   0.1811          \n 2.63   0.1833          \n 2.68   0.1795          \n 2.72   0.1809          \n 2.76   0.1812          \n 2.8    0.1775          \n 2.85   0.1790          \n 2.89   0.1767          \n 2.93   0.1735          \n 2.97   0.1793          \n 3.02   0.1778          \n 3.06   0.1776          \n 3.1    0.1812          \n 3.14   0.1800          \n 3.19   0.1783          \n 3.23   0.1810          \n 3.27   0.1805          \n 3.31   0.1773          \n 3.36   0.1804          \n 3.4    0.1768          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.17.1.dev0\n- Tokenizers 0.11.0\n\n#### Evaluation Commands\n1. To evaluate on `mozilla-foundation/common_voice_7_0` with split `test`\n\n```bash\npython ./eval.py --model_id AndrewMcDowell/wav2vec2-xls-r-300m-german-de --dataset mozilla-foundation/common_voice_7_0 --config de --split test --log_outputs\n```\n\n2. To evaluate on test dev data\n```bash\npython ./eval.py --model_id AndrewMcDowell/wav2vec2-xls-r-300m-german-de --dataset speech-recognition-community-v2/dev_data --config de --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```", "qas": [{"id": "q1", "question": "What is the model architecture of AndrewMcDowell/wav2vec2-xls-r-300m-german-de?", "answers": [{"text": "wav2vec2", "answer_start": 1544, "answer_end": 1551}]}, {"id": "q2", "question": "What is the model task of AndrewMcDowell/wav2vec2-xls-r-300m-german-de?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}]}]}, {"title": "Andrija/SRoBERTa-F", "paragraphs": [{"context": "---\ndatasets:\n- oscar\n- srwac\n- leipzig\n- cc100\n- hrwac\n\nlanguage: \n- hr\n- sr\n- multilingual\n\ntags:\n- masked-lm\n\nwidget:\n- text: \"Ovo je po\u010detak <mask>.\"\n\nlicense: apache-2.0\n\n---\n# Transformer language model for Croatian and Serbian\nTrained on 43GB datasets that contain Croatian and Serbian language for one epochs (9.6 mil. steps, 3 epochs).\nLeipzig Corpus, OSCAR, srWac, hrWac, cc100-hr and cc100-sr  datasets\n\nValidation number of exampels run for perplexity:1620487 sentences\nPerplexity:6.02\nStart loss: 8.6\nFinal loss: 2.0\nThoughts: Model could be trained more, the training did not stagnate.\n\n #params                         Training data                     |\n-------------------------------------------------------------------|\n 80M    Leipzig Corpus, OSCAR, srWac, hrWac, cc100-hr and cc100-sr  (43 GB of text)             |", "qas": []}]}, {"title": "Andrija/SRoBERTa-NER", "paragraphs": [{"context": "---\ndatasets:\n- hr500k\n\nlanguage: \n- hr\n- sr\n- multilingual\n\nwidget:\n- text: \"Moje ime je Aleksandar i zivim u Beogradu pored Vlade Republike Srbije\"\nlicense: apache-2.0\n\n---\nNamed Entity Recognition (Token Classification Head) for Serbian / Croatian languges.\n\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity\nI-MIS | Miscellaneous entity\nB-PER |Beginning of a person\u2019s name right after another person\u2019s name\nB-DERIV-PER| Begginning derivative that describes relation to a person\nI-PER |Person\u2019s name\nB-ORG |Beginning of an organization right after another organization\nI-ORG |organization\nB-LOC |Beginning of a location right after another location\nI-LOC |Location", "qas": []}]}, {"title": "Aruden/DialoGPT-medium-harrypotterall", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Aruden/DialoGPT-medium-harrypotterall?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "ArvinZhuang/BiTAG-t5-large", "paragraphs": [{"context": "---\ninference:\n  parameters:\n    do_sample: True\n    max_length: 500\n    top_p: 0.9\n    top_k: 20\n    temperature: 1\n    num_return_sequences: 10\n    \nwidget:\n- text: \"abstract: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n  example_title: \"BERT abstract\"\n---\n\n```\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ArvinZhuang/BiTAG-t5-large\")\ntokenizer = AutoTokenizer.from_pretrained(\"ArvinZhuang/BiTAG-t5-large\")\n\ntext = \"abstract: [your abstract]\"  # use 'title:' as the prefix for title_to_abs task.\ninput_ids = tokenizer.encode(text, return_tensors='pt')\n\noutputs = model.generate(\n    input_ids,\n    do_sample=True,\n    max_length=500,\n    top_p=0.9,\n    top_k=20,\n    temperature=1,\n    num_return_sequences=10,\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, output in enumerate(outputs):\n    print(\"{}: {}\".format(i+1, tokenizer.decode(output, skip_special_tokens=True)))\n\n```\nGitHub: ", "qas": [{"id": "q1", "question": "What is the model architecture of ArvinZhuang/BiTAG-t5-large?", "answers": [{"text": "t5", "answer_start": 1372, "answer_end": 1373}]}]}]}, {"title": "AryanLala/autonlp-Scientific_Title_Generator-34558227", "paragraphs": [{"context": "---\ntags: autonlp\nlanguage: en\nwidget: \n- text: \"The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at \ndatasets:\n- AryanLala/autonlp-data-Scientific_Title_Generator\nco2_eq_emissions: 137.60574081887984\n---\n\n# Model Trained Using AutoNLP\n- Model: Google's Pegasus (\n- Problem type: Summarization\n- Model ID: 34558227\n- CO2 Emissions (in grams): 137.60574081887984\n- Spaces: \n- Dataset: arXiv Dataset (\n- Data subset used: \n\n## Validation Metrics\n\n- Loss: 2.578599214553833\n- Rouge1: 44.8482\n- Rouge2: 24.4052\n- RougeL: 40.1716\n- RougeLsum: 40.1396\n- Gen Len: 11.4675\n\n## Social\n- LinkedIn: \n- Twitter: \n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_HUGGINGFACE_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoNLP\"}' \n```", "qas": []}]}, {"title": "Awsaf/large-eren", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Eren Yeager Model", "qas": [{"id": "q2", "question": "What is the model task of Awsaf/large-eren?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Axcel/DialoGPT-small-rick", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Rick DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Axcel/DialoGPT-small-rick?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Ayham/albert_bert_summarization_cnn_dailymail", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- cnn_dailymail\nmodel-index:\n- name: albert_bert_summarization_cnn_dailymail\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# albert_bert_summarization_cnn_dailymail\n\nThis model is a fine-tuned version of []( on the cnn_dailymail dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 2000\n- num_epochs: 3.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.12.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": []}]}, {"title": "BigSalmon/InformalToFormalLincoln16", "paragraphs": [{"context": "Informal to Formal:\n```\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"BigSalmon/InformalToFormalLincoln16\")\nmodel = AutoModelWithLMHead.from_pretrained(\"BigSalmon/InformalToFormalLincoln16\")\n```\n\n```\n (The model for this space changes over time)\n```\n\n```\n (The model for this space changes over time)\n```\n\n```\nHow To Make Prompt:\ninformal english: i am very ready to do that just that.\nTranslated into the Style of Abraham Lincoln: you can assure yourself of my readiness to work toward this end.\nTranslated into the Style of Abraham Lincoln: please be assured that i am most ready to undertake this laborious task.\n\ninformal english: space is huge and needs to be explored.\nTranslated into the Style of Abraham Lincoln: space awaits traversal, a new world whose boundaries are endless.\nTranslated into the Style of Abraham Lincoln: space is a ( limitless / boundless ) expanse, a vast virgin domain awaiting exploration.\n\ninformal english: corn fields are all across illinois, visible once you leave chicago.\nTranslated into the Style of Abraham Lincoln: corn fields ( permeate illinois / span the state of illinois / ( occupy / persist in ) all corners of illinois / line the horizon of illinois / envelop the landscape of illinois ), manifesting themselves visibly as one ventures beyond chicago.\n\ninformal english:\n````", "qas": []}]}, {"title": "BigSalmon/InformalToFormalLincoln17", "paragraphs": [{"context": "Informal to Formal:\n```\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"BigSalmon/InformalToFormalLincoln17\")\nmodel = AutoModelWithLMHead.from_pretrained(\"BigSalmon/InformalToFormalLincoln17\")\n```\n\n```\n (The model for this space changes over time)\n```\n\n```\n (The model for this space changes over time)\n```\n\n```\nHow To Make Prompt:\ninformal english: i am very ready to do that just that.\nTranslated into the Style of Abraham Lincoln: you can assure yourself of my readiness to work toward this end.\nTranslated into the Style of Abraham Lincoln: please be assured that i am most ready to undertake this laborious task.\n\ninformal english: space is huge and needs to be explored.\nTranslated into the Style of Abraham Lincoln: space awaits traversal, a new world whose boundaries are endless.\nTranslated into the Style of Abraham Lincoln: space is a ( limitless / boundless ) expanse, a vast virgin domain awaiting exploration.\n\ninformal english: corn fields are all across illinois, visible once you leave chicago.\nTranslated into the Style of Abraham Lincoln: corn fields ( permeate illinois / span the state of illinois / ( occupy / persist in ) all corners of illinois / line the horizon of illinois / envelop the landscape of illinois ), manifesting themselves visibly as one ventures beyond chicago.\n\ninformal english:\n````", "qas": []}]}, {"title": "BigSalmon/InformalToFormalLincoln19", "paragraphs": [{"context": "Informal to Formal:\n```\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"BigSalmon/InformalToFormalLincoln19\")\nmodel = AutoModelWithLMHead.from_pretrained(\"BigSalmon/InformalToFormalLincoln19\")\n```\n\n```\n (The model for this space changes over time)\n```\n\n```\n (The model for this space changes over time)\n```\n\n```\n (The model for this space changes over time)\n```\n\n```\nHow To Make Prompt:\ninformal english: i am very ready to do that just that.\nTranslated into the Style of Abraham Lincoln: you can assure yourself of my readiness to work toward this end.\nTranslated into the Style of Abraham Lincoln: please be assured that i am most ready to undertake this laborious task.\n\ninformal english: space is huge and needs to be explored.\nTranslated into the Style of Abraham Lincoln: space awaits traversal, a new world whose boundaries are endless.\nTranslated into the Style of Abraham Lincoln: space is a ( limitless / boundless ) expanse, a vast virgin domain awaiting exploration.\n\ninformal english: corn fields are all across illinois, visible once you leave chicago.\nTranslated into the Style of Abraham Lincoln: corn fields ( permeate illinois / span the state of illinois / ( occupy / persist in ) all corners of illinois / line the horizon of illinois / envelop the landscape of illinois ), manifesting themselves visibly as one ventures beyond chicago.\n\ninformal english:\n````\n\n```\n###\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\nText: failing to draw in the masses, the NBA has fallen into disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap solutions could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n###\n- with 2,000,000 individual articles on everything\n- wikipedia is the #8 site on the world wide web\n- created by anyone with access to a computer\n- growing at fast rate\n- proof that collaborative community-based projects are the future\nText: encompassing a staggering 2,000,000 articles on every subject conceivable, wikipedia is the 8th most visited website in the world. borne of the collective efforts of anyone with an internet connection, its contents are increasing exponentially. most compellingly, however, this effort is an affirmation that community-based initiatives is the future.\n###\n-\n```", "qas": []}]}, {"title": "BigSalmon/InformalToFormalLincoln22", "paragraphs": [{"context": "```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"BigSalmon/InformalToFormalLincoln22\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"BigSalmon/InformalToFormalLincoln22\")\n\n```\n\n```\nHow To Make Prompt:\ninformal english: i am very ready to do that just that.\nTranslated into the Style of Abraham Lincoln: you can assure yourself of my readiness to work toward this end.\nTranslated into the Style of Abraham Lincoln: please be assured that i am most ready to undertake this laborious task.\n\n***\n\ninformal english: space is huge and needs to be explored.\nTranslated into the Style of Abraham Lincoln: space awaits traversal, a new world whose boundaries are endless.\nTranslated into the Style of Abraham Lincoln: space is a ( limitless / boundless ) expanse, a vast virgin domain awaiting exploration.\n\n***\n\ninformal english: corn fields are all across illinois, visible once you leave chicago.\nTranslated into the Style of Abraham Lincoln: corn fields ( permeate illinois / span the state of illinois / ( occupy / persist in ) all corners of illinois / line the horizon of illinois / envelop the landscape of illinois ), manifesting themselves visibly as one ventures beyond chicago.\n\ninformal english:\n```\n```\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\nText: failing to draw in the masses, the NBA has fallen into disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap solutions could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n\n***\n-\n\n```\n\n```\ninfill: chrome extensions [MASK] accomplish everyday tasks.\nTranslated into the Style of Abraham Lincoln: chrome extensions ( expedite the ability to / unlock the means to more readily ) accomplish everyday tasks.\n\ninfill: at a time when nintendo has become inflexible, [MASK] consoles that are tethered to a fixed iteration, sega diligently curates its legacy of classic video games on handheld devices.\nTranslated into the Style of Abraham Lincoln: at a time when nintendo has become inflexible, ( stubbornly [MASK] on / firmly set on / unyielding in its insistence on ) consoles that are tethered to a fixed iteration, sega diligently curates its legacy of classic video games on handheld devices.\n\ninfill:\n\n```\n```\nEssay Intro (California High-Speed Rail): built with an eye on the future, california's high-speed rail service resolves to change the face of travel.\n\nEssay Intro (YIMBY's Need To Win): home to the most expensive housing market in the united states, san francisco is the city in which the yimby and anti-yimby hordes wage an eternal battle.\n\nEssay Intro (\n```\n\n```\nSearch: What is the definition of Checks and Balances?\n\n\nChecks and Balances is the idea of having a system where each and every action in government should be subject to one or more checks that would not allow one branch or the other to overly dominate.\n\n\nChecks and Balances is a system that allows each branch of government to limit the powers of the other branches in order to prevent abuse of power\n\n\nChecks and Balances is a system of separation through which branches of government can control the other, thus preventing excess power.\n\n***\n\nSearch: What is the definition of Separation of Powers?\n\n\nThe separation of powers is a principle in government, whereby governmental powers are separated into different branches, each with their own set of powers, that are prevent one branch from aggregating too much power.\n\n\nSeparation of Powers is the division of governmental functions between the executive, legislative and judicial branches, clearly demarcating each branch's authority, in the interest of ensuring that individual liberty or security is not undermined.\n\n***\n\nSearch: What is the definition of Connection of Powers?\n\n\nConnection of Powers is a feature of some parliamentary forms of government where different branches of government are intermingled, typically the executive and legislative branches.\n\n\nThe term Connection of Powers describes a system of government in which there is overlap between different parts of the government.\n\n***\nSearch: What is the definition of\n```\n\n```\nSearch: What are phrase synonyms for \"second-guess\"?\n\nShortest to Longest:\n- feel dubious about\n- raise an eyebrow at\n- wrinkle their noses at\n- cast a jaundiced eye at\n- teeter on the fence about\n\n***\n\nSearch: What are phrase synonyms for \"mean to newbies\"?\n\nShortest to Longest:\n- readiness to balk at rookies\n- absence of tolerance for novices\n- hostile attitude toward newcomers\n\n***\n\nSearch: What are phrase synonyms for \"make use of\"?\n\nShortest to Longest:\n- call upon\n- glean value from\n- reap benefits from\n- derive utility from\n- seize on the merits of\n- draw on the strength of\n- tap into the potential of\n\n***\n\nSearch: What are phrase synonyms for \"hurting itself\"?\n\nShortest to Longest:\n- erring\n- slighting itself\n- forfeiting its integrity\n- doing itself a disservice\n- evincing a lack of backbone\n\n***\n\nSearch: What are phrase synonyms for \"\n```", "qas": []}]}, {"title": "BigSalmon/Points", "paragraphs": [{"context": "Converting Points to Paragraphs\n\nExample Prompts:\n```\n###\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\nText: failing to draw in the masses, the NBA has fallen into disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap solutions could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n###\n- with 2,000,000 individual articles on everything\n- wikipedia is the #8 site on the world wide web\n- created by anyone with access to a computer\n- growing at fast rate\n- proof that collaborative community-based projects are the future\nText: encompassing a staggering 2,000,000 articles on every subject conceivable, wikipedia is the 8th most visited website in the world. borne of the collective efforts of anyone with an internet connection, its contents are increasing exponentially. most compellingly, however, this effort is an affirmation that community-based initiatives is the future.\n###\n-\n```", "qas": []}]}, {"title": "BigSalmon/SimplifyText", "paragraphs": [{"context": "- All credit goes to \n- This is a copy of their repository for future training purposes. \n- It is supposed to simplify text.\n- Their model card gives instructions on how to use it.", "qas": []}]}, {"title": "BigTooth/DialoGPT-Megumin", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Megumin model", "qas": [{"id": "q2", "question": "What is the model task of BigTooth/DialoGPT-Megumin?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Daivakai/DialoGPT-small-saitama", "paragraphs": [{"context": "---\ntags:\n - conversational\n---\n\n#Saitama DialoGPT model", "qas": [{"id": "q2", "question": "What is the model task of Daivakai/DialoGPT-small-saitama?", "answers": [{"text": "conversational", "answer_start": 13, "answer_end": 26}]}]}]}, {"title": "DanL/scientific-challenges-and-directions", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\n- text-classification\n\nlanguage:\n- en\n\ndatasets:\n- DanL/scientific-challenges-and-directions-dataset\n\nwidget:\n- text: \"severe atypical cases of pneumonia emerged and quickly spread worldwide.\"\n  example_title: \"challenge\"\n- text: \"we speculate that studying IL-6 will be beneficial.\"\n  example_title: \"direction\"\n- text: \"in future studies, both PRRs should be tested as the cause for multiple deaths.\"\n  example_title: \"both\"\n- text: \"IbMADS1-transformed potatoes exhibited tuber morphogenesis in the fibrous roots.\"\n  example_title: \"neither\"\n\n\nmetrics:\n- precision\n- recall\n- f1\nmodel-index:\n- name: scientific-challenges-and-directions\n  results: []\n---\n\n# scientific-challenges-and-directions\n\nWe present a novel resource to help scientists and medical professionals discover challenges and potential directions across scientific literature, focusing on a broad corpus pertaining to the COVID-19 pandemic and related historical research. At a high level, the _challenges_ and _directions_ are defined as follows:\n\n* **Challenge**: A sentence mentioning a problem, difficulty, flaw, limitation, failure, lack of clarity, or knowledge gap.\n* **Research direction**: A sentence mentioning suggestions or needs for further research, hypotheses, speculations, indications or hints that an issue is worthy of exploration.  \n\n* This model here is described in our paper: [A Search Engine for Discovery of Scientific Challenges and Directions]( (though we've upgraded the infrastructure since the paper was released - there are slight differences in the results).\n* Our dataset can be found [here](\n* Please cite our paper if you use our datasets or models in your project. See the [BibTeX](#citation). \n* Feel free to [email us](#contact-us). \n* Also, check out [our search engine]( as an example application.\n\n## Model description\nThis model is a fine-tuned version of [PubMedBERT]( on the [scientific-challenges-and-directions-dataset]( designed for multi-label text classification.\n\n## Training and evaluation data\nThe scientific-challenges-and-directions model is trained based on a dataset that is a collection of 2894 sentences and their surrounding contexts, from 1786 full-text papers in the CORD-19 corpus, labeled for classification of challenges and directions by expert annotators with biomedical and bioNLP backgrounds. For full details on the train/test/split of the data see section 3.1 in our [paper](\n\n## Example notebook\nWe include an example notebook that uses the model for inference in our [repo]( See `Inference_Notebook.ipynb`.\nA training notebook is also included.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning rate: 2e-05\n- train batch size: 8\n- eval batch size: 4\n- seed: 4\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr scheduler type: linear\n- lr scheduler warmup steps: 500\n- num epochs: 30\n\n### Training results\nThe achieves the following results on the test set:\n- Precision Challenge: 0.768719  \n- Recall Challenge: 0.780405  \n- F1 Challenge: 0.774518\n- Precision Direction: 0.758112  \n- Recall Direction: 0.774096  \n- F1 Direction: 0.766021\n- Precision (micro avg. on both labels): 0.764894  \n- Recall (micro avg. on both labels): 0.778139  \n- F1 (micro avg. on both labels): 0.771459\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n\n## Citation\n\nIf using our dataset and models, please cite:\n\n```\n@misc{lahav2021search,\n      title={A Search Engine for Discovery of Scientific Challenges and Directions}, \n      author={Dan Lahav and Jon Saad Falcon and Bailey Kuehl and Sophie Johnson and Sravanthi Parasa and Noam Shomron and Duen Horng Chau and Diyi Yang and Eric Horvitz and Daniel S. Weld and Tom Hope},\n      year={2021},\n      eprint={2108.13751},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## Contact us\n\nPlease don't hesitate to reach out.\n\n**Email:** `lahav@mail.tau.ac.il`,`tomh@allenai.org`.\n", "qas": [{"id": "q2", "question": "What is the model task of DanL/scientific-challenges-and-directions?", "answers": [{"text": "text-classification", "answer_start": 37, "answer_end": 55}]}]}]}, {"title": "DataikuNLP/paraphrase-multilingual-MiniLM-L12-v2", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\nlicense: apache-2.0\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\n# DataikuNLP/paraphrase-multilingual-MiniLM-L12-v2\n\n**This model is a copy of [this model repository]( from sentence-transformers at the specific commit `d66eff4d8a8598f264f166af8db67f7797164651`.**\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Evaluation Results\n\n\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [\n\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\nThis model was trained by [sentence-transformers]( \n        \nIf you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of DataikuNLP/paraphrase-multilingual-MiniLM-L12-v2?", "answers": [{"text": "bert", "answer_start": 3271, "answer_end": 3274}]}, {"id": "q2", "question": "What is the model task of DataikuNLP/paraphrase-multilingual-MiniLM-L12-v2?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "Davlan/bert-base-multilingual-cased-finetuned-igbo", "paragraphs": [{"context": "Hugging Face's logo\n---\nlanguage: ig\ndatasets:\n\n---\n# bert-base-multilingual-cased-finetuned-igbo\n## Model description\n**bert-base-multilingual-cased-finetuned-igbo** is a **Igbo BERT** model obtained by fine-tuning **bert-base-multilingual-cased** model on Igbo language texts.  It provides **better performance** than the multilingual BERT on text classification and named entity recognition datasets.  \n\nSpecifically, this model is a *bert-base-multilingual-cased* model that was fine-tuned on Igbo corpus. \n## Intended uses & limitations\n#### How to use\nYou can use this model with Transformers *pipeline* for masked token prediction.\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='Davlan/bert-base-multilingual-cased-finetuned-igbo')\n>>> unmasker(\"Reno Omokri na G\u1ecd\u1ecdment\u1ecb [MASK] enwegh\u1ecb ihe ha ga-eji hiwe ya b\u1ee5 mmachi.\")\n\n```\n#### Limitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. \n## Training data\nThis model was fine-tuned on JW300 + OPUS CC-Align + [IGBO NLP Corpus]( +[Igbo CC-100](\n\n## Training procedure\nThis model was trained on a single NVIDIA V100 GPU\n\n## Eval results on Test set (F-score, average over 5 runs)\nDataset ig_bert F1\n--\n[MasakhaNER](  86.75\n\n### BibTeX entry and citation info\nBy David Adelani\n```\n\n```\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of Davlan/bert-base-multilingual-cased-finetuned-igbo?", "answers": [{"text": "bert", "answer_start": 54, "answer_end": 57}]}, {"id": "q2", "question": "What is the model task of Davlan/bert-base-multilingual-cased-finetuned-igbo?", "answers": [{"text": "fill-mask", "answer_start": 712, "answer_end": 720}]}]}]}, {"title": "Davlan/bert-base-multilingual-cased-ner-hrl", "paragraphs": [{"context": "Hugging Face's logo\n---\nlanguage: \n- ar\n- de\n- en\n- es\n- fr\n- it\n- lv\n- nl\n- pt\n- zh\n- multilingual\n\n---\n# bert-base-multilingual-cased-ner-hrl\n## Model description\n**bert-base-multilingual-cased-ner-hrl** is a **Named Entity Recognition** model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned  mBERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER). \nSpecifically, this model is a *bert-base-multilingual-cased* model that was fine-tuned on an aggregation of 10 high-resourced languages\n## Intended uses & limitations\n#### How to use\nYou can use this model with Transformers *pipeline* for NER.\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\nner_results = nlp(example)\nprint(ner_results)\n```\n#### Limitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  \n## Training data\nThe training data for the 10 languages are from: \n\nLanguage|Dataset\n-|-\nArabic | [ANERcorp](\nGerman | [conll 2003](\nEnglish | [conll 2003](\nSpanish | [conll 2002](\nFrench | [Europeana Newspapers](\nItalian | [Italian I-CAB](\nLatvian | [Latvian NER](\nDutch | [conll 2002](\nPortuguese |[Paramopama + Second Harem](\nChinese | [MSRA](\n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-PER |Beginning of a person\u2019s name right after another person\u2019s name\nI-PER |Person\u2019s name\nB-ORG |Beginning of an organisation right after another organisation\nI-ORG |Organisation\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n## Training procedure\nThis model was trained on NVIDIA V100 GPU with recommended hyperparameters from HuggingFace code.\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of Davlan/bert-base-multilingual-cased-ner-hrl?", "answers": [{"text": "bert", "answer_start": 107, "answer_end": 110}]}]}]}, {"title": "Davlan/byt5-base-eng-yor-mt", "paragraphs": [{"context": "Hugging Face's logo\n---\nlanguage: \n- yo\n- en\ndatasets:\n- JW300 + [Menyo-20k](\n---\n# byt5-base-eng-yor-mt\n## Model description\n**byt5-base-eng-yor-mt** is a **machine translation** model from English language to Yor\u00f9b\u00e1 language based on a fine-tuned  byt5-base  model.  It establishes a **strong baseline** for automatically translating texts from English to Yor\u00f9b\u00e1.  \n\nSpecifically, this model is a *byt5-base* model that was fine-tuned on  JW300 Yor\u00f9b\u00e1 corpus and [Menyo-20k](\n\n#### Limitations and bias\nThis model is limited by its training dataset. This may not generalize well for all use cases in different domains.  \n## Training data\nThis model was fine-tuned on on  JW300 corpus and [Menyo-20k]( dataset\n\n## Training procedure\nThis model was trained on NVIDIA V100 GPU\n\n## Eval results on Test set (BLEU score)\nFine-tuning byt5-base achieves **12.23 BLEU** on [Menyo-20k test set]( while mt5-base achieves 9.82\n\n### BibTeX entry and citation info\nBy David Adelani\n```\n\n```\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of Davlan/byt5-base-eng-yor-mt?", "answers": [{"text": "t5", "answer_start": 86, "answer_end": 87}]}]}]}, {"title": "Davlan/distilbert-base-multilingual-cased-masakhaner", "paragraphs": [{"context": "Hugging Face's logo\n---\nlanguage: \n- ha\n- ig\n- rw\n- lg\n- luo\n- pcm\n- sw\n- wo\n- yo\n- multilingual\n\n\ndatasets:\n- masakhaner\n---\n# bert-base-multilingual-cased-masakhaner\n## Model description\n**distilbert-base-multilingual-cased-masakhaner** is the first **Named Entity Recognition** model for 9 African languages (Hausa, Igbo, Kinyarwanda, Luganda, Nigerian Pidgin, Swahilu, Wolof, and Yor\u00f9b\u00e1) based on a fine-tuned  BERT base model. It has been trained to recognize four types of entities: dates & times (DATE), location (LOC), organizations (ORG), and person (PER). \nSpecifically, this model is a *distilbert-base-multilingual-cased* model that was fine-tuned on an aggregation of African language datasets obtained from Masakhane [MasakhaNER]( dataset. \n## Intended uses & limitations\n#### How to use\nYou can use this model with Transformers *pipeline* for NER.\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/distilbert-base-multilingual-cased-masakhaner\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Davlan/distilbert-base-multilingual-cased-masakhaner\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"Emir of Kano turban Zhang wey don spend 18 years for Nigeria\"\nner_results = nlp(example)\nprint(ner_results)\n```\n#### Limitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  \n## Training data\nThis model was fine-tuned on 9 African NER datasets (Hausa, Igbo, Kinyarwanda, Luganda, Nigerian Pidgin, Swahilu, Wolof, and Yor\u00f9b\u00e1) Masakhane [MasakhaNER]( dataset\n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-DATE |Beginning of a DATE entity right after another DATE entity\nI-DATE |DATE entity\nB-PER |Beginning of a person\u2019s name right after another person\u2019s name\nI-PER |Person\u2019s name\nB-ORG |Beginning of an organisation right after another organisation\nI-ORG |Organisation\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n## Training procedure\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original MasakhaNER paper]( which trained & evaluated the model on MasakhaNER corpus. \n## Eval results on Test set (F-score)\nlanguage|F1-score\n-|-\nhau |88.88\nibo |84.87\nkin |74.19\nlug |78.43\nluo |73.32\npcm |87.98\nswa |86.20\nwol |64.67\nyor |78.10\n\n### BibTeX entry and citation info\n```\n@article{adelani21tacl,\n    title = {Masakha{NER}: Named Entity Recognition for African Languages},\n    author = {David Ifeoluwa Adelani and Jade Abbott and Graham Neubig and Daniel D'souza and Julia Kreutzer and Constantine Lignos and Chester Palen-Michel and Happy Buzaaba and Shruti Rijhwani and Sebastian Ruder and Stephen Mayhew and Israel Abebe Azime and Shamsuddeen Muhammad and Chris Chinenye Emezue and Joyce Nakatumba-Nabende and Perez Ogayo and Anuoluwapo Aremu and Catherine Gitau and Derguene Mbaye and Jesujoba Alabi and Seid Muhie Yimam and Tajuddeen Gwadabe and Ignatius Ezeani and Rubungo Andre Niyongabo and Jonathan Mukiibi and Verrah Otiende and Iroro Orife and Davis David and Samba Ngom and Tosin Adewumi and Paul Rayson and Mofetoluwa Adeyemi and Gerald Muriuki and Emmanuel Anebi and Chiamaka Chukwuneke and Nkiruka Odu and Eric Peter Wairagala and Samuel Oyerinde and Clemencia Siro and Tobius Saul Bateesa and Temilola Oloyede and Yvonne Wambui and Victor Akinode and Deborah Nabagereka and Maurice Katusiime and Ayodele Awokoya and Mouhamadane MBOUP and Dibora Gebreyohannes and Henok Tilaye and Kelechi Nwaike and Degaga Wolde and Abdoulaye Faye and Blessing Sibanda and Orevaoghene Ahia and Bonaventure F. P. Dossou and Kelechi Ogueji and Thierno Ibrahima DIOP and Abdoulaye Diallo and Adewale Akinfaderin and Tendai Marengereke and Salomey Osei},\n    journal = {Transactions of the Association for Computational Linguistics (TACL)},\n    month = {},\n    url = {\n    year = {2021}\n}\n```\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of Davlan/distilbert-base-multilingual-cased-masakhaner?", "answers": [{"text": "distilbert", "answer_start": 191, "answer_end": 200}]}]}]}, {"title": "Davlan/xlm-roberta-base-finetuned-shona", "paragraphs": [{"context": "---\nlicense: apache-2.0\n---\n", "qas": []}]}, {"title": "Davlan/xlm-roberta-base-finetuned-swahili", "paragraphs": [{"context": "Hugging Face's logo\n---\nlanguage: sw\ndatasets:\n\n---\n# xlm-roberta-base-finetuned-swahili\n## Model description\n**xlm-roberta-base-finetuned-swahili** is a **Swahili RoBERTa** model obtained by fine-tuning **xlm-roberta-base** model on Swahili language texts.  It provides **better performance** than the XLM-RoBERTa on text classification and named entity recognition datasets.  \n\nSpecifically, this model is a *xlm-roberta-base* model that was fine-tuned on Swahili corpus. \n## Intended uses & limitations\n#### How to use\nYou can use this model with Transformers *pipeline* for masked token prediction.\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='Davlan/xlm-roberta-base-finetuned-swahili')\n>>> unmasker(\"Jumatatu, Bwana Kagame alielezea shirika la France24 huko <mask> kwamba hakuna uhalifu ulitendwa\")\n                    \n[{'sequence': 'Jumatatu, Bwana Kagame alielezea shirika la France24 huko Ufaransa kwamba hakuna uhalifu ulitendwa', \n'score': 0.5077782273292542, \n'token': 190096, \n'token_str': 'Ufaransa'}, \n{'sequence': 'Jumatatu, Bwana Kagame alielezea shirika la France24 huko Paris kwamba hakuna uhalifu ulitendwa', \n'score': 0.3657738268375397, \n'token': 7270, \n'token_str': 'Paris'}, \n{'sequence': 'Jumatatu, Bwana Kagame alielezea shirika la France24 huko Gabon kwamba hakuna uhalifu ulitendwa', \n'score': 0.01592041552066803, \n'token': 176392, \n'token_str': 'Gabon'}, \n{'sequence': 'Jumatatu, Bwana Kagame alielezea shirika la France24 huko France kwamba hakuna uhalifu ulitendwa', \n'score': 0.010881908237934113, \n'token': 9942, \n'token_str': 'France'}, \n{'sequence': 'Jumatatu, Bwana Kagame alielezea shirika la France24 huko Marseille kwamba hakuna uhalifu ulitendwa', \n'score': 0.009554869495332241, \n'token': 185918, \n'token_str': 'Marseille'}]\n\n\n```\n#### Limitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. \n## Training data\nThis model was fine-tuned on [Swahili CC-100](\n\n## Training procedure\nThis model was trained on a single NVIDIA V100 GPU\n\n## Eval results on Test set (F-score, average over 5 runs)\nDataset sw_roberta F1\n--\n[MasakhaNER](  89.46\n\n### BibTeX entry and citation info\nBy David Adelani\n```\n\n```\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of Davlan/xlm-roberta-base-finetuned-swahili?", "answers": [{"text": "xlm-roberta", "answer_start": 54, "answer_end": 64}]}, {"id": "q2", "question": "What is the model task of Davlan/xlm-roberta-base-finetuned-swahili?", "answers": [{"text": "fill-mask", "answer_start": 676, "answer_end": 684}]}]}]}, {"title": "Duc/distilbert-base-uncased-finetuned-ner", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- conll2003\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      args: conll2003\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.9261715296198055\n    - name: Recall\n      type: recall\n      value: 0.9374650408323079\n    - name: F1\n      type: f1\n      value: 0.9317840662700839\n    - name: Accuracy\n      type: accuracy\n      value: 0.9840659602522758\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-ner\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the conll2003 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0604\n- Precision: 0.9262\n- Recall: 0.9375\n- F1: 0.9318\n- Accuracy: 0.9841\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0684           0.9206  0.9813   |\n 2.0    0.0607           0.9349  0.9835   |\n 3.0    0.0604           0.9375  0.9841   |\n\n\n### Framework versions\n\n- Transformers 4.12.3\n- Pytorch 1.9.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Duc/distilbert-base-uncased-finetuned-ner?", "answers": [{"text": "distilbert", "answer_start": 144, "answer_end": 153}]}, {"id": "q2", "question": "What is the model task of Duc/distilbert-base-uncased-finetuned-ner?", "answers": [{"text": "token-classification", "answer_start": 248, "answer_end": 267}]}]}]}, {"title": "DueLinx0402/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of DueLinx0402/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Elron/bleurt-large-512", "paragraphs": [{"context": "## BLEURT\n\nPytorch version of the original BLEURT models from ACL paper [\"BLEURT: Learning Robust Metrics for Text Generation\"]( by \nThibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research.\n\nThe code for model conversion was originated from [this notebook]( mentioned [here](\n\n## Usage Example\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-large-512\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-large-512\")\nmodel.eval()\n\nreferences = [\"hello world\", \"hello world\"]\ncandidates = [\"hi universe\", \"bye world\"]\n\nwith torch.no_grad():\n  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\nprint(scores) # tensor([0.9877, 0.0475])\n```\n", "qas": []}]}, {"title": "EmileAjar/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags :\n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of EmileAjar/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 13, "answer_end": 26}]}]}]}, {"title": "Emmanuel/bert-finetuned-ner", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- conll2003\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: bert-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      args: conll2003\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.9317394888705688\n    - name: Recall\n      type: recall\n      value: 0.9510265903736116\n    - name: F1\n      type: f1\n      value: 0.9412842508536686\n    - name: Accuracy\n      type: accuracy\n      value: 0.9865779713898863\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-finetuned-ner\n\nThis model is a fine-tuned version of [bert-base-cased]( on the conll2003 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0603\n- Precision: 0.9317\n- Recall: 0.9510\n- F1: 0.9413\n- Accuracy: 0.9866\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0660           0.9350  0.9827   |\n 2.0    0.0579           0.9498  0.9864   |\n 3.0    0.0603           0.9510  0.9866   |\n\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Emmanuel/bert-finetuned-ner?", "answers": [{"text": "bert", "answer_start": 144, "answer_end": 147}]}, {"id": "q2", "question": "What is the model task of Emmanuel/bert-finetuned-ner?", "answers": [{"text": "token-classification", "answer_start": 229, "answer_end": 248}]}]}]}, {"title": "Erfan/mT5-small_Farsi_Title_Generator", "paragraphs": [{"context": "---\nlanguage:\n- fa\ntags:\n- Title-Generation\nmetrics:\n- ROUGH\n---\n", "qas": []}]}, {"title": "Finnish-NLP/gpt2-medium-finnish", "paragraphs": [{"context": "---\nlanguage:\n- fi\nlicense: apache-2.0\ntags:\n- finnish\n- gpt2\ndatasets:\n- Finnish-NLP/mc4_fi_cleaned\n- wikipedia\nwidget:\n- text: \"Teksti\u00e4 tuottava teko\u00e4ly on\"\n\n---\n\n# GPT-2 medium for Finnish\n\nPretrained GPT-2 medium model on Finnish language using a causal language modeling (CLM) objective. GPT-2 was introduced in\n[this paper](\nand first released at [this page](\n\n**Note**: this model is 345M parameter variant as in Huggingface's [GPT-2-medium config]( so not the famous big 1.5B parameter variant by OpenAI. We also have bigger 774M parameter variant [gpt2-large-finnish]( available which performs better compared to this model.\n\n## Model description\n\nFinnish GPT-2 is a transformers model pretrained on a very large corpus of Finnish data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the Finnish language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.\n\n## Intended uses & limitations\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub]( to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\n>>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model='Finnish-NLP/gpt2-medium-finnish')\n>>> generator(\"Teksti\u00e4 tuottava teko\u00e4ly on\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': 'Teksti\u00e4 tuottava teko\u00e4ly on tullut ihmisten arkeen viime vuosina. Se auttaa hahmottamaan ja tulkitsemaan monimutkaisia kokonaisuuksia ja ilmi\u00f6it\u00e4, joita ihmiset tekev\u00e4t esimerkiksi ruokakaupassa'},\n {'generated_text': 'Teksti\u00e4 tuottava teko\u00e4ly on jo ottanut haltuunsa my\u00f6s ihmisten k\u00e4ytt\u00e4mi\u00e4 sovelluksia ja esimerkiksi pankkipalveluita. Sen vuoksi teko\u00e4ly on t\u00e4rke\u00e4 kumppani etenkin yritysten liiketoiminnan kehitt\u00e4misess\u00e4.-'},\n {'generated_text': 'Teksti\u00e4 tuottava teko\u00e4ly on teko\u00e4lylle luonnollinen valinta, sill\u00e4 sen avulla voi kommunikoida ihmisten kanssa hyvin pitk\u00e4lle samalla tavalla kuin tietokoneiden kanssa. Se on kehittynyt muun'},\n {'generated_text': 'Teksti\u00e4 tuottava teko\u00e4ly on ihmisen kehitt\u00e4m\u00e4 teko\u00e4ly, jota ei viel\u00e4 ole pystytty rakentamaan. Teko\u00e4ly kykenee toimimaan esimerkiksi matemaattisissa, tilastollisissa ja sosiaalisissa'},\n {'generated_text': 'Teksti\u00e4 tuottava teko\u00e4ly on jo niin iso juttu ettei sit\u00e4 kannata rajoittaakaan. Ja jos se saadaan k\u00e4ytt\u00f6\u00f6n, niin se voi jo pian syrj\u00e4ytt\u00e4\u00e4 perinteisen'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('Finnish-NLP/gpt2-medium-finnish')\nmodel = GPT2Model.from_pretrained('Finnish-NLP/gpt2-medium-finnish')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('Finnish-NLP/gpt2-medium-finnish')\nmodel = TFGPT2Model.from_pretrained('Finnish-NLP/gpt2-medium-finnish', from_pt=True)\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model can have biased predictions. This bias will also affect all fine-tuned versions of this model.\n\nAs with all language models, it is hard to predict in advance how the Finnish GPT-2 will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.\n\n## Training data\n\nThis Finnish GPT-2 model was pretrained on the combination of six datasets:\n- [mc4_fi_cleaned]( the dataset mC4 is a multilingual colossal, cleaned version of Common Crawl's web crawl corpus. We used the Finnish subset of the mC4 dataset and further cleaned it with our own text data cleaning codes (check the dataset repo).\n- [wikipedia]( We used the Finnish subset of the wikipedia (August 2021) dataset\n- [Yle Finnish News Archive 2011-2018](\n- [Yle Finnish News Archive 2019-2020](\n- [Finnish News Agency Archive (STT)](\n- [The Suomi24 Sentences Corpus](\n\nRaw datasets were cleaned to filter out bad quality and non-Finnish examples. Together these cleaned datasets were around 84GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 512 consecutive tokens.\n\n### Pretraining\n\nThe model was trained on TPUv3-8 VM, sponsored by the [Google TPU Research Cloud]( for 360k steps (a bit over 1 epoch, 128 batch size). The optimizer used was a AdamW with learning rate 1e-4, learning rate warmup for 4000 steps and cosine decay of the learning rate after.\n\n\n## Evaluation results\n\nEvaluation was done using the *validation* split of the [mc4_fi_cleaned]( dataset with [Perplexity]( (smaller score the better) as the evaluation metric. As seen from the table below, this model (the first row of the table) performs better than our smaller [gpt2-finnish]( model variant but loses to our bigger [gpt2-large-finnish]( model.\n\n Perplexity |\n------------|\n34.08       |\n44.19       |\n**30.74**   |\n\n## Acknowledgements\n\nThis project would not have been possible without compute generously provided by Google through the\n[TPU Research Cloud](\n\n## Team Members\n\n- Aapo Tanskanen, [Hugging Face profile]( [LinkedIn profile](\n- Rasmus Toivanen, [Hugging Face profile]( [LinkedIn profile](\n\nFeel free to contact us for more details \ud83e\udd17", "qas": [{"id": "q1", "question": "What is the model architecture of Finnish-NLP/gpt2-medium-finnish?", "answers": [{"text": "gpt2", "answer_start": 57, "answer_end": 60}]}, {"id": "q2", "question": "What is the model task of Finnish-NLP/gpt2-medium-finnish?", "answers": [{"text": "text-generation", "answer_start": 2000, "answer_end": 2014}]}]}]}, {"title": "Finnish-NLP/roberta-large-finnish", "paragraphs": [{"context": "---\nlanguage:\n- fi\nlicense: apache-2.0\ntags:\n- finnish\n- roberta\ndatasets:\n- Finnish-NLP/mc4_fi_cleaned\n- wikipedia\nwidget:\n- text: \"Moikka olen <mask> kielimalli.\"\n\n---\n\n# RoBERTa large model for Finnish\n\nPretrained RoBERTa model on Finnish language using a masked language modeling (MLM) objective. RoBERTa was introduced in\n[this paper]( and first released in\n[this repository]( This model is case-sensitive: it\nmakes a difference between finnish and Finnish.\n\n## Model description\n\nFinnish RoBERTa is a transformers model pretrained on a large corpus of Finnish data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the Finnish language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the RoBERTa model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='Finnish-NLP/roberta-large-finnish')\n>>> unmasker(\"Moikka olen <mask> kielimalli.\")\n\n[{'sequence': 'Moikka olen hyv\u00e4 kielimalli.',\n  'score': 0.1535797119140625,\n  'token': 767,\n  'token_str': ' hyv\u00e4'},\n {'sequence': 'Moikka olen paras kielimalli.',\n  'score': 0.04795042425394058,\n  'token': 2888,\n  'token_str': ' paras'},\n {'sequence': 'Moikka olen huono kielimalli.',\n  'score': 0.04251479730010033,\n  'token': 3217,\n  'token_str': ' huono'},\n {'sequence': 'Moikka olen my\u00f6s kielimalli.',\n  'score': 0.027469098567962646,\n  'token': 520,\n  'token_str': ' my\u00f6s'},\n {'sequence': 'Moikka olen se kielimalli.',\n  'score': 0.013878575526177883,\n  'token': 358,\n  'token_str': ' se'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('Finnish-NLP/roberta-large-finnish')\nmodel = RobertaModel.from_pretrained('Finnish-NLP/roberta-large-finnish')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('Finnish-NLP/roberta-large-finnish')\nmodel = TFRobertaModel.from_pretrained('Finnish-NLP/roberta-large-finnish', from_pt=True)\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions.\n\n## Training data\n\nThis Finnish RoBERTa model was pretrained on the combination of five datasets:\n- [mc4]( the dataset mC4 is a multilingual colossal, cleaned version of Common Crawl's web crawl corpus. We used the Finnish subset of the mC4 dataset\n- [wikipedia]( We used the Finnish subset of the wikipedia (August 2021) dataset\n- [Yle Finnish News Archive](\n- [Finnish News Agency Archive (STT)](\n- [The Suomi24 Sentences Corpus](\n\nRaw datasets were cleaned to filter out bad quality and non-Finnish examples. Together these cleaned datasets were around 78GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50265. The inputs of\nthe model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on TPUv3-8 VM, sponsored by the [Google TPU Research Cloud]( for 2 epochs with a sequence length of 128 and continuing for one more epoch with a sequence length of 512. The optimizer used is Adafactor with a learning rate of 2e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and \\\\(\\epsilon = 1e-6\\\\), learning rate warmup for 1500 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nEvaluation was done by fine-tuning the model on downstream text classification task with two different labeled datasets: [Yle News]( and [Eduskunta]( Yle News classification fine-tuning was done with two different sequence lengths: 128 and 512 but Eduskunta only with 128 sequence length.\nWhen fine-tuned on those datasets, this model (the first row of the table) achieves the following accuracy results compared to the [FinBERT (Finnish BERT)]( and to our previous [Finnish RoBERTa-large]( trained during the Hugging Face JAX/Flax community week:\n\n Average   Yle News 512 length \n-------------------------------\n88.02     95.23                \n**88.82** **95.49**            \n87.72     95.06                \n\nTo conclude, this model improves on our previous [Finnish RoBERTa-large]( model trained during the Hugging Face JAX/Flax community week but is still slightly (~ 1%) losing to the [FinBERT (Finnish BERT)]( model.\n\n## Acknowledgements\n\nThis project would not have been possible without compute generously provided by Google through the\n[TPU Research Cloud](\n\n## Team Members\n\n- Aapo Tanskanen, [Hugging Face profile]( [LinkedIn profile](\n- Rasmus Toivanen [Hugging Face profile]( [LinkedIn profile](\n- Tommi Vehvil\u00e4inen [Hugging Face profile](\n\nFeel free to contact us for more details \ud83e\udd17", "qas": [{"id": "q1", "question": "What is the model architecture of Finnish-NLP/roberta-large-finnish?", "answers": [{"text": "roberta", "answer_start": 57, "answer_end": 63}]}, {"id": "q2", "question": "What is the model task of Finnish-NLP/roberta-large-finnish?", "answers": [{"text": "fill-mask", "answer_start": 2258, "answer_end": 2266}]}]}]}, {"title": "Firat/albert-base-v2-finetuned-squad", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: albert-base-v2-finetuned-squad\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# albert-base-v2-finetuned-squad\n\nThis model is a fine-tuned version of [albert-base-v2]( on the squad dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9901\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.9056          |\n 2.0    0.8975          |\n 3.0    0.9901          |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Firat/albert-base-v2-finetuned-squad?", "answers": [{"text": "albert", "answer_start": 94, "answer_end": 99}]}]}]}, {"title": "Galaxy/DialoGPT-small-hermoine", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Galaxy/DialoGPT-small-hermoine?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Galuh/id-journal-gpt2", "paragraphs": [{"context": "---\nlanguage: id\nwidget:\n- text: \"Penelitian ini bertujuan untuk menentukan identitas invertebrata laut dari Perairan Papua dengan teknik DNA barcoding\"\n---\n\n# Indonesian GPT-2 finetuned on Indonesian academic journals\nThis is the [Indonesian gpt2-small model]( fine-tuned to abstracts of Indonesian academic journals. All training was done on a TPUv2-8 VM sponsored by [TPU Research Cloud](\n\nThe demo can be found [here](\n\n## How to use\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, \nwe set a seed for reproducibility:\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='Galuh/id-journal-gpt2')\n>>> set_seed(42)\n>>> generator(\"Penelitian ini menggunakan teknik DNA barcoding untuk\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk mendeteksi perubahan genetik bakteri pada udang windu. Empat tahap telah dilakukan, meliputi preparasi media untuk larva,'},\n {'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk identifikasi gen pengasil flavonoid.  Data yang diperoleh dari hasil PCR diidentifikasi dengan teknik sekuensing'},\n {'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk mengekstraksi fragmen DNA dari sampel kulit buaya dan tulang anjing, di mana proses ini melibatkan karakterisasi enzim yang'},\n {'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk melakukan transformasi. Tahapan transformasi meliputi seleksi sel dengan urutan (2, 8, 16,..., 18) dan'},\n {'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk amplifikasi genom DNA dengan menggunakan primer TG8226 dan TG806. Metode pol'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('Galuh/id-journal-gpt2')\nmodel = GPT2Model.from_pretrained('Galuh/id-journal-gpt2')\ntext = \"Ubah dengan teks apa saja.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('Galuh/id-journal-gpt2')\nmodel = TFGPT2Model.from_pretrained('Galuh/id-journal-gpt2')\ntext = \"Ubah dengan teks apa saja.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n## Limitations and bias  \nThis model is originally the [Indonesian gpt2-small model]( thus this model is also subject to the same [limitations and bias as the original model]( More detailed bias and analysis on this specific model is coming soon.\n\n## Training data\nThe model was trained on a dataset of Indonesian journals. We only trained this model on the abstracts. We extract the abstract by writing a script to find any text that is located between the word \"Abstrak\" (abstract) and \"Kata kunci\" (keywords). The extraction script can be found [here]( To separate each abstract, we also add an end of text token (`<>`) between each abstract.\n\nThe information of the sub-dataset and the distribution of the training and evaluation dataset are as follows:\n\n count \n ---------- \n 146,248      \n 16,250      \n\n## Training procedure \nThe model was trained on a TPUv2-8 VM provided by [TPU Research Cloud]( The training duration was `2h 30m 57s`.\n\n### Evaluation results \nThe model achieves the following results without any fine-tuning (zero-shot):\n\n train loss  eval perplexity |\n ----------  ---------- |\n 2.913       17.37   |\n\n### Tracking\nThe training process was tracked in [TensorBoard](", "qas": [{"id": "q1", "question": "What is the model architecture of Galuh/id-journal-gpt2?", "answers": [{"text": "gpt2", "answer_start": 243, "answer_end": 246}]}, {"id": "q2", "question": "What is the model task of Galuh/id-journal-gpt2?", "answers": [{"text": "text-generation", "answer_start": 675, "answer_end": 689}]}]}]}, {"title": "Galuh/wav2vec2-large-xlsr-indonesian", "paragraphs": [{"context": "---\nlanguage: id\ndatasets:\n- common_voice \nmetrics:\n- wer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 Indonesian by Galuh\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice id\n      type: common_voice\n      args: id\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 21.07\n---\n\n# Wav2Vec2-Large-XLSR-Indonesian\n\nThis is the model for Wav2Vec2-Large-XLSR-Indonesian, a fine-tuned \n[facebook/wav2vec2-large-xlsr-53](\nmodel on the [Indonesian Common Voice dataset](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\ntest_dataset = load_dataset(\"common_voice\", \"id\", split=\"test[:2%]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"Galuh/wav2vec2-large-xlsr-indonesian\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"Galuh/wav2vec2-large-xlsr-indonesian\")\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n  speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n  batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n  return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n  logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on the Indonesian test data of Common Voice.\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\ntest_dataset = load_dataset(\"common_voice\", \"id\", split=\"test\")\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"Galuh/wav2vec2-large-xlsr-indonesian\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"Galuh/wav2vec2-large-xlsr-indonesian\") \nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\'\\\u201d\\\ufffd]'\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n  batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n  speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n  resampler = torchaudio.transforms.Resample(sampling_rate, 16_000)\n  batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n  return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n  inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n  with torch.no_grad():\n    logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n\n  pred_ids = torch.argmax(logits, dim=-1)\n  batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n  return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n```\n\n**Test Result**: 18.32 %\n\n## Training\n\nThe Common Voice `train`, `validation`, and ... datasets were used for training as well as ... and ...  # TODO\n\nThe script used for training can be found [here]( \n(will be available soon)", "qas": [{"id": "q1", "question": "What is the model architecture of Galuh/wav2vec2-large-xlsr-indonesian?", "answers": [{"text": "wav2vec2", "answer_start": 581, "answer_end": 588}]}, {"id": "q2", "question": "What is the model task of Galuh/wav2vec2-large-xlsr-indonesian?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 74, "answer_end": 101}]}, {"id": "q3", "question": "What is the model category of Galuh/wav2vec2-large-xlsr-indonesian?", "answers": [{"text": "audio", "answer_start": 66, "answer_end": 70}]}]}]}, {"title": "GamerMan02/DialoGPT-medium-gamerbot", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Gamer Bot DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of GamerMan02/DialoGPT-medium-gamerbot?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "GammaPTest/e_bot", "paragraphs": [{"context": "This be a test", "qas": []}]}, {"title": "GanjinZero/coder_all", "paragraphs": [{"context": "---\nlanguage: \n  - en\nlicense: apache-2.0\n\ntags:\n- bert\n- biomedical\n\n---\nCODER: Knowledge infused cross-lingual medical term embedding for term normalization.\n\nMulti lingual Version.\n\nGithub Link: \n\n```\n@article{YUAN2022103983,\ntitle = {CODER: Knowledge-infused cross-lingual medical term embedding for term normalization},\njournal = {Journal of Biomedical Informatics},\npages = {103983},\nyear = {2022},\nissn = {1532-0464},\ndoi = {\nurl = {\nauthor = {Zheng Yuan and Zhengyun Zhao and Haixia Sun and Jiao Li and Fei Wang and Sheng Yu},\nkeywords = {medical term normalization, cross-lingual, medical term representation, knowledge graph embedding, contrastive learning}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of GanjinZero/coder_all?", "answers": [{"text": "bert", "answer_start": 51, "answer_end": 54}]}]}]}, {"title": "GanjinZero/coder_eng", "paragraphs": [{"context": "---\nlanguage: \n  - en\nlicense: apache-2.0\n\ntags:\n- bert\n- biomedical\n\n---\nCODER: Knowledge infused cross-lingual medical term embedding for term normalization.\nEnglish Version.\n\nGithub Link: \n\n```\n@article{YUAN2022103983,\ntitle = {CODER: Knowledge-infused cross-lingual medical term embedding for term normalization},\njournal = {Journal of Biomedical Informatics},\npages = {103983},\nyear = {2022},\nissn = {1532-0464},\ndoi = {\nurl = {\nauthor = {Zheng Yuan and Zhengyun Zhao and Haixia Sun and Jiao Li and Fei Wang and Sheng Yu},\nkeywords = {medical term normalization, cross-lingual, medical term representation, knowledge graph embedding, contrastive learning}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of GanjinZero/coder_eng?", "answers": [{"text": "bert", "answer_start": 51, "answer_end": 54}]}]}]}, {"title": "Gappy/DialoGPT-small-Zhongli", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Zhongli DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Gappy/DialoGPT-small-Zhongli?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Geotrend/bert-base-en-el-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n\nwidget:\n- text: \"Google generated 46 billion [MASK] in revenue.\"\n- text: \"Paris is the capital of [MASK].\"\n- text: \"Algiers is the largest city in [MASK].\"\n---\n\n# bert-base-en-el-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-el-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-el-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.\n", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-el-cased?", "answers": [{"text": "bert", "answer_start": 233, "answer_end": 236}]}]}]}, {"title": "Geotrend/bert-base-en-es-it-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-es-it-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-es-it-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-es-it-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-es-it-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-es-zh-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-es-zh-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-es-zh-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-es-zh-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-es-zh-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-ar-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-ar-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-ar-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-ar-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-ar-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n\nwidget:\n- text: \"Google generated 46 billion [MASK] in revenue.\"\n- text: \"Paris is the capital of [MASK].\"\n- text: \"Algiers is the largest city in [MASK].\"\n- text: \"Paris est la [MASK] de la France.\"\n- text: \"Paris est la capitale de la [MASK].\"\n- text: \"L'\u00e9lection am\u00e9ricaine a eu [MASK] en novembre 2020.\"\n---\n\n# bert-base-en-fr-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Multilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.\n", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-cased?", "answers": [{"text": "bert", "answer_start": 385, "answer_end": 388}]}]}]}, {"title": "Geotrend/bert-base-en-fr-da-ja-vi-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-da-ja-vi-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-da-ja-vi-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-da-ja-vi-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-da-ja-vi-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-de-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-de-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-de-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-de-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-de-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-de-no-da-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-de-no-da-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-de-no-da-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-de-no-da-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-de-no-da-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-es-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-es-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-es-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-es-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-es-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-es-de-zh-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-es-de-zh-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-es-de-zh-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-es-de-zh-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-es-de-zh-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-es-pt-it-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-es-pt-it-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-es-pt-it-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-es-pt-it-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Multilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-es-pt-it-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-it-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-it-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-it-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-it-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-it-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/bert-base-en-fr-uk-el-ro-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# bert-base-en-fr-uk-el-ro-cased\n\nWe are sharing smaller versions of [bert-base-multilingual-cased]( that handle a custom number of languages.\n\nUnlike [distilbert-base-multilingual-cased]( our versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-fr-uk-el-ro-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/bert-base-en-fr-uk-el-ro-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/bert-base-en-fr-uk-el-ro-cased?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "Geotrend/distilbert-base-en-fr-lt-no-pl-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-en-fr-lt-no-pl-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-en-fr-lt-no-pl-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-en-fr-lt-no-pl-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-en-fr-lt-no-pl-cased?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}]}]}, {"title": "Geotrend/distilbert-base-en-ja-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-en-ja-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-en-ja-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-en-ja-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-en-ja-cased?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}]}]}, {"title": "Geotrend/distilbert-base-en-lt-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-en-lt-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-en-lt-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-en-lt-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-en-lt-cased?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}]}]}, {"title": "Geotrend/distilbert-base-en-nl-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-en-nl-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-en-nl-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-en-nl-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-en-nl-cased?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}]}]}, {"title": "Geotrend/distilbert-base-en-no-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-en-no-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-en-no-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-en-no-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-en-no-cased?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}]}]}, {"title": "Geotrend/distilbert-base-en-pl-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-en-pl-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-en-pl-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-en-pl-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-en-pl-cased?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}]}]}, {"title": "Geotrend/distilbert-base-en-pt-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-en-pt-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-en-pt-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-en-pt-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-en-pt-cased?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}]}]}, {"title": "Geotrend/distilbert-base-en-ro-cased", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-en-ro-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-en-ro-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-en-ro-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-en-ro-cased?", "answers": [{"text": "distilbert", "answer_start": 76, "answer_end": 85}]}]}]}, {"title": "Geotrend/distilbert-base-es-cased", "paragraphs": [{"context": "---\nlanguage: es\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-es-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-es-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-es-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-es-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Geotrend/distilbert-base-hi-cased", "paragraphs": [{"context": "---\nlanguage: hi\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-hi-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-hi-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-hi-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-hi-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Geotrend/distilbert-base-lt-cased", "paragraphs": [{"context": "---\nlanguage: lt\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-lt-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-lt-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-lt-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-lt-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Geotrend/distilbert-base-no-cased", "paragraphs": [{"context": "---\nlanguage: no\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-no-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-no-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-no-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-no-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Geotrend/distilbert-base-pl-cased", "paragraphs": [{"context": "---\nlanguage: pl\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-pl-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-pl-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-pl-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-pl-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Geotrend/distilbert-base-pt-cased", "paragraphs": [{"context": "---\nlanguage: pt\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-pt-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-pt-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-pt-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-pt-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Geotrend/distilbert-base-ro-cased", "paragraphs": [{"context": "---\nlanguage: ro\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-ro-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-ro-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-ro-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-ro-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Geotrend/distilbert-base-ru-cased", "paragraphs": [{"context": "---\nlanguage: ru\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-ru-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-ru-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-ru-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-ru-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Geotrend/distilbert-base-sw-cased", "paragraphs": [{"context": "---\nlanguage: sw\n\ndatasets: wikipedia\n\nlicense: apache-2.0\n---\n\n# distilbert-base-sw-cased\n\nWe are sharing smaller versions of [distilbert-base-multilingual-cased]( that handle a custom number of languages.\n\nOur versions give exactly the same representations produced by the original model which preserves the original accuracy.\n\n\nFor more information please visit our paper: [Load What You Need: Smaller Versions of Multilingual BERT](\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"Geotrend/distilbert-base-sw-cased\")\nmodel = AutoModel.from_pretrained(\"Geotrend/distilbert-base-sw-cased\")\n\n```\n\nTo generate other smaller versions of multilingual transformers please visit [our Github repo](\n\n### How to cite\n\n```bibtex\n@inproceedings{smallermdistilbert,\n  title={Load What You Need: Smaller Versions of Mutlilingual BERT},\n  author={Abdaoui, Amine and Pradel, Camille and Sigel, Gr\u00e9goire},\n  booktitle={SustaiNLP / EMNLP},\n  year={2020}\n}\n```\n\n## Contact \n\nPlease contact amine@geotrend.fr for any question, feedback or request.", "qas": [{"id": "q1", "question": "What is the model architecture of Geotrend/distilbert-base-sw-cased?", "answers": [{"text": "distilbert", "answer_start": 66, "answer_end": 75}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ca-de", "paragraphs": [{"context": "---\nlanguage: \n- ca\n- de\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### cat-deu\n\n* source group: Catalan \n* target group: German \n*  OPUS readme: [cat-deu](\n\n*  model: transformer-align\n* source language(s): cat\n* target language(s): deu\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm12k,spm12k)\n* download original weights: [opus-2020-06-16.zip](\n* test set translations: [opus-2020-06-16.test.txt](\n* test set scores: [opus-2020-06-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 39.5 \t\n\n\n### System Info: \n- hf_name: cat-deu\n\n- source_languages: cat\n\n- target_languages: deu\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ca', 'de']\n\n- src_constituents: {'cat'}\n\n- tgt_constituents: {'deu'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm12k,spm12k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: cat\n\n- tgt_alpha3: deu\n\n- short_pair: ca-de\n\n- chrF2_score: 0.593\n\n- bleu: 39.5\n\n- brevity_penalty: 1.0\n\n- ref_len: 5643.0\n\n- src_name: Catalan\n\n- tgt_name: German\n\n- train_date: 2020-06-16\n\n- src_alpha2: ca\n\n- tgt_alpha2: de\n\n- prefer_old: False\n\n- long_pair: cat-deu\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ca-de?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-crs-es", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-crs-es\n\n* source languages: crs\n* target languages: es\n*  OPUS readme: [crs-es](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-15.zip](\n* test set translations: [opus-2020-01-15.test.txt](\n* test set scores: [opus-2020-01-15.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 26.1 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-crs-es?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-crs-fi", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-crs-fi\n\n* source languages: crs\n* target languages: fi\n*  OPUS readme: [crs-fi](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 25.6 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-crs-fi?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-crs-fr", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-crs-fr\n\n* source languages: crs\n* target languages: fr\n*  OPUS readme: [crs-fr](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 29.4 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-crs-fr?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-crs-sv", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-crs-sv\n\n* source languages: crs\n* target languages: sv\n*  OPUS readme: [crs-sv](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 29.3 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-crs-sv?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-cs-de", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-cs-de\n\n* source languages: cs\n* target languages: de\n*  OPUS readme: [cs-de](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-20.zip](\n* test set translations: [opus-2020-01-20.test.txt](\n* test set scores: [opus-2020-01-20.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 22.0 \t\n 21.1 \t\n 22.2 \t\n 22.1 \t\n 21.6 \t\n 22.2 \t\n 24.8 \t\n 23.6 \t\n 51.6 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-cs-de?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-cs-en", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-cs-en\n\n* source languages: cs\n* target languages: en\n*  OPUS readme: [cs-en](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2019-12-18.zip](\n* test set translations: [opus-2019-12-18.test.txt](\n* test set scores: [opus-2019-12-18.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 34.1 \t\n 30.4 \t\n 31.8 \t\n 28.7 \t\n 30.3 \t\n 58.0 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-cs-en?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-cs-eo", "paragraphs": [{"context": "---\nlanguage: \n- cs\n- eo\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### ces-epo\n\n* source group: Czech \n* target group: Esperanto \n*  OPUS readme: [ces-epo](\n\n*  model: transformer-align\n* source language(s): ces\n* target language(s): epo\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm4k,spm4k)\n* download original weights: [opus-2020-06-16.zip](\n* test set translations: [opus-2020-06-16.test.txt](\n* test set scores: [opus-2020-06-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 26.0 \t\n\n\n### System Info: \n- hf_name: ces-epo\n\n- source_languages: ces\n\n- target_languages: epo\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['cs', 'eo']\n\n- src_constituents: {'ces'}\n\n- tgt_constituents: {'epo'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm4k,spm4k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: ces\n\n- tgt_alpha3: epo\n\n- short_pair: cs-eo\n\n- chrF2_score: 0.45899999999999996\n\n- bleu: 26.0\n\n- brevity_penalty: 0.94\n\n- ref_len: 24901.0\n\n- src_name: Czech\n\n- tgt_name: Esperanto\n\n- train_date: 2020-06-16\n\n- src_alpha2: cs\n\n- tgt_alpha2: eo\n\n- prefer_old: False\n\n- long_pair: ces-epo\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-cs-eo?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-cs-fi", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-cs-fi\n\n* source languages: cs\n* target languages: fi\n*  OPUS readme: [cs-fi](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 25.5 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-cs-fi?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-cs-fr", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-cs-fr\n\n* source languages: cs\n* target languages: fr\n*  OPUS readme: [cs-fr](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 21.0 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-cs-fr?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-cs-sv", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-cs-sv\n\n* source languages: cs\n* target languages: sv\n*  OPUS readme: [cs-sv](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 30.6 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-cs-sv?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-loz", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-loz\n\n* source languages: en\n* target languages: loz\n*  OPUS readme: [en-loz](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 40.1 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-loz?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-lu", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-lu\n\n* source languages: en\n* target languages: lu\n*  OPUS readme: [en-lu](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 34.1 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-lu?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-lua", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-lua\n\n* source languages: en\n* target languages: lua\n*  OPUS readme: [en-lua](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 35.3 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-lua?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-lue", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-lue\n\n* source languages: en\n* target languages: lue\n*  OPUS readme: [en-lue](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 30.1 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-lue?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-lun", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-lun\n\n* source languages: en\n* target languages: lun\n*  OPUS readme: [en-lun](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 28.9 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-lun?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-luo", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-luo\n\n* source languages: en\n* target languages: luo\n*  OPUS readme: [en-luo](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-20.zip](\n* test set translations: [opus-2020-01-20.test.txt](\n* test set scores: [opus-2020-01-20.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 27.6 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-luo?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-mg", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-mg\n\n* source languages: en\n* target languages: mg\n*  OPUS readme: [en-mg](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 22.3 \t\n 35.5 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-mg?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-mk", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-mk\n\n* source languages: en\n* target languages: mk\n*  OPUS readme: [en-mk](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2019-12-18.zip](\n* test set translations: [opus-2019-12-18.test.txt](\n* test set scores: [opus-2019-12-18.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 52.1 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-mk?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-pon", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-pon\n\n* source languages: en\n* target languages: pon\n*  OPUS readme: [en-pon](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 32.4 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-pon?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-ru", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-ru\n\n* source languages: en\n* target languages: ru\n*  OPUS readme: [en-ru](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-02-11.zip](\n* test set translations: [opus-2020-02-11.test.txt](\n* test set scores: [opus-2020-02-11.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 31.1 \t\n 23.5 \t\n 27.5 \t\n 26.4 \t\n 29.1 \t\n 25.4 \t\n 27.1 \t\n 48.4 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-ru?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-run", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-run\n\n* source languages: en\n* target languages: run\n*  OPUS readme: [en-run](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-20.zip](\n* test set translations: [opus-2020-01-20.test.txt](\n* test set scores: [opus-2020-01-20.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 34.2 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-run?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-st", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-st\n\n* source languages: en\n* target languages: st\n*  OPUS readme: [en-st](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 49.8 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-st?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-swc", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-swc\n\n* source languages: en\n* target languages: swc\n*  OPUS readme: [en-swc](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 40.1 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-swc?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-en-tiv", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-tiv\n\n* source languages: en\n* target languages: tiv\n*  OPUS readme: [en-tiv](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 31.6 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-en-tiv?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-eo-es", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-eo-es\n\n* source languages: eo\n* target languages: es\n*  OPUS readme: [eo-es](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 44.2 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-eo-es?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-eo-fi", "paragraphs": [{"context": "---\nlanguage: \n- eo\n- fi\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### epo-fin\n\n* source group: Esperanto \n* target group: Finnish \n*  OPUS readme: [epo-fin](\n\n*  model: transformer-align\n* source language(s): epo\n* target language(s): fin\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm4k,spm4k)\n* download original weights: [opus-2020-06-16.zip](\n* test set translations: [opus-2020-06-16.test.txt](\n* test set scores: [opus-2020-06-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 15.9 \t\n\n\n### System Info: \n- hf_name: epo-fin\n\n- source_languages: epo\n\n- target_languages: fin\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['eo', 'fi']\n\n- src_constituents: {'epo'}\n\n- tgt_constituents: {'fin'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm4k,spm4k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: epo\n\n- tgt_alpha3: fin\n\n- short_pair: eo-fi\n\n- chrF2_score: 0.371\n\n- bleu: 15.9\n\n- brevity_penalty: 0.894\n\n- ref_len: 15881.0\n\n- src_name: Esperanto\n\n- tgt_name: Finnish\n\n- train_date: 2020-06-16\n\n- src_alpha2: eo\n\n- tgt_alpha2: fi\n\n- prefer_old: False\n\n- long_pair: epo-fin\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-eo-fi?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-eo-fr", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-eo-fr\n\n* source languages: eo\n* target languages: fr\n*  OPUS readme: [eo-fr](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-08.zip](\n* test set translations: [opus-2020-01-08.test.txt](\n* test set scores: [opus-2020-01-08.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 50.9 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-eo-fr?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-fi-mg", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-fi-mg\n\n* source languages: fi\n* target languages: mg\n*  OPUS readme: [fi-mg](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-24.zip](\n* test set translations: [opus-2020-01-24.test.txt](\n* test set scores: [opus-2020-01-24.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 21.7 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-fi-mg?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-fi-mh", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-fi-mh\n\n* source languages: fi\n* target languages: mh\n*  OPUS readme: [fi-mh](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-24.zip](\n* test set translations: [opus-2020-01-24.test.txt](\n* test set scores: [opus-2020-01-24.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 20.8 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-fi-mh?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-fr-kg", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-fr-kg\n\n* source languages: fr\n* target languages: kg\n*  OPUS readme: [fr-kg](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-09.zip](\n* test set translations: [opus-2020-01-09.test.txt](\n* test set scores: [opus-2020-01-09.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 30.4 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-fr-kg?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-fr-no", "paragraphs": [{"context": "---\nlanguage: \n- fr\n- no\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### fra-nor\n\n* source group: French \n* target group: Norwegian \n*  OPUS readme: [fra-nor](\n\n*  model: transformer-align\n* source language(s): fra\n* target language(s): nno nob\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm4k,spm4k)\n* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 36.1 \t\n\n\n### System Info: \n- hf_name: fra-nor\n\n- source_languages: fra\n\n- target_languages: nor\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['fr', 'no']\n\n- src_constituents: {'fra'}\n\n- tgt_constituents: {'nob', 'nno'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm4k,spm4k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: fra\n\n- tgt_alpha3: nor\n\n- short_pair: fr-no\n\n- chrF2_score: 0.555\n\n- bleu: 36.1\n\n- brevity_penalty: 0.981\n\n- ref_len: 3089.0\n\n- src_name: French\n\n- tgt_name: Norwegian\n\n- train_date: 2020-06-17\n\n- src_alpha2: fr\n\n- tgt_alpha2: no\n\n- prefer_old: False\n\n- long_pair: fra-nor\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-fr-no?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-fr-nso", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-fr-nso\n\n* source languages: fr\n* target languages: nso\n*  OPUS readme: [fr-nso](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 33.3 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-fr-nso?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-fr-ru", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-fr-ru\n\n* source languages: fr\n* target languages: ru\n*  OPUS readme: [fr-ru](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-24.zip](\n* test set translations: [opus-2020-01-24.test.txt](\n* test set scores: [opus-2020-01-24.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 37.9 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-fr-ru?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-fr-run", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-fr-run\n\n* source languages: fr\n* target languages: run\n*  OPUS readme: [fr-run](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-20.zip](\n* test set translations: [opus-2020-01-20.test.txt](\n* test set scores: [opus-2020-01-20.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 23.8 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-fr-run?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-gem-en", "paragraphs": [{"context": "---\nlanguage: \n- da\n- sv\n- af\n- nn\n- fy\n- fo\n- de\n- nb\n- nl\n- is\n- en\n- lb\n- yi\n- gem\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### gem-eng\n\n* source group: Germanic languages \n* target group: English \n*  OPUS readme: [gem-eng](\n\n*  model: transformer\n* source language(s): afr ang_Latn dan deu enm_Latn fao frr fry gos got_Goth gsw isl ksh ltz nds nld nno nob nob_Hebr non_Latn pdc sco stq swe swg yid\n* target language(s): eng\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus2m-2020-08-01.zip](\n* test set translations: [opus2m-2020-08-01.test.txt](\n* test set scores: [opus2m-2020-08-01.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 27.2 \t\n 26.3 \t\n 25.1 \t\n 28.3 \t\n 26.0 \t\n 26.8 \t\n 30.2 \t\n 30.7 \t\n 32.1 \t\n 36.9 \t\n 32.8 \t\n 40.2 \t\n 36.8 \t\n 62.8 \t\n 10.5 \t\n 61.6 \t\n 49.7 \t\n 23.9 \t\n 23.4 \t\n 10.2 \t\n 29.6 \t\n 17.8 \t\n 0.1 \t\n 15.3 \t\n 51.0 \t\n 6.7 \t\n 33.0 \t\n 54.0 \t\n 33.6 \t\n 58.9 \t\n 37.3 \t\n 54.9 \t\n 29.6 \t\n 40.5 \t\n 14.5 \t\n 62.0 \t\n 17.1 \t\n 19.4 \t\n\n\n### System Info: \n- hf_name: gem-eng\n\n- source_languages: gem\n\n- target_languages: eng\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['da', 'sv', 'af', 'nn', 'fy', 'fo', 'de', 'nb', 'nl', 'is', 'en', 'lb', 'yi', 'gem']\n\n- src_constituents: {'ksh', 'enm_Latn', 'got_Goth', 'stq', 'dan', 'swe', 'afr', 'pdc', 'gos', 'nno', 'fry', 'gsw', 'fao', 'deu', 'swg', 'sco', 'nob', 'nld', 'isl', 'eng', 'ltz', 'nob_Hebr', 'ang_Latn', 'frr', 'non_Latn', 'yid', 'nds'}\n\n- tgt_constituents: {'eng'}\n\n- src_multilingual: True\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: gem\n\n- tgt_alpha3: eng\n\n- short_pair: gem-en\n\n- chrF2_score: 0.687\n\n- bleu: 54.0\n\n- brevity_penalty: 0.993\n\n- ref_len: 72120.0\n\n- src_name: Germanic languages\n\n- tgt_name: English\n\n- train_date: 2020-08-01\n\n- src_alpha2: gem\n\n- tgt_alpha2: en\n\n- prefer_old: False\n\n- long_pair: gem-eng\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-gem-en?", "answers": [{"text": "translation", "answer_start": 95, "answer_end": 105}]}]}]}, {"title": "Helsinki-NLP/opus-mt-guw-en", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-guw-en\n\n* source languages: guw\n* target languages: en\n*  OPUS readme: [guw-en](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-09.zip](\n* test set translations: [opus-2020-01-09.test.txt](\n* test set scores: [opus-2020-01-09.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 44.8 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-guw-en?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-guw-es", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-guw-es\n\n* source languages: guw\n* target languages: es\n*  OPUS readme: [guw-es](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 27.2 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-guw-es?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-guw-fr", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-guw-fr\n\n* source languages: guw\n* target languages: fr\n*  OPUS readme: [guw-fr](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-09.zip](\n* test set translations: [opus-2020-01-09.test.txt](\n* test set scores: [opus-2020-01-09.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 29.7 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-guw-fr?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-he-de", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-he-de\n\n* source languages: he\n* target languages: de\n*  OPUS readme: [he-de](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-26.zip](\n* test set translations: [opus-2020-01-26.test.txt](\n* test set scores: [opus-2020-01-26.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 45.5 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-he-de?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-he-es", "paragraphs": [{"context": "---\nlanguage:\n- he\n- es\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n### he-es\n\n* source group: Hebrew \n* target group: Spanish \n*  OPUS readme: [heb-spa](\n\n*  model: transformer\n* source language(s): heb\n* target language(s): spa\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-12-10.zip](\n* test set translations: [opus-2020-12-10.test.txt](\n* test set scores: [opus-2020-12-10.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 51.3 \t\n\n\n### System Info: \n- hf_name: he-es\n\n- source_languages: heb\n\n- target_languages: spa\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['he', 'es']\n\n- src_constituents: ('Hebrew', {'heb'})\n\n- tgt_constituents: ('Spanish', {'spa'})\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- long_pair: heb-spa\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: heb\n\n- tgt_alpha3: spa\n\n- chrF2_score: 0.6890000000000001\n\n- bleu: 51.3\n\n- brevity_penalty: 0.97\n\n- ref_len: 14213.0\n\n- src_name: Hebrew\n\n- tgt_name: Spanish\n\n- train_date: 2020-12-10 00:00:00\n\n- src_alpha2: he\n\n- tgt_alpha2: es\n\n- prefer_old: False\n\n- short_pair: he-es\n\n- helsinki_git_sha: b317f78a3ec8a556a481b6a53dc70dc11769ca96\n\n- transformers_git_sha: 1310e1a758edc8e89ec363db76863c771fbeb1de\n\n- port_machine: LM0-400-22516.local\n\n- port_time: 2020-12-11-09:15", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-he-es?", "answers": [{"text": "translation", "answer_start": 33, "answer_end": 43}]}]}]}, {"title": "Helsinki-NLP/opus-mt-he-it", "paragraphs": [{"context": "---\nlanguage:\n- he\n- it\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n### he-it\n\n* source group: Hebrew \n* target group: Italian \n*  OPUS readme: [heb-ita](\n\n*  model: transformer\n* source language(s): heb\n* target language(s): ita\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-12-10.zip](\n* test set translations: [opus-2020-12-10.test.txt](\n* test set scores: [opus-2020-12-10.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 41.1 \t\n\n\n### System Info: \n- hf_name: he-it\n\n- source_languages: heb\n\n- target_languages: ita\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['he', 'it']\n\n- src_constituents: ('Hebrew', {'heb'})\n\n- tgt_constituents: ('Italian', {'ita'})\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- long_pair: heb-ita\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: heb\n\n- tgt_alpha3: ita\n\n- chrF2_score: 0.643\n\n- bleu: 41.1\n\n- brevity_penalty: 0.997\n\n- ref_len: 11464.0\n\n- src_name: Hebrew\n\n- tgt_name: Italian\n\n- train_date: 2020-12-10 00:00:00\n\n- src_alpha2: he\n\n- tgt_alpha2: it\n\n- prefer_old: False\n\n- short_pair: he-it\n\n- helsinki_git_sha: b317f78a3ec8a556a481b6a53dc70dc11769ca96\n\n- transformers_git_sha: 1310e1a758edc8e89ec363db76863c771fbeb1de\n\n- port_machine: LM0-400-22516.local\n\n- port_time: 2020-12-11-11:50", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-he-it?", "answers": [{"text": "translation", "answer_start": 33, "answer_end": 43}]}]}]}, {"title": "Helsinki-NLP/opus-mt-he-ru", "paragraphs": [{"context": "---\nlanguage:\n- he\n- ru\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### he-ru\n\n* source group: Hebrew \n* target group: Russian \n*  OPUS readme: [heb-rus](\n\n*  model: transformer\n* source language(s): heb\n* target language(s): rus\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-10-04.zip](\n* test set translations: [opus-2020-10-04.test.txt](\n* test set scores: [opus-2020-10-04.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 40.5 \t\n\n\n### System Info: \n- hf_name: he-ru\n\n- source_languages: heb\n\n- target_languages: rus\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['he', 'ru']\n\n- src_constituents: ('Hebrew', {'heb'})\n\n- tgt_constituents: ('Russian', {'rus'})\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- long_pair: heb-rus\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: heb\n\n- tgt_alpha3: rus\n\n- chrF2_score: 0.599\n\n- bleu: 40.5\n\n- brevity_penalty: 0.963\n\n- ref_len: 16583.0\n\n- src_name: Hebrew\n\n- tgt_name: Russian\n\n- train_date: 2020-10-04 00:00:00\n\n- src_alpha2: he\n\n- tgt_alpha2: ru\n\n- prefer_old: False\n\n- short_pair: he-ru\n\n- helsinki_git_sha: 61fd6908b37d9a7b21cc3e27c1ae1fccedc97561\n\n- transformers_git_sha: b0a907615aca0d728a9bc90f16caef0848f6a435\n\n- port_machine: LM0-400-22516.local\n\n- port_time: 2020-10-26-16:16", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-he-ru?", "answers": [{"text": "translation", "answer_start": 33, "answer_end": 43}]}]}]}, {"title": "Helsinki-NLP/opus-mt-is-es", "paragraphs": [{"context": "---\nlanguage: \n- is\n- es\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### isl-spa\n\n* source group: Icelandic \n* target group: Spanish \n*  OPUS readme: [isl-spa](\n\n*  model: transformer-align\n* source language(s): isl\n* target language(s): spa\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 51.2 \t\n\n\n### System Info: \n- hf_name: isl-spa\n\n- source_languages: isl\n\n- target_languages: spa\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['is', 'es']\n\n- src_constituents: {'isl'}\n\n- tgt_constituents: {'spa'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: isl\n\n- tgt_alpha3: spa\n\n- short_pair: is-es\n\n- chrF2_score: 0.665\n\n- bleu: 51.2\n\n- brevity_penalty: 0.985\n\n- ref_len: 1229.0\n\n- src_name: Icelandic\n\n- tgt_name: Spanish\n\n- train_date: 2020-06-17\n\n- src_alpha2: is\n\n- tgt_alpha2: es\n\n- prefer_old: False\n\n- long_pair: isl-spa\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-is-es?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-is-fr", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-is-fr\n\n* source languages: is\n* target languages: fr\n*  OPUS readme: [is-fr](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-09.zip](\n* test set translations: [opus-2020-01-09.test.txt](\n* test set scores: [opus-2020-01-09.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 25.0 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-is-fr?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-is-it", "paragraphs": [{"context": "---\nlanguage: \n- is\n- it\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### isl-ita\n\n* source group: Icelandic \n* target group: Italian \n*  OPUS readme: [isl-ita](\n\n*  model: transformer-align\n* source language(s): isl\n* target language(s): ita\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 46.7 \t\n\n\n### System Info: \n- hf_name: isl-ita\n\n- source_languages: isl\n\n- target_languages: ita\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['is', 'it']\n\n- src_constituents: {'isl'}\n\n- tgt_constituents: {'ita'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: isl\n\n- tgt_alpha3: ita\n\n- short_pair: is-it\n\n- chrF2_score: 0.662\n\n- bleu: 46.7\n\n- brevity_penalty: 0.977\n\n- ref_len: 1450.0\n\n- src_name: Icelandic\n\n- tgt_name: Italian\n\n- train_date: 2020-06-17\n\n- src_alpha2: is\n\n- tgt_alpha2: it\n\n- prefer_old: False\n\n- long_pair: isl-ita\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-is-it?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ja-ms", "paragraphs": [{"context": "---\nlanguage: \n- ja\n- ms\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### jpn-msa\n\n* source group: Japanese \n* target group: Malay (macrolanguage) \n*  OPUS readme: [jpn-msa](\n\n*  model: transformer-align\n* source language(s): jpn jpn_Hani jpn_Hira jpn_Kana\n* target language(s): ind zlm_Latn zsm_Latn\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 21.5 \t\n\n\n### System Info: \n- hf_name: jpn-msa\n\n- source_languages: jpn\n\n- target_languages: msa\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ja', 'ms']\n\n- src_constituents: {'jpn_Hang', 'jpn', 'jpn_Yiii', 'jpn_Kana', 'jpn_Hani', 'jpn_Bopo', 'jpn_Latn', 'jpn_Hira'}\n\n- tgt_constituents: {'zsm_Latn', 'ind', 'max_Latn', 'zlm_Latn', 'min'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: jpn\n\n- tgt_alpha3: msa\n\n- short_pair: ja-ms\n\n- chrF2_score: 0.469\n\n- bleu: 21.5\n\n- brevity_penalty: 0.9259999999999999\n\n- ref_len: 17028.0\n\n- src_name: Japanese\n\n- tgt_name: Malay (macrolanguage)\n\n- train_date: 2020-06-17\n\n- src_alpha2: ja\n\n- tgt_alpha2: ms\n\n- prefer_old: False\n\n- long_pair: jpn-msa\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ja-ms?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ja-nl", "paragraphs": [{"context": "---\nlanguage: \n- ja\n- nl\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### jpn-nld\n\n* source group: Japanese \n* target group: Dutch \n*  OPUS readme: [jpn-nld](\n\n*  model: transformer-align\n* source language(s): jpn jpn_Hani jpn_Hira jpn_Kana jpn_Latn\n* target language(s): nld\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 34.7 \t\n\n\n### System Info: \n- hf_name: jpn-nld\n\n- source_languages: jpn\n\n- target_languages: nld\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ja', 'nl']\n\n- src_constituents: {'jpn_Hang', 'jpn', 'jpn_Yiii', 'jpn_Kana', 'jpn_Hani', 'jpn_Bopo', 'jpn_Latn', 'jpn_Hira'}\n\n- tgt_constituents: {'nld'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: jpn\n\n- tgt_alpha3: nld\n\n- short_pair: ja-nl\n\n- chrF2_score: 0.534\n\n- bleu: 34.7\n\n- brevity_penalty: 0.938\n\n- ref_len: 25849.0\n\n- src_name: Japanese\n\n- tgt_name: Dutch\n\n- train_date: 2020-06-17\n\n- src_alpha2: ja\n\n- tgt_alpha2: nl\n\n- prefer_old: False\n\n- long_pair: jpn-nld\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ja-nl?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ja-pl", "paragraphs": [{"context": "---\nlanguage: \n- ja\n- pl\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### jpn-pol\n\n* source group: Japanese \n* target group: Polish \n*  OPUS readme: [jpn-pol](\n\n*  model: transformer-align\n* source language(s): jpn jpn_Bopo jpn_Hani jpn_Hira jpn_Kana jpn_Latn\n* target language(s): pol\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 15.7 \t\n\n\n### System Info: \n- hf_name: jpn-pol\n\n- source_languages: jpn\n\n- target_languages: pol\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ja', 'pl']\n\n- src_constituents: {'jpn_Hang', 'jpn', 'jpn_Yiii', 'jpn_Kana', 'jpn_Hani', 'jpn_Bopo', 'jpn_Latn', 'jpn_Hira'}\n\n- tgt_constituents: {'pol'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: jpn\n\n- tgt_alpha3: pol\n\n- short_pair: ja-pl\n\n- chrF2_score: 0.386\n\n- bleu: 15.7\n\n- brevity_penalty: 1.0\n\n- ref_len: 69904.0\n\n- src_name: Japanese\n\n- tgt_name: Polish\n\n- train_date: 2020-06-17\n\n- src_alpha2: ja\n\n- tgt_alpha2: pl\n\n- prefer_old: False\n\n- long_pair: jpn-pol\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ja-pl?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ja-pt", "paragraphs": [{"context": "---\nlanguage: \n- ja\n- pt\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### jpn-por\n\n* source group: Japanese \n* target group: Portuguese \n*  OPUS readme: [jpn-por](\n\n*  model: transformer-align\n* source language(s): jpn jpn_Hani jpn_Hira jpn_Kana jpn_Latn jpn_Yiii\n* target language(s): por por_Hira\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 22.2 \t\n\n\n### System Info: \n- hf_name: jpn-por\n\n- source_languages: jpn\n\n- target_languages: por\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ja', 'pt']\n\n- src_constituents: {'jpn_Hang', 'jpn', 'jpn_Yiii', 'jpn_Kana', 'jpn_Hani', 'jpn_Bopo', 'jpn_Latn', 'jpn_Hira'}\n\n- tgt_constituents: {'por'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: jpn\n\n- tgt_alpha3: por\n\n- short_pair: ja-pt\n\n- chrF2_score: 0.444\n\n- bleu: 22.2\n\n- brevity_penalty: 0.922\n\n- ref_len: 15570.0\n\n- src_name: Japanese\n\n- tgt_name: Portuguese\n\n- train_date: 2020-06-17\n\n- src_alpha2: ja\n\n- tgt_alpha2: pt\n\n- prefer_old: False\n\n- long_pair: jpn-por\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ja-pt?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ja-ru", "paragraphs": [{"context": "---\nlanguage: \n- ja\n- ru\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### jpn-rus\n\n* source group: Japanese \n* target group: Russian \n*  OPUS readme: [jpn-rus](\n\n*  model: transformer-align\n* source language(s): jpn jpn_Bopo jpn_Hani jpn_Hira jpn_Kana jpn_Latn jpn_Yiii\n* target language(s): rus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 23.2 \t\n\n\n### System Info: \n- hf_name: jpn-rus\n\n- source_languages: jpn\n\n- target_languages: rus\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ja', 'ru']\n\n- src_constituents: {'jpn_Hang', 'jpn', 'jpn_Yiii', 'jpn_Kana', 'jpn_Hani', 'jpn_Bopo', 'jpn_Latn', 'jpn_Hira'}\n\n- tgt_constituents: {'rus'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: jpn\n\n- tgt_alpha3: rus\n\n- short_pair: ja-ru\n\n- chrF2_score: 0.441\n\n- bleu: 23.2\n\n- brevity_penalty: 0.9740000000000001\n\n- ref_len: 70820.0\n\n- src_name: Japanese\n\n- tgt_name: Russian\n\n- train_date: 2020-06-17\n\n- src_alpha2: ja\n\n- tgt_alpha2: ru\n\n- prefer_old: False\n\n- long_pair: jpn-rus\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ja-ru?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ja-tr", "paragraphs": [{"context": "---\nlanguage: \n- ja\n- tr\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### jpn-tur\n\n* source group: Japanese \n* target group: Turkish \n*  OPUS readme: [jpn-tur](\n\n*  model: transformer-align\n* source language(s): jpn jpn_Bopo jpn_Hang jpn_Hani jpn_Hira jpn_Kana jpn_Yiii\n* target language(s): tur\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 16.7 \t\n\n\n### System Info: \n- hf_name: jpn-tur\n\n- source_languages: jpn\n\n- target_languages: tur\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ja', 'tr']\n\n- src_constituents: {'jpn_Hang', 'jpn', 'jpn_Yiii', 'jpn_Kana', 'jpn_Hani', 'jpn_Bopo', 'jpn_Latn', 'jpn_Hira'}\n\n- tgt_constituents: {'tur'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: jpn\n\n- tgt_alpha3: tur\n\n- short_pair: ja-tr\n\n- chrF2_score: 0.434\n\n- bleu: 16.7\n\n- brevity_penalty: 0.932\n\n- ref_len: 4755.0\n\n- src_name: Japanese\n\n- tgt_name: Turkish\n\n- train_date: 2020-06-17\n\n- src_alpha2: ja\n\n- tgt_alpha2: tr\n\n- prefer_old: False\n\n- long_pair: jpn-tur\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ja-tr?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-kwn-en", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-kwn-en\n\n* source languages: kwn\n* target languages: en\n*  OPUS readme: [kwn-en](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-09.zip](\n* test set translations: [opus-2020-01-09.test.txt](\n* test set scores: [opus-2020-01-09.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 27.5 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-kwn-en?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-kwy-en", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-kwy-en\n\n* source languages: kwy\n* target languages: en\n*  OPUS readme: [kwy-en](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-09.zip](\n* test set translations: [opus-2020-01-09.test.txt](\n* test set scores: [opus-2020-01-09.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 31.6 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-kwy-en?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ru-da", "paragraphs": [{"context": "---\nlanguage: \n- ru\n- da\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### rus-dan\n\n* source group: Russian \n* target group: Danish \n*  OPUS readme: [rus-dan](\n\n*  model: transformer-align\n* source language(s): rus\n* target language(s): dan\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 56.6 \t\n\n\n### System Info: \n- hf_name: rus-dan\n\n- source_languages: rus\n\n- target_languages: dan\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ru', 'da']\n\n- src_constituents: {'rus'}\n\n- tgt_constituents: {'dan'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: rus\n\n- tgt_alpha3: dan\n\n- short_pair: ru-da\n\n- chrF2_score: 0.7140000000000001\n\n- bleu: 56.6\n\n- brevity_penalty: 0.977\n\n- ref_len: 11746.0\n\n- src_name: Russian\n\n- tgt_name: Danish\n\n- train_date: 2020-06-17\n\n- src_alpha2: ru\n\n- tgt_alpha2: da\n\n- prefer_old: False\n\n- long_pair: rus-dan\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ru-da?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ru-fr", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-ru-fr\n\n* source languages: ru\n* target languages: fr\n*  OPUS readme: [ru-fr](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-26.zip](\n* test set translations: [opus-2020-01-26.test.txt](\n* test set scores: [opus-2020-01-26.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 18.3 \t\n 21.6 \t\n 51.5 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ru-fr?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-ru-lt", "paragraphs": [{"context": "---\nlanguage: \n- ru\n- lt\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### rus-lit\n\n* source group: Russian \n* target group: Lithuanian \n*  OPUS readme: [rus-lit](\n\n*  model: transformer-align\n* source language(s): rus\n* target language(s): lit\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 43.5 \t\n\n\n### System Info: \n- hf_name: rus-lit\n\n- source_languages: rus\n\n- target_languages: lit\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['ru', 'lt']\n\n- src_constituents: {'rus'}\n\n- tgt_constituents: {'lit'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: rus\n\n- tgt_alpha3: lit\n\n- short_pair: ru-lt\n\n- chrF2_score: 0.675\n\n- bleu: 43.5\n\n- brevity_penalty: 0.937\n\n- ref_len: 14406.0\n\n- src_name: Russian\n\n- tgt_name: Lithuanian\n\n- train_date: 2020-06-17\n\n- src_alpha2: ru\n\n- tgt_alpha2: lt\n\n- prefer_old: False\n\n- long_pair: rus-lit\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-ru-lt?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-sg-es", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-sg-es\n\n* source languages: sg\n* target languages: es\n*  OPUS readme: [sg-es](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 21.3 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-sg-es?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tl-pt", "paragraphs": [{"context": "---\nlanguage: \n- tl\n- pt\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### tgl-por\n\n* source group: Tagalog \n* target group: Portuguese \n*  OPUS readme: [tgl-por](\n\n*  model: transformer-align\n* source language(s): tgl_Latn\n* target language(s): por\n* model: transformer-align\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-06-17.zip](\n* test set translations: [opus-2020-06-17.test.txt](\n* test set scores: [opus-2020-06-17.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 28.8 \t\n\n\n### System Info: \n- hf_name: tgl-por\n\n- source_languages: tgl\n\n- target_languages: por\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['tl', 'pt']\n\n- src_constituents: {'tgl_Latn'}\n\n- tgt_constituents: {'por'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: tgl\n\n- tgt_alpha3: por\n\n- short_pair: tl-pt\n\n- chrF2_score: 0.522\n\n- bleu: 28.8\n\n- brevity_penalty: 0.981\n\n- ref_len: 12826.0\n\n- src_name: Tagalog\n\n- tgt_name: Portuguese\n\n- train_date: 2020-06-17\n\n- src_alpha2: tl\n\n- tgt_alpha2: pt\n\n- prefer_old: False\n\n- long_pair: tgl-por\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tl-pt?", "answers": [{"text": "translation", "answer_start": 34, "answer_end": 44}]}]}]}, {"title": "Helsinki-NLP/opus-mt-to-es", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-to-es\n\n* source languages: to\n* target languages: es\n*  OPUS readme: [to-es](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 26.6 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-to-es?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-to-fr", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-to-fr\n\n* source languages: to\n* target languages: fr\n*  OPUS readme: [to-fr](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 27.9 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-to-fr?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-to-sv", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-to-sv\n\n* source languages: to\n* target languages: sv\n*  OPUS readme: [to-sv](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 30.7 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-to-sv?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-toi-en", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-toi-en\n\n* source languages: toi\n* target languages: en\n*  OPUS readme: [toi-en](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 39.0 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-toi-en?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-toi-es", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-toi-es\n\n* source languages: toi\n* target languages: es\n*  OPUS readme: [toi-es](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 24.6 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-toi-es?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-mt-zne-fr", "paragraphs": [{"context": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-zne-fr\n\n* source languages: zne\n* target languages: fr\n*  OPUS readme: [zne-fr](\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-01-16.zip](\n* test set translations: [opus-2020-01-16.test.txt](\n* test set scores: [opus-2020-01-16.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 25.3 \t\n\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-zne-fr?", "answers": [{"text": "translation", "answer_start": 12, "answer_end": 22}]}]}]}, {"title": "Helsinki-NLP/opus-tatoeba-it-he", "paragraphs": [{"context": "---\nlanguage:\n- it\n- he\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n### it-he\n\n* source group: Italian \n* target group: Hebrew \n*  OPUS readme: [ita-heb](\n\n*  model: transformer\n* source language(s): ita\n* target language(s): heb\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-12-10.zip](\n* test set translations: [opus-2020-12-10.test.txt](\n* test set scores: [opus-2020-12-10.eval.txt](\n\n## Benchmarks\n\n BLEU  \n-------\n 38.5 \t\n\n\n### System Info: \n- hf_name: it-he\n\n- source_languages: ita\n\n- target_languages: heb\n\n- opus_readme_url: \n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['it', 'he']\n\n- src_constituents: ('Italian', {'ita'})\n\n- tgt_constituents: ('Hebrew', {'heb'})\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- long_pair: ita-heb\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: \n\n- url_test_set: \n\n- src_alpha3: ita\n\n- tgt_alpha3: heb\n\n- chrF2_score: 0.593\n\n- bleu: 38.5\n\n- brevity_penalty: 0.985\n\n- ref_len: 9796.0\n\n- src_name: Italian\n\n- tgt_name: Hebrew\n\n- train_date: 2020-12-10 00:00:00\n\n- src_alpha2: it\n\n- tgt_alpha2: he\n\n- prefer_old: False\n\n- short_pair: it-he\n\n- helsinki_git_sha: b317f78a3ec8a556a481b6a53dc70dc11769ca96\n\n- transformers_git_sha: 1310e1a758edc8e89ec363db76863c771fbeb1de\n\n- port_machine: LM0-400-22516.local\n\n- port_time: 2020-12-11-16:02", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-tatoeba-it-he?", "answers": [{"text": "translation", "answer_start": 33, "answer_end": 43}]}]}]}, {"title": "HenryAI/KerasBERTv1", "paragraphs": [{"context": "Thanks for checking this out! <br />\nThis video explains the ideas behind KerasBERT (still very much a work in progress)\n", "qas": []}]}, {"title": "Herais/pred_genre", "paragraphs": [{"context": "---\nlanguage:\n- zh\ntags:\n- classification\nlicense: apache-2.0\ndatasets:\n- Custom\nmetrics:\n- rouge\n---\n\nThis model predicts the time period given a synopsis of about 200 Chinese characters.\nThe model is trained on TV and Movie datasets and takes simplified Chinese as input.\n\nWe trained the model from the \"hfl/chinese-bert-wwm-ext\" checkpoint.\n\n#### Sample Usage\n    from transformers import BertTokenizer, BertForSequenceClassification\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint = \"Herais/pred_genre\"\n    tokenizer = BertTokenizer.from_pretrained(checkpoint, \n                                              problem_type=\"single_label_classification\")\n    model = BertForSequenceClassification.from_pretrained(checkpoint).to(device)\n    \n    label2id_genre = {'\u6d89\u6848': 7, '\u90fd\u5e02': 10, '\u9769\u547d': 12, '\u519c\u6751': 4, '\u4f20\u5947': 0, \n                      '\u5176\u5b83': 2, '\u4f20\u8bb0': 1, '\u9752\u5c11': 11, '\u519b\u65c5': 3, '\u6b66\u6253': 6, \n                      '\u79d1\u5e7b': 9, '\u795e\u8bdd': 8, '\u5bab\u5ef7': 5}\n\n    id2label_genre = {7: '\u6d89\u6848', 10: '\u90fd\u5e02', 12: '\u9769\u547d', 4: '\u519c\u6751', 0: '\u4f20\u5947', \n                      2: '\u5176\u5b83', 1: '\u4f20\u8bb0', 11: '\u9752\u5c11', 3: '\u519b\u65c5', 6: '\u6b66\u6253', \n                      9: '\u79d1\u5e7b', 8: '\u795e\u8bdd', 5: '\u5bab\u5ef7'}\n\n    synopsis = \"\"\"\u52a0\u6cb9\u5427\uff01\u68c0\u5bdf\u5b98\u3002\u9ca4\u5dde\u5e02\u5b89\u5e73\u533a\u68c0\u5bdf\u9662\u68c0\u5bdf\u5b98\u52a9\u7406\u8521\u6653\u4e0e\u5f90\u7f8e\u6d25\u662f\u4e24\u4e2a\u521a\u5165\u804c\u573a\u7684\u201c\u83dc\u9e1f\u201d\u3002\\\n    \u4ed6\u4eec\u5728\u8001\u68c0\u5bdf\u5b98\u51af\u6606\u7684\u6307\u5bfc\u4e0e\u9f13\u52b1\u4e0b\uff0c\u51ed\u501f\u7740\u81ea\u5df1\u7684\u4e00\u8154\u70ed\u8840\u4e0e\u5bf9\u68c0\u5bdf\u4e8b\u4e1a\u7684\u6267\u8457\u8ffd\u6c42\uff0c\u514b\u670d\u5de5\u4f5c\u4e0a\u7684\u79cd\u79cd\u56f0\u96be\uff0c\\\n    \u6210\u529f\u529e\u7406\u7535\u7ade\u8d4c\u535a\u3001\u865a\u5047\u8bc9\u8bbc\u3001\u6c34\u4ea7\u5e02\u573a\u6d89\u9ed1\u7b49\u4e00\u7cfb\u5217\u590d\u6742\u6848\u4ef6\uff0c\u60e9\u6cbb\u4e86\u72af\u7f6a\u5206\u5b50\uff0c\u7ef4\u62a4\u4e86\u4eba\u6c11\u7fa4\u4f17\u7684\u5408\u6cd5\u6743\u76ca\uff0c\\\n    \u4e3a\u793e\u4f1a\u4e3b\u4e49\u6cd5\u6cbb\u5efa\u8bbe\u8d21\u732e\u4e86\u81ea\u5df1\u7684\u4e00\u4efd\u529b\u91cf\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u8521\u6653\u4e0e\u5f90\u7f8e\u6d25\u4e0d\u4ec5\u5f97\u5230\u4e86\u4e1a\u52a1\u80fd\u529b\u4e0a\u7684\u63d0\u5347\uff0c\\\n    \u4e5f\u9886\u609f\u4e86\u4eba\u751f\u7684\u771f\u8c1b\uff0c\u5b66\u4f1a\u771f\u8bda\u5730\u9762\u5bf9\u5bb6\u4eba\u4e0e\u670b\u53cb\uff0c\u6536\u83b7\u4e86\u4eb2\u60c5\u4e0e\u53cb\u8c0a\uff0c\u6210\u957f\u4e3a\u5408\u683c\u7684\u5458\u989d\u68c0\u5bdf\u5b98\uff0c\\\n    \u7ee7\u7eed\u4e3a\u68c0\u5bdf\u4e8b\u4e1a\u8d21\u732e\u81ea\u5df1\u7684\u9752\u6625\u3002\u00a0\"\"\"\n    \n    inputs = tokenizer(synopsis, truncation=True, max_length=512, return_tensors='pt')\n    model.eval()\n    outputs = model(**input)\n        \n    label_ids_pred = torch.argmax(outputs.logits, dim=1).to('cpu').numpy()\n    labels_pred = [id2label_timeperiod[label] for label in labels_pred]\n    \n    print(labels_pred)\n    # ['\u6d89\u6848']\n    \n Citation\n TBA", "qas": [{"id": "q1", "question": "What is the model architecture of Herais/pred_genre?", "answers": [{"text": "bert", "answer_start": 318, "answer_end": 321}]}]}]}, {"title": "Hetarth/marian-finetuned-hi-hinglish", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: marian-finetuned-hi-hinglish\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# marian-finetuned-hi-hinglish\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-hi-en]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 4.1869\n- Validation Loss: 4.0607\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 5e-05, 'decay_steps': 279, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 4.0607          \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- TensorFlow 2.7.0\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Hetarth/marian-finetuned-hi-hinglish?", "answers": [{"text": "marian", "answer_start": 83, "answer_end": 88}]}]}]}, {"title": "MagnusChase7/DialoGPT-medium-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of MagnusChase7/DialoGPT-medium-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "ILoveThatLady/DialoGPT-small-rickandmorty", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Rick And Morty DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of ILoveThatLady/DialoGPT-small-rickandmorty?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "IMSyPP/hate_speech_en", "paragraphs": [{"context": "---\nwidget:\n\n- text: \"My name is Mark and I live in London. I am a postgraduate student at Queen Mary University.\"\nlanguage: \n  - en\nlicense: mit\n---\n\n# Hate Speech Classifier for Social Media Content in English Language\n\nA monolingual model for hate speech classification of social media content in English language. The model was trained on 103190 YouTube comments and tested on an independent test set of 20554 YouTube comments. It is based on English BERT base pre-trained language model.\n\n## Tokenizer\n\nDuring training the text was preprocessed using the original English BERT base tokenizer. We suggest the same tokenizer is used for inference.\n\n## Model output\n\nThe model classifies each input into one of four distinct classes:\n* 0 - acceptable\n* 1 - inappropriate\n* 2 - offensive\n* 3 - violent", "qas": []}]}, {"title": "IMSyPP/hate_speech_nl", "paragraphs": [{"context": "---\nlanguage: \n  - nl\nlicense: mit\n---\n\n# Hate Speech Classifier for Social Media Content in Dutch\n\nA monolingual model for hate speech classification of social media content in Dutch. The model was trained on 20000 social media posts (youtube, twitter, facebook) and tested on an independent test set of 2000 posts. It is based on thepre-trained language model [BERTje](\n\n## Tokenizer\n\nDuring training the text was preprocessed using the BERTje tokenizer. We suggest the same tokenizer is used for inference.\n\n## Model output\n\nThe model classifies each input into one of four distinct classes:\n* 0 - acceptable\n* 1 - inappropriate\n* 2 - offensive\n* 3 - violent", "qas": []}]}, {"title": "IMSyPP/hate_speech_targets_slo", "paragraphs": [{"context": "---\nlanguage: \n  - sl\nlicense: mit\n---", "qas": []}]}, {"title": "ITNODove/DialoGPT-medium-cyberbones", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Cyber Bones DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of ITNODove/DialoGPT-medium-cyberbones?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Iacopo/Shakespear-GPT2", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: output\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# output\n\nThis model is a fine-tuned version of [gpt2]( on a dataset of Shakespeare's plays.\n\n## Model description\n\nThe model is the original gpt-2 model fine-tuned on a custom dataset.\n\n## Intended uses & limitations\n\nThe model can be used to generate Shakespearean-like text. Consider that because it comes from plays, such a typographical structure might be reproduced.\n\n## Training and evaluation data\n\nTrained with Shakespeare's plays corpus.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Iacopo/Shakespear-GPT2?", "answers": [{"text": "gpt2", "answer_start": 332, "answer_end": 335}]}]}]}, {"title": "Icemiser/chat-test", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Hank Hill DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Icemiser/chat-test?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Ife/CA-ES", "paragraphs": [{"context": "# Similar-Languages-MT", "qas": []}]}, {"title": "Ifenna/dbert-3epoch", "paragraphs": [{"context": "---\ndatasets:\n- squad_v2\n- wiki_qa\nlanguage:\n- en\nmetrics:\n- accuracy\npipeline_tag: question-answering\n---\nA distilbert model fine-tuned for question answering.", "qas": [{"id": "q1", "question": "What is the model architecture of Ifenna/dbert-3epoch?", "answers": [{"text": "distilbert", "answer_start": 109, "answer_end": 118}]}, {"id": "q2", "question": "What is the model task of Ifenna/dbert-3epoch?", "answers": [{"text": "question-answering", "answer_start": 84, "answer_end": 101}]}]}]}, {"title": "Ifromspace/GRIEFSOFT-walr", "paragraphs": [{"context": "---\ntags:\n- ru\n- 4ulan\n---\n\n\u0417\u0430\u0431\u0430\u0432\u043d\u043e\u0435 \u0434\u043b\u044f \u0434\u0438\u0441\u043a\u043e\u0440\u0434\u0438\u043a\u0430))00)) \n\nOffers\n  work@4ulan.fun", "qas": []}]}, {"title": "Ifromspace/GRIEFSOFT", "paragraphs": [{"context": "---\nlanguage:\n- ru\ntags:\n- PyTorch\n- Transformers\n- 4ulan\n---\n\n**Fork of \n\n\u0417\u0430\u0431\u0430\u0432\u043d\u043e\u0435 \u0434\u043b\u044f \u0434\u0438\u0441\u043a\u043e\u0440\u0434\u0438\u043a\u0430))00))\n\nROADMAP:\n- \u0421\u043e\u0431\u0438\u0440\u0430\u044e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0438\u043a \u0438\u0437 \u043a\u043d\u0438\u0436\u0435\u043a \u043f\u0440\u043e \u043f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u0435\u0432. <------------------------- \u0421\u0435\u0439\u0447\u0430\u0441 \u0442\u0443\u0442.\n- \u0414\u043e\u043e\u0431\u0443\u0447\u0430\u044e.\n- \u0412\u044b\u0431\u0440\u0430\u0441\u044b\u0432\u0430\u044e \u0432 \u0434\u0438\u0441\u043a\u043e\u0440\u0434\u0438\u043a.\n\n", "qas": []}]}, {"title": "IlyaGusev/rubert_ext_sum_gazeta", "paragraphs": [{"context": "---\nlanguage:\n- ru\ntags:\n- summarization\n- token-classification\n- t5\ndatasets:\n- IlyaGusev/gazeta\nlicense: apache-2.0\ninference: false\nwidget:\n- text: \"\u0421 1 \u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f \u0432 \u0420\u043e\u0441\u0441\u0438\u0438 \u0432\u0441\u0442\u0443\u043f\u0430\u044e\u0442 \u0432 \u0441\u0438\u043b\u0443 \u043f\u043e\u043f\u0440\u0430\u0432\u043a\u0438 \u0432 \u0437\u0430\u043a\u043e\u043d \u00ab\u041e \u0431\u0430\u043d\u043a\u0440\u043e\u0442\u0441\u0442\u0432\u0435\u00bb \u2014 \u0442\u0435\u043f\u0435\u0440\u044c \u0434\u043e\u043b\u0436\u043d\u0438\u043a\u0438 \u0441\u043c\u043e\u0433\u0443\u0442 \u043e\u0441\u0432\u043e\u0431\u043e\u0436\u0434\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u043d\u0435\u043f\u043e\u0441\u0438\u043b\u044c\u043d\u044b\u0445 \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432 \u0432\u043e \u0432\u043d\u0435\u0441\u0443\u0434\u0435\u0431\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435, \u0435\u0441\u043b\u0438 \u0441\u0443\u043c\u043c\u0430 \u0437\u0430\u0434\u043e\u043b\u0436\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u043d\u0435 \u043c\u0435\u043d\u0435\u0435 50 \u0442\u044b\u0441. \u0440\u0443\u0431\u043b\u0435\u0439 \u0438 \u043d\u0435 \u043f\u0440\u0435\u0432\u044b\u0448\u0430\u0435\u0442 500 \u0442\u044b\u0441. \u0440\u0443\u0431\u043b\u0435\u0439 \u0431\u0435\u0437 \u0443\u0447\u0435\u0442\u0430 \u0448\u0442\u0440\u0430\u0444\u043e\u0432, \u043f\u0435\u043d\u0438, \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432 \u0437\u0430 \u043f\u0440\u043e\u0441\u0440\u043e\u0447\u043a\u0443 \u043f\u043b\u0430\u0442\u0435\u0436\u0430 \u0438 \u043f\u0440\u043e\u0447\u0438\u0445 \u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u0438\u043b\u0438 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0445 \u0441\u0430\u043d\u043a\u0446\u0438\u0439.[SEP]\u0423 \u0444\u0438\u0437\u043b\u0438\u0446 \u0438 \u0438\u043d\u0434\u0438\u0432\u0438\u0434\u0443\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u0435\u0439 \u043f\u043e\u044f\u0432\u0438\u043b\u0430\u0441\u044c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u043e\u0439\u0442\u0438 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0443 \u0431\u0430\u043d\u043a\u0440\u043e\u0442\u0441\u0442\u0432\u0430 \u0431\u0435\u0437 \u0443\u0447\u0430\u0441\u0442\u0438\u044f \u0441\u0443\u0434\u0430 \u0438 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u043e\u0433\u043e \u0443\u043f\u0440\u0430\u0432\u043b\u044f\u044e\u0449\u0435\u0433\u043e \u2014 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043f\u043e\u0434\u0430\u0442\u044c \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0435 \u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0435 \u0447\u0435\u0440\u0435\u0437 \u041c\u0424\u0426.[SEP]\u0421\u0443\u043c\u043c\u0443 \u0437\u0430\u0434\u043e\u043b\u0436\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043f\u0438\u0441\u043e\u043a \u0432\u0441\u0435\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0437\u0430\u044f\u0432\u0438\u0442\u0435\u043b\u044e \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0440\u043e\u0432 \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e.[SEP]\u0415\u0441\u043b\u0438 \u0432\u0441\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f \u0441\u043e\u0431\u043b\u044e\u0434\u0435\u043d\u044b, \u0441\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u0432\u043d\u0435\u0441\u0443\u0442 \u0432 \u0415\u0434\u0438\u043d\u044b\u0439 \u0444\u0435\u0434\u0435\u0440\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0435\u0441\u0442\u0440 \u0432 \u0442\u0435\u0447\u0435\u043d\u0438\u0435 \u0442\u0440\u0435\u0445 \u0440\u0430\u0431\u043e\u0447\u0438\u0445 \u0434\u043d\u0435\u0439.[SEP]\u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u043d\u0430 \u043c\u043e\u043c\u0435\u043d\u0442 \u043f\u043e\u0434\u0430\u0447\u0438 \u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 \u0437\u0430\u044f\u0432\u0438\u0442\u0435\u043b\u044f \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u043e\u043a\u043e\u043d\u0447\u0435\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u0441 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u0432\u0437\u044b\u0441\u043a\u0430\u0442\u0435\u043b\u044e.[SEP]\u042d\u0442\u043e \u0437\u043d\u0430\u0447\u0438\u0442, \u0447\u0442\u043e \u0443 \u043f\u043e\u0442\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0440\u043e\u0442\u0430 \u043d\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u0437\u044b\u0441\u043a\u0430\u0442\u044c.[SEP]\u041a\u0440\u043e\u043c\u0435 \u0442\u043e\u0433\u043e, \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 \u0433\u0440\u0430\u0436\u0434\u0430\u043d\u0438\u043d\u0430 \u043d\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u0432\u043e\u0437\u0431\u0443\u0436\u0434\u0435\u043d\u043e \u0434\u0440\u0443\u0433\u043e\u0435 \u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u043e.[SEP]\u0412 \u043f\u0435\u0440\u0438\u043e\u0434 \u0432\u0441\u0435\u0439 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u044b \u0437\u0430\u044f\u0432\u0438\u0442\u0435\u043b\u044c \u043d\u0435 \u0441\u043c\u043e\u0436\u0435\u0442 \u0431\u0440\u0430\u0442\u044c \u0437\u0430\u0439\u043c\u044b, \u043a\u0440\u0435\u0434\u0438\u0442\u044b, \u0432\u044b\u0434\u0430\u0432\u0430\u0442\u044c \u043f\u043e\u0440\u0443\u0447\u0438\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430, \u0441\u043e\u0432\u0435\u0440\u0448\u0430\u0442\u044c \u0438\u043d\u044b\u0435 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0441\u0434\u0435\u043b\u043a\u0438.[SEP]\u0412\u043d\u0435\u0441\u0443\u0434\u0435\u0431\u043d\u043e\u0435 \u0431\u0430\u043d\u043a\u0440\u043e\u0442\u0441\u0442\u0432\u043e \u0431\u0443\u0434\u0435\u0442 \u0434\u043b\u0438\u0442\u044c\u0441\u044f \u0448\u0435\u0441\u0442\u044c \u043c\u0435\u0441\u044f\u0446\u0435\u0432, \u0432 \u0442\u0435\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0442\u0430\u043a\u0436\u0435 \u0431\u0443\u0434\u0435\u0442 \u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u043c\u043e\u0440\u0430\u0442\u043e\u0440\u0438\u0439 \u043d\u0430 \u0443\u0434\u043e\u0432\u043b\u0435\u0442\u0432\u043e\u0440\u0435\u043d\u0438\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0440\u043e\u0432, \u043e\u0442\u043c\u0435\u0447\u0435\u043d\u043d\u044b\u0445 \u0432 \u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0438 \u0434\u043e\u043b\u0436\u043d\u0438\u043a\u0430, \u0438 \u043c\u043e\u0440\u0430\u0442\u043e\u0440\u0438\u0439 \u043e\u0431 \u0443\u043f\u043b\u0430\u0442\u0435 \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439.[SEP]\u041a\u0440\u043e\u043c\u0435 \u0442\u043e\u0433\u043e, \u043f\u0440\u0435\u043a\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u043d\u0430\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043d\u0435\u0443\u0441\u0442\u043e\u0435\u043a \u0438 \u0438\u043d\u044b\u0445 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0445 \u0441\u0430\u043d\u043a\u0446\u0438\u0439; \u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u0432\u0437\u044b\u0441\u043a\u0430\u043d\u0438\u044f (\u043a\u0440\u043e\u043c\u0435 \u0430\u043b\u0438\u043c\u0435\u043d\u0442\u043e\u0432) \u0442\u0430\u043a\u0436\u0435 \u0431\u0443\u0434\u0443\u0442 \u043f\u0440\u0438\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u044b.[SEP]\u041f\u043e \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0438\u044e \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u044b \u0437\u0430\u044f\u0432\u0438\u0442\u0435\u043b\u044f \u043e\u0441\u0432\u043e\u0431\u043e\u0434\u044f\u0442 \u043e\u0442 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0440\u043e\u0432, \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u0432 \u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0438 \u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043d\u0438\u0438 \u0435\u0433\u043e \u0431\u0430\u043d\u043a\u0440\u043e\u0442\u043e\u043c, \u0430 \u044d\u0442\u0430 \u0437\u0430\u0434\u043e\u043b\u0436\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u0435\u0442\u0441\u044f \u0431\u0435\u0437\u043d\u0430\u0434\u0435\u0436\u043d\u043e\u0439.[SEP]\u0412 \u043f\u0440\u043e\u0448\u043b\u043e\u043c \u043c\u0435\u0441\u044f\u0446\u0435 \u0441\u0442\u0430\u043b\u043e \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e, \u0447\u0442\u043e \u0437\u0430 \u043f\u0435\u0440\u0432\u043e\u0435 \u043f\u043e\u043b\u0443\u0433\u043e\u0434\u0438\u0435 2020 \u0433\u043e\u0434\u0430 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u0438\u0435 \u0441\u0443\u0434\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043b\u0438 \u0431\u0430\u043d\u043a\u0440\u043e\u0442\u0430\u043c\u0438 42,7 \u0442\u044b\u0441. \u0433\u0440\u0430\u0436\u0434\u0430\u043d (\u0432 \u0442\u043e\u043c \u0447\u0438\u0441\u043b\u0435 \u0438\u043d\u0434\u0438\u0432\u0438\u0434\u0443\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u0435\u0439) \u2014 \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u0435\u0434\u0438\u043d\u043e\u0433\u043e \u0440\u0435\u0435\u0441\u0442\u0440\u0430 \u00ab\u0424\u0435\u0434\u0440\u0435\u0441\u0443\u0440\u0441\u00bb, \u044d\u0442\u043e \u043d\u0430 47,2% \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044f \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0438\u043e\u0434\u0430 2019 \u0433\u043e\u0434\u0430.[SEP]\u0420\u043e\u0441\u0442 \u0447\u0438\u0441\u043b\u0430 \u043e\u0431\u0430\u043d\u043a\u0440\u043e\u0442\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0433\u0440\u0430\u0436\u0434\u0430\u043d \u0432\u043e \u0432\u0442\u043e\u0440\u043e\u043c \u043a\u0432\u0430\u0440\u0442\u0430\u043b\u0435 \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u043f\u0435\u0440\u0432\u044b\u043c \u0437\u0430\u043c\u0435\u0434\u043b\u0438\u043b\u0441\u044f \u2014 \u0442\u0430\u043a\u0430\u044f \u0434\u0438\u043d\u0430\u043c\u0438\u043a\u0430 \u043e\u0431\u0443\u0441\u043b\u043e\u0432\u043b\u0435\u043d\u0430 \u0442\u0435\u043c, \u0447\u0442\u043e \u0432 \u043f\u0435\u0440\u0438\u043e\u0434 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0439 \u0441 19 \u043c\u0430\u0440\u0442\u0430 \u043f\u043e 11 \u043c\u0430\u044f \u0441\u0443\u0434\u044b \u0440\u0435\u0434\u043a\u043e \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u043b\u0438 \u0431\u0430\u043d\u043a\u0440\u043e\u0442\u043d\u044b\u0435 \u0434\u0435\u043b\u0430 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0439 \u0438 \u043c\u0435\u043d\u044c\u0448\u0435, \u0447\u0435\u043c \u043e\u0431\u044b\u0447\u043d\u043e, \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 \u0433\u0440\u0430\u0436\u0434\u0430\u043d, \u043e\u0431\u044a\u044f\u0441\u043d\u044f\u043b \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c \u043f\u0440\u043e\u0435\u043a\u0442\u0430 \u00ab\u0424\u0435\u0434\u0440\u0435\u0441\u0443\u0440\u0441\u00bb \u0410\u043b\u0435\u043a\u0441\u0435\u0439 \u042e\u0445\u043d\u0438\u043d.[SEP]\"\n  example_title: \"\u041d\u043e\u0432\u043e\u0441\u0442\u0438\"\n  \n---\n\n# RuBERTExtSumGazeta\n\n## Model description\n\nModel for extractive summarization based on [rubert-base-cased](DeepPavlov/rubert-base-cased)\n\n## Intended uses & limitations\n\n#### How to use\n\nColab: [link](\n\n```python\nimport razdel\nfrom transformers import AutoTokenizer, BertForTokenClassification\n\nmodel_name = \"IlyaGusev/rubert_ext_sum_gazeta\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsep_token = tokenizer.sep_token\nsep_token_id = tokenizer.sep_token_id\n\nmodel = BertForTokenClassification.from_pretrained(model_name)\n\narticle_text = \"...\"\nsentences = [s.text for s in razdel.sentenize(article_text)]\narticle_text = sep_token.join(sentences)\n\ninputs = tokenizer(\n    [article_text],\n    max_length=500,\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\",\n)\nsep_mask = inputs[\"input_ids\"][0] == sep_token_id\n\n# Fix token_type_ids\ncurrent_token_type_id = 0 \nfor pos, input_id in enumerate(inputs[\"input_ids\"][0]):\n    inputs[\"token_type_ids\"][0][pos] = current_token_type_id\n    if input_id == sep_token_id:\n        current_token_type_id = 1 - current_token_type_id\n\n# Infer model\nwith torch.no_grad(): \n    outputs = model(**inputs) \nlogits = outputs.logits[0, :, 1]\n\n# Choose sentences \nlogits = logits[sep_mask]\nlogits, indices = logits.sort(descending=True)\nlogits, indices = logits.cpu().tolist(), indices.cpu().tolist()\npairs = list(zip(logits, indices))\npairs = pairs[:3]\nindices = list(sorted([idx for _, idx in pairs]))\nsummary = \" \".join([sentences[idx] for idx in indices])\nprint(summary)\n```\n\n#### Limitations and bias\n\n- The model should work well with Gazeta.ru articles, but for any other agencies it can suffer from domain shift\n\n\n## Training data\n\n- Dataset: [Gazeta](\n\n## Training procedure\n\nTBD\n\n## Eval results\n\nTBD\n\nEvaluation: \n\nFlags: --language ru --tokenize-after --lower\n", "qas": [{"id": "q1", "question": "What is the model architecture of IlyaGusev/rubert_ext_sum_gazeta?", "answers": [{"text": "bert", "answer_start": 2703, "answer_end": 2706}]}, {"id": "q2", "question": "What is the model task of IlyaGusev/rubert_ext_sum_gazeta?", "answers": [{"text": "token-classification", "answer_start": 43, "answer_end": 62}]}]}]}, {"title": "IlyaGusev/rubert_telegram_headlines", "paragraphs": [{"context": "---\nlanguage:\n- ru\ntags:\n- summarization\nlicense: apache-2.0\ninference:\n  parameters:\n    no_repeat_ngram_size: 4\n    \n---\n\n# RuBertTelegramHeadlines\n\n\n## Model description\n\nExample model for [Headline generation competition](\n\nBased on [RuBERT]( model\n\n## Intended uses & limitations\n\n#### How to use\n\n```python\nfrom transformers import AutoTokenizer, EncoderDecoderModel\n\nmodel_name = \"IlyaGusev/rubert_telegram_headlines\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, do_basic_tokenize=False, strip_accents=False)\nmodel = EncoderDecoderModel.from_pretrained(model_name)\n\narticle_text = \"...\"\n\ninput_ids = tokenizer(\n    [article_text],\n    add_special_tokens=True,\n    max_length=256,\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\",\n)[\"input_ids\"]\n\noutput_ids = model.generate(\n    input_ids=input_ids,\n    max_length=64,\n    no_repeat_ngram_size=3,\n    num_beams=10,\n    top_p=0.95\n)[0]\n\nheadline = tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(headline)\n```\n\n## Training data\n\n- Dataset: [ru_all_split.tar.gz](\n\n## Training procedure\n\n```python\nimport random\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import BertTokenizer, EncoderDecoderModel, Trainer, TrainingArguments, logging\n\n\ndef convert_to_tensors(\n    tokenizer,\n    text,\n    max_text_tokens_count,\n    max_title_tokens_count = None,\n    title = None\n):\n    inputs = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=max_text_tokens_count,\n        padding=\"max_length\",\n        truncation=True\n    )\n    result = {\n        \"input_ids\": torch.tensor(inputs[\"input_ids\"]),\n        \"attention_mask\": torch.tensor(inputs[\"attention_mask\"]),\n    }\n\n    if title is not None:\n        outputs = tokenizer(\n            title,\n            add_special_tokens=True,\n            max_length=max_title_tokens_count,\n            padding=\"max_length\",\n            truncation=True\n        )\n\n        decoder_input_ids = torch.tensor(outputs[\"input_ids\"])\n        decoder_attention_mask = torch.tensor(outputs[\"attention_mask\"])\n        labels = decoder_input_ids.clone()\n        labels[decoder_attention_mask == 0] = -100\n        result.update({\n            \"labels\": labels,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"decoder_attention_mask\": decoder_attention_mask\n        })\n    return result\n\n\nclass GetTitleDataset(Dataset):\n    def __init__(\n        self,\n        original_records,\n        sample_rate,\n        tokenizer,\n        max_text_tokens_count,\n        max_title_tokens_count\n    ):\n        self.original_records = original_records\n        self.sample_rate = sample_rate\n        self.tokenizer = tokenizer\n        self.max_text_tokens_count = max_text_tokens_count\n        self.max_title_tokens_count = max_title_tokens_count\n        self.records = []\n        for record in tqdm(original_records):\n            if random.random() > self.sample_rate:\n                continue\n            tensors = convert_to_tensors(\n                tokenizer=tokenizer,\n                title=record[\"title\"],\n                text=record[\"text\"],\n                max_title_tokens_count=self.max_title_tokens_count,\n                max_text_tokens_count=self.max_text_tokens_count\n            )\n            self.records.append(tensors)\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, index):\n        return self.records[index]\n\n\ndef train(\n    train_records,\n    val_records,\n    pretrained_model_path,\n    train_sample_rate=1.0,\n    val_sample_rate=1.0,\n    output_model_path=\"models\",\n    checkpoint=None,\n    max_text_tokens_count=256,\n    max_title_tokens_count=64,\n    batch_size=8,\n    logging_steps=1000,\n    eval_steps=10000,\n    save_steps=10000,\n    learning_rate=0.00003,\n    warmup_steps=2000,\n    num_train_epochs=3\n):\n    logging.set_verbosity_info()\n    tokenizer = BertTokenizer.from_pretrained(\n        pretrained_model_path,\n        do_lower_case=False,\n        do_basic_tokenize=False,\n        strip_accents=False\n    )\n    train_dataset = GetTitleDataset(\n        train_records,\n        train_sample_rate,\n        tokenizer,\n        max_text_tokens_count=max_text_tokens_count,\n        max_title_tokens_count=max_title_tokens_count\n    )\n    val_dataset = GetTitleDataset(\n        val_records,\n        val_sample_rate,\n        tokenizer,\n        max_text_tokens_count=max_text_tokens_count,\n        max_title_tokens_count=max_title_tokens_count\n    )\n    \n    model = EncoderDecoderModel.from_encoder_decoder_pretrained(pretrained_model_path, pretrained_model_path)\n    training_args = TrainingArguments(\n        output_dir=output_model_path,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        do_train=True,\n        do_eval=True,\n        overwrite_output_dir=False,\n        logging_steps=logging_steps,\n        eval_steps=eval_steps,\n        evaluation_strategy=\"steps\",\n        save_steps=save_steps,\n        learning_rate=learning_rate,\n        warmup_steps=warmup_steps,\n        num_train_epochs=num_train_epochs,\n        max_steps=-1,\n        save_total_limit=1,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n    trainer.train(checkpoint)\n    model.save_pretrained(output_model_path)\n```", "qas": [{"id": "q2", "question": "What is the model task of IlyaGusev/rubert_telegram_headlines?", "answers": [{"text": "summarization", "answer_start": 27, "answer_end": 39}]}]}]}, {"title": "IlyaGusev/rubertconv_toxic_clf", "paragraphs": [{"context": "---\nlanguage: \n- ru\ntags:\n- text-classification\nlicense: apache-2.0\n\n---\n\n# RuBERTConv Toxic Classifier\n\n## Model description\n\nBased on [rubert-base-cased-conversational]( model\n\n## Intended uses & limitations\n\n#### How to use\n\nColab: [link](\n\n```python\nfrom transformers import pipeline\n\nmodel_name = \"IlyaGusev/rubertconv_toxic_clf\"\npipe = pipeline(\"text-classification\", model=model_name, tokenizer=model_name, framework=\"pt\") \n\ntext = \"\u0422\u044b \u043f\u0440\u0438\u0434\u0443\u0440\u043e\u043a \u0438\u0437 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0430\"\npipe([text])\n```\n\n## Training data\n\nDatasets:\n- [2ch]( \n- [Odnoklassniki](\n- [Toloka Persona Chat Rus](\n- [Koziev's Conversations]( with [toxic words vocabulary](\n\nAugmentations:\n- \u0451 -> \u0435\n- Remove or add \"?\" or \"!\"\n- Fix CAPS\n- Concatenate toxic and non-toxic texts\n- Concatenate two non-toxic texts\n- Add toxic words from vocabulary\n- Add typos\n- Mask toxic words with \"*\", \"@\", \"$\"\n\n\n## Training procedure\n\nTBA", "qas": [{"id": "q1", "question": "What is the model architecture of IlyaGusev/rubertconv_toxic_clf?", "answers": [{"text": "bert", "answer_start": 139, "answer_end": 142}]}, {"id": "q2", "question": "What is the model task of IlyaGusev/rubertconv_toxic_clf?", "answers": [{"text": "text-classification", "answer_start": 28, "answer_end": 46}]}]}]}, {"title": "Intel/bert-large-uncased-sparse-90-unstructured-pruneofa", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\ntags: \n- fill-mask\ndatasets: \n- wikipedia\n- bookcorpus\n---\n## Model Details: 90% Sparse BERT-Large (uncased) Prune Once for All\nThis model is a sparse pre-trained model that can be fine-tuned for a wide range of language tasks. The process of weight pruning is forcing some of the weights of the neural network to zero. Setting some of the weights to zero results in sparser matrices. Updating neural network weights does involve matrix multiplication, and if we can keep the matrices sparse while retaining enough important information, we can reduce the overall computational overhead. The term \"sparse\" in the title of the model indicates a ratio of sparsity in the weights; for more details, you can read [Zafrir et al. (2021)](\n\nVisualization of Prunce Once for All method from [Zafrir et al. (2021)](\n![Zafrir2021_Fig1.png](\n\n Description |\n ----------- | \n Intel | \n September 30, 2021 | \n 1 | \n NLP - General sparse language model | \n \"The method consists of two steps, teacher preparation and student pruning. The sparse pre-trained model we trained is the model we use for transfer learning while maintaining its sparsity pattern. We call the method Prune Once for All since we show how to fine-tune the sparse pre-trained models for several language tasks while we prune the pre-trained model only once.\" [(Zafrir et al., 2021)]( |\n [Zafrir et al. (2021)]( [GitHub Repo]( | \n Apache 2.0 |\n [Community Tab]( and [Intel Developers Discord](\n\n Description |\n ----------- | \n This is a general sparse language model; in its current form, it is not ready for downstream prediction tasks, but it can be fine-tuned for several language tasks including (but not limited to) question-answering, genre natural language inference, and sentiment classification. | \n Anyone who needs an efficient general language model for other downstream tasks. | \n  The model should not be used to intentionally create hostile or alienating environments for people.|\n\n### How to use\n\nHere is an example of how to import this model in Python:\n\n```python\n\nimport transformers\n\nmodel = transformers.AutoModelForQuestionAnswering.from_pretrained('Intel/bert-large-uncased-sparse-90-unstructured-pruneofa')\n\n```\n\nFor more code examples, refer to the [GitHub Repo](\n\n### Metrics (Model Performance):\n Model Size  MNLI-m (Acc)  QQP (Acc/F1)  SST-2 (Acc) |\n:----------::------------::------------::-----------:|\n   -        -     -     -    |\n   Medium        82.71      91.15/88.00     91.46    |\n   Medium        81.45      90.93/87.72     90.88    |\n    Large        83.74      91.48/88.43     92.95    |\n    Small        81.35      90.29/86.97     90.60    |\n    Small        80.68      90.05/86.67     90.02    |\n\nAll the results are the mean of two seperate experiments with the same hyper-parameters and different seeds.\n\n\n Description | \n ----------- | \n [English Wikipedia Dataset]( (2500M words). |\n To build an efficient and accurate base model for several downstream language tasks. |\n \"We use the English Wikipedia dataset (2500M words) for training the models on the pre-training task. We split the data into train (95%) and validation (5%) sets. Both sets are preprocessed as described in the models\u2019 original papers ([Devlin et al., 2019]( [Sanh et al., 2019]( We process the data to use the maximum sequence length allowed by the models, however, we allow shorter sequences at a probability of 0:1.\" | \n\n Description | \n ----------- | \n The training data come from Wikipedia articles |\n The model is not intended to inform decisions central to human life or flourishing. It is an aggregated set of labelled Wikipedia articles. | \n No additional risk mitigation strategies were considered during model development. |\n Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al., 2021]( and [Bender et al., 2021]( Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Beyond this, the extent of the risks involved by using the model remain unknown.|\n - | \n\n\n \n\n\n### BibTeX entry and citation info\n```bibtex\n@article{zafrir2021prune,\n  title={Prune Once for All: Sparse Pre-Trained Language Models},\n  author={Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen, Haihao and Wasserblat, Moshe},\n  journal={arXiv preprint arXiv:2111.05754},\n  year={2021}\n}\n```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of Intel/bert-large-uncased-sparse-90-unstructured-pruneofa?", "answers": [{"text": "bert", "answer_start": 2171, "answer_end": 2174}]}, {"id": "q2", "question": "What is the model task of Intel/bert-large-uncased-sparse-90-unstructured-pruneofa?", "answers": [{"text": "fill-mask", "answer_start": 46, "answer_end": 54}]}]}]}, {"title": "Intel/bert-large-uncased-squadv1.1-sparse-90-unstructured", "paragraphs": [{"context": "---\nlanguage: en\n---\n# 90% Sparse BERT-Large (uncased) Fine Tuned on SQuADv1.1\nThis model is a result of fine-tuning a Prune OFA 90% sparse pre-trained BERT-Large combined with knowledge distillation.\nThis model yields the following results on SQuADv1.1 development set:<br>\n`{\"exact_match\": 83.56669820245979, \"f1\": 90.20829352733487}`\n\nFor further details see our paper, [Prune Once for All: Sparse Pre-Trained Language Models]( and our open source implementation available [here](\n", "qas": []}]}, {"title": "Intel/distilbert-base-uncased-sparse-85-unstructured-pruneofa", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\ndatasets: \n- wikipedia\n---\n## Model Details: 85% Sparse DistilBERT-Base (uncased) Prune Once for All\nThis model is a sparse pre-trained model that can be fine-tuned for a wide range of language tasks. The process of weight pruning is forcing some of the weights of the neural network to zero. Setting some of the weights to zero results in sparser matrices. Updating neural network weights does involve matrix multiplication, and if we can keep the matrices sparse while retaining enough important information, we can reduce the overall computational overhead. The term \"sparse\" in the title of the model indicates a ratio of sparsity in the weights; for more details, you can read [Zafrir et al. (2021)](\n\nVisualization of Prunce Once for All method from [Zafrir et al. (2021)](\n![Zafrir2021_Fig1.png](\n\n Description |\n ----------- | \n Intel | \n September 30, 2021 | \n 1 | \n NLP - General sparse language model | \n \"The method consists of two steps, teacher preparation and student pruning. The sparse pre-trained model we trained is the model we use for transfer learning while maintaining its sparsity pattern. We call the method Prune Once for All since we show how to fine-tune the sparse pre-trained models for several language tasks while we prune the pre-trained model only once.\" [(Zafrir et al., 2021)]( |\n [Zafrir et al. (2021)]( [GitHub Repo]( | \n Apache 2.0 |\n [Community Tab]( and [Intel Developers Discord](\n\n Description |\n ----------- | \n This is a general sparse language model; in its current form, it is not ready for downstream prediction tasks, but it can be fine-tuned for several language tasks including (but not limited to) question-answering, genre natural language inference, and sentiment classification. | \n Anyone who needs an efficient general language model for other downstream tasks. | \n  The model should not be used to intentionally create hostile or alienating environments for people.|\n\n### How to use\n\nHere is an example of how to import this model in Python:\n\n```python\n\nimport transformers\n\nmodel = transformers.AutoModelForQuestionAnswering.from_pretrained('Intel/distilbert-base-uncased-sparse-85-unstructured-pruneofa')\n\n```\n\nFor more code examples, refer to the [GitHub Repo](\n\n### Metrics (Model Performance):\n Model Size  MNLI-m (Acc)  QQP (Acc/F1)  SST-2 (Acc) |\n:----------::------------::------------::-----------:|\n   -        -     -     -    |\n   Medium        82.71      91.15/88.00     91.46    |\n   Medium        81.45      90.93/87.72     90.88    |\n    Large        83.74      91.48/88.43     92.95    |\n    Small        81.35      90.29/86.97     90.60    |\n    Small        80.68      90.05/86.67     90.02    |\n\nAll the results are the mean of two seperate experiments with the same hyper-parameters and different seeds.\n\n\n Description | \n ----------- | \n [English Wikipedia Dataset]( (2500M words). |\n To build an efficient and accurate base model for several downstream language tasks. |\n \"We use the English Wikipedia dataset (2500M words) for training the models on the pre-training task. We split the data into train (95%) and validation (5%) sets. Both sets are preprocessed as described in the models\u2019 original papers ([Devlin et al., 2019]( [Sanh et al., 2019]( We process the data to use the maximum sequence length allowed by the models, however, we allow shorter sequences at a probability of 0:1.\" | \n\n Description | \n ----------- | \n The training data come from Wikipedia articles |\n The model is not intended to inform decisions central to human life or flourishing. It is an aggregated set of labelled Wikipedia articles. | \n No additional risk mitigation strategies were considered during model development. |\n Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al., 2021]( and [Bender et al., 2021]( Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Beyond this, the extent of the risks involved by using the model remain unknown.|\n - | \n\n\n \n\n\n### BibTeX entry and citation info\n```bibtex\n@article{zafrir2021prune,\n  title={Prune Once for All: Sparse Pre-Trained Language Models},\n  author={Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen, Haihao and Wasserblat, Moshe},\n  journal={arXiv preprint arXiv:2111.05754},\n  year={2021}\n}\n```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of Intel/distilbert-base-uncased-sparse-85-unstructured-pruneofa?", "answers": [{"text": "distilbert", "answer_start": 2144, "answer_end": 2153}]}]}]}, {"title": "Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- wikipedia\n---\n### Model Details: 90% Sparse DistilBERT-Base (uncased) Prune Once for All\nThis model is a sparse pre-trained model that can be fine-tuned for a wide range of language tasks. The process of weight pruning is forcing some of the weights of the neural network to zero. Setting some of the weights to zero results in sparser matrices. Updating neural network weights does involve matrix multiplication, and if we can keep the matrices sparse while retaining enough important information, we can reduce the overall computational overhead. The term \"sparse\" in the title of the model indicates a ratio of sparsity in the weights; for more details, you can read [Zafrir et al. (2021)](\n\nVisualization of Prunce Once for All method from [Zafrir et al. (2021)](\n![Zafrir2021_Fig1.png](\n\n Description |\n ----------- | \n Intel | \n September 30, 2021 | \n 1 | \n NLP - General sparse language model | \n \"The method consists of two steps, teacher preparation and student pruning. The sparse pre-trained model we trained is the model we use for transfer learning while maintaining its sparsity pattern. We call the method Prune Once for All since we show how to fine-tune the sparse pre-trained models for several language tasks while we prune the pre-trained model only once.\" [(Zafrir et al., 2021)]( |\n [Zafrir et al. (2021)]( [GitHub Repo]( | \n Apache 2.0 |\n [Community Tab]( and [Intel Developers Discord](\n\n Description |\n ----------- | \n This is a general sparse language model; in its current form, it is not ready for downstream prediction tasks, but it can be fine-tuned for several language tasks including (but not limited to) question-answering, genre natural language inference, and sentiment classification. | \n Anyone who needs an efficient general language model for other downstream tasks. | \n  The model should not be used to intentionally create hostile or alienating environments for people.|\n\n### How to use\n\nHere is an example of how to import this model in Python:\n\n```python\n\nimport transformers\n\nmodel = transformers.AutoModelForQuestionAnswering.from_pretrained('Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa')\n\n```\n\nFor more code examples, refer to the [GitHub Repo](\n\n### Metrics (Model Performance):\n Model Size  MNLI-m (Acc)  QQP (Acc/F1)  SST-2 (Acc) |\n:----------::------------::------------::-----------:|\n   -        -     -     -    |\n   Medium        82.71      91.15/88.00     91.46    |\n   Medium        81.45      90.93/87.72     90.88    |\n    Large        83.74      91.48/88.43     92.95    |\n    Small        81.35      90.29/86.97     90.60    |\n    Small        80.68      90.05/86.67     90.02    |\n\nAll the results are the mean of two seperate experiments with the same hyper-parameters and different seeds.\n\n\n Description | \n ----------- | \n [English Wikipedia Dataset]( (2500M words). |\n To build an efficient and accurate base model for several downstream language tasks. |\n \"We use the English Wikipedia dataset (2500M words) for training the models on the pre-training task. We split the data into train (95%) and validation (5%) sets. Both sets are preprocessed as described in the models\u2019 original papers ([Devlin et al., 2019]( [Sanh et al., 2019]( We process the data to use the maximum sequence length allowed by the models, however, we allow shorter sequences at a probability of 0:1.\" | \n\n Description | \n ----------- | \n The training data come from Wikipedia articles |\n The model is not intended to inform decisions central to human life or flourishing. It is an aggregated set of labelled Wikipedia articles. | \n No additional risk mitigation strategies were considered during model development. |\n Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al., 2021]( and [Bender et al., 2021]( Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Beyond this, the extent of the risks involved by using the model remain unknown.|\n - | \n\n\n \n\n\n### BibTeX entry and citation info\n```bibtex\n@article{zafrir2021prune,\n  title={Prune Once for All: Sparse Pre-Trained Language Models},\n  author={Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen, Haihao and Wasserblat, Moshe},\n  journal={arXiv preprint arXiv:2111.05754},\n  year={2021}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa?", "answers": [{"text": "distilbert", "answer_start": 2144, "answer_end": 2153}]}]}]}, {"title": "Intel/dynamic_tinybert", "paragraphs": [{"context": "---\ntags:\n- question-answering\n- bert\nlicense: apache-2.0\ndatasets:\n- squad\nlanguage:\n- en\nmodel-index:\n- name: dynamic-tinybert\n  results: \n  - task:\n      type: question-answering\n      name: question-answering\n    metrics:\n    - type: f1\n      value: 88.71\n\n---\n\n## Model Details: Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic Sequence Length\n\nDynamic-TinyBERT has been fine-tuned for the NLP task of question answering, trained on the SQuAD 1.1 dataset. [Guskin et al. (2021)]( note:\n\n> Dynamic-TinyBERT is a TinyBERT model that utilizes sequence-length reduction and Hyperparameter Optimization for enhanced inference efficiency per any computational budget. Dynamic-TinyBERT is trained only once, performing on-par with BERT and achieving an accuracy-speedup trade-off superior to any other efficient approaches (up to 3.3x with <1% loss-drop).\n\n\n\n Description |\n ----------- | \n Intel | \n Intel in collaboration with Hugging Face | \n November 22, 2021 | \n 1 | \n NLP - Question Answering | \n \"For our Dynamic-TinyBERT model we use the architecture of TinyBERT6L: a small BERT model with 6 layers, a hidden size of 768, a feed forward size of 3072 and 12 heads.\" [Guskin et al. (2021)]( |\n [Paper]( [Poster]( [GitHub Repo]( | \n Apache 2.0 |\n [Community Tab]( and [Intel Developers Discord](\n\n Description |\n ----------- | \n You can use the model for the NLP task of question answering: given a corpus of text, you can ask it a question about that text, and it will find the answer in the text. | \n Anyone doing question answering | \n  The model should not be used to intentionally create hostile or alienating environments for people.|\n\n### How to use\n\nHere is how to import this model in Python:\n\n <details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\ntokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")\n ```\n</details>\n\n\n Description | \n ----------- | \n Many Wikipedia articles with question and answer labels are contained in the training data | \n - |\n Training was completed on a Titan GPU. |\n Model deployment on alternate hardware and software will change model performance |\n\n Description | \n ----------- | \n F1 |\n - | \n - | \n\n Description | \n ----------- | \n SQuAD1.1: \"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\" (\n To build an efficient and accurate model for the question answering task. |\n \"We start with a pre-trained general-TinyBERT student, which was trained to learn the general knowledge of BERT using the general-distillation method presented by TinyBERT. We perform transformer distillation from a fine- tuned BERT teacher to the student, following the same training steps used in the original TinyBERT: (1) intermediate-layer distillation (ID) \u2014 learning the knowledge residing in the hidden states and attentions matrices, and (2) prediction-layer distillation (PD) \u2014 fitting the predictions of the teacher.\" ([Guskin et al., 2021]( \n\nModel Performance Analysis:\n\n Max F1 (full model) \n---------------------\n 88.71               \n\n Description | \n ----------- | \n The training data come from Wikipedia articles |\n The model is not intended to inform decisions central to human life or flourishing. It is an aggregated set of labelled Wikipedia articles. | \n No additional risk mitigation strategies were considered during model development. |\n Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al., 2021]( and [Bender et al., 2021]( Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Beyond this, the extent of the risks involved by using the model remain unknown.|\n - | \n\n\n\n \n\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{\n  doi = {10.48550/ARXIV.2111.09645},\n  \n  url = {\n  \n  author = {Guskin, Shira and Wasserblat, Moshe and Ding, Ke and Kim, Gyuwan},\n  \n  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic Sequence Length},\n  \n  publisher = {arXiv},\n  \n  year = {2021},\n```", "qas": [{"id": "q1", "question": "What is the model architecture of Intel/dynamic_tinybert?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of Intel/dynamic_tinybert?", "answers": [{"text": "question-answering", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "Invincible/Chat_bot-Harrypotter-medium", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n#harry potter", "qas": [{"id": "q2", "question": "What is the model task of Invincible/Chat_bot-Harrypotter-medium?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Jean-Baptiste/camembert-ner-with-dates", "paragraphs": [{"context": "---\nlanguage: fr\ndatasets:\n- Jean-Baptiste/wikiner_fr\nwidget:\n- text: \"Je m'appelle jean-baptiste et j'habite \u00e0 montr\u00e9al depuis fevr 2012\"\n---\n\n# camembert-ner: model fine-tuned from camemBERT for NER task (including DATE tag).\n\n## Introduction\n\n[camembert-ner-with-dates] is an extension of french camembert-ner model with an additionnal tag for dates.\nModel was trained on enriched version of wikiner-fr dataset (~170 634  sentences).\n\nOn my test data (mix of chat and email), this model got an f1 score of ~83% (in comparison dateparser was ~70%).\nDateparser library can still be be used on the output of this model in order to convert text to python datetime object \n(\n\n\n## How to use camembert-ner-with-dates with HuggingFace\n\n##### Load camembert-ner-with-dates and its sub-word tokenizer :\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\")\n\n\n##### Process text sample (from wikipedia)\n\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nnlp(\"Apple est cr\u00e9\u00e9e le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs \u00e0 Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitu\u00e9e sous forme de soci\u00e9t\u00e9 le 3 janvier 1977 \u00e0 l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refl\u00e9ter la diversification de ses produits, le mot \u00ab computer \u00bb est retir\u00e9 le 9 janvier 2015.\")\n\n\n[{'entity_group': 'ORG',\n  'score': 0.9776379466056824,\n  'word': 'Apple',\n  'start': 0,\n  'end': 5},\n {'entity_group': 'DATE',\n  'score': 0.9793774570737567,\n  'word': 'le 1er avril 1976 dans le',\n  'start': 15,\n  'end': 41},\n {'entity_group': 'PER',\n  'score': 0.9958226680755615,\n  'word': 'Steve Jobs',\n  'start': 74,\n  'end': 85},\n {'entity_group': 'LOC',\n  'score': 0.995087186495463,\n  'word': 'Los Altos',\n  'start': 87,\n  'end': 97},\n {'entity_group': 'LOC',\n  'score': 0.9953305125236511,\n  'word': 'Californie',\n  'start': 100,\n  'end': 111},\n {'entity_group': 'PER',\n  'score': 0.9961076378822327,\n  'word': 'Steve Jobs',\n  'start': 115,\n  'end': 126},\n {'entity_group': 'PER',\n  'score': 0.9960325956344604,\n  'word': 'Steve Wozniak',\n  'start': 127,\n  'end': 141},\n {'entity_group': 'PER',\n  'score': 0.9957776467005411,\n  'word': 'Ronald Wayne',\n  'start': 144,\n  'end': 157},\n {'entity_group': 'DATE',\n  'score': 0.994030773639679,\n  'word': 'le 3 janvier 1977 \u00e0',\n  'start': 198,\n  'end': 218},\n {'entity_group': 'ORG',\n  'score': 0.9720810294151306,\n  'word': \"d'Apple Computer\",\n  'start': 240,\n  'end': 257},\n {'entity_group': 'DATE',\n  'score': 0.9924157659212748,\n  'word': '30 ans et',\n  'start': 272,\n  'end': 282},\n {'entity_group': 'DATE',\n  'score': 0.9934852868318558,\n  'word': 'le 9 janvier 2015.',\n  'start': 363,\n  'end': 382}]\n\n```\n\n\n## Model performances (metric: seqeval)\n\nGlobal\n```\n'precision': 0.928\n'recall': 0.928\n'f1': 0.928\n```\n\nBy entity\n```\nLabel LOC: (precision:0.929, recall:0.932, f1:0.931, support:9510)\nLabel PER: (precision:0.952, recall:0.965, f1:0.959, support:9399)\nLabel MISC: (precision:0.878, recall:0.844, f1:0.860, support:5364)\nLabel ORG: (precision:0.848, recall:0.883, f1:0.865, support:2299)\nLabel DATE: Not relevant because of method used to add date tag on wikiner dataset (estimated f1 ~90%)\n\n\n ```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of Jean-Baptiste/camembert-ner-with-dates?", "answers": [{"text": "camembert", "answer_start": 146, "answer_end": 154}]}]}]}, {"title": "JonatanGk/roberta-base-bne-finetuned-cyberbullying-spanish", "paragraphs": [{"context": "---\nlanguage: es\ntags:\n- \"spanish\"\nmetrics:\n- accuracy\nwidget:\n - text: \"Eres mas peque\u00f1o que un pitufo!\"\n - text: \"Eres muy feo!\"\n - text: \"Odio tu forma de hablar!\"\n - text: \"Eres tan fea que cuando eras peque\u00f1a te echaban de comer por debajo de la puerta.\"\n\n---\n\n# roberta-base-bne-finetuned-ciberbullying-spanish\n\nThis model is a fine-tuned version of [BSC-TeMU/roberta-base-bne]( on the dataset generated scrapping all social networks (Twitter, Youtube ...) to detect ciberbullying on Spanish.\n\nIt achieves the following results on the evaluation set:\n\n- Loss: 0.1657\n- Accuracy: 0.9607\n\n## Training and evaluation data\n\nI use the concatenation from multiple datasets generated scrapping social networks (Twitter,Youtube,Discord...)  to fine-tune this model. The total number of sentence pairs is above 360k sentences.\n\n## Training procedure\n\n<details>\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n\n### Training results\n\n Epoch  Accuracy \n:-----::--------:\n 1.0    0.9501   \n 2.0    0.9567   \n 3.0    0.9594   \n 4.0    0.9607   \n\n</details>\n\n### Model in action \ud83d\ude80\n\nFast usage with **pipelines**:\n\n```python\nfrom transformers import pipeline\n\nmodel_path = \"JonatanGk/roberta-base-bne-finetuned-ciberbullying-spanish\"\nbullying_analysis = pipeline(\"text-classification\", model=model_path, tokenizer=model_path)\n\nbullying_analysis(\n    \"Desde que te vi me enamor\u00e9 de ti.\"\n    )\n\n# Output:\n[{'label': 'Not_bullying', 'score': 0.9995710253715515}]\n\nbullying_analysis(\n    \"Eres tan fea que cuando eras peque\u00f1a te echaban de comer por debajo de la puerta.\"\n    )\n# Output:\n[{'label': 'Bullying', 'score': 0.9918262958526611}] \n    \n```\n\n[![Open In Colab](\n\n### Framework versions\n\n- Transformers 4.10.3\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n\n\n> Special thx to [Manuel Romero/@mrm8488]( as my mentor & R.C.\n\n> Created by [Jonatan Luna]( | [LinkedIn](", "qas": [{"id": "q1", "question": "What is the model architecture of JonatanGk/roberta-base-bne-finetuned-cyberbullying-spanish?", "answers": [{"text": "roberta", "answer_start": 268, "answer_end": 274}]}, {"id": "q2", "question": "What is the model task of JonatanGk/roberta-base-bne-finetuned-cyberbullying-spanish?", "answers": [{"text": "text-classification", "answer_start": 1476, "answer_end": 1494}]}]}]}, {"title": "JonatanGk/roberta-base-ca-finetuned-catalonia-independence-detector", "paragraphs": [{"context": "---\nlicense: apache-2.0\nlanguage: ca\ntags:\n- \"catalan\"\ndatasets:\n- catalonia_independence\nmetrics:\n- accuracy\nmodel-index:\n- name: roberta-base-ca-finetuned-mnli\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: catalonia_independence\n      type: catalonia_independence\n      args: catalan\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.7611940298507462\nwidget:\n - text: \"Puigdemont, a l'estat espanyol: Quatre anys despr\u00e9s, ens hem guanyat el dret a dir prou\"\n - text: \"Llarena demana la detenci\u00f3 de Com\u00edn i Ponsat\u00ed aprofitant que s\u00f3n a It\u00e0lia amb Puigdemont\"\n - text: \"Assegura l'expert que en un 46% els catalans s'inclouen dins del que es denomina com el doble sentiment identitari. \u00c9s a dir, se senten tant catalans com espanyols. 1 de cada cinc, en canvi, t\u00e9 un sentiment excloent, nom\u00e9s se senten catalans, i un 4% sol espanyol.\"\n---\n\n# roberta-base-ca-finetuned-catalonia-independence-detector\n\nThis model is a fine-tuned version of [BSC-TeMU/roberta-base-ca]( on the catalonia_independence dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6065\n- Accuracy: 0.7612\n\n<details>\n\n## Training and evaluation data\n\nThe data was collected over 12 days during February and March of 2019 from tweets posted in Barcelona, and during September of 2018 from tweets posted in the town of Terrassa, Catalonia.\n\nEach corpus is annotated with three classes: AGAINST, FAVOR and NEUTRAL, which express the stance towards the target - independence of Catalonia.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.6311          \n 2.0    0.6065          \n 3.0    0.6340          \n 4.0    0.6777          \n 5.0    0.7232          \n\n\n</details>\n\n### Model in action \ud83d\ude80\n\nFast usage with **pipelines**:\n\n```python\n\nfrom transformers import pipeline\n\nmodel_path = \"JonatanGk/roberta-base-ca-finetuned-catalonia-independence-detector\"\nindependence_analysis = pipeline(\"text-classification\", model=model_path, tokenizer=model_path)\n\nindependence_analysis(\n    \"Assegura l'expert que en un 46% els catalans s'inclouen dins del que es denomina com el doble sentiment identitari. \u00c9s a dir, se senten tant catalans com espanyols. 1 de cada cinc, en canvi, t\u00e9 un sentiment excloent, nom\u00e9s se senten catalans, i un 4% sol espanyol.\"\n    )\n# Output:\n[{'label': 'AGAINST', 'score': 0.7457581758499146}]\n\nindependence_analysis(\n    \"Llarena demana la detenci\u00f3 de Com\u00edn i Ponsat\u00ed aprofitant que s\u00f3n a It\u00e0lia amb Puigdemont\"\n    )\n# Output:\n[{'label': 'NEUTRAL', 'score': 0.7436802983283997}] \n    \nindependence_analysis(\n    \"Puigdemont, a l'estat espanyol: Quatre anys despr\u00e9s, ens hem guanyat el dret a dir prou\"\n    )\n# Output:\n[{'label': 'FAVOR', 'score': 0.9040119647979736}]\n\n\n```\n\n[![Open In Colab](\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.9.0+cu111\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n\n\n## Citation\n\nThx to HF.co & [@lewtun]( for Dataset ;)\n\n> Special thx to [Manuel Romero/@mrm8488]( as my mentor & R.C.\n\n> Created by [Jonatan Luna]( | [LinkedIn](", "qas": [{"id": "q1", "question": "What is the model architecture of JonatanGk/roberta-base-ca-finetuned-catalonia-independence-detector?", "answers": [{"text": "roberta", "answer_start": 131, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of JonatanGk/roberta-base-ca-finetuned-catalonia-independence-detector?", "answers": [{"text": "text-classification", "answer_start": 227, "answer_end": 245}]}]}]}, {"title": "JonatanGk/roberta-base-ca-finetuned-hate-speech-offensive-catalan", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: roberta-base-ca-finetuned-mnli\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-base-ca-finetuned-mnli\n\nThis model is a fine-tuned version of [BSC-TeMU/roberta-base-ca]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4137\n- Accuracy: 0.8778\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.3712          \n 2.0    0.3401          \n 3.0    0.4137          \n 4.0    0.4671          \n 5.0    0.5205          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.9.0+cu111\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of JonatanGk/roberta-base-ca-finetuned-hate-speech-offensive-catalan?", "answers": [{"text": "roberta", "answer_start": 96, "answer_end": 102}]}]}]}, {"title": "JonatanGk/roberta-base-ca-finetuned-tecla", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- tecla\nmetrics:\n- accuracy\nmodel-index:\n- name: roberta-base-ca-finetuned-mnli\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: tecla\n      type: tecla\n      args: tecla\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.7361816335412737\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-base-ca-finetuned-mnli\n\nThis model is a fine-tuned version of [BSC-TeMU/roberta-base-ca]( on the tecla dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9354\n- Accuracy: 0.7362\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.8222          \n 2.0    0.7872          \n 3.0    0.8060          \n 4.0    0.8470          \n 5.0    0.9354          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.9.0+cu111\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of JonatanGk/roberta-base-ca-finetuned-tecla?", "answers": [{"text": "roberta", "answer_start": 114, "answer_end": 120}]}, {"id": "q2", "question": "What is the model task of JonatanGk/roberta-base-ca-finetuned-tecla?", "answers": [{"text": "text-classification", "answer_start": 210, "answer_end": 228}]}]}]}, {"title": "Jonesy/DialoGPT-medium_Barney", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n# Barney Calhoun DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Jonesy/DialoGPT-medium_Barney?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Jonesy/FG_OLD", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n# Family Guy DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Jonesy/FG_OLD?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Jonesy/DialoGPT-small_JT", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n# Johnny Test DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Jonesy/DialoGPT-small_JT?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "JorgeSarry/est5-summarize", "paragraphs": [{"context": "---\nlanguage: es\n---\nThis is a smaller version of the google/mt5-base model with only Spanish and some English embeddings trained on 60k Spanish MLSum for summarization.\n\nYou can use it with the command \"summarize:\"\n", "qas": [{"id": "q1", "question": "What is the model architecture of JorgeSarry/est5-summarize?", "answers": [{"text": "mt5", "answer_start": 61, "answer_end": 63}]}]}]}, {"title": "KAIHATSU/DialoGPT-small-rick", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#Rick DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of KAIHATSU/DialoGPT-small-rick?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "KBLab/bert-base-swedish-cased-reallysimple-ner", "paragraphs": [{"context": "---\ntags:\n- token-classification\n- sequence-tagger-model\n- bert\nlanguage: sv\ndatasets:\n- KBLab/sucx3_ner\nwidget:\n- text: \"Emil bor i L\u00f6nneberga\"\n---\n\n# KB-BERT for NER\n\n## Cased data\n\nThis model is based on [KB-BERT]( and was fine-tuned on the [SUCX 3.0 - NER]( corpus, using the _simple_ tags and cased data.\nFor this model we used a variation of the data that did **not** use BIO-encoding to differentiate between the beginnings (B), and insides (I) of named entity tags.\n\nThe model was trained on the training data only, with the best model chosen by its performance on the validation data.\nYou find more information about the model and the performance on our blog: ", "qas": [{"id": "q2", "question": "What is the model task of KBLab/bert-base-swedish-cased-reallysimple-ner?", "answers": [{"text": "token-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "KBLab/bert-base-swedish-lowermix-reallysimple-ner", "paragraphs": [{"context": "---\nmodel:\n- KB/bert-base-swedish-cased\ntags:\n- token-classification\n- sequence-tagger-model\n- bert\nlanguage: sv\ndatasets:\n- KBLab/sucx3_ner\nwidget:\n- text: \"Emil bor i L\u00f6nneberga\"\n---\n\n# KB-BERT for NER\n\n## Mixed cased and uncased data\n\nThis model is based on [KB-BERT]( and was fine-tuned on the [SUCX 3.0 - NER]( corpus, using the _simple_ tags and partially lowercased data.\nFor this model we used a variation of the data that did **not** use BIO-encoding to differentiate between the beginnings (B), and insides (I) of named entity tags.\n\nThe model was trained on the training data only, with the best model chosen by its performance on the validation data.\nYou find more information about the model and the performance on our blog: ", "qas": [{"id": "q1", "question": "What is the model architecture of KBLab/bert-base-swedish-lowermix-reallysimple-ner?", "answers": [{"text": "bert", "answer_start": 16, "answer_end": 19}]}, {"id": "q2", "question": "What is the model task of KBLab/bert-base-swedish-lowermix-reallysimple-ner?", "answers": [{"text": "token-classification", "answer_start": 48, "answer_end": 67}]}]}]}, {"title": "KBLab/megatron-bert-base-swedish-cased-600k", "paragraphs": [{"context": "---\nlanguage:\n- sv\n\n---\n\n# Megatron-BERT-base Swedish 600k\n\nThis BERT model was trained using the Megatron-LM library.\nThe size of the model is a regular BERT-base with 110M parameters.\nThe model was trained on about 70GB of data, consisting mostly of OSCAR and Swedish newspaper text curated by the National Library of Sweden.\n\nTraining was done for 600k training steps. Its [sister model]( used the same setup, but was instead trained for only 125k steps.\n\n\nThe model has three sister models trained on the same dataset:\n- [\ud83e\udd17 BERT Swedish](\n- [Megatron-BERT-base-125k](\n- [Megatron-BERT-large-110k](\n\n## Acknowledgements\n\nWe gratefully acknowledge the HPC RIVR consortium ( and EuroHPC JU ( for funding this research by providing computing resources of the HPC system Vega at the Institute of Information Science (", "qas": []}]}, {"title": "KBLab/megatron-bert-base-swedish-cased-125k", "paragraphs": [{"context": "---\nlanguage:\n- sv\n\n---\n\n# Megatron-BERT-base Swedish 125k\n\nThis BERT model was trained using the Megatron-LM library.\nThe size of the model is a regular BERT-base with 110M parameters.\nThe model was trained on about 70GB of data, consisting mostly of OSCAR and Swedish newspaper text curated by the National Library of Sweden.\n\nTraining was done for 125k training steps. Its [sister model]( used the same setup, but was instead trained for 600k steps.\n\n\nThe model has three sister models trained on the same dataset:\n- [\ud83e\udd17 BERT Swedish](\n- [Megatron-BERT-base-600k](\n- [Megatron-BERT-large-110k](\n\n## Acknowledgements\n\nWe gratefully acknowledge the HPC RIVR consortium ( and EuroHPC JU ( for funding this research by providing computing resources of the HPC system Vega at the Institute of Information Science (", "qas": []}]}, {"title": "KBLab/roberta-base-swedish-cased", "paragraphs": [{"context": "# Roberta base TEST", "qas": []}]}, {"title": "KBLab/sentence-bert-swedish-cased", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\nlang:\n  - sv\ntags:\n  - sentence-transformers\n  - feature-extraction\n  - sentence-similarity\n  - transformers\nwidget:\n- source_sentence: \"Mannen \u00e5t mat.\"\n  sentences:\n    - \"Han f\u00f6rt\u00e4rde en n\u00e4rande och nyttig m\u00e5ltid.\"\n    - \"Det var ett sunkigt hak med ganska gott k\u00e4k.\"\n    - \"Han inmundigade middagen tillsammans med ett glas r\u00f6dvin.\"\n    - \"Potatischips \u00e4r j\u00e4ttegoda.\"\n    - \"Tryck p\u00e5 knappen f\u00f6r att f\u00e5 tala med kundsupporten.\"\n  example_title: \"Mat\"\n- source_sentence: \"Kan jag deklarera digitalt fr\u00e5n utlandet?\"\n  sentences:\n    - \"Du som befinner dig i utlandet kan deklarera digitalt p\u00e5 flera olika s\u00e4tt.\"\n    - \"Du som har kvarskatt att betala ska g\u00f6ra en inbetalning till ditt skattekonto.\"\n    - \"Efter att du har deklarerat g\u00e5r vi igenom uppgifterna i din deklaration och r\u00e4knar ut din skatt.\"\n    - \"I din deklaration som du f\u00e5r fr\u00e5n oss har vi r\u00e4knat ut vad du ska betala eller f\u00e5 tillbaka.\"\n    - \"Tryck p\u00e5 knappen f\u00f6r att f\u00e5 tala med kundsupporten.\"\n  example_title: \"Skatteverket FAQ\"\n- source_sentence: \"Hon kunde g\u00f6ra bak\u00e5tvolter.\"\n  sentences:\n    - \"Hon var atletisk.\"\n    - \"Hon var bra p\u00e5 gymnastik.\"\n    - \"Hon var inte atletisk.\"\n    - \"Hon var of\u00f6rm\u00f6gen att flippa bakl\u00e4nges.\"\n  example_title: \"Gymnastik\"\n---\n\n# KBLab/sentence-bert-swedish-cased\n\nThis is a [sentence-transformers]( model: It maps Swedish sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. This model is a bilingual Swedish-English model trained according to instructions in the paper [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation]( and the [documentation]( accompanying its companion python package. We have used the strongest available pretrained English Bi-Encoder ([all-mpnet-base-v2]( as a teacher model, and the pretrained Swedish [KB-BERT]( as the student model. \n\nA more detailed description of the model can be found in an article we published on the KBLab blog [here]( and for the updated model [here]( \n\n**Update**: We have released updated versions of the model since the initial release. The original model described in the blog post is **v1.0**. The current version is **v2.0**. The newer versions are trained on longer paragraphs, and have a longer max sequence length. **v2.0** is trained with a stronger teacher model and is the current default.\n\n Teacher Model \n---------\n [paraphrase-mpnet-base-v2](  \n [paraphrase-mpnet-base-v2](  \n [all-mpnet-base-v2](  \n\n<!--- Describe your model here -->\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"Det h\u00e4r \u00e4r en exempelmening\", \"Varje exempel blir konverterad\"]\n\nmodel = SentenceTransformer('KBLab/sentence-bert-swedish-cased')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### Loading an older model version (Sentence-Transformers)\n\nCurrently, the easiest way to load an older model version is to clone the model repository and load it from disk. For example, to clone the **v1.0** model:\n\n```bash\ngit clone --depth 1 --branch v1.0 \n```\n\nThen you can load the model by pointing to the local folder where you cloned the model:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"path_to_model_folder/sentence-bert-swedish-cased\")\n```\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['Det h\u00e4r \u00e4r en exempelmening', 'Varje exempel blir konverterad']\n\n# Load model from HuggingFace Hub\n# To load an older version, e.g. v1.0, add the argument revision=\"v1.0\" \ntokenizer = AutoTokenizer.from_pretrained('KBLab/sentence-bert-swedish-cased')\nmodel = AutoModel.from_pretrained('KBLab/sentence-bert-swedish-cased')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n### Loading an older model (Hugginfface Transformers)\n\nTo load an older model specify the version tag with the `revision` arg. For example, to load the **v1.0** model, use the following code: \n\n```python\nAutoTokenizer.from_pretrained('KBLab/sentence-bert-swedish-cased', revision=\"v1.0\")\nAutoModel.from_pretrained('KBLab/sentence-bert-swedish-cased', revision=\"v1.0\")\n```\n\n## Evaluation Results\n\n<!--- Describe how your model was evaluated -->\n\nThe model was evaluated on [SweParaphrase v1.0]( and **SweParaphrase v2.0**. This test set is part of [SuperLim]( -- a Swedish evaluation suite for natural langage understanding tasks.  We calculated Pearson and Spearman correlation between predicted model similarity scores and the human similarity score labels. Results from **SweParaphrase v1.0** are displayed below. \n\n Pearson \n---------\n 0.9183  \n 0.9183  \n **0.9283**  \n\nThe following code snippet can be used to reproduce the above results:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"sweparaphrase-dev-165.csv\",\n    sep=\"\\t\",\n    header=None,\n    names=[\n        \"original_id\",\n        \"source\",\n        \"type\",\n        \"sentence_swe1\",\n        \"sentence_swe2\",\n        \"score\",\n        \"sentence1\",\n        \"sentence2\",\n    ],\n)\n\nmodel = SentenceTransformer(\"KBLab/sentence-bert-swedish-cased\")\n\nsentences1 = df[\"sentence_swe1\"].tolist()\nsentences2 = df[\"sentence_swe2\"].tolist()\n\n# Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\n\n# Compute cosine similarity after normalizing\nembeddings1 /= embeddings1.norm(dim=-1, keepdim=True)\nembeddings2 /= embeddings2.norm(dim=-1, keepdim=True)\n\ncosine_scores = embeddings1 @ embeddings2.t()\nsentence_pair_scores = cosine_scores.diag()\n\ndf[\"model_score\"] = sentence_pair_scores.cpu().tolist()\nprint(df[[\"score\", \"model_score\"]].corr(method=\"spearman\"))\nprint(df[[\"score\", \"model_score\"]].corr(method=\"pearson\"))\n```\n\n### Sweparaphrase v2.0\n\nIn general, **v1.1** correlates the most with human assessment of text similarity on SweParaphrase v2.0. Below, we present zero-shot evaluation results on all data splits. They display the model's performance out of the box, without any fine-tuning.\n\n Data split  Spearman   |\n------------------------|\n train       0.8256     |\n train       **0.8302** |\n train       0.8059     |\n dev         0.8774     |\n dev         **0.8833** |\n dev         0.8668     |\n test        0.8476     |\n test        **0.8550** |\n test        0.8213     |\n\n### SweFAQ v2.0\n\nWhen it comes to retrieval tasks, **v2.0** performs the best by quite a substantial margin. It is better at matching the correct answer to a question compared to v1.1 and v1.0.\n\n Data split \n------------\n train      \n train      \n train      \n dev        \n dev        \n dev        \n test       \n test       \n test       \n\n\nExamples how to evaluate the models on some of the test sets of the SuperLim suites can be found on the following links: [evaluate_faq.py]( (Swedish FAQ), [evaluate_swesat.py]( (SweSAT synonyms), [evaluate_supersim.py]( (SuperSim).\n\n## Training\n\nAn article with more details on data and v1.0 of the model can be found on the [KBLab blog]( \n\nAround 14.6 million sentences from English-Swedish parallel corpuses were used to train the model. Data was sourced from the [Open Parallel Corpus]( (OPUS) and downloaded via the python package [opustools]( Datasets used were: JW300, Europarl, DGT-TM, EMEA, ELITR-ECA, TED2020, Tatoeba and OpenSubtitles. \n\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 180513 with parameters:\n```\n{'batch_size': 64, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.MSELoss.MSELoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"epochs\": 2,\n    \"evaluation_steps\": 1000,\n    \"evaluator\": \"sentence_transformers.evaluation.SequentialEvaluator.SequentialEvaluator\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'torch.optim.adamw.AdamW'>\",\n    \"optimizer_params\": {\n        \"eps\": 1e-06,\n        \"lr\": 8e-06\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": null,\n    \"warmup_steps\": 5000,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->\nThis model was trained by KBLab, a data lab at the National Library of Sweden. \n\nYou can cite the article on our blog:  .\n\n```\n@misc{rekathati2021introducing,  \n  author = {Rekathati, Faton},  \n  title = {The KBLab Blog: Introducing a Swedish Sentence Transformer},  \n  url = {  \n  year = {2021}  \n}\n```\n\n## Acknowledgements\n\nWe gratefully acknowledge the HPC RIVR consortium ([ and EuroHPC JU ([eurohpc-ju.europa.eu/]( for funding this research by providing computing resources of the HPC system Vega at the Institute of Information Science ([\n", "qas": [{"id": "q1", "question": "What is the model architecture of KBLab/sentence-bert-swedish-cased?", "answers": [{"text": "bert", "answer_start": 1291, "answer_end": 1294}]}, {"id": "q2", "question": "What is the model task of KBLab/sentence-bert-swedish-cased?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "KBLab/wav2vec2-base-voxpopuli-sv-swedish", "paragraphs": [{"context": "---\nlanguage: sv-SE\ndatasets:\n- common_voice\n- NST Swedish ASR Database\nmetrics:\n- wer\n#- cer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- voxpopuli\nlicense: cc-by-nc-4.0\nmodel-index:\n- name: Wav2vec 2.0 base VoxPopuli-sv swedish\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: NST Swedish ASR Database\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 5.619804368919309\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice\n      type: common_voice\n      args: sv-SE\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 19.145252414798616\n---\n# Wav2vec 2.0 base-voxpopuli-sv-swedish\nFinetuned version of Facebooks [VoxPopuli-sv base]( model using NST and Common Voice data. Evalutation without a language model gives the following: WER for NST + Common Voice test set (2% of total sentences) is **5.62%**, WER for Common Voice test set is **19.15%**.\n\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\ntest_dataset = load_dataset(\"common_voice\", \"sv-SE\", split=\"test[:2%]\").\nprocessor = Wav2Vec2Processor.from_pretrained(\"KBLab/wav2vec2-base-voxpopuli-sv-swedish\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"KBLab/wav2vec2-base-voxpopuli-sv-swedish\")\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\npredicted_ids = torch.argmax(logits, dim=-1)\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```", "qas": [{"id": "q1", "question": "What is the model architecture of KBLab/wav2vec2-base-voxpopuli-sv-swedish?", "answers": [{"text": "wav2vec2", "answer_start": 1460, "answer_end": 1467}]}, {"id": "q2", "question": "What is the model task of KBLab/wav2vec2-base-voxpopuli-sv-swedish?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 110, "answer_end": 137}]}, {"id": "q3", "question": "What is the model category of KBLab/wav2vec2-base-voxpopuli-sv-swedish?", "answers": [{"text": "audio", "answer_start": 102, "answer_end": 106}]}]}]}, {"title": "KBLab/wav2vec2-large-voxpopuli-sv-swedish", "paragraphs": [{"context": "---\nlanguage: sv-SE\ndatasets:\n- common_voice\n- NST Swedish ASR Database\nmetrics:\n- wer\n- cer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- voxpopuli\nlicense: cc-by-nc-4.0\nmodel-index:\n- name: Wav2vec 2.0 large VoxPopuli-sv swedish\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice\n      type: common_voice\n      args: sv-SE\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 10.994764\n       - name: Test CER\n         type: cer\n         value: 3.946846\n---\n# Wav2vec 2.0 large-voxpopuli-sv-swedish\n\n**PLEASE NOTE that [this]( model performs better and has a less restrictive license.**\n\n\nAdditionally pretrained and finetuned version of Facebooks [VoxPopuli-sv large]( model using Swedish radio broadcasts, NST and Common Voice data. Evalutation without a language model gives the following: WER for NST + Common Voice test set (2% of total sentences) is **3.95%**. WER for Common Voice test set is **10.99%** directly and **7.82%** with a 4-gram language model.\n\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Training\nThis model has additionally pretrained on 1000h of Swedish local radio broadcasts, fine-tuned for 120000 updates on NST + CommonVoice and then for an additional 20000 updates on CommonVoice only. The additional fine-tuning on CommonVoice hurts performance on the NST+CommonVoice test set somewhat and, unsurprisingly, improves it on the CommonVoice test set. It seems to perform generally better though [citation needed].\n\n## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\ntest_dataset = load_dataset(\"common_voice\", \"sv-SE\", split=\"test[:2%]\").\nprocessor = Wav2Vec2Processor.from_pretrained(\"KBLab/wav2vec2-large-voxpopuli-sv-swedish\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"KBLab/wav2vec2-large-voxpopuli-sv-swedish\")\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\npredicted_ids = torch.argmax(logits, dim=-1)\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```", "qas": [{"id": "q1", "question": "What is the model architecture of KBLab/wav2vec2-large-voxpopuli-sv-swedish?", "answers": [{"text": "wav2vec2", "answer_start": 1929, "answer_end": 1936}]}, {"id": "q2", "question": "What is the model task of KBLab/wav2vec2-large-voxpopuli-sv-swedish?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 109, "answer_end": 136}]}, {"id": "q3", "question": "What is the model category of KBLab/wav2vec2-large-voxpopuli-sv-swedish?", "answers": [{"text": "audio", "answer_start": 101, "answer_end": 105}]}]}]}, {"title": "KBLab/wav2vec2-large-voxrex-swedish", "paragraphs": [{"context": "---\nlanguage: sv\ndatasets:\n- common_voice\n- NST Swedish ASR Database\n- P4\nmetrics:\n- wer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- hf-asr-leaderboard\nlicense: cc0-1.0\nmodel-index:\n- name: Wav2vec 2.0 large VoxRex Swedish\n  results:\n  - task:\n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice\n      type: common_voice\n      args: sv-SE\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 8.49\n---\n# Wav2vec 2.0 large VoxRex Swedish (C)\n\n**Disclaimer:** This is a work in progress. See [VoxRex]( for more details.\n\n**Update 2022-01-10:** Updated to VoxRex-C version.\n\n**Update 2022-05-16:** Paper is is [here](\n\nFinetuned version of KBs [VoxRex large]( model using Swedish radio broadcasts, NST and Common Voice data. Evalutation without a language model gives the following: WER for NST + Common Voice test set (2% of total sentences) is **2.5%**. WER for Common Voice test set is **8.49%** directly and **7.37%** with a 4-gram language model.\n\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n# Performance\\*\n\n![Comparison](comparison.png \"Comparison\")\n<center><del>*<i>Chart shows performance without the additional 20k steps of Common Voice fine-tuning</i></del></center>\n\n## Training\nThis model has been fine-tuned for 120000 updates on NST + CommonVoice<del> and then for an additional 20000 updates on CommonVoice only. The additional fine-tuning on CommonVoice hurts performance on the NST+CommonVoice test set somewhat and, unsurprisingly, improves it on the CommonVoice test set. It seems to perform generally better though [citation needed]</del>.\n\n![WER during training](chart_1.svg \"WER\")\n\n## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\ntest_dataset = load_dataset(\"common_voice\", \"sv-SE\", split=\"test[:2%]\").\nprocessor = Wav2Vec2Processor.from_pretrained(\"KBLab/wav2vec2-large-voxrex-swedish\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"KBLab/wav2vec2-large-voxrex-swedish\")\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\npredicted_ids = torch.argmax(logits, dim=-1)\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of KBLab/wav2vec2-large-voxrex-swedish?", "answers": [{"text": "wav2vec2", "answer_start": 2054, "answer_end": 2061}]}, {"id": "q2", "question": "What is the model task of KBLab/wav2vec2-large-voxrex-swedish?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 105, "answer_end": 132}]}, {"id": "q3", "question": "What is the model category of KBLab/wav2vec2-large-voxrex-swedish?", "answers": [{"text": "audio", "answer_start": 97, "answer_end": 101}]}]}]}, {"title": "KBLab/wav2vec2-large-xlsr-53-swedish", "paragraphs": [{"context": "---\nlanguage: sv-SE\ndatasets:\n- common_voice\n- NST Swedish ASR Database\nmetrics:\n- wer\n- cer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 Swedish by KBLab\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice sv-SE\n      type: common_voice\n      args: sv-SE\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 14.298610\n       - name: Test CER\n         type: cer\n         value: 4.925294\n---\n\n# Wav2Vec2-Large-XLSR-53-Swedish\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( in Swedish using the [NST Swedish Dictation](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\n\nThe model can be used directly (without a language model) as follows:\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\ntest_dataset = load_dataset(\"common_voice\", \"sv-SE\", split=\"test[:2%]\").\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"KBLab/wav2vec2-large-xlsr-53-swedish\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"KBLab/wav2vec2-large-xlsr-53-swedish\")\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on the Swedish test data of Common Voice.\n\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\ntest_dataset = load_dataset(\"common_voice\", \"sv-SE\", split=\"test\")\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"KBLab/wav2vec2-large-xlsr-53-swedish\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"KBLab/wav2vec2-large-xlsr-53-swedish\")\nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[,?.!\\\\-;:\"\u201c]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\nprint(\"CER: {:2f}\".format(100 * wer.compute(predictions=[\" \".join(list(entry)) for entry in result[\"pred_strings\"]], references=[\" \".join(list(entry)) for entry in result[\"sentence\"]])))\n```\n\n**WER**: 14.298610%\n**CER**: 4.925294%\n\n## Training\n\nFirst the XLSR model was further pre-trained for 50 epochs with a corpus consisting of 1000 hours spoken Swedish from various radio stations. Secondly [NST Swedish Dictation]( was used for fine tuning as well as [Common Voice]( Lastly only Common Voice dataset was used for final finetuning. The [Fairseq]( scripts were used.\n", "qas": [{"id": "q1", "question": "What is the model architecture of KBLab/wav2vec2-large-xlsr-53-swedish?", "answers": [{"text": "wav2vec2", "answer_start": 634, "answer_end": 641}]}, {"id": "q2", "question": "What is the model task of KBLab/wav2vec2-large-xlsr-53-swedish?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 109, "answer_end": 136}]}, {"id": "q3", "question": "What is the model category of KBLab/wav2vec2-large-xlsr-53-swedish?", "answers": [{"text": "audio", "answer_start": 101, "answer_end": 105}]}]}]}, {"title": "KENNETHFOO/DialoGPT-medium-harrypotter", "paragraphs": [{"context": "---\ntags: \n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of KENNETHFOO/DialoGPT-medium-harrypotter?", "answers": [{"text": "conversational", "answer_start": 13, "answer_end": 26}]}]}]}, {"title": "Mads/wav2vec2-xlsr-large-53-kor-financial-engineering", "paragraphs": [{"context": "# WIP\n\n\n\n\n", "qas": []}]}, {"title": "MagicalCat29/model_save_test2", "paragraphs": [{"context": "---\nlicense: other\n---\n", "qas": []}]}, {"title": "MagmaCubes1133/DialoGPT-large-rick", "paragraphs": [{"context": "---\ntags: \n  conversational\n---\n#Rick Sanchez DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of MagmaCubes1133/DialoGPT-large-rick?", "answers": [{"text": "conversational", "answer_start": 13, "answer_end": 26}]}]}]}, {"title": "Mahalakshmi/wav2vec2-xls-r-300m-demo-colab", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-xls-r-300m-demo-colab\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-xls-r-300m-demo-colab\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- eval_loss: 0.9475\n- eval_wer: 1.0377\n- eval_runtime: 70.5646\n- eval_samples_per_second: 25.239\n- eval_steps_per_second: 3.16\n- epoch: 21.05\n- step: 2000\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 300\n- mixed_precision_training: Native AMP\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Mahalakshmi/wav2vec2-xls-r-300m-demo-colab?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "Maheshwaranr/bert-analyzer", "paragraphs": [{"context": "---\nlicense: afl-3.0\n---\n", "qas": []}]}, {"title": "MaiaMaiaMaia/DialoGPT-medium-PeterParkerBot", "paragraphs": [{"context": "----\ntags:\n- conversational\n---\n\n#Peter Parker DialoGPT Model", "qas": []}]}, {"title": "MalawiUniST/ISO6392.nya.ny", "paragraphs": [{"context": "This model trained on nyanja dataset in Longformer", "qas": []}]}, {"title": "Maltehb/aelaectra-danish-electra-small-cased-ner-dane", "paragraphs": [{"context": "---\nlanguage: \"da\"\ntags:\n- \u00e6l\u00e6ctra\n- pytorch\n- danish\n- ELECTRA-Small\n- replaced token detection\nlicense: \"mit\"\ndatasets:\n- DAGW\nwidget:\n- text: \"Chili Jensen, som bor p\u00e5 Danmarksgade 12, k\u00f8ber chilifrugter fra Netto.\"\nmetrics:\n- f1\n---\n\n# \u00c6l\u00e6ctra - Finetuned for Named Entity Recognition on the [DaNE dataset]( (Hvingelby et al., 2020) by Malte H\u00f8jmark-Bertelsen.\n**\u00c6l\u00e6ctra** is a Danish Transformer-based language model created to enhance the variety of Danish NLP resources with a more efficient model compared to previous state-of-the-art (SOTA) models. \n\n\u00c6l\u00e6ctra was pretrained with the ELECTRA-Small (Clark et al., 2020) pretraining approach by using the Danish Gigaword Corpus (Str\u00f8mberg-Derczynski et al., 2020) and evaluated on Named Entity Recognition (NER) tasks. Since NER only presents a limited picture of \u00c6l\u00e6ctra's capabilities I am very interested in further evaluations. Therefore, if you employ it for any task, feel free to hit me up your findings!\n\n\u00c6l\u00e6ctra was, as mentioned, created to enhance the Danish NLP capabilties and please do note how this GitHub still does not support the Danish characters \"*\u00c6, \u00d8 and \u00c5*\" as the title of this repository becomes \"*-l-ctra*\". How ironic.\ud83d\ude42\n\nHere is an example on how to load the finetuned \u00c6l\u00e6ctra-cased model for Named Entity Recognition in [PyTorch]( using the [\ud83e\udd17Transformers]( library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Maltehb/-l-ctra-danish-electra-small-cased-ner-dane\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Maltehb/-l-ctra-danish-electra-small-cased-ner-dane\")\n```\n\n### Evaluation of current Danish Language Models \n\n\u00c6l\u00e6ctra, Danish BERT (DaBERT) and multilingual BERT (mBERT) were evaluated:\n\n Layers  Params  Average Inference Time (Sec/Epoch)  \n ---  ---  --- \n 12  13.7M  10.91  \n 12  14.7M  10.92  \n 12  110M  43.03  \n 12  167M  72.10  \n 12  177M  70.56  \n\n\nOn [DaNE]( (Hvingelby et al., 2020) without the *MISC-tag*, \u00c6l\u00e6ctra scores slightly worse than both cased and uncased Multilingual BERT (Devlin et al., 2019) and Danish BERT (Danish BERT, 2019/2020), however, \u00c6l\u00e6ctra is less than one third the size, and uses significantly fewer computational resources to pretrain and instantiate.\n\n### Pretraining\nTo pretrain \u00c6l\u00e6ctra it is recommended to build a Docker Container from the [Dockerfile]( Next, simply follow the [pretraining notebooks]( \n\nThe pretraining was done by utilizing a single NVIDIA Tesla V100 GPU with 16 GiB, endowed by the Danish data company [KMD]( The pretraining took approximately 4 days and 9.5 hours for both the cased and uncased model\n\n### Fine-tuning\nTo fine-tune any \u00c6l\u00e6ctra model follow the [fine-tuning notebooks](\n\n### References\nClark, K., Luong, M.-T., Le, Q. V., & Manning, C. D. (2020). ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. ArXiv:2003.10555 [Cs]. \n\nDanish BERT. (2020). BotXO.  (Original work published 2019)\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv:1810.04805 [Cs]. \n\nHvingelby, R., Pauli, A. B., Barrett, M., Rosted, C., Lidegaard, L. M., & S\u00f8gaard, A. (2020). DaNE: A Named Entity Resource for Danish. Proceedings of the 12th Language Resources and Evaluation Conference, 4597\u20134604. \n\nStr\u00f8mberg-Derczynski, L., Baglini, R., Christiansen, M. H., Ciosici, M. R., Dalsgaard, J. A., Fusaroli, R., Henrichsen, P. J., Hvingelby, R., Kirkedal, A., Kjeldsen, A. S., Ladefoged, C., Nielsen, F. \u00c5., Petersen, M. L., Rystr\u00f8m, J. H., & Varab, D. (2020). The Danish Gigaword Project. ArXiv:2005.03521 [Cs]. \n\n\n#### Acknowledgements\nAs the majority of this repository is build upon [the works]( by the team at Google who created ELECTRA, a HUGE thanks to them is in order. \n\nA Giga thanks also goes out to the incredible people who collected The Danish Gigaword Corpus (Str\u00f8mberg-Derczynski et al., 2020). \n\nFurthermore, I would like to thank my supervisor [Riccardo Fusaroli]( for the support with the thesis, and a special thanks goes out to [Kenneth Enevoldsen]( for his continuous feedback. \n\nLastly, i would like to thank KMD, my colleagues from KMD, and my peers and co-students from Cognitive Science for encouriging me to keep on working hard and holding my head up high!\n\n#### Contact\n\nFor help or further information feel free to connect with the author Malte H\u00f8jmark-Bertelsen on [hjb@kmd.dk](mailto:hjb@kmd.dk?subject=[GitHub]%20\u00c6l\u00e6ctraCasedNER) or any of the following platforms:\n\n[<img align=\"left\" alt=\"MalteHB | Twitter\" width=\"22px\" src=\" />][twitter]\n[<img align=\"left\" alt=\"MalteHB | LinkedIn\" width=\"22px\" src=\" />][linkedin]\n[<img align=\"left\" alt=\"MalteHB | Instagram\" width=\"22px\" src=\" />][instagram]\n\n<br />\n\n</details>\n\n[twitter]: \n[instagram]: \n[linkedin]: ", "qas": [{"id": "q1", "question": "What is the model architecture of Maltehb/aelaectra-danish-electra-small-cased-ner-dane?", "answers": [{"text": "electra", "answer_start": 1501, "answer_end": 1507}]}]}]}, {"title": "Norod78/hebrew-gpt_neo-xl-poetry", "paragraphs": [{"context": "---\nlanguage: he\n\nthumbnail: \nwidget:\n- text: \"\u05e2\u05d5\u05d3 \u05d1\u05d9\u05de\u05d9 \u05e7\u05d3\u05dd\"\n- text: \"\u05ea\u05e8\u05d9\u05e1\u05e8 \u05de\u05db\u05e9\u05e4\u05d5\u05ea \u05e1\u05d2\"\n- text:  \"\\n\\n\u05d4\u05d0\u05d9\u05e9 \u05d4\u05d0\u05d7\u05e8\u05d5\u05df \u05d1\u05e2\u05d5\u05dc\u05dd /\"\n- text: \"\u05e4\u05e2\u05dd \u05d0\u05d7\u05ea, \u05dc\u05e4\u05e0\u05d9 \u05e9\u05e0\u05d9\u05dd \u05e8\u05d1\u05d5\u05ea\"\n- text: \"\u05d4\u05e8\u05de\u05d9\u05d5\u05e0\u05d9 \u05d4\u05e1\u05ea\u05d9\u05e8\u05d4 \u05d0\u05ea\"\n- text: \"\u05dc\u05e4\u05ea\u05e2, \u05d0\u05d5\u05e8 \u05d9\u05e8\u05d5\u05e7\"\n\nlicense: mit\n---\n\n# hebrew-gpt_neo-xl-poetry\n\nHebrew poetry text generation model which was fine tuned upon on [hebrew-gpt_neo-xl](\n## Datasets\n\nAn assortment of various Hebrew books, magazines and poetry corpuses\n\n## Training Config\n\nSimilar to [this one]( <BR>\n\n## Usage\n\n### Google Colab Notebook\n\nAvailable [here ]( <BR>\n\n\n#### Simple usage sample code\n\n```python\n\n!pip install tokenizers==0.10.3 transformers==4.8.0\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n  \ntokenizer = AutoTokenizer.from_pretrained(\"Norod78/hebrew-gpt_neo-xl-poetry\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Norod78/hebrew-gpt_neo-xl-poetry\", pad_token_id=tokenizer.eos_token_id)\n\nprompt_text = \"\u05d0\u05e0\u05d9 \u05d0\u05d5\u05d4\u05d1 \u05e9\u05d5\u05e7\u05d5\u05dc\u05d3 \u05d5\u05e2\u05d5\u05d2\u05d5\u05ea\"\nmax_len = 512\nsample_output_num = 3\nseed = 1000\n\nimport numpy as np\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = 0 if torch.cuda.is_available()==False else torch.cuda.device_count()\n\nprint(f\"device: {device}, n_gpu: {n_gpu}\")\n\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif n_gpu > 0:\n    torch.cuda.manual_seed_all(seed)\n\nmodel.to(device)\n\nencoded_prompt = tokenizer.encode(\n    prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n\nencoded_prompt = encoded_prompt.to(device)\n\nif encoded_prompt.size()[-1] == 0:\n        input_ids = None\nelse:\n        input_ids = encoded_prompt\n\nprint(\"input_ids = \" + str(input_ids))\n\nif input_ids != None:\n  max_len += len(encoded_prompt[0])\n  if max_len > 2048:\n    max_len = 2048\n\nprint(\"Updated max_len = \" + str(max_len))\n\nstop_token = \"<>\"\nnew_lines = \"\\n\\n\\n\"\n\nsample_outputs = model.generate(\n    input_ids,\n    do_sample=True, \n    max_length=max_len, \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=sample_output_num\n)\n\nprint(100 * '-' + \"\\n\\t\\tOutput\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n\n  text = tokenizer.decode(sample_output, skip_special_tokens=True)\n  \n  # Remove all text after the stop token\n  text = text[: text.find(stop_token) if stop_token else None]\n\n  # Remove all text after 3 newlines\n  text = text[: text.find(new_lines) if new_lines else None]\n\n  print(\"\\n{}: {}\".format(i, text))\n  print(\"\\n\" + 100 * '-')\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of Norod78/hebrew-gpt_neo-xl-poetry?", "answers": [{"text": "gpt_neo", "answer_start": 237, "answer_end": 243}]}]}]}, {"title": "Norod78/hebrew_stories-gpt_neo-small", "paragraphs": [{"context": "---\nlanguage: he\n\nthumbnail: \nwidget:\n- text: \"\u05ea\u05e8\u05d9\u05e1\u05e8 \u05de\u05db\u05e9\u05e4\u05d5\u05ea \u05e1\u05d2\"\n- text:  \"\\n\\n\u05d4\u05d0\u05d9\u05e9 \u05d4\u05d0\u05d7\u05e8\u05d5\u05df \u05d1\u05e2\u05d5\u05dc\u05dd /\"\n- text: \"\u05e4\u05e2\u05dd \u05d0\u05d7\u05ea, \u05dc\u05e4\u05e0\u05d9 \u05e9\u05e0\u05d9\u05dd \u05e8\u05d1\u05d5\u05ea\"\n- text: \"\u05d4\u05e8\u05de\u05d9\u05d5\u05e0\u05d9 \u05d4\u05e1\u05ea\u05d9\u05e8\u05d4 \u05d0\u05ea\"\n- text: \"\u05dc\u05e4\u05ea\u05e2, \u05d0\u05d5\u05e8 \u05d9\u05e8\u05d5\u05e7\"\n\nlicense: mit\n---\n\n# hebrew_stories-gpt_neo-small\n\nHebrew story-text generation model, fined tuned upon [hebrew-gpt_neo-small]( which was trained using [EleutherAI's gpt-neo](\n\n## Dataset\n\nText from various Hebrew books\n", "qas": [{"id": "q1", "question": "What is the model architecture of Norod78/hebrew_stories-gpt_neo-small?", "answers": [{"text": "gpt_neo", "answer_start": 222, "answer_end": 228}]}]}]}, {"title": "Nova/DialoGPT-medium-Lelouch", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#Lelouch DialoGPT model", "qas": [{"id": "q2", "question": "What is the model task of Nova/DialoGPT-medium-Lelouch?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "NovaChrono/twervy", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# My Awesome Model", "qas": [{"id": "q2", "question": "What is the model task of NovaChrono/twervy?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Rexhaif/rubert-base-srl-seqlabeling", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: rubert-base-srl-seqlabeling\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# rubert-base-srl-seqlabeling\n\nThis model is a fine-tuned version of [./ruBert-base/]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1723\n- Causator Precision: 0.8539\n- Causator Recall: 0.8352\n- Causator F1: 0.8444\n- Causator Number: 91\n- Expiriencer Precision: 0.9259\n- Expiriencer Recall: 0.9740\n- Expiriencer F1: 0.9494\n- Expiriencer Number: 77\n- Instrument Precision: 0.375\n- Instrument Recall: 1.0\n- Instrument F1: 0.5455\n- Instrument Number: 3\n- Other Precision: 0.0\n- Other Recall: 0.0\n- Other F1: 0.0\n- Other Number: 1\n- Predicate Precision: 0.9352\n- Predicate Recall: 0.9902\n- Predicate F1: 0.9619\n- Predicate Number: 102\n- Overall Precision: 0.8916\n- Overall Recall: 0.9307\n- Overall F1: 0.9107\n- Overall Accuracy: 0.9667\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.98) and epsilon=1e-06\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.06\n- num_epochs: 10.0\n\n### Training results\n\n Epoch  Validation Loss  Causator Recall  Causator Number  Expiriencer Recall  Expiriencer Number  Instrument Recall  Instrument Number  Other Recall  Other Number  Predicate Recall  Predicate Number  Overall Recall  Overall Accuracy |\n:-----::---------------::---------------::---------------::------------------::------------------::-----------------::-----------------::------------::------------::----------------::----------------::--------------::----------------:|\n 1.0    0.3471           0.6703           91               0.8312              77                  0.0                3                  0.0           1             0.9804            102               0.8212          0.9203           |\n 2.0    0.1608           0.7802           91               0.9740              77                  0.6667             3                  0.0           1             0.9706            102               0.9015          0.9554           |\n 3.0    0.1311           0.8022           91               0.9740              77                  1.0                3                  0.0           1             0.9804            102               0.9161          0.9673           |\n 4.0    0.1507           0.8242           91               0.9481              77                  1.0                3                  0.0           1             0.9804            102               0.9161          0.9637           |\n 5.0    0.1830           0.7912           91               0.9870              77                  1.0                3                  0.0           1             0.9902            102               0.9197          0.9560           |\n 6.0    0.1994           0.8462           91               0.9870              77                  1.0                3                  0.0           1             0.9902            102               0.9380          0.9572           |\n 7.0    0.1657           0.8462           91               0.9740              77                  1.0                3                  0.0           1             0.9902            102               0.9343          0.9673           |\n 8.0    0.1716           0.8462           91               0.9740              77                  1.0                3                  0.0           1             0.9902            102               0.9343          0.9673           |\n 9.0    0.1715           0.8352           91               0.9740              77                  1.0                3                  0.0           1             0.9902            102               0.9307          0.9667           |\n 10.0   0.1723           0.8352           91               0.9740              77                  1.0                3                  0.0           1             0.9902            102               0.9307          0.9667           |\n\n\n### Framework versions\n\n- Transformers 4.13.0.dev0\n- Pytorch 1.10.0+cu102\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rexhaif/rubert-base-srl-seqlabeling?", "answers": [{"text": "bert", "answer_start": 58, "answer_end": 61}]}]}]}, {"title": "RifsxD/DialoGPT-medium-raifu", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# My Awesome Model", "qas": [{"id": "q2", "question": "What is the model task of RifsxD/DialoGPT-medium-raifu?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "RishabhRawatt/DialoGPT-small-kela", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Kela DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of RishabhRawatt/DialoGPT-small-kela?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Ritchie/DialoGPT-small-Rickandmorty", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Rick and Morty DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of Ritchie/DialoGPT-small-Rickandmorty?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "RizqFarIDN/DialoGPT-medium-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#harry potter DialoGPT model", "qas": [{"id": "q2", "question": "What is the model task of RizqFarIDN/DialoGPT-medium-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "RizqFarIDN/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#harry potter DialoGPT model", "qas": [{"id": "q2", "question": "What is the model task of RizqFarIDN/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "RobW/distilbert-base-cased-finetuned-chunk", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilbert-base-cased-finetuned-chunk\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-cased-finetuned-chunk\n\nThis model is a fine-tuned version of [distilbert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5180\n- Precision: 0.8615\n- Recall: 0.9088\n- F1: 0.8845\n- Accuracy: 0.8239\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.5871           0.9035  0.8054   |\n 2.0    0.5447           0.8983  0.8142   |\n 3.0    0.5180           0.9088  0.8239   |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.9.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of RobW/distilbert-base-cased-finetuned-chunk?", "answers": [{"text": "distilbert", "answer_start": 122, "answer_end": 131}]}]}]}, {"title": "Roberta55/deberta-base-mnli-finetuned-cola", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: deberta-base-mnli-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.6281691768918801\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# deberta-base-mnli-finetuned-cola\n\nThis model is a fine-tuned version of [microsoft/deberta-base-mnli]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8205\n- Matthews Correlation: 0.6282\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5110          \n 2.0    0.6648          \n 3.0    0.6681          \n 4.0    0.8205          \n 5.0    1.0413          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.9.0+cu111\n- Datasets 1.14.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Roberta55/deberta-base-mnli-finetuned-cola?", "answers": [{"text": "deberta", "answer_start": 118, "answer_end": 124}]}, {"id": "q2", "question": "What is the model task of Roberta55/deberta-base-mnli-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 216, "answer_end": 234}]}]}]}, {"title": "RobinMari/DialoGPT-small-mikoto", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Mikoto Jinba DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of RobinMari/DialoGPT-small-mikoto?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Rocketknight1/bert-base-cased-finetuned-swag", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/bert-base-cased-finetuned-swag\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/bert-base-cased-finetuned-swag\n\nThis model is a fine-tuned version of [bert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.8709\n- Train Accuracy: 0.6465\n- Validation Loss: 0.6167\n- Validation Accuracy: 0.7590\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 5e-05, 'decay_steps': 9192, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Train Accuracy  Validation Accuracy \n:--------------::-------------------:\n 0.6465          0.7590              \n\n\n### Framework versions\n\n- Transformers 4.21.0.dev0\n- TensorFlow 2.9.1\n- Datasets 2.3.3.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/bert-base-cased-finetuned-swag?", "answers": [{"text": "bert", "answer_start": 97, "answer_end": 100}]}]}]}, {"title": "Rocketknight1/bert-base-cased-finetuned-wikitext2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/bert-base-cased-finetuned-wikitext2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/bert-base-cased-finetuned-wikitext2\n\nThis model is a fine-tuned version of [bert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 6.3982\n- Validation Loss: 6.2664\n- Epoch: 1\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': 2e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 6.4768          \n 6.2664          \n\n\n### Framework versions\n\n- Transformers 4.21.0.dev0\n- TensorFlow 2.9.1\n- Datasets 2.3.3.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/bert-base-cased-finetuned-wikitext2?", "answers": [{"text": "bert", "answer_start": 97, "answer_end": 100}]}]}]}, {"title": "Rocketknight1/bert-base-uncased-finetuned-swag", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/bert-base-uncased-finetuned-swag\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/bert-base-uncased-finetuned-swag\n\nThis model is a fine-tuned version of [bert-base-uncased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.8360\n- Train Accuracy: 0.6631\n- Validation Loss: 0.5885\n- Validation Accuracy: 0.7706\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 5e-05, 'decay_steps': 9192, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Train Accuracy  Validation Accuracy \n:--------------::-------------------:\n 0.6631          0.7706              \n\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- TensorFlow 2.8.0-rc0\n- Datasets 2.0.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/bert-base-uncased-finetuned-swag?", "answers": [{"text": "bert", "answer_start": 97, "answer_end": 100}]}]}]}, {"title": "Rocketknight1/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/distilbert-base-uncased-finetuned-cola\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.3182\n- Validation Loss: 0.4914\n- Train Matthews Correlation: 0.5056\n- Epoch: 1\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 1602, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Validation Loss  Epoch |\n:---------------::-----:|\n 0.4638           0     |\n 0.4914           1     |\n\n\n### Framework versions\n\n- Transformers 4.22.0.dev0\n- TensorFlow 2.9.1\n- Datasets 2.4.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 97, "answer_end": 106}]}]}]}, {"title": "Rocketknight1/distilbert-base-uncased-finetuned-ner", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/distilbert-base-uncased-finetuned-ner\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/distilbert-base-uncased-finetuned-ner\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.2026\n- Validation Loss: 0.0726\n- Train Precision: 0.8945\n- Train Recall: 0.9220\n- Train F1: 0.9081\n- Train Accuracy: 0.9793\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 2631, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss  Train Recall  Train Accuracy \n:---------------::------------::--------------:\n 0.0726           0.9220        0.9793         \n\n\n### Framework versions\n\n- Transformers 4.21.0.dev0\n- TensorFlow 2.9.1\n- Datasets 2.3.3.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/distilbert-base-uncased-finetuned-ner?", "answers": [{"text": "distilbert", "answer_start": 97, "answer_end": 106}]}]}]}, {"title": "Rocketknight1/distilbert-base-uncased-finetuned-squad", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/distilbert-base-uncased-finetuned-squad\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/distilbert-base-uncased-finetuned-squad\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 1.5124\n- Train End Logits Accuracy: 0.6041\n- Train Start Logits Accuracy: 0.5680\n- Validation Loss: 1.1534\n- Validation End Logits Accuracy: 0.6849\n- Validation Start Logits Accuracy: 0.6443\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 11064, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Train End Logits Accuracy  Validation Loss  Validation Start Logits Accuracy \n:-------------------------::---------------::--------------------------------:\n 0.6041                     1.1534           0.6443                           \n\n\n### Framework versions\n\n- Transformers 4.21.0.dev0\n- TensorFlow 2.9.1\n- Datasets 2.3.3.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/distilbert-base-uncased-finetuned-squad?", "answers": [{"text": "distilbert", "answer_start": 97, "answer_end": 106}]}]}]}, {"title": "Rocketknight1/distilgpt2-finetuned-wikitext2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/distilgpt2-finetuned-wikitext2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/distilgpt2-finetuned-wikitext2\n\nThis model is a fine-tuned version of [distilgpt2]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 3.8577\n- Validation Loss: 3.6752\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': 2e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 3.6752          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- TensorFlow 2.8.0-rc0\n- Datasets 1.17.0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/distilgpt2-finetuned-wikitext2?", "answers": [{"text": "gpt2", "answer_start": 103, "answer_end": 106}]}]}]}, {"title": "Rocketknight1/distilroberta-base-finetuned-wikitext2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: distilroberta-base-finetuned-wikitext2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# distilroberta-base-finetuned-wikitext2\n\nThis model is a fine-tuned version of [distilroberta-base]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': 2e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- TensorFlow 2.8.0-rc0\n- Datasets 1.17.0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/distilroberta-base-finetuned-wikitext2?", "answers": [{"text": "roberta", "answer_start": 89, "answer_end": 95}]}]}]}, {"title": "Rocketknight1/gbert-base-germaner", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/gbert-base-germaner\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/gbert-base-germaner\n\nThis model is a fine-tuned version of [deepset/gbert-base]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.0340\n- Validation Loss: 0.0881\n- Epoch: 2\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'inner_optimizer': {'class_name': 'AdamWeightDecay', 'config': {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 4176, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}}, 'dynamic': True, 'initial_scale': 32768.0, 'dynamic_growth_steps': 2000}\n- training_precision: mixed_float16\n\n### Training results\n\n Validation Loss \n:---------------:\n 0.0865          \n 0.0878          \n 0.0881          \n\n\n### Framework versions\n\n- Transformers 4.15.0.dev0\n- TensorFlow 2.6.0\n- Datasets 1.16.2.dev0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/gbert-base-germaner?", "answers": [{"text": "bert", "answer_start": 91, "answer_end": 94}]}]}]}, {"title": "Rocketknight1/gpt2-finetuned-wikitext2", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/gpt2-finetuned-wikitext2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/gpt2-finetuned-wikitext2\n\nThis model is a fine-tuned version of [gpt2]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 7.3062\n- Validation Loss: 6.7676\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': 2e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 6.7676          \n\n\n### Framework versions\n\n- Transformers 4.21.0.dev0\n- TensorFlow 2.9.1\n- Datasets 2.3.3.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/gpt2-finetuned-wikitext2?", "answers": [{"text": "gpt2", "answer_start": 90, "answer_end": 93}]}]}]}, {"title": "Rocketknight1/marian-finetuned-kde4-en-to-fr", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/marian-finetuned-kde4-en-to-fr\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/marian-finetuned-kde4-en-to-fr\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-en-fr]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.6862\n- Validation Loss: 0.8050\n- Epoch: 2\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 5e-05, 'decay_steps': 17733, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 0.8832          \n 0.8211          \n 0.8050          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- TensorFlow 2.7.0\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/marian-finetuned-kde4-en-to-fr?", "answers": [{"text": "marian", "answer_start": 97, "answer_end": 102}]}]}]}, {"title": "Rocketknight1/model-card-callback-test-new", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/model-card-callback-test-new\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/model-card-callback-test-new\n\nThis model is a fine-tuned version of [distilbert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.0031\n- Train Accuracy: 1.0\n- Validation Loss: 0.0000\n- Validation Accuracy: 1.0\n- Epoch: 1\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Train Accuracy  Validation Accuracy \n:--------------::-------------------:\n 0.6406          1.0                 \n 1.0             1.0                 \n\n\n### Framework versions\n\n- Transformers 4.14.0.dev0\n- TensorFlow 2.6.0\n- Datasets 1.16.2.dev0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/model-card-callback-test-new?", "answers": [{"text": "distilbert", "answer_start": 412, "answer_end": 421}]}]}]}, {"title": "Rocketknight1/model_card_test2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: model_card_test2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# model_card_test2\n\nThis model is a fine-tuned version of [distilbert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.0031\n- Train Accuracy: 1.0\n- Validation Loss: 0.0000\n- Validation Accuracy: 1.0\n- Epoch: 1\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Train Accuracy  Validation Accuracy \n:--------------::-------------------:\n 0.6406          1.0                 \n 1.0             1.0                 \n\n\n### Framework versions\n\n- Transformers 4.14.0.dev0\n- TensorFlow 2.6.0\n- Datasets 1.16.2.dev0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/model_card_test2?", "answers": [{"text": "distilbert", "answer_start": 360, "answer_end": 369}]}]}]}, {"title": "Rocketknight1/opus-mt-en-ROMANCE-finetuned-en-to-ro", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/opus-mt-en-ROMANCE-finetuned-en-to-ro\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/opus-mt-en-ROMANCE-finetuned-en-to-ro\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-en-ROMANCE]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.7140\n- Validation Loss: 1.2757\n- Train Bleu: 26.7914\n- Train Gen Len: 41.4932\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': 2e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss  Train Gen Len \n:---------------::-------------:\n 1.2757           41.4932       \n\n\n### Framework versions\n\n- Transformers 4.21.0.dev0\n- TensorFlow 2.9.1\n- Datasets 2.4.0\n- Tokenizers 0.11.0\n", "qas": []}]}, {"title": "Rocketknight1/t5-small-finetuned-xsum", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/t5-small-finetuned-xsum\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/t5-small-finetuned-xsum\n\nThis model is a fine-tuned version of [t5-small]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 2.7172\n- Validation Loss: 2.3977\n- Train Rouge1: 28.7469\n- Train Rouge2: 7.9005\n- Train Rougel: 22.5917\n- Train Rougelsum: 22.6162\n- Train Gen Len: 18.875\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': 2e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss  Train Rouge2  Train Rougelsum  Epoch |\n:---------------::------------::---------------::-----:|\n 2.3977           7.9005        22.6162          0     |\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- TensorFlow 2.8.0-rc0\n- Datasets 1.17.0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/t5-small-finetuned-xsum?", "answers": [{"text": "t5", "answer_start": 97, "answer_end": 98}]}]}]}, {"title": "Rocketknight1/test-model-tf", "paragraphs": [{"context": "---\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: test-model-tf\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# test-model-tf\n\nThis model is a fine-tuned version of []( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: None\n- training_precision: float32\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.14.0.dev0\n- TensorFlow 2.6.0\n- Datasets 1.16.2.dev0\n- Tokenizers 0.10.3\n", "qas": []}]}, {"title": "Rocketknight1/transformers-qa", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: transformers-qa\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# transformers-qa\n\nThis model is a fine-tuned version of [distilbert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.9300\n- Validation Loss: 1.1437\n- Epoch: 1\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': 5e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n- training_precision: mixed_float16\n\n### Training results\n\n Validation Loss \n:---------------:\n 1.1500          \n 1.1437          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- TensorFlow 2.6.0\n- Datasets 1.16.2.dev0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/transformers-qa?", "answers": [{"text": "distilbert", "answer_start": 358, "answer_end": 367}]}]}]}, {"title": "Rolv-Arild/xls-r-300m-npsc-4", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- NbAiLab/NPSC\n- generated_from_trainer\nmodel-index:\n- name: ''\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the NBAILAB/NPSC - 16K_MP3 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1957\n- Wer: 0.1697\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 7.5e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 2000\n- num_epochs: 20.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.28   4.0144          \n 0.56   3.1369          \n 0.85   3.0183          \n 1.13   2.9991          \n 1.41   2.9000          \n 1.69   1.7688          \n 1.98   0.6842          \n 2.26   0.5096          \n 2.54   0.4479          \n 2.82   0.4056          \n 3.11   0.3870          \n 3.39   0.3646          \n 3.67   0.3499          \n 3.95   0.3345          \n 4.24   0.3320          \n 4.52   0.3117          \n 4.8    0.3198          \n 5.08   0.3071          \n 5.37   0.3011          \n 5.65   0.2875          \n 5.93   0.2926          \n 6.21   0.2695          \n 6.5    0.2602          \n 6.78   0.2603          \n 7.06   0.2540          \n 7.34   0.2614          \n 7.63   0.2707          \n 7.91   0.2483          \n 8.19   0.2483          \n 8.47   0.2487          \n 8.76   0.2456          \n 9.04   0.2397          \n 9.32   0.2374          \n 9.6    0.2206          \n 9.89   0.2247          \n 10.17  0.2325          \n 10.45  0.2301          \n 10.73  0.2192          \n 11.02  0.2266          \n 11.3   0.2193          \n 11.58  0.2309          \n 11.86  0.2268          \n 12.15  0.2322          \n 12.43  0.2197          \n 12.71  0.2211          \n 12.99  0.2079          \n 13.28  0.2054          \n 13.56  0.2031          \n 13.84  0.2059          \n 14.12  0.2122          \n 14.41  0.2072          \n 14.69  0.2105          \n 14.97  0.2035          \n 15.25  0.2035          \n 15.54  0.1964          \n 15.82  0.1984          \n 16.1   0.2022          \n 16.38  0.1969          \n 16.67  0.1963          \n 16.95  0.1960          \n 17.23  0.1929          \n 17.51  0.1928          \n 17.8   0.1937          \n 18.08  0.1978          \n 18.36  0.1956          \n 18.64  0.1933          \n 18.93  0.1962          \n 19.21  0.1962          \n 19.49  0.1965          \n 19.77  0.1957          \n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.0+cu113\n- Datasets 1.18.1\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rolv-Arild/xls-r-300m-npsc-4?", "answers": [{"text": "wav2vec2", "answer_start": 384, "answer_end": 391}]}, {"id": "q2", "question": "What is the model task of Rolv-Arild/xls-r-300m-npsc-4?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 32, "answer_end": 59}]}]}]}, {"title": "Rolv-Arild/xls-r-300m-npsc-seq2seq", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: ''\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model was trained from scratch on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2965\n- Wer: 0.3144\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 20.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.51   3.7320          \n 1.02   2.9188          \n 1.53   2.3347          \n 2.04   0.6678          \n 2.55   0.4605          \n 3.06   0.4266          \n 3.57   0.3786          \n 4.08   0.3161          \n 4.59   0.3029          \n 5.1    0.2988          \n 5.61   0.2873          \n 6.12   0.3129          \n 6.63   0.2963          \n 7.14   0.2755          \n 7.65   0.2706          \n 8.16   0.2823          \n 8.67   0.2754          \n 9.18   0.2917          \n 9.69   0.2885          \n 10.2   0.2810          \n 10.71  0.2689          \n 11.22  0.2899          \n 11.73  0.2798          \n 12.24  0.2894          \n 12.75  0.2838          \n 13.27  0.2959          \n 13.77  0.2922          \n 14.29  0.2903          \n 14.8   0.2868          \n 15.31  0.2959          \n 15.82  0.2966          \n 16.33  0.2941          \n 16.84  0.2980          \n 17.35  0.2965          \n 17.86  0.2935          \n 18.37  0.2964          \n 18.88  0.2967          \n 19.39  0.2955          \n 19.9   0.2965          \n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.0+cu113\n- Datasets 1.18.1\n- Tokenizers 0.11.0\n", "qas": []}]}, {"title": "Rolv-Arild/xls-r-300m-npsc", "paragraphs": [{"context": "", "qas": []}]}, {"title": "Rostlab/prot_t5_base_mt_uniref50", "paragraphs": [{"context": "---\ntags:\n- summarization\nwidget:\n- text: \"predict protein ms : Met Gly Leu Pro Val Ser Trp Ala Pro Pro Ala Leu\"\n\n---\n\n", "qas": [{"id": "q2", "question": "What is the model task of Rostlab/prot_t5_base_mt_uniref50?", "answers": [{"text": "summarization", "answer_start": 12, "answer_end": 24}]}]}]}, {"title": "Rostlab/prot_t5_xl_bfd", "paragraphs": [{"context": "---\nlanguage: protein\ntags:\n- protein language model\ndatasets:\n- BFD\n---\n\n# ProtT5-XL-BFD model\n\nPretrained model on protein sequences using a masked language modeling (MLM) objective. It was introduced in\n[this paper]( and first released in\n[this repository]( This model is trained on uppercase amino acids: it only works with capital letter amino acids.\n\n\n## Model description\n\nProtT5-XL-BFD is based on the `t5-3b` model and was pretrained on a large corpus of protein sequences in a self-supervised fashion.\nThis means it was pretrained on the raw protein sequences only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those protein sequences.\n\nOne important difference between this T5 model and the original T5 version is the denosing objective.\nThe original T5-3B model was pretrained using a span denosing objective, while this model was pre-trained with a Bart-like MLM denosing objective.\nThe masking probability is consistent with the original T5 training by randomly masking 15% of the amino acids in the input.\n\nIt has been shown that the features extracted from this self-supervised model (LM-embeddings) captured important biophysical properties governing protein shape.\nshape.\nThis implied learning some of the grammar of the language of life realized in protein sequences.\n\n## Intended uses & limitations\n\nThe model could be used for protein feature extraction or to be fine-tuned on downstream tasks.\nWe have noticed in some tasks on can gain more accuracy by fine-tuning the model rather than using it as a feature extractor.\nWe have also noticed that for feature extraction, its better to use the feature extracted from the encoder not from the decoder.\n\n### How to use\n\nHere is how to use this model to extract the features of a given protein sequence in PyTorch:\n\n```python\nfrom transformers import T5Tokenizer, T5Model\nimport re\nimport torch\n\ntokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_bfd', do_lower_case=False)\n\nmodel = T5Model.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n\nsequences_Example = [\"A E T C Z A O\",\"S K T Z P\"]\n\nsequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]\n\nids = tokenizer.batch_encode_plus(sequences_Example, add_special_tokens=True, padding=True)\n\ninput_ids = torch.tensor(ids['input_ids'])\nattention_mask = torch.tensor(ids['attention_mask'])\n\nwith torch.no_grad():\n    embedding = model(input_ids=input_ids,attention_mask=attention_mask,decoder_input_ids=None)\n\n# For feature extraction we recommend to use the encoder embedding\nencoder_embedding = embedding[2].cpu().numpy()\ndecoder_embedding = embedding[0].cpu().numpy()\n```\n\n## Training data\n\nThe ProtT5-XL-BFD model was pretrained on [BFD]( a dataset consisting of 2.1 billion protein sequences.\n\n## Training procedure\n\n### Preprocessing\n\nThe protein sequences are uppercased and tokenized using a single space and a vocabulary size of 21. The rare amino acids \"U,Z,O,B\" were mapped to \"X\".\nThe inputs of the model are then of the form:\n\n```\nProtein Sequence [EOS]\n```\n\nThe preprocessing step was performed on the fly, by cutting and padding the protein sequences up to 512 tokens.\n\nThe details of the masking procedure for each sequence are as follows:\n- 15% of the amino acids are masked.\n- In 90% of the cases, the masked amino acids are replaced by `[MASK]` token.\n- In 10% of the cases, the masked amino acids are replaced by a random amino acid (different) from the one they replace.\n\n### Pretraining\n\nThe model was trained on a single TPU Pod V3-1024 for 1.2 million steps in total, using sequence length 512 (batch size 4k).\nIt has a total of approximately 3B parameters and was trained using the encoder-decoder architecture.\nThe optimizer used is AdaFactor with inverse square root learning rate schedule for pre-training.\n\n\n## Evaluation results\n\nWhen the model is used for feature etraction, this model achieves the following results:\n\nTest results :\n\n secondary structure (3-states)   Localization \n:-----::-----:\n 77     \n 85      \n 84     \n     77 \n\n### BibTeX entry and citation info\n\n```bibtex\n@article {Elnaggar2020.07.12.199554,\n\tauthor = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n\ttitle = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n\telocation-id = {2020.07.12.199554},\n\tyear = {2020},\n\tdoi = {10.1101/2020.07.12.199554},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tabstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22- and 112 times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8 states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs. Availability ProtTrans: \\&lt;a href=\" Interest StatementThe authors have declared no competing interest.},\n\tURL = {\n\teprint = {\n\tjournal = {bioRxiv}\n}\n```\n\n> Created by [Ahmed Elnaggar/@Elnaggar_AI]( | [LinkedIn](\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rostlab/prot_t5_xl_bfd?", "answers": [{"text": "t5", "answer_start": 411, "answer_end": 412}]}]}]}, {"title": "Rostlab/prot_t5_xl_uniref50", "paragraphs": [{"context": "---\ntags:\n- protein language model\ndatasets:\n- UniRef50\n---\n\n# ProtT5-XL-UniRef50 model\n\nPretrained model on protein sequences using a masked language modeling (MLM) objective. It was introduced in\n[this paper]( and first released in\n[this repository]( This model is trained on uppercase amino acids: it only works with capital letter amino acids.\n\n\n## Model description\n\nProtT5-XL-UniRef50 is based on the `t5-3b` model and was pretrained on a large corpus of protein sequences in a self-supervised fashion.\nThis means it was pretrained on the raw protein sequences only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those protein sequences.\n\nOne important difference between this T5 model and the original T5 version is the denosing objective.\nThe original T5-3B model was pretrained using a span denosing objective, while this model was pre-trained with a Bart-like MLM denosing objective.\nThe masking probability is consistent with the original T5 training by randomly masking 15% of the amino acids in the input.\n\nIt has been shown that the features extracted from this self-supervised model (LM-embeddings) captured important biophysical properties governing protein shape.\nshape.\nThis implied learning some of the grammar of the language of life realized in protein sequences.\n\n## Intended uses & limitations\n\nThe model could be used for protein feature extraction or to be fine-tuned on downstream tasks.\nWe have noticed in some tasks on can gain more accuracy by fine-tuning the model rather than using it as a feature extractor.\nWe have also noticed that for feature extraction, its better to use the feature extracted from the encoder not from the decoder.\n\n### How to use\n\nHere is how to use this model to extract the features of a given protein sequence in PyTorch:\n\n```python\nsequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\nsequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n\n# tokenize sequences and pad up to the longest sequence in the batch\nids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\ninput_ids = torch.tensor(ids['input_ids']).to(device)\nattention_mask = torch.tensor(ids['attention_mask']).to(device)\n\n# generate embeddings\nwith torch.no_grad():\n    embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n\n# extract embeddings for the first ([0,:]) sequence in the batch while removing padded & special tokens ([0,:7]) \nemb_0 = embedding_repr.last_hidden_state[0,:7] # shape (7 x 1024)\nprint(f\"Shape of per-residue embedding of first sequences: {emb_0.shape}\")\n# do the same for the second ([1,:]) sequence in the batch while taking into account different sequence lengths ([1,:8])\nemb_1 = embedding_repr.last_hidden_state[1,:8] # shape (8 x 1024)\n\n# if you want to derive a single representation (per-protein embedding) for the whole protein\nemb_0_per_protein = emb_0.mean(dim=0) # shape (1024)\n\nprint(f\"Shape of per-protein embedding of first sequences: {emb_0_per_protein.shape}\")\n```\n\n## Training data\n\nThe ProtT5-XL-UniRef50 model was pretrained on [UniRef50]( a dataset consisting of 45 million protein sequences.\n\n## Training procedure\n\n### Preprocessing\n\nThe protein sequences are uppercased and tokenized using a single space and a vocabulary size of 21. The rare amino acids \"U,Z,O,B\" were mapped to \"X\".\nThe inputs of the model are then of the form:\n\n```\nProtein Sequence [EOS]\n```\n\nThe preprocessing step was performed on the fly, by cutting and padding the protein sequences up to 512 tokens.\n\nThe details of the masking procedure for each sequence are as follows:\n- 15% of the amino acids are masked.\n- In 90% of the cases, the masked amino acids are replaced by `[MASK]` token.\n- In 10% of the cases, the masked amino acids are replaced by a random amino acid (different) from the one they replace.\n\n### Pretraining\n\nThe model was trained on a single TPU Pod V2-256 for 991.5 thousand steps in total, using sequence length 512 (batch size 2k).\nIt was trained using ProtT5-XL-BFD model as an initial checkpoint, rather than training from scratch.  \nIt has a total of approximately 3B parameters and was trained using the encoder-decoder architecture.\nThe optimizer used is AdaFactor with inverse square root learning rate schedule for pre-training.\n\n\n## Evaluation results\n\nWhen the model is used for feature extraction, this model achieves the following results:\n\nTest results :\n\n secondary structure (3-states)   Localization \n:-----::-----:\n 81     \n 87      \n 86     \n     81 \n\n### BibTeX entry and citation info\n\n```bibtex\n@article {Elnaggar2020.07.12.199554,\n\tauthor = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n\ttitle = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n\telocation-id = {2020.07.12.199554},\n\tyear = {2020},\n\tdoi = {10.1101/2020.07.12.199554},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tabstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22- and 112 times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8 states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs. Availability ProtTrans: \\&lt;a href=\" Interest StatementThe authors have declared no competing interest.},\n\tURL = {\n\teprint = {\n\tjournal = {bioRxiv}\n}\n```\n\n> Created by [Ahmed Elnaggar/@Elnaggar_AI]( | [LinkedIn](\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rostlab/prot_t5_xl_uniref50?", "answers": [{"text": "t5", "answer_start": 408, "answer_end": 409}]}]}]}, {"title": "Roy029/distilroberta-base-finetuned-wikitext2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: distilroberta-base-finetuned-wikitext2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilroberta-base-finetuned-wikitext2\n\nThis model is a fine-tuned version of [distilroberta-base]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.2005\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    2.2650          |\n 2.0    2.2408          |\n 3.0    2.1696          |\n\n\n### Framework versions\n\n- Transformers 4.12.3\n- Pytorch 1.9.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Roy029/distilroberta-base-finetuned-wikitext2?", "answers": [{"text": "roberta", "answer_start": 82, "answer_end": 88}]}]}]}, {"title": "SetFit/deberta-v3-large__sst2__train-8-6", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: deberta-v3-large__sst2__train-8-6\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# deberta-v3-large__sst2__train-8-6\n\nThis model is a fine-tuned version of [microsoft/deberta-v3-large]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.4331\n- Accuracy: 0.7106\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.7901          \n 2.0    0.9259          \n 3.0    1.0574          \n 4.0    1.1372          \n 5.0    0.6090          \n 6.0    0.4435          \n 7.0    0.2804          \n 8.0    0.2205          \n 9.0    0.1282          \n 10.0   0.0643          \n 11.0   0.0361          \n 12.0   0.0211          \n 13.0   0.0155          \n 14.0   0.0158          \n 15.0   0.0189          \n 16.0   0.0254          \n 17.0   0.0305          \n 18.0   0.0287          \n 19.0   0.0215          \n 20.0   0.0163          \n 21.0   0.0138          \n 22.0   0.0131          \n 23.0   0.0132          \n 24.0   0.0126          \n 25.0   0.0125          \n 26.0   0.0119          \n 27.0   0.0110          \n 28.0   0.0106          \n 29.0   0.0095          \n 30.0   0.0089          \n 31.0   0.0083          \n 32.0   0.0075          \n 33.0   0.0066          \n 34.0   0.0059          \n 35.0   0.0054          \n 36.0   0.0051          \n 37.0   0.0049          \n 38.0   0.0047          \n 39.0   0.0045          \n 40.0   0.0046          \n 41.0   0.0045          \n 42.0   0.0044          \n 43.0   0.0043          \n 44.0   0.0044          \n 45.0   0.0045          \n 46.0   0.0043          \n 47.0   0.0043          \n 48.0   0.0041          \n 49.0   0.0042          \n 50.0   0.0042          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": []}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-0", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-0\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-0\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.2707\n- Accuracy: 0.517\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1095          \n 2.0    1.1086          \n 3.0    1.1165          \n 4.0    1.1377          \n 5.0    1.0126          \n 6.0    0.9298          \n 7.0    0.9555          \n 8.0    0.8543          \n 9.0    0.9876          \n 10.0   0.8383          \n 11.0   0.8056          \n 12.0   0.8915          \n 13.0   0.8722          \n 14.0   1.0064          \n 15.0   1.0479          \n 16.0   1.0723          \n 17.0   1.0758          \n 18.0   1.1236          \n 19.0   1.1480          \n 20.0   1.1651          \n 21.0   1.1832          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-0?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-2\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9210\n- Accuracy: 0.5635\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1051          \n 2.0    1.0794          \n 3.0    1.0664          \n 4.0    1.0729          \n 5.0    1.0175          \n 6.0    0.9624          \n 7.0    0.9924          \n 8.0    1.0136          \n 9.0    1.0683          \n 10.0   1.2376          \n 11.0   1.2537          \n 12.0   1.4387          \n 13.0   1.5702          \n 14.0   1.6795          \n 15.0   1.7228          \n 16.0   1.7892          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-2?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-3", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-3\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0675\n- Accuracy: 0.44\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1346          \n 2.0    1.1120          \n 3.0    1.1002          \n 4.0    1.0838          \n 5.0    1.0935          \n 6.0    1.0867          \n 7.0    1.1145          \n 8.0    1.1278          \n 9.0    1.2801          \n 10.0   1.3296          \n 11.0   1.2913          \n 12.0   1.3692          \n 13.0   1.4642          \n 14.0   1.5568          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-3?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-4", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-4\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-4\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0903\n- Accuracy: 0.4805\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1139          \n 2.0    1.0988          \n 3.0    1.1013          \n 4.0    1.0769          \n 5.0    1.0484          \n 6.0    1.0223          \n 7.0    0.9190          \n 8.0    1.1370          \n 9.0    1.1728          \n 10.0   1.1998          \n 11.0   1.3700          \n 12.0   1.3329          \n 13.0   1.2697          \n 14.0   1.4195          \n 15.0   1.5342          \n 16.0   1.5999          \n 17.0   1.6327          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-4?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-5", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-5\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-5\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9907\n- Accuracy: 0.49\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1287          \n 2.0    1.1136          \n 3.0    1.1200          \n 4.0    1.0771          \n 5.0    0.9733          \n 6.0    1.0626          \n 7.0    1.0787          \n 8.0    1.3183          \n 9.0    1.2204          \n 10.0   1.6892          \n 11.0   1.6967          \n 12.0   1.5436          \n 13.0   1.7447          \n 14.0   1.8999          \n 15.0   1.9004          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-5?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-6", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-6\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-6\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8331\n- Accuracy: 0.625\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1248          \n 2.0    1.1162          \n 3.0    1.1199          \n 4.0    1.0740          \n 5.0    1.0183          \n 6.0    1.0259          \n 7.0    0.8699          \n 8.0    1.0615          \n 9.0    1.0164          \n 10.0   1.0620          \n 11.0   1.1829          \n 12.0   1.2815          \n 13.0   1.2607          \n 14.0   1.3695          \n 15.0   1.4397          \n 16.0   1.4388          \n 17.0   1.4242          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-6?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-7", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-7\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-7\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9011\n- Accuracy: 0.578\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1309          \n 2.0    1.1237          \n 3.0    1.1254          \n 4.0    1.1206          \n 5.0    1.0831          \n 6.0    0.9830          \n 7.0    0.9919          \n 8.0    1.0472          \n 9.0    1.1617          \n 10.0   1.2789          \n 11.0   1.4091          \n 12.0   1.4974          \n 13.0   1.4845          \n 14.0   1.4924          \n 15.0   1.5206          \n 16.0   1.5858          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-7?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-8", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-8\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-8\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0704\n- Accuracy: 0.394\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1286          \n 2.0    1.1157          \n 3.0    1.1412          \n 4.0    1.2053          \n 5.0    1.1466          \n 6.0    1.1783          \n 7.0    1.2992          \n 8.0    1.3483          \n 9.0    1.4533          \n 10.0   1.6292          \n 11.0   1.8381          \n 12.0   2.0781          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-8?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-9", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-16-9\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-16-9\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1121\n- Accuracy: 0.16\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1243          \n 2.0    1.1182          \n 3.0    1.1442          \n 4.0    1.2239          \n 5.0    1.2023          \n 6.0    1.2329          \n 7.0    1.2971          \n 8.0    1.3913          \n 9.0    1.4670          \n 10.0   1.7961          \n 11.0   1.8168          \n 12.0   1.9307          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-9?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-0", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-32-0\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-32-0\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7714\n- Accuracy: 0.705\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0704          \n 2.0    1.0167          \n 3.0    0.9134          \n 4.0    0.8430          \n 5.0    0.8315          \n 6.0    0.8585          \n 7.0    0.9443          \n 8.0    1.1019          \n 9.0    1.1420          \n 10.0   1.2773          \n 11.0   1.2454          \n 12.0   1.2785          \n 13.0   1.3834          \n 14.0   1.4139          \n 15.0   1.4056          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-0?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-1", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-32-1\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-32-1\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0606\n- Accuracy: 0.4745\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1045          \n 2.0    1.1164          \n 3.0    1.1570          \n 4.0    1.2403          \n 5.0    1.3815          \n 6.0    1.8102          \n 7.0    2.1439          \n 8.0    2.4368          \n 9.0    2.5994          \n 10.0   2.7388          \n 11.0   2.8287          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-1?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-32-2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-32-2\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7136\n- Accuracy: 0.679\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0726          \n 2.0    1.0225          \n 3.0    0.9164          \n 4.0    0.8251          \n 5.0    0.8908          \n 6.0    0.6772          \n 7.0    0.7792          \n 8.0    1.0657          \n 9.0    1.2228          \n 10.0   1.1100          \n 11.0   1.1991          \n 12.0   1.2654          \n 13.0   1.2837          \n 14.0   1.2860          \n 15.0   1.3160          \n 16.0   1.3323          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-2?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-3", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-32-3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-32-3\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8286\n- Accuracy: 0.661\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0658          \n 2.0    0.9892          \n 3.0    0.8516          \n 4.0    0.7877          \n 5.0    0.7592          \n 6.0    0.9437          \n 7.0    1.0315          \n 8.0    1.3513          \n 9.0    1.1702          \n 10.0   1.2272          \n 11.0   1.2889          \n 12.0   1.3073          \n 13.0   1.3595          \n 14.0   1.4443          \n 15.0   1.4709          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-3?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-5", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-32-5\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-32-5\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1327\n- Accuracy: 0.57\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0470          \n 2.0    0.9244          \n 3.0    0.8612          \n 4.0    0.6759          \n 5.0    0.7273          \n 6.0    0.6444          \n 7.0    0.7671          \n 8.0    0.7599          \n 9.0    0.8140          \n 10.0   0.7861          \n 11.0   0.8318          \n 12.0   0.8777          \n 13.0   0.8501          \n 14.0   0.8603          \n 15.0   0.8787          \n 16.0   0.8969          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-5?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-6", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-32-6\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-32-6\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0523\n- Accuracy: 0.663\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0696          \n 2.0    1.0047          \n 3.0    0.8358          \n 4.0    0.7641          \n 5.0    0.5931          \n 6.0    0.5570          \n 7.0    0.5017          \n 8.0    0.3115          \n 9.0    0.4353          \n 10.0   0.5461          \n 11.0   0.5045          \n 12.0   0.5014          \n 13.0   0.5070          \n 14.0   0.4681          \n 15.0   0.4701          \n 16.0   0.4862          \n 17.0   0.4742          \n 18.0   0.4652          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-6?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-7", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-32-7\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-32-7\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8210\n- Accuracy: 0.6305\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0655          \n 2.0    0.9927          \n 3.0    0.9117          \n 4.0    0.8058          \n 5.0    0.8393          \n 6.0    0.8438          \n 7.0    1.1901          \n 8.0    1.4429          \n 9.0    1.3648          \n 10.0   1.4768          \n 11.0   1.4830          \n 12.0   1.4936          \n 13.0   1.5649          \n 14.0   1.6306          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-7?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-8", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-32-8\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-32-8\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9191\n- Accuracy: 0.632\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0877          \n 2.0    1.0593          \n 3.0    0.9722          \n 4.0    0.9271          \n 5.0    0.7852          \n 6.0    0.9360          \n 7.0    1.0610          \n 8.0    1.0884          \n 9.0    1.3483          \n 10.0   1.4226          \n 11.0   1.4270          \n 12.0   1.5074          \n 13.0   1.5577          \n 14.0   1.5798          \n 15.0   1.6196          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-32-8?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-8-1", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-8-1\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-8-1\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1013\n- Accuracy: 0.0915\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1363          \n 2.0    1.1803          \n 3.0    1.2162          \n 4.0    1.2619          \n 5.0    1.2929          \n 6.0    1.3010          \n 7.0    1.3011          \n 8.0    1.2931          \n 9.0    1.3274          \n 10.0   1.3259          \n 11.0   1.2800          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-8-1?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SetFit/distilbert-base-uncased__hate_speech_offensive__train-8-2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased__hate_speech_offensive__train-8-2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased__hate_speech_offensive__train-8-2\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1019\n- Accuracy: 0.139\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.1432          \n 2.0    1.1613          \n 3.0    1.1547          \n 4.0    1.1680          \n 5.0    1.1762          \n 6.0    1.1809          \n 7.0    1.1912          \n 8.0    1.2100          \n 9.0    1.2037          \n 10.0   1.2096          \n 11.0   1.2203          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of SetFit/distilbert-base-uncased__hate_speech_offensive__train-8-2?", "answers": [{"text": "distilbert", "answer_start": 96, "answer_end": 105}]}]}]}, {"title": "SilentMyuth/stableben", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# My Awesome Model", "qas": [{"id": "q2", "question": "What is the model task of SilentMyuth/stableben?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "SirBastianXVII/DialoGPT-small-TVD", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# The Vampire Diaries DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of SirBastianXVII/DialoGPT-small-TVD?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Sired/DialoGPT-small-trumpbot", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Trump Insults GPT Bot", "qas": [{"id": "q2", "question": "What is the model task of Sired/DialoGPT-small-trumpbot?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "Siyris/DialoGPT-medium-SIY", "paragraphs": [{"context": "---\nthumbnail: \ntags:\n- conversational\nlicense: mit\n---\n# DialoGPT Trained on a customized various spiritual texts and mixed with various different character personalities.\nThis is an instance of [microsoft/DialoGPT-medium]( trained on the energy complex known as Ra. Some text has been changed from the original with the intention of making it fit our discord server better. I've also trained it on various channeling experiences. I'm testing mixing this dataset with character from popular shows with the intention of creating a more diverse dialogue.\nI built a Discord AI chatbot based on this model for internal use within Siyris, Inc.\nChat with the model:\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"Siyris/DialoGPT-medium-SIY\")\nmodel = AutoModelWithLMHead.from_pretrained(\"Siyris/DialoGPT-medium-SIY\")\n# Let's chat for 4 lines\nfor step in range(4):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature=0.8\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"SIY: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n```", "qas": [{"id": "q2", "question": "What is the model task of Siyris/DialoGPT-medium-SIY?", "answers": [{"text": "conversational", "answer_start": 24, "answer_end": 37}]}]}]}, {"title": "Siyris/SIY", "paragraphs": [{"context": "---\nthumbnail: \ntags:\n- conversational\nlicense: mit\n---\n# DialoGPT Trained on a customized version of The Law of One.\nThis is an instance of [microsoft/DialoGPT-medium]( trained on the energy complex known as Ra. Some text has been changed from the original with the intention of making it fit our discord server better.\nI built a Discord AI chatbot based on this model for internal use within Siyris, Inc.\nChat with the model:\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"Siyris/SIY\")\nmodel = AutoModelWithLMHead.from_pretrained(\"Siyris/SIY\")\n# Let's chat for 4 lines\nfor step in range(4):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature=0.8\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"SIY: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n```", "qas": [{"id": "q2", "question": "What is the model task of Siyris/SIY?", "answers": [{"text": "conversational", "answer_start": 24, "answer_end": 37}]}]}]}, {"title": "StivenLancheros/mBERT-base-Biomedical-NER", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: bert-base-multilingual-cased-finetuned-ner-4\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-multilingual-cased-finetuned-ner-4\n#This model is part of a test for creating multilingual BioMedical NER systems. Not intended for proffesional use now.\n\nThis model is a fine-tuned version of [bert-base-multilingual-cased]( on the CRAFT+BC4CHEMD+BioNLP09 datasets concatenated.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1027\n- Precision: 0.9830\n- Recall: 0.9832\n- F1: 0.9831\n- Accuracy: 0.9799\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0751           0.9795  0.9758   |\n 2.0    0.0753           0.9815  0.9786   |\n 3.0    0.0934           0.9825  0.9796   |\n 4.0    0.1027           0.9832  0.9799   |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of StivenLancheros/mBERT-base-Biomedical-NER?", "answers": [{"text": "bert", "answer_start": 122, "answer_end": 125}]}]}]}, {"title": "StivenLancheros/mBERT-base-cased-NER-CONLL", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- conll2002\n- conll2003\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: mBERT-base-cased-NER-CONLL\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: conll2002\n      type: conll2002\n      args: es\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.8621083924079579\n    - name: Recall\n      type: recall\n      value: 0.8662683823529411\n    - name: F1\n      type: f1\n      value: 0.8641833810888252\n    - name: Accuracy\n      type: accuracy\n      value: 0.9790639230580277\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mBERT-base-cased-NER-CONLL (EN-ES)\n\nThis model is a fine-tuned version of [bert-base-multilingual-cased ]( on the conll2003 and conll2002 datasets. Training was performed separately.\nIt achieves the following results on the evaluation set:\n\nConnll2003:\n- Loss: 0.0585\n- Precision: 0.9489\n- Recall: 0.9541\n- F1: 0.9515\n- Accuracy: 0.9880\n\nConll2002:\n- Loss: 0.1435\n- Precision: 0.8621\n- Recall: 0.8663\n- F1: 0.8642\n- Accuracy: 0.9791\n\n## Model description\n\nIOB tagging Scheme. PER/LOC/MISC/ORG tags\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nConll2002/2003 (ES-EN)\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n\n### Training results\n\nConll2003:\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0741           0.9181  0.9823   |\n 2.0    0.0586           0.9476  0.9870   |\n 3.0    0.0583           0.9510  0.9877   |\n 4.0    0.0585           0.9541  0.9880   |\n\n\nConll2002:\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.1322           0.8267  0.9741   |\n 2.0    0.1158           0.8614  0.9782   |\n 3.0    0.1243           0.8660  0.9783   |\n 4.0    0.1435           0.8663  0.9791   |\n\n\n### Framework versions\n\n- Transformers 4.12.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of StivenLancheros/mBERT-base-cased-NER-CONLL?", "answers": [{"text": "bert", "answer_start": 907, "answer_end": 910}]}, {"id": "q2", "question": "What is the model task of StivenLancheros/mBERT-base-cased-NER-CONLL?", "answers": [{"text": "token-classification", "answer_start": 249, "answer_end": 268}]}]}]}, {"title": "StivenLancheros/roberta-base-biomedical-clinical-es-finetuned-ner-BioNLP13", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: roberta-base-biomedical-clinical-es-finetuned-ner-BioNLP13\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-base-biomedical-clinical-es-finetuned-ner-BioNLP13\n\nThis model is a fine-tuned version of [PlanTL-GOB-ES/roberta-base-biomedical-clinical-es]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2217\n- Precision: 0.7936\n- Recall: 0.8067\n- F1: 0.8001\n- Accuracy: 0.9451\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.2182           0.7757  0.9342   |\n 2.0    0.2032           0.7865  0.9398   |\n 3.0    0.2043           0.7904  0.9443   |\n 4.0    0.2217           0.8067  0.9451   |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of StivenLancheros/roberta-base-biomedical-clinical-es-finetuned-ner-BioNLP13?", "answers": [{"text": "roberta", "answer_start": 122, "answer_end": 128}]}]}]}, {"title": "Theivaprakasham/bert-base-cased-twitter_sentiment", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: bert-base-cased-twitter_sentiment\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-cased-twitter_sentiment\n\nThis model is a fine-tuned version of [bert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6907\n- Accuracy: 0.7132\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-06\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.8592          \n 2.0    0.7600          \n 3.0    0.7170          \n 4.0    0.7018          \n 5.0    0.6926          \n 6.0    0.6910          \n 7.0    0.6902          \n 8.0    0.6910          \n 9.0    0.6925          \n 10.0   0.6907          \n\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Theivaprakasham/bert-base-cased-twitter_sentiment?", "answers": [{"text": "bert", "answer_start": 96, "answer_end": 99}]}]}]}, {"title": "Yehor/wav2vec2-xls-r-1b-uk-with-lm", "paragraphs": [{"context": "---\nlanguage:\n- uk\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- generated_from_trainer\n- hf-asr-leaderboard\n- mozilla-foundation/common_voice_7_0\n- robust-speech-event\n- uk\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmodel-index:\n- name: wav2vec2-xls-r-1b-uk-with-lm\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 7\n      type: mozilla-foundation/common_voice_7_0\n      args: uk\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 14.62\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: uk\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 48.72\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Test Data\n      type: speech-recognition-community-v2/eval_data\n      args: uk\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 40.66\n---\n\n# Ukrainian STT model (with Language Model)\n\n\ud83c\uddfa\ud83c\udde6 Join Ukrainian Speech Recognition Community - \n\n\u2b50 See other Ukrainian models - \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-1b]( on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - UK dataset.\n\nIt achieves the following results on the evaluation set without the language model:\n\n- Loss: 0.1875\n- Wer: 0.2033\n- Cer: 0.0384\n\n\n## Model description\n\nOn 100 test example the model shows the following results:\n\nWithout LM:\n\n- WER: 0.1862\n- CER: 0.0277\n\nWith LM:\n\n- WER: 0.1218\n- CER: 0.0190\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 20\n- total_train_batch_size: 160\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 100.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Cer    |\n:-----::---------------::------:|\n 7.93   0.3536           0.1009 |\n 15.86  0.2317           0.0614 |\n 23.8   0.2022           0.0521 |\n 31.74  0.1948           0.0487 |\n 39.67  0.1916           0.0464 |\n 47.61  0.1903           0.0439 |\n 55.55  0.1786           0.0423 |\n 63.49  0.1849           0.0416 |\n 71.42  0.1869           0.0413 |\n 79.36  0.1855           0.0394 |\n 87.3   0.1884           0.0389 |\n 95.24  0.1877           0.0387 |\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.18.1.dev0\n- Tokenizers 0.11.0\n\n#### Evaluation Commands\n\n1. To evaluate on `mozilla-foundation/common_voice_7_0` with split `test`\n\n```bash\npython eval.py --model_id Yehor/wav2vec2-xls-r-1b-uk-with-lm --dataset mozilla-foundation/common_voice_7_0 --config uk --split test\n```\n\n### Eval results on Common Voice 7 \"test\" (WER):\n\n With LM (run `./eval.py`) |\n---|\n 14.62 |\n", "qas": [{"id": "q1", "question": "What is the model architecture of Yehor/wav2vec2-xls-r-1b-uk-with-lm?", "answers": [{"text": "wav2vec2", "answer_start": 256, "answer_end": 263}]}, {"id": "q2", "question": "What is the model task of Yehor/wav2vec2-xls-r-1b-uk-with-lm?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}]}]}, {"title": "Yehor/wav2vec2-xls-r-1b-uk-with-news-lm", "paragraphs": [{"context": "---\nlanguage:\n- uk\nlicense: cc-by-nc-sa-4.0\ntags:\n- automatic-speech-recognition\n- mozilla-foundation/common_voice_7_0\n- generated_from_trainer\n- uk\nxdatasets:\n- mozilla-foundation/common_voice_7_0\n---\n\n# Ukrainian STT model (with the Big Language Model formed on News Dataset)\n\n\ud83c\uddfa\ud83c\udde6 Join Ukrainian Speech Recognition Community - \n\n\u2b50 See other Ukrainian models - \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-1b]( on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - UK dataset.\n\nAttribution to the dataset of Language Model:\n\n- Chaplynskyi, D. et al. (2021) lang-uk Ukrainian Ubercorpus [Data set]. \n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 20\n- total_train_batch_size: 160\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 100.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Cer    |\n:-----::---------------::------:|\n 7.93   0.3536           0.1009 |\n 15.86  0.2317           0.0614 |\n 23.8   0.2022           0.0521 |\n 31.74  0.1948           0.0487 |\n 39.67  0.1916           0.0464 |\n 47.61  0.1903           0.0439 |\n 55.55  0.1786           0.0423 |\n 63.49  0.1849           0.0416 |\n 71.42  0.1869           0.0413 |\n 79.36  0.1855           0.0394 |\n 87.3   0.1884           0.0389 |\n 95.24  0.1877           0.0387 |\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.18.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Yehor/wav2vec2-xls-r-1b-uk-with-news-lm?", "answers": [{"text": "wav2vec2", "answer_start": 411, "answer_end": 418}]}, {"id": "q2", "question": "What is the model task of Yehor/wav2vec2-xls-r-1b-uk-with-news-lm?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 52, "answer_end": 79}]}]}]}, {"title": "Yehor/wav2vec2-xls-r-300m-uk-with-lm", "paragraphs": [{"context": "---\nlanguage:\n- uk\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- mozilla-foundation/common_voice_7_0\n- generated_from_trainer\n- uk\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmodel-index:\n- name: wav2vec2-xls-r-300m-uk-with-lm\n  results:\n  - task: \n      name: Automatic Speech Recognition \n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 7\n      type: mozilla-foundation/common_voice_7_0\n      args: uk\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 26.47\n       - name: Test CER\n         type: cer\n         value: 2.90\n---\n\n\n# Ukrainian STT model (with Language Model)\n\n\ud83c\uddfa\ud83c\udde6 Join Ukrainian Speech Recognition Community - \n\n\u2b50 See other Ukrainian models - \n\n- Have a look on an updated 300m model: \n- Have a look on a better model with more parameters: \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - UK dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3015\n- Wer: 0.3377\n- Cer: 0.0708\n\nThe above results present evaluation without the language model.\n\n## Model description\n\nOn 100 test example the model shows the following results:\n\nWithout LM:\n\n- WER: 0.2647\n- CER: 0.0469\n\nWith LM:\n\n- WER: 0.1568\n- CER: 0.0289\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 20\n- total_train_batch_size: 160\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 100.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Cer    |\n:-----::---------------::------:|\n 7.93   2.5514           0.9047 |\n 15.86  0.4065           0.1201 |\n 23.8   0.3474           0.1033 |\n 31.74  0.3617           0.1005 |\n 39.67  0.3182           0.0891 |\n 47.61  0.3166           0.0875 |\n 55.55  0.3116           0.0828 |\n 63.49  0.3137           0.0807 |\n 71.42  0.2992           0.0771 |\n 79.36  0.3015           0.0740 |\n 87.3   0.3004           0.0723 |\n 95.24  0.3016           0.0713 |\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.18.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Yehor/wav2vec2-xls-r-300m-uk-with-lm?", "answers": [{"text": "wav2vec2", "answer_start": 213, "answer_end": 220}]}, {"id": "q2", "question": "What is the model task of Yehor/wav2vec2-xls-r-300m-uk-with-lm?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}]}]}, {"title": "abhi1nandy2/Europarl-roberta-base", "paragraphs": [{"context": "---\nlanguage: \n  - English\ntags:\n- Europarl\n- roberta\ndatasets:\n- Europarl\n\n---\n\n\nRefer to  \n\n## Citation\n\nIf you use this model in your work, please add the following citation -\n```\n@inproceedings{nandy-etal-2021-cs60075,\n    title = \"cs60075{\\_}team2 at {S}em{E}val-2021 Task 1 : Lexical Complexity Prediction using Transformer-based Language Models pre-trained on various text corpora\",\n    author = \"Nandy, Abhilash  and\n      Adak, Sayantan  and\n      Halder, Tanurima  and\n      Pokala, Sai Mahesh\",\n    booktitle = \"Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    doi = \"10.18653/v1/2021.semeval-1.87\",\n    pages = \"678--682\",\n    abstract = \"The main contribution of this paper is to fine-tune transformer-based language models pre-trained on several text corpora, some being general (E.g., Wikipedia, BooksCorpus), some being the corpora from which the CompLex Dataset was extracted, and others being from other specific domains such as Finance, Law, etc. We perform ablation studies on selecting the transformer models and how their individual complexity scores are aggregated to get the resulting complexity scores. Our method achieves a best Pearson Correlation of 0.784 in sub-task 1 (single word) and 0.836 in sub-task 2 (multiple word expressions).\",\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of abhi1nandy2/Europarl-roberta-base?", "answers": [{"text": "roberta", "answer_start": 46, "answer_end": 52}]}]}]}, {"title": "abhisht/DialoGPT-medium-Emilybot", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Emilybot DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of abhisht/DialoGPT-medium-Emilybot?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "abinayam/gpt-2-tamil", "paragraphs": [{"context": "---\n\nlanguage: ta\ndatasets:\n- oscar\n- IndicNLP\nwidget:\n- text: '\u0b92\u0bb0\u0bc1 \u0b8a\u0bb0\u0bbf\u0bb2\u0bc7 \u0b92\u0bb0\u0bc1 \u0b95\u0bbe\u0b95\u0bcd\u0b95\u0bc8\u0b95\u0bcd\u0b95\u0bc1'\n\n---\n# GPT2-Tamil\n\nThis repository is created as part of the Flax/Jax community week by Huggingface. The aim of this project is to pretrain a language model using GPT-2 specifically for Tamil language. \n\n## Setup:\nTo setup the project, run the following command,\n```python\npip install -r requirements.txt\n```\n\n## Model:\nPretrained model on Tamil language using a causal language modeling (CLM) objective.\n \n## Dataset Used:\nThe GTP-2 model is trained on [oscar dataset - ta]( and [IndicNLP dataset - ta](\n\n## Intended uses & limitations:\nYou can use the raw model for next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub]( to look for fine-tuned versions on a task that interests you.\n\n## How to pretrain the model:\nTo perform training, do the following steps,\n\n- Export the model directory (where you want to store the model artifacts like config, tokenizer, etc.)\n```python\n>>> export MODEL_DIR=<model_dir>\n```\n- Create the config.json by running the following command,\n```python\n>>> python src/create_config.py\n```\n- Create the tokenizer by running the following command,\n```python\n>>> python src/train_tokenizer.py\n```\n- Once the config and tokenizer is created, run the following script to start training the flax model\n```python\n>>> python scripts/train_gpt2-oscar-tamil.sh\n```\n\n## How to use:\nTo perform language generation using the model, pipeline can be used directly.\n\n- First convert the flax model to pytorch using the following command,\n```python\npython src/convert_flax_to_pytorch.py\n```\n- Use the following snippet to perform language generation,\n```python\n >>> from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n >>> model_name = 'abinayam/gpt-2-tamil'\n >>> model = AutoModelWithLMHead.from_pretrained(model_name)\n >>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n >>> set_seed(42)\n >>> input_text = \"\u0b92\u0bb0\u0bc1 \u0b8a\u0bb0\u0bbf\u0bb2\u0bc7 \u0b92\u0bb0\u0bc1 \u0b95\u0bbe\u0b95\u0bcd\u0b95\u0bc8\u0b95\u0bcd\u0b95\u0bc1\"\n >>> max_len = 300\n >>> no_seq = 5\n >>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n >>> sequence = generator(input_text, max_length=max_len, num_return_sequences=no_seq)\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of abinayam/gpt-2-tamil?", "answers": [{"text": "gpt2", "answer_start": 1406, "answer_end": 1409}]}, {"id": "q2", "question": "What is the model task of abinayam/gpt-2-tamil?", "answers": [{"text": "text-generation", "answer_start": 2080, "answer_end": 2094}]}]}]}, {"title": "addy88/t5-base-finetuned-sn-to-en", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- itihasa\nmodel-index:\n- name: t5-base-finetuned-sn-to-en\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-finetuned-sn-to-en\n\nThis model is a fine-tuned version of [google/t5-v1_1-base]( on the itihasa dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/t5-base-finetuned-sn-to-en?", "answers": [{"text": "t5", "answer_start": 96, "answer_end": 97}]}]}]}, {"title": "addy88/t5-grammar-correction", "paragraphs": [{"context": "### How to use\nHere is how to use this model in PyTorch:\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"addy88/t5-grammar-correction\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"addy88/t5-grammar-correction\")\n\ninput_ids = tokenizer('grammar: This sentences has has bads grammar.', return_tensors='pt').input_ids\n\noutputs = model.generate(input_ids)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n```", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/t5-grammar-correction?", "answers": [{"text": "t5", "answer_start": 180, "answer_end": 181}]}]}]}, {"title": "addy88/wav2vec-odia-stt", "paragraphs": [{"context": "## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport soundfile as sf\nimport torch\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport argparse\ndef parse_transcription(wav_file):\n    # load pretrained model\n    processor = Wav2Vec2Processor.from_pretrained(\"addy88/wav2vec2-odia-stt\")\n    model = Wav2Vec2ForCTC.from_pretrained(\"addy88/wav2vec2-odia-stt\")\n    # load audio\n    audio_input, sample_rate = sf.read(wav_file)\n    # pad input values and return pt tensor\n    input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n    # INFERENCE\n    # retrieve logits & take argmax\n    logits = model(input_values).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    # transcribe\n    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n    print(transcription)\n```", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/wav2vec-odia-stt?", "answers": [{"text": "wav2vec2", "answer_start": 321, "answer_end": 328}]}, {"id": "q3", "question": "What is the model category of addy88/wav2vec-odia-stt?", "answers": [{"text": "audio", "answer_start": 423, "answer_end": 427}]}]}]}, {"title": "addy88/wav2vec2-assamese-stt", "paragraphs": [{"context": "## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport soundfile as sf\nimport torch\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport argparse\ndef parse_transcription(wav_file):\n    # load pretrained model\n    processor = Wav2Vec2Processor.from_pretrained(\"addy88/addy88/wav2vec2-assamese-stt\")\n    model = Wav2Vec2ForCTC.from_pretrained(\"addy88/addy88/wav2vec2-assamese-stt\")\n    # load audio\n    audio_input, sample_rate = sf.read(wav_file)\n    # pad input values and return pt tensor\n    input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n    # INFERENCE\n    # retrieve logits & take argmax\n    logits = model(input_values).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    # transcribe\n    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n    print(transcription)\n```", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/wav2vec2-assamese-stt?", "answers": [{"text": "wav2vec2", "answer_start": 328, "answer_end": 335}]}, {"id": "q3", "question": "What is the model category of addy88/wav2vec2-assamese-stt?", "answers": [{"text": "audio", "answer_start": 445, "answer_end": 449}]}]}]}, {"title": "addy88/wav2vec2-base-finetuned-ks", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- superb\nmetrics:\n- accuracy\nmodel-index:\n- name: wav2vec2-base-finetuned-ks\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-finetuned-ks\n\nThis model is a fine-tuned version of [facebook/wav2vec2-base]( on the superb dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1339\n- Accuracy: 0.9768\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0087          \n 2.0    0.4266          \n 3.0    0.2037          \n 4.0    0.1444          \n 5.0    0.1339          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.14.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/wav2vec2-base-finetuned-ks?", "answers": [{"text": "wav2vec2", "answer_start": 115, "answer_end": 122}]}]}]}, {"title": "addy88/wav2vec2-sanskrit-stt", "paragraphs": [{"context": "## Usage\n\nThe model can be used directly (without a language model) as follows:\n\n```python\nimport soundfile as sf\nimport torch\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport argparse\n\ndef parse_transcription(wav_file):\n    # load pretrained model\n    processor = Wav2Vec2Processor.from_pretrained(\"addy88/wav2vec2-sanskrit-stt\")\n    model = Wav2Vec2ForCTC.from_pretrained(\"addy88/wav2vec2-sanskrit-stt\")\n\n    # load audio\n    audio_input, sample_rate = sf.read(wav_file)\n\n    # pad input values and return pt tensor\n    input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n\n    # INFERENCE\n    # retrieve logits & take argmax\n    logits = model(input_values).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n\n    # transcribe\n    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n    print(transcription)\n\n```", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/wav2vec2-sanskrit-stt?", "answers": [{"text": "wav2vec2", "answer_start": 324, "answer_end": 331}]}, {"id": "q3", "question": "What is the model category of addy88/wav2vec2-sanskrit-stt?", "answers": [{"text": "audio", "answer_start": 435, "answer_end": 439}]}]}]}, {"title": "addy88/wav2vec2-tamil-stt", "paragraphs": [{"context": "## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport soundfile as sf\nimport torch\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport argparse\ndef parse_transcription(wav_file):\n    # load pretrained model\n    processor = Wav2Vec2Processor.from_pretrained(\"addy88/wav2vec2-tamil-stt\")\n    model = Wav2Vec2ForCTC.from_pretrained(\"addy88/wav2vec2-tamil-stt\")\n    # load audio\n    audio_input, sample_rate = sf.read(wav_file)\n    # pad input values and return pt tensor\n    input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n    # INFERENCE\n    # retrieve logits & take argmax\n    logits = model(input_values).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    # transcribe\n    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n    print(transcription)\n```", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/wav2vec2-tamil-stt?", "answers": [{"text": "wav2vec2", "answer_start": 321, "answer_end": 328}]}, {"id": "q3", "question": "What is the model category of addy88/wav2vec2-tamil-stt?", "answers": [{"text": "audio", "answer_start": 425, "answer_end": 429}]}]}]}, {"title": "addy88/wav2vec2-telugu-stt", "paragraphs": [{"context": "## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport soundfile as sf\nimport torch\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport argparse\ndef parse_transcription(wav_file):\n    # load pretrained model\n    processor = Wav2Vec2Processor.from_pretrained(\"addy88/wav2vec2-telugu-stt\")\n    model = Wav2Vec2ForCTC.from_pretrained(\"addy88/wav2vec2-telugu-stt\")\n    # load audio\n    audio_input, sample_rate = sf.read(wav_file)\n    # pad input values and return pt tensor\n    input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n    # INFERENCE\n    # retrieve logits & take argmax\n    logits = model(input_values).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    # transcribe\n    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n    print(transcription)\n```", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/wav2vec2-telugu-stt?", "answers": [{"text": "wav2vec2", "answer_start": 321, "answer_end": 328}]}, {"id": "q3", "question": "What is the model category of addy88/wav2vec2-telugu-stt?", "answers": [{"text": "audio", "answer_start": 427, "answer_end": 431}]}]}]}, {"title": "addy88/wav2vec2-urdu-stt", "paragraphs": [{"context": "## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport soundfile as sf\nimport torch\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport argparse\ndef parse_transcription(wav_file):\n    # load pretrained model\n    processor = Wav2Vec2Processor.from_pretrained(\"addy88/wav2vec2-urdu-stt\")\n    model = Wav2Vec2ForCTC.from_pretrained(\"addy88/wav2vec2-urdu-stt\")\n    # load audio\n    audio_input, sample_rate = sf.read(wav_file)\n    # pad input values and return pt tensor\n    input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n    # INFERENCE\n    # retrieve logits & take argmax\n    logits = model(input_values).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    # transcribe\n    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n    print(transcription)\n```", "qas": [{"id": "q1", "question": "What is the model architecture of addy88/wav2vec2-urdu-stt?", "answers": [{"text": "wav2vec2", "answer_start": 321, "answer_end": 328}]}, {"id": "q3", "question": "What is the model category of addy88/wav2vec2-urdu-stt?", "answers": [{"text": "audio", "answer_start": 423, "answer_end": 427}]}]}]}, {"title": "adelgasmi/autonlp-kpmg_nlp-18833547", "paragraphs": [{"context": "---\ntags: autonlp\nlanguage: ar\nwidget:\n- text: \"I love AutoNLP \ud83e\udd17\"\ndatasets:\n- adelgasmi/autonlp-data-kpmg_nlp\nco2_eq_emissions: 64.58945483765274\n---\n\n# Model Trained Using AutoNLP\n\n- Problem type: Multi-class Classification\n- Model ID: 18833547\n- CO2 Emissions (in grams): 64.58945483765274\n\n## Validation Metrics\n\n- Loss: 0.14247722923755646\n- Accuracy: 0.9586074193404036\n- Macro F1: 0.9468339778730883\n- Micro F1: 0.9586074193404036\n- Weighted F1: 0.9585551117678807\n- Macro Precision: 0.9445436604001405\n- Micro Precision: 0.9586074193404036\n- Weighted Precision: 0.9591405429662925\n- Macro Recall: 0.9499427161888565\n- Micro Recall: 0.9586074193404036\n- Weighted Recall: 0.9586074193404036\n\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoNLP\"}' \n```\n\nOr Python API:\n\n```\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"adelgasmi/autonlp-kpmg_nlp-18833547\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"adelgasmi/autonlp-kpmg_nlp-18833547\", use_auth_token=True)\n\ninputs = tokenizer(\"I love AutoNLP\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n```", "qas": []}]}, {"title": "adilism/wav2vec2-large-xlsr-kyrgyz", "paragraphs": [{"context": "---\nlanguage: ky\ndatasets:\n- common_voice\nmetrics:\n- wer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: {Wav2Vec2-XLSR-53 Kyrgyz by adilism}\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice ky\n      type: common_voice\n      args: ky\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 34.08\n---\n\n# Wav2Vec2-Large-XLSR-53-Kyrgyz\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( on Kyrgyz using the [Common Voice]( dataset.\n\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\n\nThe model can be used directly (without a language model) as follows:\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\ntest_dataset = load_dataset(\"common_voice\", \"ky\", split=\"test[:2%]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"adilism/wav2vec2-large-xlsr-kyrgyz\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"adilism/wav2vec2-large-xlsr-kyrgyz\")\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n\tspeech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n\tbatch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n\treturn batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n\tlogits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on the Kyrgyz test data of Common Voice:\n\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\ntest_dataset = load_dataset(\"common_voice\", \"ky\", split=\"test\")\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"adilism/wav2vec2-large-xlsr-kyrgyz\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"adilism/wav2vec2-large-xlsr-kyrgyz\")\nmodel.to(\"cuda\")\n\nchars_to_ignore = [\",\", \"?\", \".\", \"!\", \"-\", \";\", \":\", \"\u2014\", \"\u2013\", \"\u201d\"]\nchars_to_ignore_regex = f'[{\"\".join(chars_to_ignore)}]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n\tbatch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n\tspeech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n\tbatch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n\treturn batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n\tinputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n\twith torch.no_grad():\n\t\tlogits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n\n\tpred_ids = torch.argmax(logits, dim=-1)\n\tbatch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n\treturn batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n```\n\n**Test Result**: 34.08 %\n\n## Training\n\nThe Common Voice `train` and `validation` datasets were used for training.\n", "qas": [{"id": "q1", "question": "What is the model architecture of adilism/wav2vec2-large-xlsr-kyrgyz?", "answers": [{"text": "wav2vec2", "answer_start": 525, "answer_end": 532}]}, {"id": "q2", "question": "What is the model task of adilism/wav2vec2-large-xlsr-kyrgyz?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 73, "answer_end": 100}]}, {"id": "q3", "question": "What is the model category of adilism/wav2vec2-large-xlsr-kyrgyz?", "answers": [{"text": "audio", "answer_start": 65, "answer_end": 69}]}]}]}, {"title": "airKlizz/mt5-base-wikinewssum-italian", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- summarization\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: mt5-base-wikinewssum-italian\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mt5-base-wikinewssum-italian\n\nThis model is a fine-tuned version of [google/mt5-base]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 10.5739\n- Rouge1: 2.1728\n- Rouge2: 0.1516\n- Rougel: 2.0846\n- Rougelsum: 2.0515\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 8\n\n### Training results\n\n Epoch  Validation Loss  Rouge2  Rougelsum |\n:-----::---------------::------::---------:|\n 1.0    16.6193          0.3829  2.2161    |\n 2.0    15.8909          0.2799  2.3523    |\n 3.0    15.4843          0.2252  2.1382    |\n 4.0    13.0850          0.1516  2.0859    |\n 5.0    11.7838          0.1516  2.0859    |\n 6.0    11.3207          0.1516  2.1171    |\n 7.0    10.7871          0.1516  1.9838    |\n 8.0    10.5739          0.1516  2.0515    |\n\n\n### Framework versions\n\n- Transformers 4.13.0\n- Pytorch 1.10.1\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of airKlizz/mt5-base-wikinewssum-italian?", "answers": [{"text": "mt5", "answer_start": 109, "answer_end": 111}]}, {"id": "q2", "question": "What is the model task of airKlizz/mt5-base-wikinewssum-italian?", "answers": [{"text": "summarization", "answer_start": 32, "answer_end": 44}]}]}]}, {"title": "airKlizz/t5-base-multi-fr-wiki-news", "paragraphs": [{"context": "---\nlanguage: fr\nlicense: mit\n---\n", "qas": []}]}, {"title": "airKlizz/t5-base-with-title-multi-fr-wiki-news", "paragraphs": [{"context": "---\nlanguage: fr\nlicense: mit\n---\n\n", "qas": []}]}, {"title": "airesearch/bert-base-multilingual-cased-finetune-qa", "paragraphs": [{"context": "---\nwidget:\n- text: \"\u0e2a\u0e27\u0e19\u0e01\u0e38\u0e2b\u0e25\u0e32\u0e1a\u0e40\u0e1b\u0e47\u0e19\u0e42\u0e23\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19\u0e2d\u0e30\u0e44\u0e23\"\n  context: \"\u0e42\u0e23\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19\u0e2a\u0e27\u0e19\u0e01\u0e38\u0e2b\u0e25\u0e32\u0e1a\u0e27\u0e34\u0e17\u0e22\u0e32\u0e25\u0e31\u0e22 (Suankularb Wittayalai School) (\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e22\u0e48\u0e2d : \u0e2a.\u0e01. / S.K.) \u0e40\u0e1b\u0e47\u0e19\u0e42\u0e23\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19\u0e0a\u0e32\u0e22\u0e25\u0e49\u0e27\u0e19 \u0e23\u0e30\u0e14\u0e31\u0e1a\u0e0a\u0e31\u0e49\u0e19\u0e21\u0e31\u0e18\u0e22\u0e21\u0e28\u0e36\u0e01\u0e29\u0e32\u0e02\u0e19\u0e32\u0e14\u0e43\u0e2b\u0e0d\u0e48\u0e1e\u0e34\u0e40\u0e28\u0e29 \u0e2a\u0e31\u0e07\u0e01\u0e31\u0e14\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e40\u0e02\u0e15\u0e1e\u0e37\u0e49\u0e19\u0e17\u0e35\u0e48\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32\u0e21\u0e31\u0e18\u0e22\u0e21\u0e28\u0e36\u0e01\u0e29\u0e32\u0e40\u0e02\u0e15 1 \u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e04\u0e13\u0e30\u0e01\u0e23\u0e23\u0e21\u0e01\u0e32\u0e23\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32\u0e02\u0e31\u0e49\u0e19\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19 (\u0e0a\u0e37\u0e48\u0e2d\u0e40\u0e14\u0e34\u0e21: \u0e01\u0e23\u0e21\u0e2a\u0e32\u0e21\u0e31\u0e0d\u0e28\u0e36\u0e01\u0e29\u0e32) \u0e01\u0e23\u0e30\u0e17\u0e23\u0e27\u0e07\u0e28\u0e36\u0e01\u0e29\u0e32\u0e18\u0e34\u0e01\u0e32\u0e23 \u0e01\u0e48\u0e2d\u0e15\u0e31\u0e49\u0e07\u0e42\u0e14\u0e22 \u0e1e\u0e23\u0e30\u0e1a\u0e32\u0e17\u0e2a\u0e21\u0e40\u0e14\u0e47\u0e08\u0e1e\u0e23\u0e30\u0e08\u0e38\u0e25\u0e08\u0e2d\u0e21\u0e40\u0e01\u0e25\u0e49\u0e32\u0e40\u0e08\u0e49\u0e32\u0e2d\u0e22\u0e39\u0e48\u0e2b\u0e31\u0e27 \u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e2a\u0e16\u0e32\u0e1b\u0e19\u0e32\u0e02\u0e36\u0e49\u0e19\u0e43\u0e19\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48 8 \u0e21\u0e35\u0e19\u0e32\u0e04\u0e21 \u0e1e.\u0e28. 2424 (\u0e02\u0e13\u0e30\u0e19\u0e31\u0e49\u0e19\u0e19\u0e31\u0e1a\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48 1 \u0e40\u0e21\u0e29\u0e32\u0e22\u0e19 \u0e40\u0e1b\u0e47\u0e19\u0e27\u0e31\u0e19\u0e02\u0e36\u0e49\u0e19\u0e1b\u0e35\u0e43\u0e2b\u0e21\u0e48 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e19\u0e31\u0e1a\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e2a\u0e32\u0e01\u0e25\u0e16\u0e37\u0e2d\u0e40\u0e1b\u0e47\u0e19 \u0e1e.\u0e28. 2425) \u0e42\u0e14\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e42\u0e23\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e31\u0e10\u0e1a\u0e32\u0e25\u0e41\u0e2b\u0e48\u0e07\u0e41\u0e23\u0e01\u0e02\u0e2d\u0e07\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22\"\n---\n\n#  bert-base-multilingual-cased\n\nFinetuning `bert-base-multilingual-cased` with the training set of `iapp_wiki_qa_squad`, `thaiqa_squad`, and `nsc_qa` (removed examples which have cosine similarity with validation and test examples over 0.8; contexts of the latter two are trimmed to be around 300 `newmm` words). Benchmarks shared on [wandb]( using validation and test sets of `iapp_wiki_qa_squad`.\nTrained with [thai2transformers](\n\nRun with:\n```\nexport MODEL_NAME=bert-base-multilingual-cased\npython train_question_answering_lm_finetuning.py \\\n  --model_name $MODEL_NAME \\\n  --dataset_name chimera_qa \\\n  --output_dir $MODEL_NAME-finetune-chimera_qa-model \\\n  --log_dir $MODEL_NAME-finetune-chimera_qa-log \\\n  --pad_on_right \\\n  --fp16\n```", "qas": [{"id": "q1", "question": "What is the model architecture of airesearch/bert-base-multilingual-cased-finetune-qa?", "answers": [{"text": "bert", "answer_start": 552, "answer_end": 555}]}]}]}, {"title": "aishanisingh/DiagloGPT-small-michaelscott", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Michael Scott DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of aishanisingh/DiagloGPT-small-michaelscott?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "aishanisingh/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of aishanisingh/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "alaggung/bart-pretrained", "paragraphs": [{"context": "---\nlanguage:\n  - ko\nwidget:\n- text: \"[BOS]\ubb50 \ud574?[SEP][MASK]\ud558\ub2e4\uac00 \uc774\uc81c [MASK]\ub824\uace0[EOS]\"\ninference:\n  parameters:\n    max_length: 64\n---\n\n# BART Pretrained\n\n[2021 \ud6c8\ubbfc\uc815\uc74c \ud55c\uad6d\uc5b4 \uc74c\uc131\u2022\uc790\uc5f0\uc5b4 \uc778\uacf5\uc9c0\ub2a5 \uacbd\uc9c4\ub300\ud68c] \ub300\ud654\uc694\uc57d \ubd80\ubb38 \uc54c\ub77c\uafcd\ub2ec\ub77c\uafcd \ud300\uc758 \ub300\ud654\uc694\uc57d \ud559\uc2b5 \uc0d8\ud50c \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4.\n\n[2021-dialogue-summary-competition]( \ub808\ud3ec\uc9c0\ud1a0\ub9ac\uc758 BART Pretrain \ub2e8\uacc4\ub97c \ud559\uc2b5\ud55c \ubaa8\ub378\uc785\ub2c8\ub2e4.\n\n\ub370\uc774\ud130\ub294 [AIHub \ud55c\uad6d\uc5b4 \ub300\ud654\uc694\uc57d]( \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.", "qas": []}]}, {"title": "alaggung/bart-r3f", "paragraphs": [{"context": "---\nlanguage:\n  - ko\ntags:\n- summarization\nwidget:\n- text: \"[BOS]\ubc25 \u3131?[SEP]\uace0\uace0\uace0\uace0 \ubb50 \uba39\uc744\uae4c?[SEP]\uc5b4\uc81c \uae40\uce58\ucc0c\uac1c \uba39\uc5b4\uc11c \ud55c\uc2dd\ub9d0\uace0 \ub534 \uac70[SEP]\uadf8\ub7fc \ub3c8\uae4c\uc2a4 \uc5b4\ub54c?[SEP]\uc624 \uc88b\ub2e4 1\uc2dc \ud559\uad00 \uc55e\uc73c\ub85c \uc624\uc148[SEP]\u3147\u314b[EOS]\"\ninference:\n  parameters:\n    max_length: 64\n    top_k: 5\n---\n\n# BART R3F\n\n[2021 \ud6c8\ubbfc\uc815\uc74c \ud55c\uad6d\uc5b4 \uc74c\uc131\u2022\uc790\uc5f0\uc5b4 \uc778\uacf5\uc9c0\ub2a5 \uacbd\uc9c4\ub300\ud68c] \ub300\ud654\uc694\uc57d \ubd80\ubb38 \uc54c\ub77c\uafcd\ub2ec\ub77c\uafcd \ud300\uc758 \ub300\ud654\uc694\uc57d \ud559\uc2b5 \uc0d8\ud50c \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4.\n\n[bart-pretrained]( \ubaa8\ub378\uc5d0 [2021-dialogue-summary-competition]( \ub808\ud3ec\uc9c0\ud1a0\ub9ac\uc758 R3F\ub97c \uc801\uc6a9\ud574 \ub300\ud654\uc694\uc57d Task\ub97c \ud559\uc2b5\ud55c \ubaa8\ub378\uc785\ub2c8\ub2e4.\n\n\ub370\uc774\ud130\ub294 [AIHub \ud55c\uad6d\uc5b4 \ub300\ud654\uc694\uc57d]( \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.", "qas": [{"id": "q1", "question": "What is the model architecture of alaggung/bart-r3f?", "answers": [{"text": "bart", "answer_start": 310, "answer_end": 313}]}, {"id": "q2", "question": "What is the model task of alaggung/bart-r3f?", "answers": [{"text": "summarization", "answer_start": 29, "answer_end": 41}]}]}]}, {"title": "alaggung/bart-rl", "paragraphs": [{"context": "---\nlanguage:\n  - ko\ntags:\n- summarization\nwidget:\n- text: \"[BOS]\ubc25 \u3131?[SEP]\uace0\uace0\uace0\uace0 \ubb50 \uba39\uc744\uae4c?[SEP]\uc5b4\uc81c \uae40\uce58\ucc0c\uac1c \uba39\uc5b4\uc11c \ud55c\uc2dd\ub9d0\uace0 \ub534 \uac70[SEP]\uadf8\ub7fc \ub3c8\uae4c\uc2a4 \uc5b4\ub54c?[SEP]\uc624 \uc88b\ub2e4 1\uc2dc \ud559\uad00 \uc55e\uc73c\ub85c \uc624\uc148[SEP]\u3147\u314b[EOS]\"\ninference:\n  parameters:\n    max_length: 64\n    top_k: 5\n---\n\n# BART R3F\n\n[2021 \ud6c8\ubbfc\uc815\uc74c \ud55c\uad6d\uc5b4 \uc74c\uc131\u2022\uc790\uc5f0\uc5b4 \uc778\uacf5\uc9c0\ub2a5 \uacbd\uc9c4\ub300\ud68c] \ub300\ud654\uc694\uc57d \ubd80\ubb38 \uc54c\ub77c\uafcd\ub2ec\ub77c\uafcd \ud300\uc758 \ub300\ud654\uc694\uc57d \ud559\uc2b5 \uc0d8\ud50c \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4.\n\n[bart-r3f]( \ubaa8\ub378\uc5d0 [2021-dialogue-summary-competition]( \ub808\ud3ec\uc9c0\ud1a0\ub9ac\uc758 RL \uae30\ubc95\uc744 \uc801\uc6a9\ud574 \ub300\ud654\uc694\uc57d Task\ub97c \ud559\uc2b5\ud55c \ubaa8\ub378\uc785\ub2c8\ub2e4.\n\n\ub370\uc774\ud130\ub294 [AIHub \ud55c\uad6d\uc5b4 \ub300\ud654\uc694\uc57d]( \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n", "qas": [{"id": "q1", "question": "What is the model architecture of alaggung/bart-rl?", "answers": [{"text": "bart", "answer_start": 310, "answer_end": 313}]}, {"id": "q2", "question": "What is the model task of alaggung/bart-rl?", "answers": [{"text": "summarization", "answer_start": 29, "answer_end": 41}]}]}]}, {"title": "alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli", "paragraphs": [{"context": "---\nlanguage: \n- multilingual\n- en\n- fr\n- es\n- de\n- el\n- bg\n- ru\n- tr\n- ar\n- vi\n- th\n- zh\n- hi\n- sw\n- ur\ntags:\n- pytorch\nlicense: apache-2.0\ndatasets:\n- multi_nli\n- xnli\nmetrics:\n- xnli\n\n---\n\n# mt5-large-finetuned-mnli-xtreme-xnli\n\n## Model Description\n\n\nThis model takes a pretrained large [multilingual-t5]( (also available from [models]( and fine-tunes it on English MNLI and the [xtreme_xnli]( training set. It is intended to be used for zero-shot text classification, inspired by [xlm-roberta-large-xnli](\n\n## Intended Use\n\nThis model is intended to be used for zero-shot text classification, especially in languages other than English. It is fine-tuned on English MNLI and the [xtreme_xnli]( training set, a multilingual NLI dataset. The model can therefore be used with any of the languages in the XNLI corpus:\n\n- Arabic\n- Bulgarian\n- Chinese\n- English\n- French\n- German\n- Greek\n- Hindi\n- Russian\n- Spanish\n- Swahili\n- Thai\n- Turkish\n- Urdu\n- Vietnamese\n\n\nAs per recommendations in [xlm-roberta-large-xnli]( for English-only classification, you might want to check out:\n- [bart-large-mnli](\n- [a distilled bart MNLI model](\n\n\n### Zero-shot example:\n\nThe model retains its text-to-text characteristic after fine-tuning. This means that our expected outputs will be text. During fine-tuning, the model learns to respond to the NLI task with a series of single token responses that map to entailment, neutral, or contradiction. The NLI task is indicated with a fixed prefix, \"xnli:\". \n\nBelow is an example, using PyTorch, of the model's use in a similar fashion to the `zero-shot-classification` pipeline. We use the logits from the LM output at the first token to represent confidence.\n\n```python\nfrom torch.nn.functional import softmax\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer\n\nmodel_name = \"alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli\"\n\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\nmodel.eval()\n\nsequence_to_classify = \"\u00bfA qui\u00e9n vas a votar en 2020?\"\ncandidate_labels = [\"Europa\", \"salud p\u00fablica\", \"pol\u00edtica\"]\nhypothesis_template = \"Este ejemplo es {}.\"\n\nENTAILS_LABEL = \"\u25810\"\nNEUTRAL_LABEL = \"\u25811\"\nCONTRADICTS_LABEL = \"\u25812\"\n\nlabel_inds = tokenizer.convert_tokens_to_ids(\n    [ENTAILS_LABEL, NEUTRAL_LABEL, CONTRADICTS_LABEL])\n\n\ndef process_nli(premise: str, hypothesis: str):\n    \"\"\" process to required xnli format with task prefix \"\"\"\n    return \"\".join(['xnli: premise: ', premise, ' hypothesis: ', hypothesis])\n\n\n# construct sequence of premise, hypothesis pairs\npairs = [(sequence_to_classify, hypothesis_template.format(label)) for label in\n        candidate_labels]\n# format for mt5 xnli task\nseqs = [process_nli(premise=premise, hypothesis=hypothesis) for\n        premise, hypothesis in pairs]\nprint(seqs)\n# ['xnli: premise: \u00bfA qui\u00e9n vas a votar en 2020? hypothesis: Este ejemplo es Europa.',\n# 'xnli: premise: \u00bfA qui\u00e9n vas a votar en 2020? hypothesis: Este ejemplo es salud p\u00fablica.',\n# 'xnli: premise: \u00bfA qui\u00e9n vas a votar en 2020? hypothesis: Este ejemplo es pol\u00edtica.']\n\ninputs = tokenizer.batch_encode_plus(seqs, return_tensors=\"pt\", padding=True)\n\nout = model.generate(**inputs, output_scores=True, return_dict_in_generate=True,\n                     num_beams=1)\n\n# sanity check that our sequences are expected length (1 + start token + end token = 3)\nfor i, seq in enumerate(out.sequences):\n    assert len(\n        seq) == 3, f\"generated sequence {i} not of expected length, 3.\" \\\\\\\\\n                   f\" Actual length: {len(seq)}\"\n\n# get the scores for our only token of interest\n# we'll now treat these like the output logits of a `*ForSequenceClassification` model\nscores = out.scores[0]\n\n# scores has a size of the model's vocab.\n# However, for this task we have a fixed set of labels\n# sanity check that these labels are always the top 3 scoring\nfor i, sequence_scores in enumerate(scores):\n    top_scores = sequence_scores.argsort()[-3:]\n    assert set(top_scores.tolist()) == set(label_inds), \\\\\\\\\n        f\"top scoring tokens are not expected for this task.\" \\\\\\\\\n        f\" Expected: {label_inds}. Got: {top_scores.tolist()}.\"\n\n# cut down scores to our task labels\nscores = scores[:, label_inds]\nprint(scores)\n# tensor([[-2.5697,  1.0618,  0.2088],\n#         [-5.4492, -2.1805, -0.1473],\n#         [ 2.2973,  3.7595, -0.1769]])\n\n\n# new indices of entailment and contradiction in scores\nentailment_ind = 0\ncontradiction_ind = 2\n\n# we can show, per item, the entailment vs contradiction probas\nentail_vs_contra_scores = scores[:, [entailment_ind, contradiction_ind]]\nentail_vs_contra_probas = softmax(entail_vs_contra_scores, dim=1)\nprint(entail_vs_contra_probas)\n# tensor([[0.0585, 0.9415],\n#         [0.0050, 0.9950],\n#         [0.9223, 0.0777]])\n\n\n# or we can show probas similar to `ZeroShotClassificationPipeline`\n# this gives a zero-shot classification style output across labels\nentail_scores = scores[:, entailment_ind]\nentail_probas = softmax(entail_scores, dim=0)\nprint(entail_probas)\n# tensor([7.6341e-03, 4.2873e-04, 9.9194e-01])\n\nprint(dict(zip(candidate_labels, entail_probas.tolist())))\n# {'Europa': 0.007634134963154793,\n# 'salud p\u00fablica': 0.0004287279152777046,\n# 'pol\u00edtica': 0.9919371604919434}\n\n```\n\nUnfortunately, the `generate` function for the TF equivalent model doesn't exactly mirror the PyTorch version so the above code won't directly transfer.\n\nThe model is currently not compatible with the existing `zero-shot-classification` pipeline.\n\n\n## Training\n\nThis model was pre-trained on a set of 101 languages in the mC4, as described in [the mt5 paper]( It was then fine-tuned on the [mt5_xnli_translate_train]( task for 8k steps in a similar manner to that described in the [offical repo]( with guidance from [Stephen Mayhew's notebook]( The resulting model was then converted to :hugging_face: format.\n\n\n## Eval results\n\nAccuracy over XNLI test set:\n\n bg    el    es    hi    sw    tr    vi    average  |\n------------------------------------------------|\n 85.0  84.3  85.3  79.9  78.0  81.6  81.7  82.4 |\n", "qas": [{"id": "q1", "question": "What is the model architecture of alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli?", "answers": [{"text": "mt5", "answer_start": 194, "answer_end": 196}]}]}]}, {"title": "alexrfelicio/t5-small-finetuned-en-to-de", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- wmt16\nmodel-index:\n- name: t5-small-finetuned-en-to-de\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-finetuned-en-to-de\n\nThis model is a fine-tuned version of [t5-small]( on the wmt16 dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Gen Len |\n:-----::---------------::-------:|\n 1.0    1.7446           17.8356 |\n\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of alexrfelicio/t5-small-finetuned-en-to-de?", "answers": [{"text": "t5", "answer_start": 94, "answer_end": 95}]}]}]}, {"title": "alexrfelicio/t5-small-finetuned128-en-to-de", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- wmt16\nmodel-index:\n- name: t5-small-finetuned128-en-to-de\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-finetuned128-en-to-de\n\nThis model is a fine-tuned version of [t5-small]( on the wmt16 dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of alexrfelicio/t5-small-finetuned128-en-to-de?", "answers": [{"text": "t5", "answer_start": 94, "answer_end": 95}]}]}]}, {"title": "alexrfelicio/t5-small-finetuned300-en-to-de", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- wmt16\nmodel-index:\n- name: t5-small-finetuned300-en-to-de\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-finetuned300-en-to-de\n\nThis model is a fine-tuned version of [t5-small]( on the wmt16 dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Gen Len |\n:-----::---------------::-------:|\n 1.0    1.1454           17.8329 |\n\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of alexrfelicio/t5-small-finetuned300-en-to-de?", "answers": [{"text": "t5", "answer_start": 94, "answer_end": 95}]}]}]}, {"title": "ali2066/correct_twitter_RoBERTa_token_itr0_1e-05_all_01_03_2022-15_36_04", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: correct_twitter_RoBERTa_token_itr0_1e-05_all_01_03_2022-15_36_04\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# correct_twitter_RoBERTa_token_itr0_1e-05_all_01_03_2022-15_36_04\n\nThis model is a fine-tuned version of [cardiffnlp/twitter-roberta-base]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2876\n- Precision: 0.2345\n- Recall: 0.4281\n- F1: 0.3030\n- Accuracy: 0.8728\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.3907           0.0824  0.7626   |\n 2.0    0.3046           0.4095  0.8598   |\n 3.0    0.2945           0.4095  0.8668   |\n 4.0    0.2687           0.4607  0.8761   |\n 5.0    0.2643           0.4444  0.8788   |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ali2066/correct_twitter_RoBERTa_token_itr0_1e-05_all_01_03_2022-15_36_04?", "answers": [{"text": "roberta", "answer_start": 500, "answer_end": 506}]}]}]}, {"title": "ali2066/correct_twitter_RoBERTa_token_itr0_1e-05_webDiscourse_01_03_2022-15_30_39", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: correct_twitter_RoBERTa_token_itr0_1e-05_webDiscourse_01_03_2022-15_30_39\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# correct_twitter_RoBERTa_token_itr0_1e-05_webDiscourse_01_03_2022-15_30_39\n\nThis model is a fine-tuned version of [cardiffnlp/twitter-roberta-base]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6169\n- Precision: 0.0031\n- Recall: 0.0357\n- F1: 0.0057\n- Accuracy: 0.6464\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.6339           0.0120  0.6662   |\n 2.0    0.6182           0.0120  0.6688   |\n 3.0    0.6139           0.0241  0.6659   |\n 4.0    0.6172           0.0241  0.6622   |\n 5.0    0.6165           0.0241  0.6599   |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ali2066/correct_twitter_RoBERTa_token_itr0_1e-05_webDiscourse_01_03_2022-15_30_39?", "answers": [{"text": "roberta", "answer_start": 518, "answer_end": 524}]}]}]}, {"title": "ali2066/distilBERT_token_itr0_0.0001_all_01_03_2022-15_22_12", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilBERT_token_itr0_0.0001_all_01_03_2022-15_22_12\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilBERT_token_itr0_0.0001_all_01_03_2022-15_22_12\n\nThis model is a fine-tuned version of [bert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2811\n- Precision: 0.3231\n- Recall: 0.5151\n- F1: 0.3971\n- Accuracy: 0.8913\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.2881           0.3621  0.8715   |\n 2.0    0.2500           0.3842  0.8845   |\n 3.0    0.2571           0.4338  0.8809   |\n 4.0    0.2479           0.4761  0.8949   |\n 5.0    0.2783           0.4761  0.8936   |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ali2066/distilBERT_token_itr0_0.0001_all_01_03_2022-15_22_12?", "answers": [{"text": "bert", "answer_start": 477, "answer_end": 480}]}]}]}, {"title": "ali2066/distilBERT_token_itr0_0.0001_editorials_01_03_2022-15_20_12", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilBERT_token_itr0_0.0001_editorials_01_03_2022-15_20_12\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilBERT_token_itr0_0.0001_editorials_01_03_2022-15_20_12\n\nThis model is a fine-tuned version of [bert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1290\n- Precision: 0.0637\n- Recall: 0.0080\n- F1: 0.0141\n- Accuracy: 0.9707\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0733           0.0055  0.9861   |\n 2.0    0.0732           0.0055  0.9861   |\n 3.0    0.0731           0.0055  0.9861   |\n 4.0    0.0716           0.0055  0.9861   |\n 5.0    0.0635           0.0055  0.9861   |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ali2066/distilBERT_token_itr0_0.0001_editorials_01_03_2022-15_20_12?", "answers": [{"text": "bert", "answer_start": 491, "answer_end": 494}]}]}]}, {"title": "ali2066/distilBERT_token_itr0_0.0001_essays_01_03_2022-15_18_35", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilBERT_token_itr0_0.0001_essays_01_03_2022-15_18_35\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilBERT_token_itr0_0.0001_essays_01_03_2022-15_18_35\n\nThis model is a fine-tuned version of [bert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1832\n- Precision: 0.6138\n- Recall: 0.7169\n- F1: 0.6613\n- Accuracy: 0.9332\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.2740           0.5460  0.8943   |\n 2.0    0.2189           0.6558  0.9193   |\n 3.0    0.2039           0.6706  0.9198   |\n 4.0    0.2097           0.6795  0.9237   |\n 5.0    0.2255           0.6825  0.9223   |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ali2066/distilBERT_token_itr0_0.0001_essays_01_03_2022-15_18_35?", "answers": [{"text": "bert", "answer_start": 483, "answer_end": 486}]}]}]}, {"title": "ali2066/distilBERT_token_itr0_0.0001_webDiscourse_01_03_2022-15_16_57", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilBERT_token_itr0_0.0001_webDiscourse_01_03_2022-15_16_57\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilBERT_token_itr0_0.0001_webDiscourse_01_03_2022-15_16_57\n\nThis model is a fine-tuned version of [bert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5923\n- Precision: 0.0039\n- Recall: 0.0212\n- F1: 0.0066\n- Accuracy: 0.7084\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.6673           0.0128  0.6652   |\n 2.0    0.6211           0.0     0.6707   |\n 3.0    0.6880           0.0128  0.6703   |\n 4.0    0.6566           0.0128  0.6690   |\n 5.0    0.6036           0.0     0.6868   |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ali2066/distilBERT_token_itr0_0.0001_webDiscourse_01_03_2022-15_16_57?", "answers": [{"text": "bert", "answer_start": 495, "answer_end": 498}]}]}]}, {"title": "ali2066/distilBERT_token_itr0_1e-05_all_01_03_2022-15_14_04", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilBERT_token_itr0_1e-05_all_01_03_2022-15_14_04\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilBERT_token_itr0_1e-05_all_01_03_2022-15_14_04\n\nThis model is a fine-tuned version of [distilbert-base-uncased-finetuned-sst-2-english]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3121\n- Precision: 0.1204\n- Recall: 0.2430\n- F1: 0.1611\n- Accuracy: 0.8538\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.4480           0.0223  0.7794   |\n 2.0    0.3521           0.1218  0.8267   |\n 3.0    0.3177           0.2504  0.8487   |\n 4.0    0.3009           0.2607  0.8602   |\n 5.0    0.2988           0.2693  0.8599   |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ali2066/distilBERT_token_itr0_1e-05_all_01_03_2022-15_14_04?", "answers": [{"text": "distilbert", "answer_start": 475, "answer_end": 484}]}]}]}, {"title": "ali2066/finetuned_sentence_itr4_2e-05_all_26_02_2022-04_20_09", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: finetuned_sentence_itr4_2e-05_all_26_02_2022-04_20_09\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# finetuned_sentence_itr4_2e-05_all_26_02_2022-04_20_09\n\nThis model is a fine-tuned version of [distilbert-base-uncased-finetuned-sst-2-english]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4676\n- Accuracy: 0.8299\n- F1: 0.8892\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.4087           0.8754 |\n 2.0    0.3952           0.8803 |\n 3.0    0.4183           0.8831 |\n 4.0    0.4596           0.8867 |\n 5.0    0.4919           0.8873 |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ali2066/finetuned_sentence_itr4_2e-05_all_26_02_2022-04_20_09?", "answers": [{"text": "distilbert", "answer_start": 458, "answer_end": 467}]}]}]}, {"title": "alireza7/TRANSFORMER-persian-base-PN-summary", "paragraphs": [{"context": "More information about models is available [here](", "qas": []}]}, {"title": "alireza7/TRANSFORMER-persian-base-perkey-summary", "paragraphs": [{"context": "More information about models is available [here](", "qas": []}]}, {"title": "alireza7/TRANSFORMER-persian-base-perkey-title", "paragraphs": [{"context": "More information about models is available [here](", "qas": []}]}, {"title": "anas-awadalla/roberta-base-few-shot-k-256-finetuned-squad-seed-8", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: roberta-base-few-shot-k-256-finetuned-squad-seed-8\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-base-few-shot-k-256-finetuned-squad-seed-8\n\nThis model is a fine-tuned version of [roberta-base]( on the squad dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 10.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of anas-awadalla/roberta-base-few-shot-k-256-finetuned-squad-seed-8?", "answers": [{"text": "roberta", "answer_start": 87, "answer_end": 93}]}]}]}, {"title": "anas-awadalla/roberta-base-few-shot-k-64-finetuned-squad-seed-4", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: roberta-base-few-shot-k-64-finetuned-squad-seed-4\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-base-few-shot-k-64-finetuned-squad-seed-4\n\nThis model is a fine-tuned version of [roberta-base]( on the squad dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- training_steps: 200\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of anas-awadalla/roberta-base-few-shot-k-64-finetuned-squad-seed-4?", "answers": [{"text": "roberta", "answer_start": 87, "answer_end": 93}]}]}]}, {"title": "anas-awadalla/spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-10", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-10\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-10\n\nThis model is a fine-tuned version of [SpanBERT/spanbert-base-cased]( on the squad dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- training_steps: 200\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of anas-awadalla/spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-10?", "answers": [{"text": "bert", "answer_start": 78, "answer_end": 81}]}]}]}, {"title": "anas-awadalla/spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-4", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-4\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-4\n\nThis model is a fine-tuned version of [SpanBERT/spanbert-base-cased]( on the squad dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- training_steps: 200\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of anas-awadalla/spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-4?", "answers": [{"text": "bert", "answer_start": 78, "answer_end": 81}]}]}]}, {"title": "anas-awadalla/spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-6", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-6\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-6\n\nThis model is a fine-tuned version of [SpanBERT/spanbert-base-cased]( on the squad dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- training_steps: 200\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of anas-awadalla/spanbert-base-cased-few-shot-k-128-finetuned-squad-seed-6?", "answers": [{"text": "bert", "answer_start": 78, "answer_end": 81}]}]}]}, {"title": "anas-awadalla/spanbert-base-cased-few-shot-k-16-finetuned-squad-seed-42", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: spanbert-base-cased-few-shot-k-16-finetuned-squad-seed-42\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# spanbert-base-cased-few-shot-k-16-finetuned-squad-seed-42\n\nThis model is a fine-tuned version of [SpanBERT/spanbert-base-cased]( on the squad dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- training_steps: 200\n\n### Training results\n\n{'exact_match': 4.541154210028382, 'f1': 10.04181288563879}\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of anas-awadalla/spanbert-base-cased-few-shot-k-16-finetuned-squad-seed-42?", "answers": [{"text": "bert", "answer_start": 78, "answer_end": 81}]}]}]}, {"title": "anirudh21/albert-base-v2-finetuned-wnli", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- accuracy\nmodel-index:\n- name: albert-base-v2-finetuned-wnli\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: wnli\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.5633802816901409\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# albert-base-v2-finetuned-wnli\n\nThis model is a fine-tuned version of [albert-base-v2]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6878\n- Accuracy: 0.5634\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.6878          \n 2.0    0.6919          \n 3.0    0.6877          \n 4.0    0.6984          \n 5.0    0.6957          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of anirudh21/albert-base-v2-finetuned-wnli?", "answers": [{"text": "albert", "answer_start": 113, "answer_end": 118}]}, {"id": "q2", "question": "What is the model task of anirudh21/albert-base-v2-finetuned-wnli?", "answers": [{"text": "text-classification", "answer_start": 208, "answer_end": 226}]}]}]}, {"title": "anirudh21/albert-large-v2-finetuned-rte", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- accuracy\nmodel-index:\n- name: albert-large-v2-finetuned-rte\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: rte\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.5487364620938628\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# albert-large-v2-finetuned-rte\n\nThis model is a fine-tuned version of [albert-large-v2]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6827\n- Accuracy: 0.5487\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.6954          \n 2.0    0.6860          \n 3.0    0.6827          \n 4.0    0.7179          \n 5.0    0.7504          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of anirudh21/albert-large-v2-finetuned-rte?", "answers": [{"text": "albert", "answer_start": 113, "answer_end": 118}]}, {"id": "q2", "question": "What is the model task of anirudh21/albert-large-v2-finetuned-rte?", "answers": [{"text": "text-classification", "answer_start": 208, "answer_end": 226}]}]}]}, {"title": "anukaver/xlm-roberta-est-qa", "paragraphs": [{"context": "---\ntags: \n- question-answering\ndatasets:\n- squad\n- anukaver/EstQA\n---\n\n# Question answering model for Estonian\nThis is a question answering model based on XLM-Roberta base model. It is fine-tuned subsequentially on:\n1. English SQuAD v1.1\n2. SQuAD v1.1 translated into Estonian\n3. Small native Estonian dataset (800 samples)\n\nThe model has retained good multilingual properties and can be used for extractive QA tasks in all languages included in XLM-Roberta. The performance is best in the fine-tuning languages of Estonian and English.\n\n F1 \n --- \n 82.4 \n 86.9 \n\nThe Estonian dataset used for fine-tuning and validating results is available in  (version 1.0)", "qas": [{"id": "q2", "question": "What is the model task of anukaver/xlm-roberta-est-qa?", "answers": [{"text": "question-answering", "answer_start": 13, "answer_end": 30}]}]}]}, {"title": "anuragshas/wav2vec2-large-xls-r-300m-hi", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-large-xls-r-300m-hi\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xls-r-300m-hi\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.4156\n- Wer: 0.7181\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 2.72   2.2274          \n 5.44   1.5812          \n 8.16   2.0590          \n 10.88  2.0324          \n 13.6   2.1396          \n 16.33  2.2090          \n 19.05  2.3907          \n 21.77  2.5294          \n 24.49  2.5024          \n 27.21  2.4715          \n 29.93  2.4156          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of anuragshas/wav2vec2-large-xls-r-300m-hi?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "anuragshas/wav2vec2-large-xls-r-300m-mr", "paragraphs": [{"context": "---\nlanguage:\n- mr\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- robust-speech-event\n- hf-asr-leaderboard\ndatasets:\n- mozilla-foundation/common_voice_8_0\nmetrics:\n- wer\nmodel-index:\n- name: wav2vec2-large-xls-r-300m-mr\n  results:\n  - task:\n      type: automatic-speech-recognition\n      name: Speech Recognition\n    dataset:\n      type: mozilla-foundation/common_voice_8_0\n      name: Common Voice 8\n      args: mr\n    metrics:\n    - type: wer\n      value: 32.811\n      name: Test WER\n    - name: Test CER\n      type: cer\n      value: 7.692\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xls-r-300m-mr\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5479\n- Wer: 0.5740\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 200\n\n### Training results\n\n Epoch   Validation Loss \n:------::---------------:\n 18.18   3.5047          \n 36.36   2.6166          \n 54.55   0.5778          \n 72.73   0.5168          \n 90.91   0.5105          \n 109.09  0.5151          \n 127.27  0.5157          \n 145.45  0.5179          \n 163.64  0.5348          \n 181.82  0.5518          \n 200.0   0.5479          \n\n\n### Framework versions\n\n- Transformers 4.16.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.1\n- Tokenizers 0.11.0\n\n#### Evaluation Commands\n1. To evaluate on `mozilla-foundation/common_voice_8_0` with split `test`\n\n```bash\npython eval.py --model_id anuragshas/wav2vec2-large-xls-r-300m-mr --dataset mozilla-foundation/common_voice_8_0 --config mr --split test\n```\n\n\n### Inference With LM\n\n```python\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCTC, AutoProcessor\nimport torchaudio.functional as F\nmodel_id = \"anuragshas/wav2vec2-large-xls-r-300m-mr\"\nsample_iter = iter(load_dataset(\"mozilla-foundation/common_voice_8_0\", \"mr\", split=\"test\", streaming=True, use_auth_token=True))\nsample = next(sample_iter)\nresampled_audio = F.resample(torch.tensor(sample[\"audio\"][\"array\"]), 48_000, 16_000).numpy()\nmodel = AutoModelForCTC.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\ninput_values = processor(resampled_audio, return_tensors=\"pt\").input_values\nwith torch.no_grad():\n    logits = model(input_values).logits\ntranscription = processor.batch_decode(logits.numpy()).text\n# => \"\u092f\u093e \u092a\u093e\u0928\u093e\u0938 \u0932\u0947\u0916\u093e\u091a\u0947 \u0938\u094d\u0935\u0930\u0942\u092a \u092f\u093e\u092f\u0932\u093e \u0939\u093e\u0935\u0947\"\n```\n\n### Eval results on Common Voice 8 \"test\" (WER):\n\n With LM (run `./eval.py`) |\n---|\n 32.811 |\n", "qas": [{"id": "q1", "question": "What is the model architecture of anuragshas/wav2vec2-large-xls-r-300m-mr?", "answers": [{"text": "wav2vec2", "answer_start": 197, "answer_end": 204}]}, {"id": "q2", "question": "What is the model task of anuragshas/wav2vec2-large-xls-r-300m-mr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 259, "answer_end": 286}]}, {"id": "q3", "question": "What is the model category of anuragshas/wav2vec2-large-xls-r-300m-mr?", "answers": [{"text": "audio", "answer_start": 2381, "answer_end": 2385}]}]}]}, {"title": "anuragshas/wav2vec2-large-xls-r-300m-or", "paragraphs": [{"context": "---\nlanguage:\n- or\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- robust-speech-event\n- hf-asr-leaderboard\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmetrics:\n- wer\nmodel-index:\n- name: wav2vec2-large-xls-r-300m-or\n  results:\n  - task:\n      type: automatic-speech-recognition\n      name: Speech Recognition\n    dataset:\n      type: mozilla-foundation/common_voice_7_0\n      name: Common Voice 7\n      args: or\n    metrics:\n    - type: wer\n      value: 47.186\n      name: Test WER\n    - name: Test CER\n      type: cer\n      value: 11.82\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xls-r-300m-or\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6618\n- Wer: 0.5166\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.12\n- num_epochs: 240\n\n### Training results\n\n Epoch   Validation Loss \n:------::---------------:\n 23.53   2.9728          \n 47.06   1.2895          \n 70.59   1.6854          \n 94.12   1.9433          \n 117.65  1.4393          \n 141.18  1.4665          \n 164.71  1.5441          \n 188.24  1.6502          \n 211.76  1.6411          \n 235.29  1.6618          \n\n\n### Framework versions\n\n- Transformers 4.16.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n\n#### Evaluation Commands\n1. To evaluate on `mozilla-foundation/common_voice_7_0` with split `test`\n\n```bash\npython eval.py --model_id anuragshas/wav2vec2-large-xls-r-300m-or --dataset mozilla-foundation/common_voice_7_0 --config or --split test\n```\n\n\n### Inference With LM\n\n```python\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCTC, AutoProcessor\nimport torchaudio.functional as F\nmodel_id = \"anuragshas/wav2vec2-large-xls-r-300m-or\"\nsample_iter = iter(load_dataset(\"mozilla-foundation/common_voice_7_0\", \"or\", split=\"test\", streaming=True, use_auth_token=True))\nsample = next(sample_iter)\nresampled_audio = F.resample(torch.tensor(sample[\"audio\"][\"array\"]), 48_000, 16_000).numpy()\nmodel = AutoModelForCTC.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\ninput_values = processor(resampled_audio, return_tensors=\"pt\").input_values\nwith torch.no_grad():\n    logits = model(input_values).logits\ntranscription = processor.batch_decode(logits.numpy()).text\n# => \"\u0b2a\u0b30\u0b30\u0b3e\u0b0f \u0b2c\u0b3e\u0b32\u0b3e \u0b17\u0b38\u0b4d\u0b24\u0b3f \u0b2b\u0b3e\u0b23\u0b4d\u0b21\u0b3f \u0b17\u0b4b\u0b2a\u0b3e\u0b33 \u0b2a\u0b30\u0b20\u0b3e\u0b30\u0b41 \u0b26\u0b47\u0b22\u0b3c\u0b15\u0b36 \u0b26\u0b42\u0b30\"\n```\n\n### Eval results on Common Voice 7 \"test\" (WER):\n\n With LM (run `./eval.py`) |\n---|\n 47.186 |\n", "qas": [{"id": "q1", "question": "What is the model architecture of anuragshas/wav2vec2-large-xls-r-300m-or?", "answers": [{"text": "wav2vec2", "answer_start": 203, "answer_end": 210}]}, {"id": "q2", "question": "What is the model task of anuragshas/wav2vec2-large-xls-r-300m-or?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}, {"id": "q3", "question": "What is the model category of anuragshas/wav2vec2-large-xls-r-300m-or?", "answers": [{"text": "audio", "answer_start": 2361, "answer_end": 2365}]}]}]}, {"title": "anuragshas/wav2vec2-large-xls-r-300m-pa-in", "paragraphs": [{"context": "---\nlanguage:\n- pa\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- robust-speech-event\n- hf-asr-leaderboard\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmetrics:\n- wer\nmodel-index:\n- name: XLS-R-300M - Punjabi\n  results:\n  - task:\n      type: automatic-speech-recognition\n      name: Speech Recognition\n    dataset:\n      type: mozilla-foundation/common_voice_7_0\n      name: Common Voice 7\n      args: pa-IN\n    metrics:\n    - type: wer\n      value: 45.611\n      name: Test WER\n    - name: Test CER\n      type: cer\n      value: 15.584\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# XLS-R-300M - Punjabi\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.2548\n- Wer: 0.5677\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.12\n- num_epochs: 120\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch   Validation Loss \n:------::---------------:\n 16.65   1.8461          \n 33.33   1.1018          \n 49.98   1.1918          \n 66.65   1.1889          \n 83.33   1.2266          \n 99.98   1.2512          \n 116.65  1.2548          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n\n\n#### Evaluation Commands\n1. To evaluate on `mozilla-foundation/common_voice_7_0` with split `test`\n\n```bash\npython eval.py --model_id anuragshas/wav2vec2-large-xls-r-300m-pa-in --dataset mozilla-foundation/common_voice_7_0 --config pa-IN --split test\n```\n\n\n### Inference With LM\n\n```python\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCTC, AutoProcessor\nimport torchaudio.functional as F\nmodel_id = \"anuragshas/wav2vec2-large-xls-r-300m-pa-in\"\nsample_iter = iter(load_dataset(\"mozilla-foundation/common_voice_7_0\", \"pa-IN\", split=\"test\", streaming=True, use_auth_token=True))\nsample = next(sample_iter)\nresampled_audio = F.resample(torch.tensor(sample[\"audio\"][\"array\"]), 48_000, 16_000).numpy()\nmodel = AutoModelForCTC.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\ninput_values = processor(resampled_audio, return_tensors=\"pt\").input_values\nwith torch.no_grad():\n    logits = model(input_values).logits\ntranscription = processor.batch_decode(logits.numpy()).text\n# => \"\u0a09\u0a28\u0a4d\u0a39\u0a3e\u0a02 \u0a28\u0a47 \u0a38\u0a3e\u0a30\u0a47 \u0a24\u0a47\u0a05\u0a30\u0a35\u0a47 \u0a35\u0a71\u0a16\u0a30\u0a40 \u0a15\u0a3f\u0a38\u0a2e \u0a26\u0a47 \u0a15\u0a40\u0a24\u0a47 \u0a39\u0a28\"\n```\n\n### Eval results on Common Voice 7 \"test\" (WER):\n\n With LM (run `./eval.py`) |\n---|\n 45.611 |\n", "qas": [{"id": "q1", "question": "What is the model architecture of anuragshas/wav2vec2-large-xls-r-300m-pa-in?", "answers": [{"text": "wav2vec2", "answer_start": 809, "answer_end": 816}]}, {"id": "q2", "question": "What is the model task of anuragshas/wav2vec2-large-xls-r-300m-pa-in?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 251, "answer_end": 278}]}, {"id": "q3", "question": "What is the model category of anuragshas/wav2vec2-large-xls-r-300m-pa-in?", "answers": [{"text": "audio", "answer_start": 2311, "answer_end": 2315}]}]}]}, {"title": "anuragshas/wav2vec2-large-xlsr-53-dv", "paragraphs": [{"context": "---\nlanguage: dv\ndatasets:\n- common_voice \nmetrics:\n- wer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: Anurag Singh XLSR Wav2Vec2 Large 53 Dhivehi\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice dv\n      type: common_voice\n      args: dv\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 55.68\n---\n# Wav2Vec2-Large-XLSR-53-Dhivehi\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( on Dhivehi using the [Common Voice](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n## Usage\nThe model can be used directly (without a language model) as follows:\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\ntest_dataset = load_dataset(\"common_voice\", \"dv\", split=\"test[:2%]\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"anuragshas/wav2vec2-large-xlsr-53-dv\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"anuragshas/wav2vec2-large-xlsr-53-dv\")\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\npredicted_ids = torch.argmax(logits, dim=-1)\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n## Evaluation\nThe model can be evaluated as follows on the Dhivehi test data of Common Voice.\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\ntest_dataset = load_dataset(\"common_voice\", \"dv\", split=\"test\")\nwer = load_metric(\"wer\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"anuragshas/wav2vec2-large-xlsr-53-dv\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"anuragshas/wav2vec2-large-xlsr-53-dv\")\nmodel.to(\"cuda\")\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\\u201d\\\u060c\\.\\\u061f\\\u2013\\'\\\u2019]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n```\n**Test Result**: 55.68 % \n## Training\nThe Common Voice `train` and `validation` datasets were used for training.", "qas": [{"id": "q1", "question": "What is the model architecture of anuragshas/wav2vec2-large-xlsr-53-dv?", "answers": [{"text": "wav2vec2", "answer_start": 532, "answer_end": 539}]}, {"id": "q2", "question": "What is the model task of anuragshas/wav2vec2-large-xlsr-53-dv?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 74, "answer_end": 101}]}, {"id": "q3", "question": "What is the model category of anuragshas/wav2vec2-large-xlsr-53-dv?", "answers": [{"text": "audio", "answer_start": 66, "answer_end": 70}]}]}]}, {"title": "aristotletan/bart-large-finetuned-xsum", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- wsj_markets\nmetrics:\n- rouge\nmodel_index:\n- name: bart-large-finetuned-xsum\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: wsj_markets\n      type: wsj_markets\n      args: default\n    metric:\n      name: Rouge1\n      type: rouge\n      value: 15.3934\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bart-large-finetuned-xsum\n\nThis model is a fine-tuned version of [facebook/bart-large]( on the wsj_markets dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8497\n- Rouge1: 15.3934\n- Rouge2: 7.0378\n- Rougel: 13.9522\n- Rougelsum: 14.3541\n- Gen Len: 20.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Rouge2   Rougelsum \n:-----::---------------::-------::---------:\n 1.0    0.9365           12.7539  18.5397   \n 2.0    0.8871           13.0938  18.8363   \n 3.0    0.8587           7.142    14.5975   \n 4.0    0.8569           11.4495  17.489    \n 5.0    0.8497           7.0378   14.3541   \n\n\n### Framework versions\n\n- Transformers 4.8.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.10.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of aristotletan/bart-large-finetuned-xsum?", "answers": [{"text": "bart", "answer_start": 110, "answer_end": 113}]}, {"id": "q2", "question": "What is the model task of aristotletan/bart-large-finetuned-xsum?", "answers": [{"text": "text2text-generation", "answer_start": 220, "answer_end": 239}]}]}]}, {"title": "armageddon/albert-xxlarge-v2-squad2-covid-qa-deepset", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- covid_qa_deepset\nmodel-index:\n- name: albert-xxlarge-v2-squad2-covid-qa-deepset\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# albert-xxlarge-v2-squad2-covid-qa-deepset\n\nThis model is a fine-tuned version of [mfeb/albert-xxlarge-v2-squad2]( on the covid_qa_deepset dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: tpu\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of armageddon/albert-xxlarge-v2-squad2-covid-qa-deepset?", "answers": [{"text": "albert", "answer_start": 85, "answer_end": 90}]}]}]}, {"title": "armageddon/bert-large-uncased-squad2-covid-qa-deepset", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- covid_qa_deepset\nmodel-index:\n- name: bert-large-uncased-squad2-covid-qa-deepset\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-large-uncased-squad2-covid-qa-deepset\n\nThis model is a fine-tuned version of [phiyodr/bert-large-finetuned-squad2]( on the covid_qa_deepset dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: tpu\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of armageddon/bert-large-uncased-squad2-covid-qa-deepset?", "answers": [{"text": "bert", "answer_start": 85, "answer_end": 88}]}]}]}, {"title": "research-backup/bart-base-squad-qg-no-paragraph", "paragraphs": [{"context": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: en\ndatasets:\n- lmqg/qg_squad\npipeline_tag: text2text-generation\ntags:\n- question generation\nwidget:\n- text: \"<hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 1\" \n- text: \"Beyonce further expanded her acting career, starring as blues singer <hl> Etta James <hl> in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 2\" \n- text: \"Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic,  <hl> Cadillac Records <hl> .\"\n  example_title: \"Question Generation Example 3\" \nmodel-index:\n- name: research-backup/bart-base-squad-qg-no-paragraph\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_squad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 23.86\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 51.43\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 25.18\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 90.7\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 63.85\n---\n\n# Model Card of `research-backup/bart-base-squad-qg-no-paragraph`\nThis model is fine-tuned version of [facebook/bart-base]( for question generation task on the [lmqg/qg_squad]( (dataset_name: default) via [`lmqg`](\nThis model is fine-tuned without pargraph information but only the sentence that contains the answer.\n\n### Overview\n- **Language model:** [facebook/bart-base](   \n- **Language:** en  \n- **Training data:** [lmqg/qg_squad]( (default)\n- **Online Demo:** [\n- **Repository:** [\n- **Paper:** [\n\n### Usage\n- With [`lmqg`](\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"en\", model=\"research-backup/bart-base-squad-qg-no-paragraph\")\n\n# model prediction\nquestions = model.generate_q(list_context=\"William Turner was an English painter who specialised in watercolour landscapes\", list_answer=\"William Turner\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"research-backup/bart-base-squad-qg-no-paragraph\")\noutput = pipe(\"<hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file]( \n\n   Score  Dataset                                                        |\n--------::---------------------------------------------------------------|\n   90.7   [lmqg/qg_squad]( |\n   55.85  [lmqg/qg_squad]( |\n   39.85  [lmqg/qg_squad]( |\n   30.44  [lmqg/qg_squad]( |\n   23.86  [lmqg/qg_squad]( |\n   25.18  [lmqg/qg_squad]( |\n   63.85  [lmqg/qg_squad]( |\n   51.43  [lmqg/qg_squad]( |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_squad\n - dataset_name: default\n - input_types: ['sentence_answer']\n - output_types: ['question']\n - prefix_types: None\n - model: facebook/bart-base\n - max_length: 128\n - max_length_output: 32\n - epoch: 3\n - batch: 64\n - lr: 0.0001\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 2\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of research-backup/bart-base-squad-qg-no-paragraph?", "answers": [{"text": "bart", "answer_start": 822, "answer_end": 825}]}, {"id": "q2", "question": "What is the model task of research-backup/bart-base-squad-qg-no-paragraph?", "answers": [{"text": "text2text-generation", "answer_start": 138, "answer_end": 157}]}]}]}, {"title": "research-backup/bart-large-squad-qg-no-answer", "paragraphs": [{"context": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: en\ndatasets:\n- lmqg/qg_squad\npipeline_tag: text2text-generation\ntags:\n- question generation\nwidget:\n- text: \"<hl>  Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. <hl>\"\n  example_title: \"Question Generation Example 1\" \n- text: \"<hl> Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. <hl>\"\n  example_title: \"Question Generation Example 2\" \n- text: \"<hl> Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records . <hl>\"\n  example_title: \"Question Generation Example 3\" \nmodel-index:\n- name: research-backup/bart-large-squad-qg-no-answer\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_squad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 23.47\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 50.25\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 24.94\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 90.28\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 63.28\n---\n\n# Model Card of `research-backup/bart-large-squad-qg-no-answer`\nThis model is fine-tuned version of [facebook/bart-large]( for question generation task on the [lmqg/qg_squad]( (dataset_name: default) via [`lmqg`](\nThis model is fine-tuned without answer information, i.e. generate a question only given a paragraph (note that normal model is fine-tuned to generate a question given a pargraph and an associated answer in the paragraph).\n\n### Overview\n- **Language model:** [facebook/bart-large](   \n- **Language:** en  \n- **Training data:** [lmqg/qg_squad]( (default)\n- **Online Demo:** [\n- **Repository:** [\n- **Paper:** [\n\n### Usage\n- With [`lmqg`](\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"en\", model=\"research-backup/bart-large-squad-qg-no-answer\")\n\n# model prediction\nquestions = model.generate_q(list_context=\"William Turner was an English painter who specialised in watercolour landscapes\", list_answer=\"William Turner\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"research-backup/bart-large-squad-qg-no-answer\")\noutput = pipe(\"<hl>  Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. <hl>\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file]( \n\n   Score  Dataset                                                        |\n--------::---------------------------------------------------------------|\n   90.28  [lmqg/qg_squad]( |\n   55.38  [lmqg/qg_squad]( |\n   39.27  [lmqg/qg_squad]( |\n   29.99  [lmqg/qg_squad]( |\n   23.47  [lmqg/qg_squad]( |\n   24.94  [lmqg/qg_squad]( |\n   63.28  [lmqg/qg_squad]( |\n   50.25  [lmqg/qg_squad]( |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_squad\n - dataset_name: default\n - input_types: ['paragraph_sentence']\n - output_types: ['question']\n - prefix_types: None\n - model: facebook/bart-large\n - max_length: 512\n - max_length_output: 32\n - epoch: 4\n - batch: 32\n - lr: 5e-05\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 4\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of research-backup/bart-large-squad-qg-no-answer?", "answers": [{"text": "bart", "answer_start": 822, "answer_end": 825}]}, {"id": "q2", "question": "What is the model task of research-backup/bart-large-squad-qg-no-answer?", "answers": [{"text": "text2text-generation", "answer_start": 138, "answer_end": 157}]}]}]}, {"title": "research-backup/t5-base-squad-qg-no-paragraph", "paragraphs": [{"context": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: en\ndatasets:\n- lmqg/qg_squad\npipeline_tag: text2text-generation\ntags:\n- question generation\nwidget:\n- text: \"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 1\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer <hl> Etta James <hl> in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 2\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic,  <hl> Cadillac Records <hl> .\"\n  example_title: \"Question Generation Example 3\" \nmodel-index:\n- name: research-backup/t5-base-squad-qg-no-paragraph\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_squad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 24.33\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 51.81\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 25.81\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 90.73\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 64.0\n---\n\n# Model Card of `research-backup/t5-base-squad-qg-no-paragraph`\nThis model is fine-tuned version of [t5-base]( for question generation task on the [lmqg/qg_squad]( (dataset_name: default) via [`lmqg`](\nThis model is fine-tuned without pargraph information but only the sentence that contains the answer.\n\n### Overview\n- **Language model:** [t5-base](   \n- **Language:** en  \n- **Training data:** [lmqg/qg_squad]( (default)\n- **Online Demo:** [\n- **Repository:** [\n- **Paper:** [\n\n### Usage\n- With [`lmqg`](\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"en\", model=\"research-backup/t5-base-squad-qg-no-paragraph\")\n\n# model prediction\nquestions = model.generate_q(list_context=\"William Turner was an English painter who specialised in watercolour landscapes\", list_answer=\"William Turner\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"research-backup/t5-base-squad-qg-no-paragraph\")\noutput = pipe(\"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file]( \n\n   Score  Dataset                                                        |\n--------::---------------------------------------------------------------|\n   90.73  [lmqg/qg_squad]( |\n   56.89  [lmqg/qg_squad]( |\n   40.62  [lmqg/qg_squad]( |\n   31.05  [lmqg/qg_squad]( |\n   24.33  [lmqg/qg_squad]( |\n   25.81  [lmqg/qg_squad]( |\n   64     [lmqg/qg_squad]( |\n   51.81  [lmqg/qg_squad]( |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_squad\n - dataset_name: default\n - input_types: ['sentence_answer']\n - output_types: ['question']\n - prefix_types: ['qg']\n - model: t5-base\n - max_length: 128\n - max_length_output: 32\n - epoch: 8\n - batch: 64\n - lr: 0.0001\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 1\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of research-backup/t5-base-squad-qg-no-paragraph?", "answers": [{"text": "t5", "answer_start": 879, "answer_end": 880}]}, {"id": "q2", "question": "What is the model task of research-backup/t5-base-squad-qg-no-paragraph?", "answers": [{"text": "text2text-generation", "answer_start": 138, "answer_end": 157}]}]}]}, {"title": "research-backup/t5-large-squad-qg-default", "paragraphs": [{"context": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: en\ndatasets:\n- lmqg/qg_squad\npipeline_tag: text2text-generation\ntags:\n- question generation\nwidget:\n- text: \"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 1\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer <hl> Etta James <hl> in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 2\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic,  <hl> Cadillac Records <hl> .\"\n  example_title: \"Question Generation Example 3\" \nmodel-index:\n- name: research-backup/t5-large-squad-qg-default\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_squad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 27.03\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 53.98\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 27.71\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 90.92\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 65.21\n---\n\n# Model Card of `research-backup/t5-large-squad-qg-default`\nThis model is fine-tuned version of [t5-large]( for question generation task on the [lmqg/qg_squad]( (dataset_name: default) via [`lmqg`](\nThis model is fine-tuned without parameter search (default configuration is taken from [ERNIE-GEN](\n\n### Overview\n- **Language model:** [t5-large](   \n- **Language:** en  \n- **Training data:** [lmqg/qg_squad]( (default)\n- **Online Demo:** [\n- **Repository:** [\n- **Paper:** [\n\n### Usage\n- With [`lmqg`](\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"en\", model=\"research-backup/t5-large-squad-qg-default\")\n\n# model prediction\nquestions = model.generate_q(list_context=\"William Turner was an English painter who specialised in watercolour landscapes\", list_answer=\"William Turner\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"research-backup/t5-large-squad-qg-default\")\noutput = pipe(\"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file]( \n\n   Score  Dataset                                                        |\n--------::---------------------------------------------------------------|\n   90.92  [lmqg/qg_squad]( |\n   59.39  [lmqg/qg_squad]( |\n   43.58  [lmqg/qg_squad]( |\n   33.91  [lmqg/qg_squad]( |\n   27.03  [lmqg/qg_squad]( |\n   27.71  [lmqg/qg_squad]( |\n   65.21  [lmqg/qg_squad]( |\n   53.98  [lmqg/qg_squad]( |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_squad\n - dataset_name: default\n - input_types: ['paragraph_answer']\n - output_types: ['question']\n - prefix_types: ['qg']\n - model: t5-large\n - max_length: 512\n - max_length_output: 32\n - epoch: 10\n - batch: 1\n - lr: 1.25e-05\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 32\n - label_smoothing: 0.1\n\nThe full configuration can be found at [fine-tuning config file](\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of research-backup/t5-large-squad-qg-default?", "answers": [{"text": "t5", "answer_start": 879, "answer_end": 880}]}, {"id": "q2", "question": "What is the model task of research-backup/t5-large-squad-qg-default?", "answers": [{"text": "text2text-generation", "answer_start": 138, "answer_end": 157}]}]}]}, {"title": "research-backup/t5-large-squad-qg-no-answer", "paragraphs": [{"context": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: en\ndatasets:\n- lmqg/qg_squad\npipeline_tag: text2text-generation\ntags:\n- question generation\nwidget:\n- text: \"generate question: <hl>  Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. <hl>\"\n  example_title: \"Question Generation Example 1\" \n- text: \"generate question: <hl> Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. <hl>\"\n  example_title: \"Question Generation Example 2\" \n- text: \"generate question: <hl> Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records . <hl>\"\n  example_title: \"Question Generation Example 3\" \nmodel-index:\n- name: research-backup/t5-large-squad-qg-no-answer\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_squad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 24.27\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 51.3\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 25.67\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 90.41\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 63.97\n---\n\n# Model Card of `research-backup/t5-large-squad-qg-no-answer`\nThis model is fine-tuned version of [t5-large]( for question generation task on the [lmqg/qg_squad]( (dataset_name: default) via [`lmqg`](\nThis model is fine-tuned without answer information, i.e. generate a question only given a paragraph (note that normal model is fine-tuned to generate a question given a pargraph and an associated answer in the paragraph).\n\n### Overview\n- **Language model:** [t5-large](   \n- **Language:** en  \n- **Training data:** [lmqg/qg_squad]( (default)\n- **Online Demo:** [\n- **Repository:** [\n- **Paper:** [\n\n### Usage\n- With [`lmqg`](\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"en\", model=\"research-backup/t5-large-squad-qg-no-answer\")\n\n# model prediction\nquestions = model.generate_q(list_context=\"William Turner was an English painter who specialised in watercolour landscapes\", list_answer=\"William Turner\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"research-backup/t5-large-squad-qg-no-answer\")\noutput = pipe(\"generate question: <hl>  Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. <hl>\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file]( \n\n   Score  Dataset                                                        |\n--------::---------------------------------------------------------------|\n   90.41  [lmqg/qg_squad]( |\n   56.44  [lmqg/qg_squad]( |\n   40.29  [lmqg/qg_squad]( |\n   30.87  [lmqg/qg_squad]( |\n   24.27  [lmqg/qg_squad]( |\n   25.67  [lmqg/qg_squad]( |\n   63.97  [lmqg/qg_squad]( |\n   51.3   [lmqg/qg_squad]( |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_squad\n - dataset_name: default\n - input_types: ['paragraph_sentence']\n - output_types: ['question']\n - prefix_types: ['qg']\n - model: t5-large\n - max_length: 512\n - max_length_output: 32\n - epoch: 7\n - batch: 16\n - lr: 5e-05\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 4\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of research-backup/t5-large-squad-qg-no-answer?", "answers": [{"text": "t5", "answer_start": 879, "answer_end": 880}]}, {"id": "q2", "question": "What is the model task of research-backup/t5-large-squad-qg-no-answer?", "answers": [{"text": "text2text-generation", "answer_start": 138, "answer_end": 157}]}]}]}, {"title": "research-backup/t5-large-squad-qg-no-paragraph", "paragraphs": [{"context": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: en\ndatasets:\n- lmqg/qg_squad\npipeline_tag: text2text-generation\ntags:\n- question generation\nwidget:\n- text: \"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 1\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer <hl> Etta James <hl> in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 2\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic,  <hl> Cadillac Records <hl> .\"\n  example_title: \"Question Generation Example 3\" \nmodel-index:\n- name: research-backup/t5-large-squad-qg-no-paragraph\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_squad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 25.36\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 52.53\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 26.28\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 90.88\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 64.44\n---\n\n# Model Card of `research-backup/t5-large-squad-qg-no-paragraph`\nThis model is fine-tuned version of [t5-large]( for question generation task on the [lmqg/qg_squad]( (dataset_name: default) via [`lmqg`](\nThis model is fine-tuned without pargraph information but only the sentence that contains the answer.\n\n### Overview\n- **Language model:** [t5-large](   \n- **Language:** en  \n- **Training data:** [lmqg/qg_squad]( (default)\n- **Online Demo:** [\n- **Repository:** [\n- **Paper:** [\n\n### Usage\n- With [`lmqg`](\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"en\", model=\"research-backup/t5-large-squad-qg-no-paragraph\")\n\n# model prediction\nquestions = model.generate_q(list_context=\"William Turner was an English painter who specialised in watercolour landscapes\", list_answer=\"William Turner\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"research-backup/t5-large-squad-qg-no-paragraph\")\noutput = pipe(\"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file]( \n\n   Score  Dataset                                                        |\n--------::---------------------------------------------------------------|\n   90.88  [lmqg/qg_squad]( |\n   57.49  [lmqg/qg_squad]( |\n   41.59  [lmqg/qg_squad]( |\n   32.1   [lmqg/qg_squad]( |\n   25.36  [lmqg/qg_squad]( |\n   26.28  [lmqg/qg_squad]( |\n   64.44  [lmqg/qg_squad]( |\n   52.53  [lmqg/qg_squad]( |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_squad\n - dataset_name: default\n - input_types: ['sentence_answer']\n - output_types: ['question']\n - prefix_types: ['qg']\n - model: t5-large\n - max_length: 128\n - max_length_output: 32\n - epoch: 6\n - batch: 16\n - lr: 5e-05\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 4\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of research-backup/t5-large-squad-qg-no-paragraph?", "answers": [{"text": "t5", "answer_start": 879, "answer_end": 880}]}, {"id": "q2", "question": "What is the model task of research-backup/t5-large-squad-qg-no-paragraph?", "answers": [{"text": "text2text-generation", "answer_start": 138, "answer_end": 157}]}]}]}, {"title": "research-backup/t5-small-squad-qg-default", "paragraphs": [{"context": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: en\ndatasets:\n- lmqg/qg_squad\npipeline_tag: text2text-generation\ntags:\n- question generation\nwidget:\n- text: \"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 1\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer <hl> Etta James <hl> in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 2\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic,  <hl> Cadillac Records <hl> .\"\n  example_title: \"Question Generation Example 3\" \nmodel-index:\n- name: research-backup/t5-small-squad-qg-default\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_squad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 22.67\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 49.54\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 24.68\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 90.17\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 63.06\n---\n\n# Model Card of `research-backup/t5-small-squad-qg-default`\nThis model is fine-tuned version of [t5-small]( for question generation task on the [lmqg/qg_squad]( (dataset_name: default) via [`lmqg`](\nThis model is fine-tuned without parameter search (default configuration is taken from [ERNIE-GEN](\n\n### Overview\n- **Language model:** [t5-small](   \n- **Language:** en  \n- **Training data:** [lmqg/qg_squad]( (default)\n- **Online Demo:** [\n- **Repository:** [\n- **Paper:** [\n\n### Usage\n- With [`lmqg`](\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"en\", model=\"research-backup/t5-small-squad-qg-default\")\n\n# model prediction\nquestions = model.generate_q(list_context=\"William Turner was an English painter who specialised in watercolour landscapes\", list_answer=\"William Turner\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"research-backup/t5-small-squad-qg-default\")\noutput = pipe(\"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file]( \n\n   Score  Dataset                                                        |\n--------::---------------------------------------------------------------|\n   90.17  [lmqg/qg_squad]( |\n   54.85  [lmqg/qg_squad]( |\n   38.46  [lmqg/qg_squad]( |\n   29.07  [lmqg/qg_squad]( |\n   22.67  [lmqg/qg_squad]( |\n   24.68  [lmqg/qg_squad]( |\n   63.06  [lmqg/qg_squad]( |\n   49.54  [lmqg/qg_squad]( |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_squad\n - dataset_name: default\n - input_types: ['paragraph_answer']\n - output_types: ['question']\n - prefix_types: ['qg']\n - model: t5-small\n - max_length: 512\n - max_length_output: 32\n - epoch: 10\n - batch: 32\n - lr: 1.25e-05\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 1\n - label_smoothing: 0.1\n\nThe full configuration can be found at [fine-tuning config file](\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of research-backup/t5-small-squad-qg-default?", "answers": [{"text": "t5", "answer_start": 879, "answer_end": 880}]}, {"id": "q2", "question": "What is the model task of research-backup/t5-small-squad-qg-default?", "answers": [{"text": "text2text-generation", "answer_start": 138, "answer_end": 157}]}]}]}, {"title": "research-backup/t5-small-squad-qg-no-paragraph", "paragraphs": [{"context": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: en\ndatasets:\n- lmqg/qg_squad\npipeline_tag: text2text-generation\ntags:\n- question generation\nwidget:\n- text: \"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 1\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer <hl> Etta James <hl> in the 2008 musical biopic, Cadillac Records.\"\n  example_title: \"Question Generation Example 2\" \n- text: \"generate question: Beyonce further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic,  <hl> Cadillac Records <hl> .\"\n  example_title: \"Question Generation Example 3\" \nmodel-index:\n- name: research-backup/t5-small-squad-qg-no-paragraph\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_squad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 23.23\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 50.18\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 24.8\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 90.36\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 63.18\n---\n\n# Model Card of `research-backup/t5-small-squad-qg-no-paragraph`\nThis model is fine-tuned version of [t5-small]( for question generation task on the [lmqg/qg_squad]( (dataset_name: default) via [`lmqg`](\nThis model is fine-tuned without pargraph information but only the sentence that contains the answer.\n\n### Overview\n- **Language model:** [t5-small](   \n- **Language:** en  \n- **Training data:** [lmqg/qg_squad]( (default)\n- **Online Demo:** [\n- **Repository:** [\n- **Paper:** [\n\n### Usage\n- With [`lmqg`](\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"en\", model=\"research-backup/t5-small-squad-qg-no-paragraph\")\n\n# model prediction\nquestions = model.generate_q(list_context=\"William Turner was an English painter who specialised in watercolour landscapes\", list_answer=\"William Turner\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"research-backup/t5-small-squad-qg-no-paragraph\")\noutput = pipe(\"generate question: <hl> Beyonce <hl> further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file]( \n\n   Score  Dataset                                                        |\n--------::---------------------------------------------------------------|\n   90.36  [lmqg/qg_squad]( |\n   55.39  [lmqg/qg_squad]( |\n   39.1   [lmqg/qg_squad]( |\n   29.7   [lmqg/qg_squad]( |\n   23.23  [lmqg/qg_squad]( |\n   24.8   [lmqg/qg_squad]( |\n   63.18  [lmqg/qg_squad]( |\n   50.18  [lmqg/qg_squad]( |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_squad\n - dataset_name: default\n - input_types: ['sentence_answer']\n - output_types: ['question']\n - prefix_types: ['qg']\n - model: t5-small\n - max_length: 128\n - max_length_output: 32\n - epoch: 8\n - batch: 64\n - lr: 0.0001\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 1\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of research-backup/t5-small-squad-qg-no-paragraph?", "answers": [{"text": "t5", "answer_start": 879, "answer_end": 880}]}, {"id": "q2", "question": "What is the model task of research-backup/t5-small-squad-qg-no-paragraph?", "answers": [{"text": "text2text-generation", "answer_start": 138, "answer_end": 157}]}]}]}, {"title": "aubmindlab/aragpt2-mega-detector-long", "paragraphs": [{"context": "---\nlanguage: ar\n\nwidget:\n - text: \"\u0648\u0625\u0630\u0627 \u0643\u0627\u0646 \u0647\u0646\u0627\u0643 \u0645\u0646 \u0644\u0627 \u064a\u0632\u0627\u0644 \u064a\u0639\u062a\u0642\u062f \u0623\u0646 \u0644\u0628\u0646\u0627\u0646 \u0647\u0648 \u0633\u0648\u064a\u0633\u0631\u0627 \u0627\u0644\u0634\u0631\u0642 \u060c \u0641\u0647\u0648 \u0645\u062e\u0637\u0626 \u0625\u0644\u0649 \u062d\u062f \u0628\u0639\u064a\u062f . \u0641\u0644\u0628\u0646\u0627\u0646 \u0644\u064a\u0633 \u0633\u0648\u064a\u0633\u0631\u0627 \u060c \u0648\u0644\u0627 \u064a\u0645\u0643\u0646 \u0623\u0646 \u064a\u0643\u0648\u0646 \u0643\u0630\u0644\u0643 . \u0644\u0642\u062f \u0639\u0627\u0634 \u0627\u0644\u0644\u0628\u0646\u0627\u0646\u064a\u0648\u0646 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0628\u0644\u062f \u0645\u0646\u0630 \u0645\u0627 \u064a\u0632\u064a\u062f \u0639\u0646 \u0623\u0644\u0641 \u0648\u062e\u0645\u0633\u0645\u0626\u0629 \u0639\u0627\u0645 \u060c \u0623\u064a \u0645\u0646\u0630 \u062a\u0623\u0633\u064a\u0633 \u0627\u0644\u0625\u0645\u0627\u0631\u0629 \u0627\u0644\u0634\u0647\u0627\u0628\u064a\u0629 \u0627\u0644\u062a\u064a \u0623\u0633\u0633\u0647\u0627 \u0627\u0644\u0623\u0645\u064a\u0631 \u0641\u062e\u0631 \u0627\u0644\u062f\u064a\u0646 \u0627\u0644\u0645\u0639\u0646\u064a \u0627\u0644\u062b\u0627\u0646\u064a ( 1697 - 1742 )\"\n---\n\n# AraGPT2 Detector\n\nMachine generated detector model from the [AraGPT2: Pre-Trained Transformer for Arabic Language Generation paper](\n\nThis model is trained on the long text passages, and achieves a 99.4% F1-Score.\n\n# How to use it:\n```python\nfrom transformers import pipeline\nfrom arabert.preprocess import ArabertPreprocessor\n\nprocessor = ArabertPreprocessor(model=\"aubmindlab/araelectra-base-discriminator\")\npipe = pipeline(\"sentiment-analysis\", model = \"aubmindlab/aragpt2-mega-detector-long\")\n\ntext = \" \"\ntext_prep = processor.preprocess(text)\nresult = pipe(text_prep)\n# [{'label': 'machine-generated', 'score': 0.9977743625640869}]\n```\n\n\n# If you used this model please cite us as :\n```\n@misc{antoun2020aragpt2,\n      title={AraGPT2: Pre-Trained Transformer for Arabic Language Generation},\n      author={Wissam Antoun and Fady Baly and Hazem Hajj},\n      year={2020},\n      eprint={2012.15520},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n# Contacts\n**Wissam Antoun**: [Linkedin](  [Github](  <wissam.antoun@gmail.com>\n\n**Fady Baly**: [Linkedin](  [Github](  <baly.fady@gmail.com>", "qas": [{"id": "q1", "question": "What is the model architecture of aubmindlab/aragpt2-mega-detector-long?", "answers": [{"text": "electra", "answer_start": 703, "answer_end": 709}]}]}]}, {"title": "aubmindlab/bert-large-arabertv02", "paragraphs": [{"context": "---\nlanguage: ar\ndatasets:\n - wikipedia\n - OSIAN\n - 1.5B Arabic Corpus\n - OSCAR Arabic Unshuffled\nwidget:\n - text: \" \u0639\u0627\u0635\u0645\u0629 \u0644\u0628\u0646\u0627\u0646 \u0647\u064a [MASK] .\"\n---\n\n# AraBERT v1 & v2 : Pre-training BERT for Arabic Language Understanding\n\n<img src=\" width=\"100\" align=\"left\"/>\n\n**AraBERT** is an Arabic pretrained lanaguage model based on [Google's BERT architechture]( AraBERT uses the same BERT-Base config. More details are available in the [AraBERT Paper]( and in the [AraBERT Meetup](\n\nThere are two versions of the model, AraBERTv0.1 and AraBERTv1, with the difference being that AraBERTv1 uses pre-segmented text where prefixes and suffixes were splitted using the [Farasa Segmenter](\n\n\nWe evalaute AraBERT models on different downstream tasks and compare them to [mBERT](( and other state of the art models (*To the extent of our knowledge*). The Tasks were Sentiment Analysis on 6 different datasets ([HARD]( [ASTD-Balanced]( [ArsenTD-Lev]( [LABR]( Named Entity Recognition with the [ANERcorp]( and Arabic Question Answering on [Arabic-SQuAD and ARCD](\n\n# AraBERTv2\n\n## What's New!\n\nAraBERT now comes in 4 new variants to replace the old v1 versions:\n\nMore Detail in the AraBERT folder and in the [README]( and in the [AraBERT Paper](\n\n Model  Size (MB/Params) DataSet (Sentences/Size/nWords) |\n ---:---::---:\nAraBERTv0.2-base  543MB / 136M  200M / 77GB / 8.6B |\n AraBERTv0.2-large 1.38G 371M  200M / 77GB / 8.6B |\nAraBERTv2-base 543MB 136M  200M / 77GB / 8.6B |\nAraBERTv2-large 1.38G 371M  200M / 77GB / 8.6B |\n AraBERTv0.1-base 543MB 136M  77M / 23GB / 2.7B |\nAraBERTv1-base 543MB 136M  77M / 23GB / 2.7B |\n\nAll models are available in the `HuggingFace` model page under the [aubmindlab]( name. Checkpoints are available in PyTorch, TF2 and TF1 formats.\n\n## Better Pre-Processing and New Vocab\n\nWe identified an issue with AraBERTv1's wordpiece vocabulary. The issue came from punctuations and numbers that were still attached to words when learned the wordpiece vocab. We now insert a space between numbers and characters and around punctuation characters.\n\nThe new vocabulary was learnt using the `BertWordpieceTokenizer` from the `tokenizers` library, and should now support the Fast tokenizer implementation from the `transformers` library.\n\n**P.S.**: All the old BERT codes should work with the new BERT, just change the model name and check the new preprocessing dunction\n**Please read the section on how to use the [preprocessing function](#Preprocessing)**\n\n## Bigger Dataset and More Compute\n\nWe used ~3.5 times more data, and trained for longer.\nFor Dataset Sources see the [Dataset Section](#Dataset)\n\nModel  num of examples with seq len (128 / 512)  512 (Batch Size/ Num of Steps)  Total Time (in Days) |\n ---:---::---::---:\nAraBERTv0.2-base  420M / 207M  384/ 2M  -\nAraBERTv0.2-large  420M / 207M  2056 / 300K  7\nAraBERTv2-base  420M / 207M  384/ 2M  -\nAraBERTv2-large  520M / 245M  2056 / 300K  7\nAraBERT-base (v1/v0.1)  -  128 / 300K 4\n\n# Dataset\n\nThe pretraining data used for the new AraBERT model is also used for Arabic **GPT2 and ELECTRA**.\n\nThe dataset consists of 77GB or 200,095,961 lines or 8,655,948,860 words or 82,232,988,358 chars (before applying Farasa Segmentation)\n\nFor the new dataset we added the unshuffled OSCAR corpus, after we thoroughly filter it, to the previous dataset used in AraBERTv1 but with out the websites that we previously crawled:\n- OSCAR unshuffled and filtered.\n- [Arabic Wikipedia dump]( from 2020/09/01\n- [The 1.5B words Arabic Corpus](\n- [The OSIAN Corpus](\n- Assafir news articles. Huge thank you for Assafir for giving us the data\n\n# Preprocessing\n\nIt is recommended to apply our preprocessing function before training/testing on any dataset.\n**Install farasapy to segment text for AraBERT v1 & v2 `pip install farasapy`**\n\n```python\nfrom arabert.preprocess import ArabertPreprocessor\n\nmodel_name=\"bert-large-arabertv02\"\narabert_prep = ArabertPreprocessor(model_name=model_name)\n\ntext = \"\u0648\u0644\u0646 \u0646\u0628\u0627\u0644\u063a \u0625\u0630\u0627 \u0642\u0644\u0646\u0627 \u0625\u0646 \u0647\u0627\u062a\u0641 \u0623\u0648 \u0643\u0645\u0628\u064a\u0648\u062a\u0631 \u0627\u0644\u0645\u0643\u062a\u0628 \u0641\u064a \u0632\u0645\u0646\u0646\u0627 \u0647\u0630\u0627 \u0636\u0631\u0648\u0631\u064a\"\narabert_prep.preprocess(text)\n```\n\n## Accepted_models\n```\nbert-base-arabertv01\nbert-base-arabert\nbert-base-arabertv02\nbert-base-arabertv2\nbert-large-arabertv02\nbert-large-arabertv2\naraelectra-base\naragpt2-base\naragpt2-medium\naragpt2-large\naragpt2-mega\n```\n\n# TensorFlow 1.x models\n\nThe TF1.x model are available in the HuggingFace models repo.\nYou can download them as follows:\n- via git-lfs: clone all the models in a repo\n```bash\ncurl -s  | sudo bash\nsudo apt-get install git-lfs\ngit lfs install\ngit clone \ntar -C ./MODEL_NAME -zxvf /content/MODEL_NAME/tf1_model.tar.gz\n```\nwhere `MODEL_NAME` is any model under the `aubmindlab` name\n\n- via `wget`:\n    - Go to the tf1_model.tar.gz file on huggingface.co/models/aubmindlab/MODEL_NAME.\n    - copy the `oid sha256`\n    - then run `wget   (ex: for `aragpt2-base`: `wget \n\n\n# If you used this model please cite us as :\nGoogle Scholar has our Bibtex wrong (missing name), use this instead\n```\n@inproceedings{antoun2020arabert,\n  title={AraBERT: Transformer-based Model for Arabic Language Understanding},\n  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},\n  booktitle={LREC 2020 Workshop Language Resources and Evaluation Conference 11--16 May 2020},\n  pages={9}\n}\n```\n# Acknowledgments\nThanks to TensorFlow Research Cloud (TFRC) for the free access to Cloud TPUs, couldn't have done it without this program, and to the [AUB MIND Lab]( Members for the continous support. Also thanks to [Yakshof]( and Assafir for data and storage access. Another thanks for Habib Rahal ( for putting a face to AraBERT.\n\n# Contacts\n**Wissam Antoun**: [Linkedin](  [Github](  <wissam.antoun@gmail.com>\n\n**Fady Baly**: [Linkedin](  [Github](  <baly.fady@gmail.com>\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of aubmindlab/bert-large-arabertv02?", "answers": [{"text": "bert", "answer_start": 3793, "answer_end": 3796}]}]}]}, {"title": "auday/paraphraser_model1", "paragraphs": [{"context": "This folder contain a Google T5 Transformer Fine-tuned to generate paraphrases using:\n   -  Para_NMT_50M_Paraphrasing_train_small.csv 134337 lines of pair sentences 19Mbytes\n   -  Para_NMT_50M_Paraphrasing_val_small.csv 14928 lines of pair sentences 2.0Mbytes\n\nTraining Start Time:  Sun Mar 14 18:27:15 2021\nTraining End Time:  Sun Mar 14 22:19:00 2021\n\n", "qas": []}]}, {"title": "auday/paraphraser_model2", "paragraphs": [{"context": "This folder contain a Google T5 Transformer Fine-tuned to generate paraphrases using:\n   -  Quora_pair_train 134337 lines of pair sentences 14 Mbytes\n   -  Quora_pair_val 14928 lines of pair sentences 1.6 Mbytes\n\ntraining epoch: 6\n\nStart Time:  Sun Mar 14 18:27:15 2021\nEnd Time:  Sun Mar 14 22:19:00 2021\n", "qas": []}]}, {"title": "augustojaba/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of augustojaba/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "augustoortiz/bert-finetuned-squad2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: augustoortiz/bert-finetuned-squad2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# augustoortiz/bert-finetuned-squad2\n\nThis model is a fine-tuned version of [bert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 1.2223\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 11091, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: mixed_float16\n\n### Training results\n\n Epoch |\n:-----:|\n 0     |\n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- TensorFlow 2.8.0\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of augustoortiz/bert-finetuned-squad2?", "answers": [{"text": "bert", "answer_start": 96, "answer_end": 99}]}]}]}, {"title": "austin/adr-ner", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: adr-ner\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# adr-ner\n\nThis model is a fine-tuned version of [austin/Austin-MeDeBERTa]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0434\n- Precision: 0.7305\n- Recall: 0.6934\n- F1: 0.7115\n- Accuracy: 0.9941\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 12\n- eval_batch_size: 12\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 15\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0630           0.0     0.9876   |\n 2.0    0.0308           0.3467  0.9900   |\n 3.0    0.0254           0.5603  0.9920   |\n 4.0    0.0280           0.5751  0.9929   |\n 5.0    0.0266           0.7146  0.9915   |\n 6.0    0.0423           0.5793  0.9939   |\n 7.0    0.0336           0.6765  0.9939   |\n 8.0    0.0370           0.6702  0.9936   |\n 9.0    0.0349           0.7040  0.9932   |\n 10.0   0.0403           0.6808  0.9938   |\n 11.0   0.0415           0.6808  0.9939   |\n 12.0   0.0440           0.6681  0.9941   |\n 13.0   0.0423           0.6977  0.9941   |\n 14.0   0.0435           0.6977  0.9941   |\n 15.0   0.0434           0.6934  0.9941   |\n\n\n### Framework versions\n\n- Transformers 4.14.1\n- Pytorch 1.10.0+cu113\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": []}]}, {"title": "bashar-talafha/multi-dialect-bert-base-arabic", "paragraphs": [{"context": "---\nlanguage: ar\nthumbnail: \ndatasets:\n- nadi\n---\n# Multi-dialect-Arabic-BERT\nThis is a repository of Multi-dialect Arabic BERT model.\n\nBy [Mawdoo3-AI]( \n\n<p align=\"center\">\n    <br>\n    <img src=\" alt=\"Background reference:  width=\"500\"/>\n    <br>\n<p>\n\n\n\n### About our Multi-dialect-Arabic-BERT model\nInstead of training the Multi-dialect Arabic BERT model from scratch, we initialized the weights of the model using [Arabic-BERT]( and trained it on 10M arabic tweets from the unlabled data of [The Nuanced Arabic Dialect Identification (NADI) shared task](\n\n### To cite this work\n\n```\n@misc{talafha2020multidialect,\n    title={Multi-Dialect Arabic BERT for Country-Level Dialect Identification},\n    author={Bashar Talafha and Mohammad Ali and Muhy Eddin Za'ter and Haitham Seelawi and Ibraheem Tuffaha and Mostafa Samir and Wael Farhan and Hussein T. Al-Natsheh},\n    year={2020},\n    eprint={2007.05612},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n### Usage\nThe model weights can be loaded using `transformers` library by HuggingFace.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")\nmodel = AutoModel.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")\n```\n\nExample using `pipeline`:\n\n```python\nfrom transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"bashar-talafha/multi-dialect-bert-base-arabic \",\n    tokenizer=\"bashar-talafha/multi-dialect-bert-base-arabic \"\n)\n\nfill_mask(\" \u0633\u0627\u0641\u0631 \u0627\u0644\u0631\u062d\u0627\u0644\u0629 \u0645\u0646 \u0645\u0637\u0627\u0631 [MASK] \")\n```\n```\n[{'sequence': '[CLS] \u0633\u0627\u0641\u0631 \u0627\u0644\u0631\u062d\u0627\u0644\u0629 \u0645\u0646 \u0645\u0637\u0627\u0631 \u0627\u0644\u0643\u0648\u064a\u062a [SEP]', 'score': 0.08296813815832138, 'token': 3226},\n {'sequence': '[CLS] \u0633\u0627\u0641\u0631 \u0627\u0644\u0631\u062d\u0627\u0644\u0629 \u0645\u0646 \u0645\u0637\u0627\u0631 \u062f\u0628\u064a [SEP]', 'score': 0.05123933032155037, 'token': 4747},\n {'sequence': '[CLS] \u0633\u0627\u0641\u0631 \u0627\u0644\u0631\u062d\u0627\u0644\u0629 \u0645\u0646 \u0645\u0637\u0627\u0631 \u0645\u0633\u0642\u0637 [SEP]', 'score': 0.046838656067848206, 'token': 13205},\n {'sequence': '[CLS] \u0633\u0627\u0641\u0631 \u0627\u0644\u0631\u062d\u0627\u0644\u0629 \u0645\u0646 \u0645\u0637\u0627\u0631 \u0627\u0644\u0642\u0627\u0647\u0631\u0629 [SEP]', 'score': 0.03234650194644928, 'token': 4003},\n {'sequence': '[CLS] \u0633\u0627\u0641\u0631 \u0627\u0644\u0631\u062d\u0627\u0644\u0629 \u0645\u0646 \u0645\u0637\u0627\u0631 \u0627\u0644\u0631\u064a\u0627\u0636 [SEP]', 'score': 0.02606341242790222, 'token': 2200}]\n```\n### Repository\nPlease check the [original repository]( for more information. \n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of bashar-talafha/multi-dialect-bert-base-arabic?", "answers": [{"text": "bert", "answer_start": 1189, "answer_end": 1192}]}, {"id": "q2", "question": "What is the model task of bashar-talafha/multi-dialect-bert-base-arabic?", "answers": [{"text": "fill-mask", "answer_start": 1395, "answer_end": 1403}]}]}]}, {"title": "batterydata/batterybert-uncased-squad-v1", "paragraphs": [{"context": "---\nlanguage: en\ntags: question answering\nlicense: apache-2.0\ndatasets: \n- squad\n- batterydata/battery-device-data-qa\nmetrics: squad\n---\n\n# BatteryBERT-uncased for QA \n**Language model:** batterybert-uncased\n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD v1\n**Eval data:** SQuAD v1\n**Code:**  See [example]( \n**Infrastructure**: 8x DGX A100\n## Hyperparameters\n```\nbatch_size = 32\nn_epochs = 3\nbase_LM_model = \"batterybert-uncased\"\nmax_seq_len = 386\nlearning_rate = 3e-5\ndoc_stride=128\nmax_query_length=64\n``` \n## Performance\nEvaluated on the SQuAD v1.0 dev set.\n```\n\"exact\": 81.08,\n\"f1\": 88.41,\n```\nEvaluated on the battery device dataset.\n```\n\"precision\": 68.27,\n\"recall\": 80.88,\n```\n## Usage\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"batterydata/batterybert-uncased-squad-v1\"\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'What is the electrolyte?',\n    'context': 'The typical non-aqueous electrolyte for commercial Li-ion cells is a solution of LiPF6 in linear and cyclic carbonates.'\n}\nres = nlp(QA_input)\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n## Authors\nShu Huang: `sh2009 [at] cam.ac.uk`\n\nJacqueline Cole: `jmc61 [at] cam.ac.uk`\n\n## Citation\nBatteryBERT: A Pre-trained Language Model for Battery Database Enhancement\n", "qas": [{"id": "q1", "question": "What is the model architecture of batterydata/batterybert-uncased-squad-v1?", "answers": [{"text": "bert", "answer_start": 195, "answer_end": 198}]}, {"id": "q2", "question": "What is the model task of batterydata/batterybert-uncased-squad-v1?", "answers": [{"text": "question-answering", "answer_start": 937, "answer_end": 954}]}]}]}, {"title": "beomi/kcbert-base", "paragraphs": [{"context": "---\nlanguage: ko\nlicense: apache-2.0\ntags:\n  - korean\n---\n\n# KcBERT: Korean comments BERT\n\n** Updates on 2021.04.07 **\n\n- KcELECTRA\uac00 \ub9b4\ub9ac\uc988 \ub418\uc5c8\uc2b5\ub2c8\ub2e4!\ud83e\udd17\n- KcELECTRA\ub294 \ubcf4\ub2e4 \ub354 \ub9ce\uc740 \ub370\uc774\ud130\uc14b, \uadf8\ub9ac\uace0 \ub354 \ud070 General vocab\uc744 \ud1b5\ud574 KcBERT \ub300\ube44 **\ubaa8\ub4e0 \ud0dc\uc2a4\ud06c\uc5d0\uc11c \ub354 \ub192\uc740 \uc131\ub2a5**\uc744 \ubcf4\uc785\ub2c8\ub2e4.\n- \uc544\ub798 \uae43\ud5d9 \ub9c1\ud06c\uc5d0\uc11c \uc9c1\uc811 \uc0ac\uc6a9\ud574\ubcf4\uc138\uc694!\n- \n\n** Updates on 2021.03.14 **\n\n- KcBERT Paper \uc778\uc6a9 \ud45c\uae30\ub97c \ucd94\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4.(bibtex)\n- KcBERT-finetune Performance score\ub97c \ubcf8\ubb38\uc5d0 \ucd94\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n** Updates on 2020.12.04 **\n\nHuggingface Transformers\uac00 v4.0.0\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8\ub428\uc5d0 \ub530\ub77c Tutorial\uc758 \ucf54\ub4dc\uac00 \uc77c\ubd80 \ubcc0\uacbd\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\uc5c5\ub370\uc774\ud2b8\ub41c KcBERT-Large NSMC Finetuning Colab: <a href=\"\n  <img src=\" alt=\"Open In Colab\"/>\n</a>\n\n** Updates on 2020.09.11 **\n\nKcBERT\ub97c Google Colab\uc5d0\uc11c TPU\ub97c \ud1b5\ud574 \ud559\uc2b5\ud560 \uc218 \uc788\ub294 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4! \uc544\ub798 \ubc84\ud2bc\uc744 \ub20c\ub7ec\ubcf4\uc138\uc694.\n\nColab\uc5d0\uc11c TPU\ub85c KcBERT Pretrain \ud574\ubcf4\uae30: <a href=\"\n  <img src=\" alt=\"Open In Colab\"/>\n</a>\n\n\ud14d\uc2a4\ud2b8 \ubd84\ub7c9\ub9cc \uc804\uccb4 12G \ud14d\uc2a4\ud2b8 \uc911 \uc77c\ubd80(144MB)\ub85c \uc904\uc5ec \ud559\uc2b5\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4. \n\n\ud55c\uad6d\uc5b4 \ub370\uc774\ud130\uc14b/\ucf54\ud37c\uc2a4\ub97c \uc880\ub354 \uc27d\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 [Korpora]( \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n** Updates on 2020.09.08 **\n\nGithub Release\ub97c \ud1b5\ud574 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc5c5\ub85c\ub4dc\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n\ub2e4\ub9cc \ud55c \ud30c\uc77c\ub2f9 2GB \uc774\ub0b4\uc758 \uc81c\uc57d\uc73c\ub85c \uc778\ud574 \ubd84\ud560\uc555\ucd95\ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4.\n\n\uc544\ub798 \ub9c1\ud06c\ub97c \ud1b5\ud574 \ubc1b\uc544\uc8fc\uc138\uc694. (\uac00\uc785 \uc5c6\uc774 \ubc1b\uc744 \uc218 \uc788\uc5b4\uc694. \ubd84\ud560\uc555\ucd95)\n\n\ub9cc\uc57d \ud55c \ud30c\uc77c\ub85c \ubc1b\uace0\uc2f6\uc73c\uc2dc\uac70\ub098/Kaggle\uc5d0\uc11c \ub370\uc774\ud130\ub97c \uc0b4\ud3b4\ubcf4\uace0 \uc2f6\uc73c\uc2dc\ub2e4\uba74 \uc544\ub798\uc758 \uce90\uae00 \ub370\uc774\ud130\uc14b\uc744 \uc774\uc6a9\ud574\uc8fc\uc138\uc694.\n\n- Github\ub9b4\ub9ac\uc988: \n\n** Updates on 2020.08.22 **\n\nPretrain Dataset \uacf5\uac1c\n\n- \uce90\uae00:  (\ud55c \ud30c\uc77c\ub85c \ubc1b\uc744 \uc218 \uc788\uc5b4\uc694. \ub2e8\uc77c\ud30c\uc77c)\n\nKaggle\uc5d0 \ud559\uc2b5\uc744 \uc704\ud574 \uc815\uc81c\ud55c(\uc544\ub798 `clean`\ucc98\ub9ac\ub97c \uac70\uce5c) Dataset\uc744 \uacf5\uac1c\ud558\uc600\uc2b5\ub2c8\ub2e4!\n\n\uc9c1\uc811 \ub2e4\uc6b4\ubc1b\uc73c\uc154\uc11c \ub2e4\uc591\ud55c Task\uc5d0 \ud559\uc2b5\uc744 \uc9c4\ud589\ud574\ubcf4\uc138\uc694 :) \n\n---\n\n\uacf5\uac1c\ub41c \ud55c\uad6d\uc5b4 BERT\ub294 \ub300\ubd80\ubd84 \ud55c\uad6d\uc5b4 \uc704\ud0a4, \ub274\uc2a4 \uae30\uc0ac, \ucc45 \ub4f1 \uc798 \uc815\uc81c\ub41c \ub370\uc774\ud130\ub97c \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ud55c \ubaa8\ub378\uc785\ub2c8\ub2e4. \ud55c\ud3b8, \uc2e4\uc81c\ub85c NSMC\uc640 \uac19\uc740 \ub313\uae00\ud615 \ub370\uc774\ud130\uc14b\uc740 \uc815\uc81c\ub418\uc9c0 \uc54a\uc558\uace0 \uad6c\uc5b4\uccb4 \ud2b9\uc9d5\uc5d0 \uc2e0\uc870\uc5b4\uac00 \ub9ce\uc73c\uba70, \uc624\ud0c8\uc790 \ub4f1 \uacf5\uc2dd\uc801\uc778 \uae00\uc4f0\uae30\uc5d0\uc11c \ub098\ud0c0\ub098\uc9c0 \uc54a\ub294 \ud45c\ud604\ub4e4\uc774 \ube48\ubc88\ud558\uac8c \ub4f1\uc7a5\ud569\ub2c8\ub2e4.\n\nKcBERT\ub294 \uc704\uc640 \uac19\uc740 \ud2b9\uc131\uc758 \ub370\uc774\ud130\uc14b\uc5d0 \uc801\uc6a9\ud558\uae30 \uc704\ud574, \ub124\uc774\ubc84 \ub274\uc2a4\uc5d0\uc11c \ub313\uae00\uacfc \ub300\ub313\uae00\uc744 \uc218\uc9d1\ud574, \ud1a0\ud06c\ub098\uc774\uc800\uc640 BERT\ubaa8\ub378\uc744 \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ud55c Pretrained BERT \ubaa8\ub378\uc785\ub2c8\ub2e4.\n\nKcBERT\ub294 Huggingface\uc758 Transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \ud1b5\ud574 \uac04\ud3b8\ud788 \ubd88\ub7ec\uc640 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\ubcc4\ub3c4\uc758 \ud30c\uc77c \ub2e4\uc6b4\ub85c\ub4dc\uac00 \ud544\uc694\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.)\n\n## KcBERT Performance\n\n- Finetune \ucf54\ub4dc\ub294  \uc5d0\uc11c \ucc3e\uc544\ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n Size<br/>(\uc6a9\ub7c9)   **Naver NER**<br/>(F1)  **KorNLI**<br/>(acc)  **Question Pair**<br/>(acc) \n :---:  :--------------------:  :------------------:  :-------------------------: \n 417M           84.34                  74.85                     93.93            \n 1.2G           85.53                  76.99                     94.06            \n 351M           86.11                  79.00                     93.93            \n 1.03G          86.26                  79.92                     93.53            \n 614M         **87.31**              **80.89**                   94.19            \n 423M           86.87                  80.85                     94.20            \n 423M           87.02                  80.61                   **94.72**          \n 108M          84.13                  70.55                     92.48            \n\n\n\\*HanBERT\uc758 Size\ub294 Bert Model\uacfc Tokenizer DB\ub97c \ud569\uce5c \uac83\uc785\ub2c8\ub2e4.\n\n\\***config\uc758 \uc138\ud305\uc744 \uadf8\ub300\ub85c \ud558\uc5ec \ub3cc\ub9b0 \uacb0\uacfc\uc774\uba70, hyperparameter tuning\uc744 \ucd94\uac00\uc801\uc73c\ub85c \ud560 \uc2dc \ub354 \uc88b\uc740 \uc131\ub2a5\uc774 \ub098\uc62c \uc218 \uc788\uc2b5\ub2c8\ub2e4.**\n\n## How to use\n\n### Requirements\n\n- `pytorch <= 1.8.0`\n- `transformers ~= 3.0.1`\n  - `transformers ~= 4.0.0` \ub3c4 \ud638\ud658\ub429\ub2c8\ub2e4.\n- `emoji ~= 0.6.0`\n- `soynlp ~= 0.0.493`\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n\n# Base Model (108M)\n\ntokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"beomi/kcbert-base\")\n\n# Large Model (334M)\n\ntokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-large\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"beomi/kcbert-large\")\n```\n\n### Pretrain & Finetune Colab \ub9c1\ud06c \ubaa8\uc74c \n\n#### Pretrain Data\n\n- [\ub370\uc774\ud130\uc14b \ub2e4\uc6b4\ub85c\ub4dc(Kaggle, \ub2e8\uc77c\ud30c\uc77c, \ub85c\uadf8\uc778 \ud544\uc694)](\n- [\ub370\uc774\ud130\uc14b \ub2e4\uc6b4\ub85c\ub4dc(Github, \uc555\ucd95 \uc5ec\ub7ec\ud30c\uc77c, \ub85c\uadf8\uc778 \ubd88\ud544\uc694)](\n\n#### Pretrain Code\n\nColab\uc5d0\uc11c TPU\ub85c KcBERT Pretrain \ud574\ubcf4\uae30: <a href=\"\n  <img src=\" alt=\"Open In Colab\"/>\n</a>\n\n#### Finetune Samples\n\n**KcBERT-Base** NSMC Finetuning with PyTorch-Lightning (Colab) <a href=\"\n  <img src=\" alt=\"Open In Colab\"/>\n</a>\n\n**KcBERT-Large** NSMC Finetuning with PyTorch-Lightning (Colab) <a href=\"\n  <img src=\" alt=\"Open In Colab\"/>\n</a>\n\n> \uc704 \ub450 \ucf54\ub4dc\ub294 Pretrain \ubaa8\ub378(base, large)\uc640 batch size\ub9cc \ub2e4\ub97c \ubfd0, \ub098\uba38\uc9c0 \ucf54\ub4dc\ub294 \uc644\uc804\ud788 \ub3d9\uc77c\ud569\ub2c8\ub2e4.\n\n## Train Data & Preprocessing\n\n### Raw Data\n\n\ud559\uc2b5 \ub370\uc774\ud130\ub294 2019.01.01 ~ 2020.06.15 \uc0ac\uc774\uc5d0 \uc791\uc131\ub41c **\ub313\uae00 \ub9ce\uc740 \ub274\uc2a4** \uae30\uc0ac\ub4e4\uc758 **\ub313\uae00\uacfc \ub300\ub313\uae00**\uc744 \ubaa8\ub450 \uc218\uc9d1\ud55c \ub370\uc774\ud130\uc785\ub2c8\ub2e4.\n\n\ub370\uc774\ud130 \uc0ac\uc774\uc988\ub294 \ud14d\uc2a4\ud2b8\ub9cc \ucd94\ucd9c\uc2dc **\uc57d 15.4GB\uc774\uba70, 1\uc5b51\ucc9c\ub9cc\uac1c \uc774\uc0c1\uc758 \ubb38\uc7a5**\uc73c\ub85c \uc774\ub904\uc838 \uc788\uc2b5\ub2c8\ub2e4.\n\n### Preprocessing\n\nPLM \ud559\uc2b5\uc744 \uc704\ud574\uc11c \uc804\ucc98\ub9ac\ub97c \uc9c4\ud589\ud55c \uacfc\uc815\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\n1. \ud55c\uae00 \ubc0f \uc601\uc5b4, \ud2b9\uc218\ubb38\uc790, \uadf8\ub9ac\uace0 \uc774\ubaa8\uc9c0(\ud83e\udd73)\uae4c\uc9c0!\n\n   \uc815\uaddc\ud45c\ud604\uc2dd\uc744 \ud1b5\ud574 \ud55c\uae00, \uc601\uc5b4, \ud2b9\uc218\ubb38\uc790\ub97c \ud3ec\ud568\ud574 Emoji\uae4c\uc9c0 \ud559\uc2b5 \ub300\uc0c1\uc5d0 \ud3ec\ud568\ud588\uc2b5\ub2c8\ub2e4.\n\n   \ud55c\ud3b8, \ud55c\uae00 \ubc94\uc704\ub97c `\u3131-\u314e\uac00-\ud7a3` \uc73c\ub85c \uc9c0\uc815\ud574 `\u3131-\ud7a3` \ub0b4\uc758 \ud55c\uc790\ub97c \uc81c\uc678\ud588\uc2b5\ub2c8\ub2e4. \n\n2. \ub313\uae00 \ub0b4 \uc911\ubcf5 \ubb38\uc790\uc5f4 \ucd95\uc57d\n\n   `\u314b\u314b\u314b\u314b\u314b`\uc640 \uac19\uc774 \uc911\ubcf5\ub41c \uae00\uc790\ub97c `\u314b\u314b`\uc640 \uac19\uc740 \uac83\uc73c\ub85c \ud569\ucce4\uc2b5\ub2c8\ub2e4.\n\n3. Cased Model\n\n   KcBERT\ub294 \uc601\ubb38\uc5d0 \ub300\ud574\uc11c\ub294 \ub300\uc18c\ubb38\uc790\ub97c \uc720\uc9c0\ud558\ub294 Cased model\uc785\ub2c8\ub2e4.\n\n4. \uae00\uc790 \ub2e8\uc704 10\uae00\uc790 \uc774\ud558 \uc81c\uac70\n\n   10\uae00\uc790 \ubbf8\ub9cc\uc758 \ud14d\uc2a4\ud2b8\ub294 \ub2e8\uc77c \ub2e8\uc5b4\ub85c \uc774\ub904\uc9c4 \uacbd\uc6b0\uac00 \ub9ce\uc544 \ud574\ub2f9 \ubd80\ubd84\uc744 \uc81c\uc678\ud588\uc2b5\ub2c8\ub2e4.\n\n5. \uc911\ubcf5 \uc81c\uac70\n\n   \uc911\ubcf5\uc801\uc73c\ub85c \uc4f0\uc778 \ub313\uae00\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 \uc911\ubcf5 \ub313\uae00\uc744 \ud558\ub098\ub85c \ud569\ucce4\uc2b5\ub2c8\ub2e4.\n\n\uc774\ub97c \ud1b5\ud574 \ub9cc\ub4e0 \ucd5c\uc885 \ud559\uc2b5 \ub370\uc774\ud130\ub294 **12.5GB, 8.9\ucc9c\ub9cc\uac1c \ubb38\uc7a5**\uc785\ub2c8\ub2e4.\n\n\uc544\ub798 \uba85\ub839\uc5b4\ub85c pip\ub85c \uc124\uce58\ud55c \ub4a4, \uc544\ub798 clean\ud568\uc218\ub85c \ud074\ub9ac\ub2dd\uc744 \ud558\uba74 Downstream task\uc5d0\uc11c \ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc88b\uc544\uc9d1\ub2c8\ub2e4. (`[UNK]` \uac10\uc18c)\n\n```bash\npip install soynlp emoji\n```\n\n\uc544\ub798 `clean` \ud568\uc218\ub97c Text data\uc5d0 \uc0ac\uc6a9\ud574\uc8fc\uc138\uc694.\n\n```python\nimport re\nimport emoji\nfrom soynlp.normalizer import repeat_normalize\n\nemojis = list({y for x in emoji.UNICODE_EMOJI.values() for y in x.keys()})\nemojis = ''.join(emojis)\npattern = re.compile(f'[^ .,?!/@$%~\uff05\u00b7\u223c()\\x00-\\x7F\u3131-\u3163\uac00-\ud7a3{emojis}]+')\nurl_pattern = re.compile(\n    r'\n\ndef clean(x):\n    x = pattern.sub(' ', x)\n    x = url_pattern.sub('', x)\n    x = x.strip()\n    x = repeat_normalize(x, num_repeats=2)\n    return x\n```\n\n### Cleaned Data (Released on Kaggle)\n\n\uc6d0\ubcf8 \ub370\uc774\ud130\ub97c \uc704 `clean`\ud568\uc218\ub85c \uc815\uc81c\ud55c 12GB\ubd84\ub7c9\uc758 txt \ud30c\uc77c\uc744 \uc544\ub798 Kaggle Dataset\uc5d0\uc11c \ub2e4\uc6b4\ubc1b\uc73c\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4 :)\n\n\n\n\n## Tokenizer Train\n\nTokenizer\ub294 Huggingface\uc758 [Tokenizers]( \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \ud1b5\ud574 \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.\n\n\uadf8 \uc911 `BertWordPieceTokenizer` \ub97c \uc774\uc6a9\ud574 \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uace0, Vocab Size\ub294 `30000`\uc73c\ub85c \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.\n\nTokenizer\ub97c \ud559\uc2b5\ud558\ub294 \uac83\uc5d0\ub294 `1/10`\ub85c \uc0d8\ud50c\ub9c1\ud55c \ub370\uc774\ud130\ub85c \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uace0, \ubcf4\ub2e4 \uace8\uace0\ub8e8 \uc0d8\ud50c\ub9c1\ud558\uae30 \uc704\ud574 \uc77c\uc790\ubcc4\ub85c stratify\ub97c \uc9c0\uc815\ud55c \ub4a4 \ud591\uc2b5\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.\n\n## BERT Model Pretrain\n\n- KcBERT Base config\n\n```json\n{\n    \"max_position_embeddings\": 300,\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_act\": \"gelu\",\n    \"initializer_range\": 0.02,\n    \"num_hidden_layers\": 12,\n    \"type_vocab_size\": 2,\n    \"vocab_size\": 30000,\n    \"hidden_size\": 768,\n    \"attention_probs_dropout_prob\": 0.1,\n    \"directionality\": \"bidi\",\n    \"num_attention_heads\": 12,\n    \"intermediate_size\": 3072,\n    \"architectures\": [\n        \"BertForMaskedLM\"\n    ],\n    \"model_type\": \"bert\"\n}\n```\n\n- KcBERT Large config\n\n```json\n{\n    \"type_vocab_size\": 2,\n    \"initializer_range\": 0.02,\n    \"max_position_embeddings\": 300,\n    \"vocab_size\": 30000,\n    \"hidden_size\": 1024,\n    \"hidden_dropout_prob\": 0.1,\n    \"model_type\": \"bert\",\n    \"directionality\": \"bidi\",\n    \"pad_token_id\": 0,\n    \"layer_norm_eps\": 1e-12,\n    \"hidden_act\": \"gelu\",\n    \"num_hidden_layers\": 24,\n    \"num_attention_heads\": 16,\n    \"attention_probs_dropout_prob\": 0.1,\n    \"intermediate_size\": 4096,\n    \"architectures\": [\n        \"BertForMaskedLM\"\n    ]\n}\n```\n\nBERT Model Config\ub294 Base, Large \uae30\ubcf8 \uc138\ud305\uac12\uc744 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. (MLM 15% \ub4f1)\n\nTPU `v3-8` \uc744 \uc774\uc6a9\ud574 \uac01\uac01 3\uc77c, N\uc77c(Large\ub294 \ud559\uc2b5 \uc9c4\ud589 \uc911)\uc744 \uc9c4\ud589\ud588\uace0, \ud604\uc7ac Huggingface\uc5d0 \uacf5\uac1c\ub41c \ubaa8\ub378\uc740 1m(100\ub9cc) step\uc744 \ud559\uc2b5\ud55c ckpt\uac00 \uc5c5\ub85c\ub4dc \ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4.\n\n\ubaa8\ub378 \ud559\uc2b5 Loss\ub294 Step\uc5d0 \ub530\ub77c \ucd08\uae30 200k\uc5d0 \uac00\uc7a5 \ube60\ub974\uac8c Loss\uac00 \uc904\uc5b4\ub4e4\ub2e4 400k\uc774\ud6c4\ub85c\ub294 \uc870\uae08\uc529 \uac10\uc18c\ud558\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n- Base Model Loss\n\n![KcBERT-Base Pretraining Loss](\n\n- Large Model Loss\n\n![KcBERT-Large Pretraining Loss](\n\n\ud559\uc2b5\uc740 GCP\uc758 TPU v3-8\uc744 \uc774\uc6a9\ud574 \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uace0, \ud559\uc2b5 \uc2dc\uac04\uc740 Base Model \uae30\uc900 2.5\uc77c\uc815\ub3c4 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4. Large Model\uc740 \uc57d 5\uc77c\uc815\ub3c4 \uc9c4\ud589\ud55c \ub4a4 \uac00\uc7a5 \ub0ae\uc740 loss\ub97c \uac00\uc9c4 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub85c \uc815\ud588\uc2b5\ub2c8\ub2e4.\n\n## Example\n\n### HuggingFace MASK LM\n\n[HuggingFace kcbert-base \ubaa8\ub378]( \uc5d0\uc11c \uc544\ub798\uc640 \uac19\uc774 \ud14c\uc2a4\ud2b8 \ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n![\uc624\ub298\uc740 \ub0a0\uc528\uac00 \"\uc88b\ub124\uc694\", KcBERT-Base](\n\n\ubb3c\ub860 [kcbert-large \ubaa8\ub378]( \uc5d0\uc11c\ub3c4 \ud14c\uc2a4\ud2b8 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n![image-20200806160624340](\n\n\n\n### NSMC Binary Classification\n\n[\ub124\uc774\ubc84 \uc601\ud654\ud3c9 \ucf54\ud37c\uc2a4]( \ub370\uc774\ud130\uc14b\uc744 \ub300\uc0c1\uc73c\ub85c Fine Tuning\uc744 \uc9c4\ud589\ud574 \uc131\ub2a5\uc744 \uac04\ub2e8\ud788 \ud14c\uc2a4\ud2b8\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n\nBase Model\uc744 Fine Tune\ud558\ub294 \ucf54\ub4dc\ub294 <a href=\"\n  <img src=\" alt=\"Open In Colab\"/>\n</a> \uc5d0\uc11c \uc9c1\uc811 \uc2e4\ud589\ud574\ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\nLarge Model\uc744 Fine Tune\ud558\ub294 \ucf54\ub4dc\ub294 <a href=\"\n  <img src=\" alt=\"Open In Colab\"/>\n</a> \uc5d0\uc11c \uc9c1\uc811 \uc2e4\ud589\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n- GPU\ub294 P100 x1\ub300 \uae30\uc900 1epoch\uc5d0 2-3\uc2dc\uac04, TPU\ub294 1epoch\uc5d0 1\uc2dc\uac04 \ub0b4\ub85c \uc18c\uc694\ub429\ub2c8\ub2e4.\n- GPU RTX Titan x4\ub300 \uae30\uc900 30\ubd84/epoch \uc18c\uc694\ub429\ub2c8\ub2e4.\n- \uc608\uc2dc \ucf54\ub4dc\ub294 [pytorch-lightning]( \uac1c\ubc1c\ud588\uc2b5\ub2c8\ub2e4.\n\n#### \uc2e4\ud5d8\uacb0\uacfc\n\n- KcBERT-Base Model \uc2e4\ud5d8\uacb0\uacfc: Val acc `.8905`\n\n  ![KcBERT Base finetune on NSMC](\n\n- KcBERT-Large Model \uc2e4\ud5d8 \uacb0\uacfc: Val acc `.9089`\n\n  ![image-20200806190242834](\n\n> \ub354 \ub2e4\uc591\ud55c Downstream Task\uc5d0 \ub300\ud574 \ud14c\uc2a4\ud2b8\ub97c \uc9c4\ud589\ud558\uace0 \uacf5\uac1c\ud560 \uc608\uc815\uc785\ub2c8\ub2e4.\n\n## \uc778\uc6a9\ud45c\uae30/Citation\n\nKcBERT\ub97c \uc778\uc6a9\ud558\uc2e4 \ub54c\ub294 \uc544\ub798 \uc591\uc2dd\uc744 \ud1b5\ud574 \uc778\uc6a9\ud574\uc8fc\uc138\uc694.\n\n```\n@inproceedings{lee2020kcbert,\n  title={KcBERT: Korean Comments BERT},\n  author={Lee, Junbum},\n  booktitle={Proceedings of the 32nd Annual Conference on Human and Cognitive Language Technology},\n  pages={437--440},\n  year={2020}\n}\n```\n\n- \ub17c\ubb38\uc9d1 \ub2e4\uc6b4\ub85c\ub4dc \ub9c1\ud06c:  (*\ud639\uc740  )\n\n## Acknowledgement\n\nKcBERT Model\uc744 \ud559\uc2b5\ud558\ub294 GCP/TPU \ud658\uacbd\uc740 [TFRC]( \ud504\ub85c\uadf8\ub7a8\uc758 \uc9c0\uc6d0\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4.\n\n\ubaa8\ub378 \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c \ub9ce\uc740 \uc870\uc5b8\uc744 \uc8fc\uc2e0 [Monologg]( \ub2d8 \uac10\uc0ac\ud569\ub2c8\ub2e4 :)\n\n## Reference\n\n### Github Repos\n\n- [BERT by Google](\n- [KoBERT by SKT](\n- [KoELECTRA by Monologg](\n\n- [Transformers by Huggingface](\n- [Tokenizers by Hugginface](\n\n### Papers\n\n- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](\n\n### Blogs\n\n- [Monologg\ub2d8\uc758 KoELECTRA \ud559\uc2b5\uae30](\n- [Colab\uc5d0\uc11c TPU\ub85c BERT \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\uc2dc\ud0a4\uae30 - Tensorflow/Google ver.](\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of beomi/kcbert-base?", "answers": [{"text": "bert", "answer_start": 2958, "answer_end": 2961}]}]}]}, {"title": "bergum/xtremedistil-l6-h384-go-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ndatasets:\n- go_emotions\n\nmetrics:\n- accuracy\nmodel-index:\n- name: xtremedistil-emotion\n  results:\n  - task:\n      name: Multi Label Text Classification\n      type: multi_label_classification\n    dataset:\n      name: go_emotions\n      type: emotion\n      args: default\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: NaN\n      \n---\n# xtremedistil-l6-h384-go-emotion\nThis model is a fine-tuned version of [microsoft/xtremedistil-l6-h384-uncased]( on the \n[go_emotions dataset]( \n\nSee notebook for how the model was trained and converted to ONNX format [![Training Notebook](\n\nThis model is deployed to [aiserv.cloud]( for live demo of the model. \n\nSee [ for how to reproduce. \n\n\n### Training hyperparameters\n- batch size 128 \n- learning_rate=3e-05\n- epocs 4 \n<pre>\n  Num examples = 211225\n  Num Epochs = 4\n  Instantaneous batch size per device = 128\n  Total train batch size (w. parallel, distributed & accumulation) = 128\n  Gradient Accumulation steps = 1\n  Total optimization steps = 6604\n [6604/6604 53:23, Epoch 4/4]\nStep\tTraining Loss\n500\t0.263200\n1000\t0.156900\n1500\t0.152500\n2000\t0.145400\n2500\t0.140500\n3000\t0.135900\n3500\t0.132800\n4000\t0.129400\n4500\t0.127200\n5000\t0.125700\n5500\t0.124400\n6000\t0.124100\n6500\t0.123400\n</pre>", "qas": []}]}, {"title": "bergurth/XLMR-ENIS-finetuned-ner", "paragraphs": [{"context": "---\nlicense: agpl-3.0\ntags:\n- generated_from_trainer\ndatasets:\n- mim_gold_ner\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nwidget:\n- text: B\u00f3nus fe\u00f0garnir J\u00f3hannes J\u00f3nsson og J\u00f3n \u00c1sgeir J\u00f3hannesson opnu\u00f0u fyrstu B\u00f3nusb\u00fa\u00f0ina \u00ed 400 fermetra h\u00fasn\u00e6\u00f0i vi\u00f0 Sk\u00fatuvog laugardaginn 8. apr\u00edl 1989\nmodel-index:\n- name: XLMR-ENIS-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: mim_gold_ner\n      type: mim_gold_ner\n      args: mim-gold-ner\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.861851332398317\n    - name: Recall\n      type: recall\n      value: 0.8384309266628767\n    - name: F1\n      type: f1\n      value: 0.849979828251974\n    - name: Accuracy\n      type: accuracy\n      value: 0.9830620929487668\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# XLMR-ENIS-finetuned-ner\n\nThis model is a fine-tuned version of [vesteinn/XLMR-ENIS]( on the mim_gold_ner dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0938\n- Precision: 0.8619\n- Recall: 0.8384\n- F1: 0.8500\n- Accuracy: 0.9831\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0983           0.8061  0.9795   |\n 2.0    0.0991           0.8235  0.9811   |\n 3.0    0.0938           0.8384  0.9831   |\n\n\n### Framework versions\n\n- Transformers 4.11.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of bergurth/XLMR-ENIS-finetuned-ner?", "answers": [{"text": "token-classification", "answer_start": 399, "answer_end": 418}]}]}]}, {"title": "bgoel4132/tweet-disaster-classifier", "paragraphs": [{"context": "---\ntags: autonlp\nlanguage: en\nwidget:\n- text: \"I love AutoNLP \ud83e\udd17\"\ndatasets:\n- bgoel4132/autonlp-data-tweet-disaster-classifier\nco2_eq_emissions: 27.22397099134103\n---\n\n# Model Trained Using AutoNLP\n\n- Problem type: Multi-class Classification\n- Model ID: 28716412\n- CO2 Emissions (in grams): 27.22397099134103\n\n## Validation Metrics\n\n- Loss: 0.4146720767021179\n- Accuracy: 0.8066924731182795\n- Macro F1: 0.7835463282531184\n- Micro F1: 0.8066924731182795\n- Weighted F1: 0.7974252447208724\n- Macro Precision: 0.8183917344767431\n- Micro Precision: 0.8066924731182795\n- Weighted Precision: 0.8005510296861892\n- Macro Recall: 0.7679676081852519\n- Micro Recall: 0.8066924731182795\n- Weighted Recall: 0.8066924731182795\n\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoNLP\"}' \n```\n\nOr Python API:\n\n```\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bgoel4132/autonlp-tweet-disaster-classifier-28716412\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bgoel4132/autonlp-tweet-disaster-classifier-28716412\", use_auth_token=True)\n\ninputs = tokenizer(\"I love AutoNLP\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n```", "qas": []}]}, {"title": "bhadresh-savani/distilbert-base-uncased-go-emotion", "paragraphs": [{"context": "---\nlanguage: \n- en\nthumbnail: \ntags:\n- text-classification\n- go-emotion\n- pytorch\nlicense: apache-2.0\ndatasets:\n- go_emotions\nmetrics:\n- Accuracy\n---\n# Distilbert-Base-Uncased-Go-Emotion\n\n## Model description:\n\n**Not working fine**\n\n## Training Parameters:\n```\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 15831\n```\n\n## TrainOutput:\n```\n'train_loss': 0.105500\n```\n\n## Evalution Output:\n```\n 'eval_accuracy_thresh': 0.962023913860321,\n 'eval_loss': 0.11090277135372162,\n```\n\n## Colab Notebook:\n[Notebook](", "qas": [{"id": "q2", "question": "What is the model task of bhadresh-savani/distilbert-base-uncased-go-emotion?", "answers": [{"text": "text-classification", "answer_start": 40, "answer_end": 58}]}]}]}, {"title": "bhadresh-savani/distilbert-base-uncased-sentiment-sst2", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- sst2\n---\n\n# distilbert-base-uncased-sentiment-sst2\nThis model will be able to identify positivity or negativity present in the sentence\n## Dataset:\nThe Stanford Sentiment Treebank from GLUE\n\n## Results:\n```\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_accuracy           =     0.9094\n  eval_loss               =     0.3514\n  eval_runtime            = 0:00:03.60\n  eval_samples            =        872\n  eval_samples_per_second =    242.129\n  eval_steps_per_second   =     30.266\n```", "qas": [{"id": "q1", "question": "What is the model architecture of bhadresh-savani/distilbert-base-uncased-sentiment-sst2?", "answers": [{"text": "distilbert", "answer_start": 61, "answer_end": 70}]}]}]}, {"title": "bhadresh-savani/roberta-base-emotion", "paragraphs": [{"context": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-classification\n- emotion\n- pytorch\ndatasets:\n- emotion\nmetrics:\n- Accuracy, F1 Score\nthumbnail: \nmodel-index:\n- name: bhadresh-savani/roberta-base-emotion\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: emotion\n      type: emotion\n      config: default\n      split: test\n    metrics:\n    - type: accuracy\n      value: 0.931\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjg5OTI4ZTlkY2VmZjYzNGEzZGQ3ZjczYzY5YjJmMGVmZDQ4ZWNiYTAyZTJiZjlmMTU2MjE1NTllMWFhYzU0MiIsInZlcnNpb24iOjF9.dc44cEsbu900M2s64GyVIWKPagBzwI-dPlfvh0NGyJFMGKOcypke9P2ary9fBZITrH3UF6lza3sCh7vWYZFHBQ\n    - type: precision\n      value: 0.9168321948556312\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2EzYTcxNTExNGU1MmFiZjE3NGE5MDIyMDU2M2U3OGExOTdjZDE5YWU2NDhmOTJlYWMzY2NkN2U5MmRmZTE0MiIsInZlcnNpb24iOjF9.4U7vJ3ALdUUxySMhVeb4Qa1tSp3wphSIZkRYNMujz-KrOZW8kkcmCde3ioStBg3Qqyf1powYd88uk1R7DuWRBA\n    - type: precision\n      value: 0.931\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjhmZGRlYWE5ZTAzMmJiMzlmMWZiM2VlYjdiNzI0NjVmN2M2YzcxM2EzYTg0OTFiZTE1MjVmNzE5NGEzYTg2ZCIsInZlcnNpb24iOjF9.8eCHAK0rlZWnhBNQdh9kcuAeItmDUAgK3KkZ7eC-GyYhi4HT5dZiS6btcC5EjkYVOS4czcjzqxfVz4PuZgtLDQ\n    - type: precision\n      value: 0.9357445689014415\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhZTdkNzYzMjhjZjc4MTAxNWZiYjgzMjhhNjRiZWRmYjc5YTA0NTQ1MzllMTYxMTVkMDk4OTE0ZGEyMTNhMiIsInZlcnNpb24iOjF9.YIZfj2Eo1nMX2GVSfqJy-Cp7VBubfUh2LuOnU60sG5Lci8FdlNbAanS1IzAyxU3U29lqiTasxfS_yrwAj5cmBQ\n    - type: recall\n      value: 0.8743657671177089\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2Y2YTcyNzMwYzZiMmM1Yzc4YWZhNDM3ZDQyMjI1NWZhMjQyNmU5NTA0YmE2ZDBiZmY1MmUyZWRlMjRhMjFmYSIsInZlcnNpb24iOjF9.XKlFy_Cx4T4l7Otd8aAwWcI-fJ_dJ6V1Kp3uZm6OWjwCb1Do6mSdPFfwiMeBZZyfEIsNBnguegssZvHsOfTSAQ\n    - type: recall\n      value: 0.931\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzgzN2JkNzAzZDRjNjJmZjNkY2RmYzVkMWEzYTMzZDU4NzJlYzBmOWE4MTU0MGU0MTJhM2JjZDdjODhlZDExOCIsInZlcnNpb24iOjF9.9tSVB4yNBdFXpH3equwo1ZaEnVUktO6lm93UEJ-luKhxo6wgS54OLjgDq7IpJYwa3lvYyjy-sxzQEe9ri31WAg\n    - type: recall\n      value: 0.931\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGVhZTIyMmVmOTU1YWNjMmZiZjNmOTNlNzlhZTk3NjhlZmMwZGFkZWQxZTlhZWUwZGQyN2JhOWQyNWQ3MTVhOCIsInZlcnNpb24iOjF9.2odv2fK7zH0_S_7wC3obONzjxOipDdjWvddhnGdMnrIN6CiZwLp7XgizpqcWbwAQ_9YJwjC-6wXpbq2jTvN0Bw\n    - type: f1\n      value: 0.8821236522209227\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDI0YTUxOTA2M2ZjNGM1OTJlZDAzZTAxNTg4YjY3OWNmMjNmMTk0YWRjZTE2Y2ZmYWI1ZmU3ZmJmNzNjMjBlOCIsInZlcnNpb24iOjF9.P5-TbuEUrCtX9H7F-tKn8LI1RBPhoJwjJm_l853WTSzdLioThAtIK5HBG0xgXT2uB0Q8v94qH2b8cz1j_WonDg\n    - type: f1\n      value: 0.931\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYjNmNDgyMmFjODYwNjcwOTJiOGM2N2YwYjUyMDk5Yjk2Y2I3NmFmZGFhYjU0NGM2OGUwZmRjNjcxYTU3YzgzNSIsInZlcnNpb24iOjF9.2ZoRJwQWVIcl_Ykxce1MnZ3mSxBGxGeNYFPxt9mivo9yTi3gUE7ua6JRpVEOnOUbevlWxVkUUNnmOPFqBN1sCQ\n    - type: f1\n      value: 0.9300782840205046\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGE1OTcxNmNmMjQ3ZDAzYzk0N2Q1MGFjM2VhNWMyYmRjY2E3ZThjODExOTNlNWMxYzdlMWM2MDBiMTZhY2M2OSIsInZlcnNpb24iOjF9.r63SEArCiFB5m0ccV2q_t5uSOtjVnWdz4PfvCYUchm0JlrRC9YAm5oWKeO419wdyFY4rZFe014yv7sRcV-CgBQ\n    - type: loss\n      value: 0.15155883133411407\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2M4MmVlNjAzZjhiMWJlNWQxMDg5ZTRiYjFlZGYyMGMyYzU4M2IwY2E1M2E2MzA5NmU5ZjgwZTZmMDI5YjgzMyIsInZlcnNpb24iOjF9.kjgFJohkTxLKtzHJDlBvd6qolGQDSZLbrDE7C07xNGmarhTLc_A3MmLeC4MmQGOl1DxfnHflImIkdqPylyylDA\n---\n# robert-base-emotion\n\n## Model description:\n[roberta]( is Bert with better hyperparameter choices so they said it's Robustly optimized Bert during pretraining.\n\n[roberta-base]( finetuned on the emotion dataset using HuggingFace Trainer with below Hyperparameters\n```\n learning rate 2e-5, \n batch size 64,\n num_train_epochs=8,\n```\n\n## Model Performance Comparision on Emotion Dataset from Twitter:\n\n Accuracy   Test Sample per Second |\n ---  --- |\n 93.8  398.69 |\n 94.05  190.152 |\n 93.95  195.639 |\n 93.6  182.794 |\n\n## How to Use the model:\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='bhadresh-savani/roberta-base-emotion', return_all_scores=True)\nprediction = classifier(\"I love using transformers. The best part is wide range of support and its easy to use\", )\nprint(prediction)\n\n\"\"\"\nOutput:\n[[\n{'label': 'sadness', 'score': 0.002281982684507966}, \n{'label': 'joy', 'score': 0.9726489186286926}, \n{'label': 'love', 'score': 0.021365027874708176}, \n{'label': 'anger', 'score': 0.0026395076420158148}, \n{'label': 'fear', 'score': 0.0007162453257478774}, \n{'label': 'surprise', 'score': 0.0003483477921690792}\n]]\n\"\"\"\n```\n\n## Dataset:\n[Twitter-Sentiment-Analysis](\n\n## Training procedure\n[Colab Notebook](\nfollow the above notebook by changing the model name to roberta\n\n## Eval results\n```json\n{\n 'test_accuracy': 0.9395,\n 'test_f1': 0.9397328860104454,\n 'test_loss': 0.14367154240608215,\n 'test_runtime': 10.2229,\n 'test_samples_per_second': 195.639,\n 'test_steps_per_second': 3.13\n }\n```\n\n## Reference:\n* [Natural Language Processing with Transformer By Lewis Tunstall, Leandro von Werra, Thomas Wolf](", "qas": [{"id": "q1", "question": "What is the model architecture of bhadresh-savani/roberta-base-emotion?", "answers": [{"text": "roberta", "answer_start": 186, "answer_end": 192}]}, {"id": "q2", "question": "What is the model task of bhadresh-savani/roberta-base-emotion?", "answers": [{"text": "text-classification", "answer_start": 47, "answer_end": 65}]}]}]}, {"title": "bhan/distilbert-base-uncased-finetuned-squad", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: distilbert-base-uncased-finetuned-squad\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-squad\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the squad dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    5.8757          |\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.17.0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of bhan/distilbert-base-uncased-finetuned-squad?", "answers": [{"text": "distilbert", "answer_start": 94, "answer_end": 103}]}]}]}, {"title": "bhavikardeshna/multilingual-bert-base-cased-chinese", "paragraphs": [{"context": "# BibTeX entry and citation info\n\n```\n@misc{pandya2021cascading,\n      title={Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages}, \n      author={Hariom A. Pandya and Bhavik Ardeshna and Dr. Brijesh S. Bhatt},\n      year={2021},\n      eprint={2112.09866},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": []}]}, {"title": "bhavikardeshna/multilingual-bert-base-cased-english", "paragraphs": [{"context": "# BibTeX entry and citation info\n\n```\n@misc{pandya2021cascading,\n      title={Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages}, \n      author={Hariom A. Pandya and Bhavik Ardeshna and Dr. Brijesh S. Bhatt},\n      year={2021},\n      eprint={2112.09866},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": []}]}, {"title": "bhavikardeshna/multilingual-bert-base-cased-spanish", "paragraphs": [{"context": "# BibTeX entry and citation info\n\n```\n@misc{pandya2021cascading,\n      title={Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages}, \n      author={Hariom A. Pandya and Bhavik Ardeshna and Dr. Brijesh S. Bhatt},\n      year={2021},\n      eprint={2112.09866},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": []}]}, {"title": "bhavikardeshna/multilingual-bert-base-cased-vietnamese", "paragraphs": [{"context": "# BibTeX entry and citation info\n\n```\n@misc{pandya2021cascading,\n      title={Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages}, \n      author={Hariom A. Pandya and Bhavik Ardeshna and Dr. Brijesh S. Bhatt},\n      year={2021},\n      eprint={2112.09866},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": []}]}, {"title": "bhavikardeshna/xlm-roberta-base-arabic", "paragraphs": [{"context": "# BibTeX entry and citation info\n\n```\n@misc{pandya2021cascading,\n      title={Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages}, \n      author={Hariom A. Pandya and Bhavik Ardeshna and Dr. Brijesh S. Bhatt},\n      year={2021},\n      eprint={2112.09866},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": []}]}, {"title": "bhavikardeshna/xlm-roberta-base-german", "paragraphs": [{"context": "# BibTeX entry and citation info\n\n```\n@misc{pandya2021cascading,\n      title={Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages}, \n      author={Hariom A. Pandya and Bhavik Ardeshna and Dr. Brijesh S. Bhatt},\n      year={2021},\n      eprint={2112.09866},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": []}]}, {"title": "bhavikardeshna/xlm-roberta-base-hindi", "paragraphs": [{"context": "# BibTeX entry and citation info\n\n```\n@misc{pandya2021cascading,\n      title={Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages}, \n      author={Hariom A. Pandya and Bhavik Ardeshna and Dr. Brijesh S. Bhatt},\n      year={2021},\n      eprint={2112.09866},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": []}]}, {"title": "chip/DialoGPT-small-chizuru", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\nChizuru Ichinose DialoGPT Model. ", "qas": [{"id": "q2", "question": "What is the model task of chip/DialoGPT-small-chizuru?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "chitra/finetuned-adversarial-paraphrase-model", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: finetuned-adversarial-paraphrase-model\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# finetuned-adversarial-paraphrase-model\n\nThis model is a fine-tuned version of [coderpotter/adversarial-paraphrasing-detector]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 7.5680\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    5.4633          |\n 2.0    6.0352          |\n 3.0    7.5680          |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": []}]}, {"title": "chkla/roberta-argument", "paragraphs": [{"context": "---\nlanguage: en\nwidget:\n- text: \"It has been determined that the amount of greenhouse gases have decreased by almost half because of the prevalence in the utilization of nuclear power.\"\n---\n\n### Welcome to RoBERTArg!\n\n\ud83e\udd16 **Model description**\n\nThis model was trained on ~25k heterogeneous manually annotated sentences (\ud83d\udcda [Stab et al. 2018]( of controversial topics to classify text into one of two labels: \ud83c\udff7 **NON-ARGUMENT** (0) and **ARGUMENT** (1).\n\n\ud83d\uddc3 **Dataset**\n\nThe dataset (\ud83d\udcda Stab et al. 2018) consists of **ARGUMENTS** (\\~11k) that either support or oppose a topic if it includes a relevant reason for supporting or opposing the topic, or as a **NON-ARGUMENT** (\\~14k) if it does not include reasons. The authors focus on controversial topics, i.e., topics that include \"an obvious polarity to the possible outcomes\" and compile a final set of eight controversial topics: _abortion, school uniforms, death penalty, marijuana legalization, nuclear energy, cloning, gun control, and minimum wage_.\n\n ARGUMENT \n----\n 2213 \n 325 \n 325 \n 325 \n 325 \n 325 \n 325 \n 325 \n\n\ud83c\udfc3\ud83c\udffc\u200d\u2642\ufe0f**Model training**\n\n**RoBERTArg** was fine-tuned on a RoBERTA (base) pre-trained model from HuggingFace using the HuggingFace trainer with the following hyperparameters:\n\n```\ntraining_args = TrainingArguments(\n    num_train_epochs=2,\n    learning_rate=2.3102e-06,\n    seed=8,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n)\n```\n\n\ud83d\udcca **Evaluation**\n\nThe model was evaluated on an evaluation set (20%):\n\n Acc  R arg  P arg \n------------\n 0.8193  0.8463  0.7623 \n\nShowing the **confusion matrix** using again the evaluation set:\n\n ARGUMENT \n----\n 2213 \n 325 \n\n\u26a0\ufe0f **Intended Uses & Potential Limitations**\n\nThe model can only be a starting point to dive into the exciting field of argument mining. But be aware. An argument is a complex structure, with multiple dependencies. Therefore, the model may perform less well on different topics and text types not included in the training set.\n\nEnjoy and stay tuned! \ud83d\ude80\n\n\ud83d\udc26 Twitter: [@chklamm](", "qas": []}]}, {"title": "chmanoj/xls-r-1B-te", "paragraphs": [{"context": "---\nlanguage:\n- te\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- openslr_SLR66\n- generated_from_trainer\n- robust-speech-event\n- hf-asr-leaderboard\ndatasets:\n- openslr\n- SLR66\nmetrics:\n- wer\nmodel-index:\n- name: xls-r-1B-te\n  results:\n  - task:\n      type: automatic-speech-recognition\n      name: Speech Recognition\n    dataset:\n      type: openslr\n      name: Open SLR\n      args: SLR66\n    metrics:\n    - type: wer\n      value: 20.624\n      name: Test WER\n    - type: cer\n      value: 3.979\n      name: Test CER\n    - type: wer\n      value: 26.14777618364419\n      name: Test WER (without LM)\n    - type: cer\n      value: 4.932543184970369\n      name: Test CER (without LM)\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-1b]( on the OPENSLR_SLR66 - NA dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3119\n- Wer: 0.2613\n\n\n### Evaluation metrics\n\n Split   Value     |\n:------::---------:|\n Train   5.36      |\n Train   1.11      |\n Test    26.14     |\n Test    4.93      |\n Train   5.04      |\n Train   1.07      |\n Test    20.69     |\n Test    3.986     |\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 4\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 2000\n- num_epochs: 150.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch   Validation Loss \n:------::---------------:\n 4.8     3.0125          \n 9.61    0.8681          \n 14.42   0.6256          \n 19.23   0.5244          \n 24.04   0.4585          \n 28.84   0.4072          \n 33.65   0.3590          \n 38.46   0.3678          \n 43.27   0.3474          \n 48.08   0.3224          \n 52.88   0.3233          \n 57.69   0.3029          \n 62.5    0.3195          \n 67.31   0.3004          \n 72.11   0.2826          \n 76.92   0.2962          \n 81.73   0.2990          \n 86.54   0.2834          \n 91.34   0.2886          \n 96.15   0.3093          \n 100.96  0.3123          \n 105.77  0.2968          \n 110.57  0.3106          \n 115.38  0.3030          \n 120.19  0.2964          \n 125.0   0.3101          \n 129.8   0.3063          \n 134.61  0.3082          \n 139.42  0.3121          \n 144.23  0.3105          \n 149.04  0.3114          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.17.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of chmanoj/xls-r-1B-te?", "answers": [{"text": "wav2vec2", "answer_start": 934, "answer_end": 941}]}, {"id": "q2", "question": "What is the model task of chmanoj/xls-r-1B-te?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}]}]}, {"title": "chmanoj/xls-r-2B-te", "paragraphs": [{"context": "---\nlanguage:\n- te\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- openslr_SLR66\n- generated_from_trainer\n- robust-speech-event\n- hf-asr-leaderboard\ndatasets:\n- openslr\n- SLR66\nmetrics:\n- wer\nmodel-index:\n- name: xls-r-1B-te\n  results:\n  - task:\n      type: automatic-speech-recognition\n      name: Speech Recognition\n    dataset:\n      type: openslr\n      name: Open SLR\n      args: SLR66\n    metrics:\n    - type: wer\n      value: 0.51\n      name: Test WER\n    - type: cer\n      value: 0.097\n      name: Test CER\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-2b]( on the OPENSLR_SLR66 - NA dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4253\n- Wer: 0.5109\n\n\n### Evaluation metrics\n\n Split   Value     |\n:------::---------:|\n Train        |\n Train        |\n Test         |\n Test         |\n Train        |\n Train        |\n Test         |\n Test         |\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- gradient_accumulation_steps: 12\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- learning_rate: 3e-6\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 2000\n- num_epochs: 150.0\n- hidden_dropout: 0.15\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.17.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of chmanoj/xls-r-2B-te?", "answers": [{"text": "wav2vec2", "answer_start": 770, "answer_end": 777}]}, {"id": "q2", "question": "What is the model task of chmanoj/xls-r-2B-te?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}]}]}, {"title": "chmanoj/xls-r-300m-sv", "paragraphs": [{"context": "---\nlanguage:\n- sv-SE\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- mozilla-foundation/common_voice_7_0\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: ''\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - SV-SE dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8004\n- Wer: 0.7139\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 7.5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 2000\n- num_epochs: 10.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.45   1.7698          \n 2.91   1.0890          \n 4.36   1.0878          \n 5.81   1.1501          \n 7.27   1.0452          \n 8.72   0.9153          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.0+cu113\n- Datasets 1.18.1.dev0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of chmanoj/xls-r-300m-sv?", "answers": [{"text": "wav2vec2", "answer_start": 450, "answer_end": 457}]}, {"id": "q2", "question": "What is the model task of chmanoj/xls-r-300m-sv?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 50, "answer_end": 77}]}]}]}, {"title": "chmanoj/xls-r-300m-te", "paragraphs": [{"context": "---\nlanguage:\n- te\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- openslr_SLR66\n- generated_from_trainer\n- robust-speech-event\n- hf-asr-leaderboard\ndatasets:\n- openslr\n- SLR66\nmetrics:\n- wer\nmodel-index:\n- name: xls-r-300m-te\n  results:\n  - task:\n      type: automatic-speech-recognition\n      name: Speech Recognition\n    dataset:\n      type: openslr\n      name: Open SLR\n      args: SLR66\n    metrics:\n    - type: wer\n      value: 24.695121951219512\n      name: Test WER\n    - type: cer\n      value: 4.861934182322532\n      name: Test CER\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the OPENSLR_SLR66 - NA dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2680\n- Wer: 0.3467\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 7.5e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 2000\n- num_epochs: 10.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 4.81   1.5676          \n 9.61   0.4693          \n 14.42  0.4368          \n 19.23  0.4360          \n 24.04  0.4213          \n 28.84  0.4329          \n 33.65  0.4074          \n 38.46  0.3866          \n 43.27  0.3860          \n 48.08  0.3590          \n 52.88  0.3283          \n 57.69  0.3162          \n 62.5   0.3126          \n 67.31  0.2990          \n 72.12  0.2870          \n 76.92  0.2791          \n 81.73  0.2770          \n 86.54  0.2841          \n 91.35  0.2721          \n 96.15  0.2681          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.17.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of chmanoj/xls-r-300m-te?", "answers": [{"text": "wav2vec2", "answer_start": 798, "answer_end": 805}]}, {"id": "q2", "question": "What is the model task of chmanoj/xls-r-300m-te?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}]}]}, {"title": "chmanoj/xls-r-demo-test", "paragraphs": [{"context": "---\nlanguage:\n- ab\ntags:\n- automatic-speech-recognition\n- mozilla-foundation/common_voice_7_0\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: ''\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# \n\nThis model is a fine-tuned version of [hf-test/xls-r-dummy]( on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - AB dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 156.8786\n- Wer: 1.3460\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 2\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- training_steps: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.0+cu113\n- Datasets 1.18.1.dev0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of chmanoj/xls-r-demo-test?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 27, "answer_end": 54}]}]}]}, {"title": "chompk/wav2vec2-large-xlsr-thai-tokenized", "paragraphs": [{"context": "---\nlanguage: th\ndatasets:\n- common_voice\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning\nlicense: apache-2.0\n---\n\n# Wav2Vec2-Large-XLSR-53 in Thai Language (Train with deepcut tokenizer)\n", "qas": [{"id": "q2", "question": "What is the model task of chompk/wav2vec2-large-xlsr-thai-tokenized?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 58, "answer_end": 85}]}, {"id": "q3", "question": "What is the model category of chompk/wav2vec2-large-xlsr-thai-tokenized?", "answers": [{"text": "audio", "answer_start": 50, "answer_end": 54}]}]}]}, {"title": "chopey/testmntdv", "paragraphs": [{"context": "Test English-Dhivehi/Dhivehi-English NMT\n\nWould need a lot more data to get accurate translations. ", "qas": []}]}, {"title": "chrisjay/fonxlsr", "paragraphs": [{"context": "---\nlanguage: fon\ndatasets:\n- fon_dataset \nmetrics:\n- wer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\n- hf-asr-leaderboard\nlicense: apache-2.0\nmodel-index:\n- name: Fon XLSR Wav2Vec2 Large 53\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: fon\n      type: fon_dataset\n      args: fon\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 14.97\n---\n# Wav2Vec2-Large-XLSR-53-Fon\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( on [Fon (or Fongbe)]( using the [Fon Dataset](\n\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\n\nThe model can be used directly (without a language model) as follows:\n\n```python\nimport json\nimport random\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\n\n#Load test_dataset from saved files in folder\nfrom datasets import load_dataset, load_metric\n\n#for test\nfor root, dirs, files in os.walk(test/):\n  test_dataset= load_dataset(\"json\", data_files=[os.path.join(root,i) for i in files],split=\"train\")\n\n#Remove unnecessary chars\nchars_to_ignore_regex = '[\\\\,\\\\?\\\\.\\\\!\\\\-\\\\;\\\\:\\\\\"\\\\\u201c\\\\%\\\\\u2018\\\\\u201d]'\ndef remove_special_characters(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n    return batch\n\ntest_dataset = test_dataset.map(remove_special_characters)\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"chrisjay/wav2vec2-large-xlsr-53-fon\") \nmodel = Wav2Vec2ForCTC.from_pretrained(\"chrisjay/wav2vec2-large-xlsr-53-fon\") \n\n#No need for resampling because audio dataset already at 16kHz\n#resampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n  speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n  batch[\"speech\"]=speech_array.squeeze().numpy()\n  return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n  tlogits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on our unique Fon test data. \n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\nfor root, dirs, files in os.walk(test/):\n  test_dataset = load_dataset(\"json\", data_files=[os.path.join(root,i) for i in files],split=\"train\")\n\nchars_to_ignore_regex = '[\\\\,\\\\?\\\\.\\\\!\\\\-\\\\;\\\\:\\\\\"\\\\\u201c\\\\%\\\\\u2018\\\\\u201d]'\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n    return batch\n\ntest_dataset = test_dataset.map(remove_special_characters)\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"chrisjay/wav2vec2-large-xlsr-53-fon\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"chrisjay/wav2vec2-large-xlsr-53-fon\")\nmodel.to(\"cuda\")\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech_array[0].numpy()\n    batch[\"sampling_rate\"] = sampling_rate\n    batch[\"target_text\"] = batch[\"sentence\"]\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n#Evaluation on test dataset\ndef evaluate(batch):\n  inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n  \n  with torch.no_grad():\n    logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n  \n  pred_ids = torch.argmax(logits, dim=-1)\n  batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n  return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n\n```\n\n**Test Result**: 14.97 % \n\n## Training\n\nThe [Fon dataset]( was split into `train`(8235 samples), `validation`(1107 samples), and `test`(1061 samples).\n\nThe script used for training can be found [here](\n\n\n# Collaborators on this project\n\n  - Chris C. Emezue  ([Twitter](\n  - Bonaventure F.P. Dossou (HuggingFace Username: [bonadossou](\n      \n## This is a joint project continuing our research on [OkwuGb\u00e9: End-to-End Speech Recognition for Fon and Igbo](", "qas": [{"id": "q1", "question": "What is the model architecture of chrisjay/fonxlsr?", "answers": [{"text": "wav2vec2", "answer_start": 521, "answer_end": 528}]}, {"id": "q2", "question": "What is the model task of chrisjay/fonxlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 74, "answer_end": 101}]}, {"id": "q3", "question": "What is the model category of chrisjay/fonxlsr?", "answers": [{"text": "audio", "answer_start": 66, "answer_end": 70}]}]}]}, {"title": "chrisliu298/arxiv_ai_gpt2", "paragraphs": [{"context": "---\nlanguage: \"en\"\ntags:\n- gpt2\n- arxiv\n- transformers\ndatasets:\n- \n---\n\n# ArXiv AI GPT-2\n\n## Model description\n\nThis GPT-2 (774M) model is capable of generating abstracts given paper titles. It was trained using all research paper titles and abstracts under artificial intelligence (AI), machine learning (LG), computation and language (CL), and computer vision and pattern recognition (CV) on arXiv.\n\n## Intended uses & limitations\n\n#### How to use\n\nTo generate paper abstracts, use the provided `generate.py` [here]( This is very similar to the HuggingFace's `run_generation.py` [here]( You can simply replace the text with with your own model path (line 89) and change the input string to your paper title (line 127). If you want to use your own script, make sure to prepend `<> ` at the front and append ` <>` at the end of the paper title.\n\n## Training data\nI selected a subset of the [arXiv Archive]( dataset (Geiger, 2019) as the training and evaluation data to fine-tune GPT-2. The original arXiv Archive dataset contains a full archive of metadata about papers on arxiv.org, from the start of the site in 1993 to the end of 2019. Our subset includes all the paper titles (query) and abstracts (context) under the Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Computation and Language (cs.CL), and Computer Vision and Pattern Recognition (cs.CV) categories. I provide the information  of the sub-dataset and the distribution of the training and evaluation dataset as follows.\n\n\n   Count     BPE Token Count |\n :--------:  :-------------: |\n   90,000      20,834,012    |\n    4,940       1,195,056    |\n    4,940       1,218,754    |\n **99,880**  **23,247,822**  |\n\nThe original dataset is in the format of a tab-separated value, so we wrote a simple preprocessing script to convert it into a text file format, which is the input file type (a document) of the GPT-2 model. An example of a paper\u2019s title and its abstract is shown below.\n\n```text\n<> Some paper title <> Some paper abstract <>\n```\n\nBecause there are a lot of cross-domain papers in the dataset, I deduplicate the dataset using the arXiv ID, which is unique for every paper. I sort the paper by submission date, by doing so, one can examine GPT-2\u2019s ability to use learned terminologies when it is prompted with paper titles from the \u201cfuture.\u201d\n\n\n## Training procedure\n\nI used block size = 512, batch size = 1, gradidnet accumulation = 1, learning rate = 1e-5, epochs = 5, and everything else follows the default model configuration.\n\n## Eval results\n\nThe resulting GPT-2 large model's perplexity score on the test set is **14.9413**.\n\n## Reference\n\n```bibtex\n@dataset{r_stuart_geiger_2019_2533436,\n    author= {R. Stuart Geiger},\n    title={{ArXiV Archive: A tidy and complete archive of metadata for papers on arxiv.org, 1993-2019}},\n    month=jan,\n    year= 2019,\n    publisher={Zenodo},\n    version= {v1.0.1},\n    doi={10.5281/zenodo.2533436},\n    url={\n}\n```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of chrisliu298/arxiv_ai_gpt2?", "answers": [{"text": "gpt2", "answer_start": 27, "answer_end": 30}]}]}]}, {"title": "christopherastone/distilgpt2-proofs", "paragraphs": [{"context": "---\nwidget:\n- text: \"Let MATH be given.\"\n- text: \"If MATH is a nonempty\"\n- text: \"By the inductive hypothesis,\"\n---\n[DistilGPT2](  English language model fine-tuned on mathematical proofs extracted from [arXiv.org]( LaTeX sources from 1992 to 2020.\n\n Proofs have been cleaned up a bit. In particular, they use\n * `CITE` for any citation\n * `REF` for any reference\n * `MATH` for any LaTeX mathematical formula\n * `CASE:` for any `\\item` or labeled subcase.", "qas": []}]}, {"title": "chrommium/rubert-base-cased-sentence-finetuned-headlines_X", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: rubert-base-cased-sentence-finetuned-headlines_X\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.952\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# rubert-base-cased-sentence-finetuned-headlines_X\n\nThis model is a fine-tuned version of [DeepPavlov/rubert-base-cased-sentence]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2535\n- Accuracy: 0.952\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.2759          \n 2.0    0.2538          \n 3.0    0.2556          \n 4.0    0.2601          \n 5.0    0.2535          \n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of chrommium/rubert-base-cased-sentence-finetuned-headlines_X?", "answers": [{"text": "bert", "answer_start": 78, "answer_end": 81}]}, {"id": "q2", "question": "What is the model task of chrommium/rubert-base-cased-sentence-finetuned-headlines_X?", "answers": [{"text": "text-classification", "answer_start": 190, "answer_end": 208}]}]}]}, {"title": "chrommium/rubert-base-cased-sentence-finetuned-sent_in_news_sents", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: rubert-base-cased-sentence-finetuned-sent_in_news_sents\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.7224199288256228\n    - name: F1\n      type: f1\n      value: 0.5137303178348194\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# rubert-base-cased-sentence-finetuned-sent_in_news_sents\n\nThis model is a fine-tuned version of [DeepPavlov/rubert-base-cased-sentence]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.9506\n- Accuracy: 0.7224\n- F1: 0.5137\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 14\n- eval_batch_size: 14\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 20\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    1.0045           0.1388 |\n 2.0    0.9574           0.2980 |\n 3.0    1.0259           0.3208 |\n 4.0    1.1262           0.4033 |\n 5.0    1.3377           0.3909 |\n 6.0    1.5716           0.3624 |\n 7.0    1.6286           0.4130 |\n 8.0    1.6450           0.4775 |\n 9.0    1.7108           0.4920 |\n 10.0   1.8792           0.5028 |\n 11.0   1.8670           0.4992 |\n 12.0   1.8856           0.4934 |\n 13.0   1.9506           0.5137 |\n 14.0   2.0363           0.4761 |\n 15.0   2.0601           0.5053 |\n 16.0   2.0813           0.5038 |\n 17.0   2.0960           0.5065 |\n 18.0   2.1060           0.5098 |\n 19.0   2.1153           0.5086 |\n 20.0   2.1187           0.5086 |\n\n\n### Framework versions\n\n- Transformers 4.10.3\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of chrommium/rubert-base-cased-sentence-finetuned-sent_in_news_sents?", "answers": [{"text": "bert", "answer_start": 83, "answer_end": 86}]}, {"id": "q2", "question": "What is the model task of chrommium/rubert-base-cased-sentence-finetuned-sent_in_news_sents?", "answers": [{"text": "text-classification", "answer_start": 202, "answer_end": 220}]}]}]}, {"title": "dbmdz/bert-base-finnish-europeana-cased", "paragraphs": [{"context": "---\nlanguage: finnish \nlicense: mit\nwidget:\n- text: \"T\u00e4k\u00e4l\u00e4inen sanomalehdist\u00f6 [MASK] erit - t\u00e4in\"\n---\n\n# Historic Language Models (HLMs)\n\n## Languages\n\nOur Historic Language Models Zoo contains support for the following languages - incl. their training data source:\n\n Training data | Size \n ------------- | ----\n [Europeana](       | 13-28GB (filtered)\n [Europeana](       | 11-31GB (filtered)\n [British Library]( | 24GB (year filtered)\n [Europeana](       | 1.2GB\n [Europeana](       | 1.1GB\n\n## Models\n\nAt the moment, the following models are available on the model hub:\n\n Model Hub link\n --------------------------------------------------------------------------\n [here](\n [here](\n [here](\n [here](\n\n# Corpora Stats\n\n## German Europeana Corpus\n\nWe provide some statistics using different thresholds of ocr confidences, in order to shrink down the corpus size\nand use less-noisier data:\n\n Size\n ----\n 28GB\n 18GB\n 13GB\n\nFor the final corpus we use a OCR confidence of 0.6 (28GB). The following plot shows a tokens per year distribution:\n\n![German Europeana Corpus Stats](stats/figures/german_europeana_corpus_stats.png)\n\n## French Europeana Corpus\n\nLike German, we use different ocr confidence thresholds:\n\n Size\n ----\n 31GB\n 27GB\n 27GB\n 23GB\n 11GB\n\nFor the final corpus we use a OCR confidence of 0.7 (27GB). The following plot shows a tokens per year distribution:\n\n![French Europeana Corpus Stats](stats/figures/french_europeana_corpus_stats.png)\n\n## British Library Corpus\n\nMetadata is taken from [here]( Stats incl. year filtering:\n\n Size\n ----\n 24GB\n 24GB\n\nWe use the year filtered variant. The following plot shows a tokens per year distribution:\n\n![British Library Corpus Stats](stats/figures/bl_corpus_stats.png)\n\n## Finnish Europeana Corpus\n\n Size\n ----\n 1.2GB\n\nThe following plot shows a tokens per year distribution:\n\n![Finnish Europeana Corpus Stats](stats/figures/finnish_europeana_corpus_stats.png)\n\n## Swedish Europeana Corpus\n\n Size\n ----\n 1.1GB\n\nThe following plot shows a tokens per year distribution:\n\n![Swedish Europeana Corpus Stats](stats/figures/swedish_europeana_corpus_stats.png)\n\n## All Corpora\n\nThe following plot shows a tokens per year distribution of the complete training corpus:\n\n![All Corpora Stats](stats/figures/all_corpus_stats.png)\n\n# Multilingual Vocab generation\n\nFor the first attempt, we use the first 10GB of each pretraining corpus. We upsample both Finnish and Swedish to ~10GB.\nThe following tables shows the exact size that is used for generating a 32k and 64k subword vocabs:\n\n Size\n ----\n 10GB\n 10GB\n 10GB\n 9.5GB\n 9.7GB\n\nWe then calculate the subword fertility rate and portion of `[UNK]`s over the following NER corpora:\n\n NER corpora\n ------------------\n CLEF-HIPE, NewsEye\n CLEF-HIPE, NewsEye\n CLEF-HIPE\n NewsEye\n NewsEye\n\nBreakdown of subword fertility rate and unknown portion per language for the 32k vocab:\n\n Subword fertility  | Unknown portion\n ------------------ | ---------------\n 1.43               | 0.0004\n 1.25               | 0.0001\n 1.25               | 0.0\n 1.69               | 0.0007\n 1.43               | 0.0\n\nBreakdown of subword fertility rate and unknown portion per language for the 64k vocab:\n\n Subword fertility  | Unknown portion\n ------------------ | ---------------\n 1.31               | 0.0004\n 1.16               | 0.0001\n 1.17               | 0.0\n 1.54               | 0.0007\n 1.32               | 0.0\n\n# Final pretraining corpora\n\nWe upsample Swedish and Finnish to ~27GB. The final stats for all pretraining corpora can be seen here:\n\n Size\n ----\n 28GB\n 27GB\n 24GB\n 27GB\n 27GB\n\nTotal size is 130GB.\n\n# Pretraining\n\n## Multilingual model\n\nWe train a multilingual BERT model using the 32k vocab with the official BERT implementation\non a v3-32 TPU using the following parameters:\n\n```bash\npython3 run_pretraining.py --input_file gs://histolectra/historic-multilingual-tfrecords/*.tfrecord \\\n--output_dir gs://histolectra/bert-base-historic-multilingual-cased \\\n--bert_config_file ./config.json \\\n--max_seq_length=512 \\\n--max_predictions_per_seq=75 \\\n--do_train=True \\\n--train_batch_size=128 \\\n--num_train_steps=3000000 \\\n--learning_rate=1e-4 \\\n--save_checkpoints_steps=100000 \\\n--keep_checkpoint_max=20 \\\n--use_tpu=True \\\n--tpu_name=electra-2 \\\n--num_tpu_cores=32\n```\n\nThe following plot shows the pretraining loss curve:\n\n![Training loss curve](stats/figures/pretraining_loss_historic-multilingual.png)\n\n## English model\n\nThe English BERT model - with texts from British Library corpus - was trained with the Hugging Face\nJAX/FLAX implementation for 10 epochs (approx. 1M steps) on a v3-8 TPU, using the following command:\n\n```bash\npython3 run_mlm_flax.py --model_type bert \\\n--config_name /mnt/datasets/bert-base-historic-english-cased/ \\\n--tokenizer_name /mnt/datasets/bert-base-historic-english-cased/ \\\n--train_file /mnt/datasets/bl-corpus/bl_1800-1900_extracted.txt \\\n--validation_file /mnt/datasets/bl-corpus/english_validation.txt \\\n--max_seq_length 512 \\\n--per_device_train_batch_size 16 \\\n--learning_rate 1e-4 \\\n--num_train_epochs 10 \\\n--preprocessing_num_workers 96 \\\n--output_dir /mnt/datasets/bert-base-historic-english-cased-512-noadafactor-10e \\\n--save_steps 2500 \\\n--eval_steps 2500 \\\n--warmup_steps 10000 \\\n--line_by_line \\\n--pad_to_max_length\n```\n\nThe following plot shows the pretraining loss curve:\n\n![Training loss curve](stats/figures/pretraining_loss_historic_english.png)\n\n## Finnish model\n\nThe BERT model - with texts from Finnish part of Europeana - was trained with the Hugging Face\nJAX/FLAX implementation for 40 epochs (approx. 1M steps) on a v3-8 TPU, using the following command:\n\n```bash\npython3 run_mlm_flax.py --model_type bert \\\n--config_name /mnt/datasets/bert-base-finnish-europeana-cased/ \\\n--tokenizer_name /mnt/datasets/bert-base-finnish-europeana-cased/ \\\n--train_file /mnt/datasets/hlms/extracted_content_Finnish_0.6.txt \\\n--validation_file /mnt/datasets/hlms/finnish_validation.txt \\\n--max_seq_length 512 \\\n--per_device_train_batch_size 16 \\\n--learning_rate 1e-4 \\\n--num_train_epochs 40 \\\n--preprocessing_num_workers 96 \\\n--output_dir /mnt/datasets/bert-base-finnish-europeana-cased-512-dupe1-noadafactor-40e \\\n--save_steps 2500 \\\n--eval_steps 2500 \\\n--warmup_steps 10000 \\\n--line_by_line \\\n--pad_to_max_length\n```\n\nThe following plot shows the pretraining loss curve:\n\n![Training loss curve](stats/figures/pretraining_loss_finnish_europeana.png)\n\n## Swedish model\n\nThe BERT model - with texts from Swedish part of Europeana - was trained with the Hugging Face\nJAX/FLAX implementation for 40 epochs (approx. 660K steps) on a v3-8 TPU, using the following command:\n\n```bash\npython3 run_mlm_flax.py --model_type bert \\\n--config_name /mnt/datasets/bert-base-swedish-europeana-cased/ \\\n--tokenizer_name /mnt/datasets/bert-base-swedish-europeana-cased/ \\\n--train_file /mnt/datasets/hlms/extracted_content_Swedish_0.6.txt \\\n--validation_file /mnt/datasets/hlms/swedish_validation.txt \\\n--max_seq_length 512 \\\n--per_device_train_batch_size 16 \\\n--learning_rate 1e-4 \\\n--num_train_epochs 40 \\\n--preprocessing_num_workers 96 \\\n--output_dir /mnt/datasets/bert-base-swedish-europeana-cased-512-dupe1-noadafactor-40e \\\n--save_steps 2500 \\\n--eval_steps 2500 \\\n--warmup_steps 10000 \\\n--line_by_line \\\n--pad_to_max_length\n```\n\nThe following plot shows the pretraining loss curve:\n\n![Training loss curve](stats/figures/pretraining_loss_swedish_europeana.png)\n\n# Acknowledgments\n\nResearch supported with Cloud TPUs from Google's TPU Research Cloud (TRC) program, previously known as\nTensorFlow Research Cloud (TFRC). Many thanks for providing access to the TRC \u2764\ufe0f\n\nThanks to the generous support from the [Hugging Face]( team,\nit is possible to download both cased and uncased models from their S3 storage \ud83e\udd17\n", "qas": [{"id": "q1", "question": "What is the model architecture of dbmdz/bert-base-finnish-europeana-cased?", "answers": [{"text": "bert", "answer_start": 3905, "answer_end": 3908}]}]}]}, {"title": "flax-sentence-embeddings/mpnet_stackexchange_v1", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# mpnet_stackexchange_v1\n\n## Model Description\n\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used a pretrained [mpnet-base]( model and trained it using Siamese Network setup and contrastive learning objective. Question and answer pairs from StackExchange was used as training data to make the model robust to Question / Answer embedding similarity.\n\nWe developped this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developped this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence encoder for a search engine. Given an input sentence, it ouptuts a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\n\n## How to use\n\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/mpnet_stackexchange_v1')\ntext = \"Replace me by any question / answer you'd like.\"\ntext_embbedding = model.encode(text)\n# array([-0.01559514,  0.04046123,  0.1317083 ,  0.00085931,  0.04585106,\n#        -0.05607086,  0.0138078 ,  0.03569756,  0.01420381,  0.04266302 ...],\n#        dtype=float32)\n```\n\n# Training procedure\n\n## Pre-training \n\nWe use the pretrained [`Mpnet-base`]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n\n## Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n### Hyper parameters\n\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n\n### Training data\n\nWe used the concatenation from multiple Stackexchange Question-Answer datasets to fine-tune our model.\nWe sampled each StackExchange given a weighted probability of following equation.\n\n```\nint((stackexchange_length[path] / total_stackexchange_length) * total_weight)\n```\n\nMSMARCO, NQ & other question-answer datasets were also used. Sampling ratio for StackExchange vs remaining : 2 vs 1.\n\n\n Paper                                    \n:----------------------------------------:\n - \n [paper]( \n [paper]( \n [paper]( \n - \n [paper]( ", "qas": [{"id": "q1", "question": "What is the model architecture of flax-sentence-embeddings/mpnet_stackexchange_v1?", "answers": [{"text": "mpnet", "answer_start": 118, "answer_end": 122}]}, {"id": "q2", "question": "What is the model task of flax-sentence-embeddings/mpnet_stackexchange_v1?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/multi-QA_v1-mpnet-asymmetric-Q", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n# multi-QA_v1-mpnet-asymmetric-Q\n## Model Description\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used two separate pretrained [mpnet-base]( models and trained them using contrastive learning objective. Question and answer pairs from StackExchange and other datasets were used as training data to make the model robust to Question / Answer embedding similarity.\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n## Intended uses\nThis model set is intended to be used as a sentence encoder for a search engine. Given an input sentence, it ouptuts a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\nTwo models should be used on conjunction for Semantic Search purposes.\n1. [multi-QA_v1-mpnet-asymmetric-Q]( - Model to encode Questions\n1. [multi-QA_v1-mpnet-asymmetric-Q]( - Model to encode Answers\n## How to use\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n```python\nfrom sentence_transformers import SentenceTransformer\nmodel_Q = SentenceTransformer('flax-sentence-embeddings/multi-QA_v1-mpnet-asymmetric-Q')\nmodel_A = SentenceTransformer('flax-sentence-embeddings/multi-QA_v1-mpnet-asymmetric-A')\nquestion = \"Replace me by any question you'd like.\"\nquestion_embbedding = model_Q.encode(text)\nanswer = \"Replace me by any answer you'd like.\"\nanswer_embbedding = model_A.encode(text)\nanswer_likeliness = cosine_similarity(question_embedding, answer_embedding)\n```\n# Training procedure\n## Pre-training \nWe use the pretrained [`Mpnet-base`]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n## Fine-tuning \nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n### Hyper parameters\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n### Training data\nWe used the concatenation from multiple Stackexchange Question-Answer datasets to fine-tune our model. MSMARCO, NQ & other question-answer datasets were also used.\n Paper                                    \n:----------------------------------------:\n - \n - \n - \n [paper]( \n - \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n - \n [paper]( ", "qas": [{"id": "q1", "question": "What is the model architecture of flax-sentence-embeddings/multi-QA_v1-mpnet-asymmetric-Q?", "answers": [{"text": "mpnet", "answer_start": 129, "answer_end": 133}]}, {"id": "q2", "question": "What is the model task of flax-sentence-embeddings/multi-QA_v1-mpnet-asymmetric-Q?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/multi-qa_v1-MiniLM-L6-cls_dot", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# multi-qa_v1-MiniLM-L6-cls_dot\n\n## Model Description\n\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used a pretrained [nreimers/MiniLM-L6-H384-uncased]( model and trained it using Siamese Network setup and contrastive learning objective. Question and answer pairs from StackExchange was used as training data to make the model robust to Question / Answer embedding similarity. For this model, cls output was used instead of mean pooling as sentence embeddings. Dot product was used to calculate similarity for learning objective.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence encoder for a search engine. Given an input sentence, it outputs a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\n\n## How to use\n\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/multi-qa_v1-MiniLM-L6-cls_dot')\ntext = \"Replace me by any question / answer you'd like.\"\ntext_embbedding = model.encode(text)\n# array([-0.01559514,  0.04046123,  0.1317083 ,  0.00085931,  0.04585106,\n#        -0.05607086,  0.0138078 ,  0.03569756,  0.01420381,  0.04266302 ...],\n#        dtype=float32)\n```\n\n# Training procedure\n\n## Pre-training \n\nWe use the pretrained [nreimers/MiniLM-L6-H384-uncased]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n\n## Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n### Hyper parameters\n\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n\n### Training data\n\nWe used the concatenation from multiple Stackexchange Question-Answer datasets to fine-tune our model. MSMARCO, NQ & other question-answer datasets were also used.\n\n\n Paper                                    \n:----------------------------------------:\n - \n - \n - \n [paper]( \n - \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n - \n [paper]( ", "qas": [{"id": "q2", "question": "What is the model task of flax-sentence-embeddings/multi-qa_v1-MiniLM-L6-cls_dot?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/multi-qa_v1-MiniLM-L6-mean_cos", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# multi-qa_v1-MiniLM-L6-mean_cos\n\n## Model Description\n\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used a pretrained [nreimers/MiniLM-L6-H384-uncased]( model and trained it using Siamese Network setup and contrastive learning objective. Question and answer pairs from StackExchange was used as training data to make the model robust to Question / Answer embedding similarity. For this model, mean pooling of the hidden states were used as sentence embeddings.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence encoder for a search engine. Given an input sentence, it outputs a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\n\n## How to use\n\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/multi-qa_v1-MiniLM-L6-mean_cos')\ntext = \"Replace me by any question / answer you'd like.\"\ntext_embbedding = model.encode(text)\n# array([-0.01559514,  0.04046123,  0.1317083 ,  0.00085931,  0.04585106,\n#        -0.05607086,  0.0138078 ,  0.03569756,  0.01420381,  0.04266302 ...],\n#        dtype=float32)\n```\n\n# Training procedure\n\n## Pre-training \n\nWe use the pretrained [nreimers/MiniLM-L6-H384-uncased]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n\n## Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n### Hyper parameters\n\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n\n### Training data\n\nWe used the concatenation from multiple Stackexchange Question-Answer datasets to fine-tune our model. MSMARCO, NQ & other question-answer datasets were also used.\n\n\n Paper                                    \n:----------------------------------------:\n - \n - \n - \n [paper]( \n - \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n - \n [paper]( \n\n", "qas": [{"id": "q2", "question": "What is the model task of flax-sentence-embeddings/multi-qa_v1-MiniLM-L6-mean_cos?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/multi-qa_v1-distilbert-cls_dot", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# multi-qa_v1-distilbert-cls_dot\n\n## Model Description\n\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used a pretrained [distilbert-base-uncased]( model and trained it using Siamese Network setup and contrastive learning objective. Question and answer pairs from StackExchange was used as training data to make the model robust to Question / Answer embedding similarity. For this model, cls output was used instead of mean pooling as sentence embeddings. Dot product was used to calculate similarity for learning objective.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence encoder for a search engine. Given an input sentence, it outputs a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\n\n## How to use\n\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/multi-qa_v1-distilbert-cls_dot')\ntext = \"Replace me by any question / answer you'd like.\"\ntext_embbedding = model.encode(text)\n# array([-0.01559514,  0.04046123,  0.1317083 ,  0.00085931,  0.04585106,\n#        -0.05607086,  0.0138078 ,  0.03569756,  0.01420381,  0.04266302 ...],\n#        dtype=float32)\n```\n\n# Training procedure\n\n## Pre-training \n\nWe use the pretrained [distilbert-base-uncased]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n\n## Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n### Hyper parameters\n\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n\n### Training data\n\nWe used the concatenation from multiple Stackexchange Question-Answer datasets to fine-tune our model. MSMARCO, NQ & other question-answer datasets were also used.\n\n\n Paper                                    \n:----------------------------------------:\n - \n - \n - \n [paper]( \n - \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n - \n [paper]( ", "qas": [{"id": "q1", "question": "What is the model architecture of flax-sentence-embeddings/multi-qa_v1-distilbert-cls_dot?", "answers": [{"text": "distilbert", "answer_start": 130, "answer_end": 139}]}, {"id": "q2", "question": "What is the model task of flax-sentence-embeddings/multi-qa_v1-distilbert-cls_dot?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/multi-qa_v1-distilbert-mean_cos", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# multi-qa_v1-distilbert-mean_cos\n\n## Model Description\n\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used a pretrained [distilbert-base-uncased]( model and trained it using Siamese Network setup and contrastive learning objective. Question and answer pairs from StackExchange was used as training data to make the model robust to Question / Answer embedding similarity. For this model, mean pooling of hidden states were used as sentence embeddings.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence encoder for a search engine. Given an input sentence, it outputs a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\n\n## How to use\n\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/multi-qa_v1-distilbert-mean_cos')\ntext = \"Replace me by any question / answer you'd like.\"\ntext_embbedding = model.encode(text)\n# array([-0.01559514,  0.04046123,  0.1317083 ,  0.00085931,  0.04585106,\n#        -0.05607086,  0.0138078 ,  0.03569756,  0.01420381,  0.04266302 ...],\n#        dtype=float32)\n```\n\n# Training procedure\n\n## Pre-training \n\nWe use the pretrained [distilbert-base-uncased]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n\n## Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n### Hyper parameters\n\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n\n### Training data\n\nWe used the concatenation from multiple Stackexchange Question-Answer datasets to fine-tune our model. MSMARCO, NQ & other question-answer datasets were also used.\n\n\n Paper                                    \n:----------------------------------------:\n - \n - \n - \n [paper]( \n - \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n - \n [paper]( ", "qas": [{"id": "q1", "question": "What is the model architecture of flax-sentence-embeddings/multi-qa_v1-distilbert-mean_cos?", "answers": [{"text": "distilbert", "answer_start": 130, "answer_end": 139}]}, {"id": "q2", "question": "What is the model task of flax-sentence-embeddings/multi-qa_v1-distilbert-mean_cos?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/multi-qa_v1-mpnet-cls_dot", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# multi-qa_v1-mpnet-cls_dot\n\n## Model Description\n\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used a pretrained [microsoft/mpnet-base]( model and trained it using Siamese Network setup and contrastive learning objective. Question and answer pairs from StackExchange was used as training data to make the model robust to Question / Answer embedding similarity. For this model, cls output was used instead of mean pooling as sentence embeddings. Dot product was used to calculate similarity for learning objective.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence encoder for a search engine. Given an input sentence, it outputs a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\n\n## How to use\n\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/multi-qa_v1-mpnet-cls_dot')\ntext = \"Replace me by any question / answer you'd like.\"\ntext_embbedding = model.encode(text)\n# array([-0.01559514,  0.04046123,  0.1317083 ,  0.00085931,  0.04585106,\n#        -0.05607086,  0.0138078 ,  0.03569756,  0.01420381,  0.04266302 ...],\n#        dtype=float32)\n```\n\n# Training procedure\n\n## Pre-training \n\nWe use the pretrained [microsoft/mpnet-base]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n\n## Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n### Hyper parameters\n\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n\n### Training data\n\nWe used the concatenation from multiple Stackexchange Question-Answer datasets to fine-tune our model. MSMARCO, NQ & other question-answer datasets were also used.\n\n\n Paper                                    \n:----------------------------------------:\n - \n - \n - \n [paper]( \n - \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n - \n [paper]( \n\n", "qas": [{"id": "q1", "question": "What is the model architecture of flax-sentence-embeddings/multi-qa_v1-mpnet-cls_dot?", "answers": [{"text": "mpnet", "answer_start": 130, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of flax-sentence-embeddings/multi-qa_v1-mpnet-cls_dot?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/multi-qa_v1-mpnet-mean_cos", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# multi-qa_v1-mpnet-mean_cos\n\n## Model Description\n\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used a pretrained [microsoft/mpnet-base]( model and trained it using Siamese Network setup and contrastive learning objective. Question and answer pairs from StackExchange was used as training data to make the model robust to Question / Answer embedding similarity. For this model, mean pooling of hidden states were used as sentence embeddings.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence encoder for a search engine. Given an input sentence, it outputs a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\n\n## How to use\n\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/multi-qa_v1-mpnet-mean_cos')\ntext = \"Replace me by any question / answer you'd like.\"\ntext_embbedding = model.encode(text)\n# array([-0.01559514,  0.04046123,  0.1317083 ,  0.00085931,  0.04585106,\n#        -0.05607086,  0.0138078 ,  0.03569756,  0.01420381,  0.04266302 ...],\n#        dtype=float32)\n```\n\n# Training procedure\n\n## Pre-training \n\nWe use the pretrained [microsoft/mpnet-base]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n\n## Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n### Hyper parameters\n\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n\n### Training data\n\nWe used the concatenation from multiple Stackexchange Question-Answer datasets to fine-tune our model. MSMARCO, NQ & other question-answer datasets were also used.\n\n\n Paper                                    \n:----------------------------------------:\n - \n - \n - \n [paper]( \n - \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n [paper]( \n - \n [paper]( \n\n", "qas": [{"id": "q1", "question": "What is the model architecture of flax-sentence-embeddings/multi-qa_v1-mpnet-mean_cos?", "answers": [{"text": "mpnet", "answer_start": 130, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of flax-sentence-embeddings/multi-qa_v1-mpnet-mean_cos?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/st-codesearch-distilroberta-base", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\ndatasets:\n- code_search_net\n---\n\n# flax-sentence-embeddings/st-codesearch-distilroberta-base\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\nIt was trained on the [code_search_net]( dataset and can be used to search program code given text.\n\n## Usage:\n\n```python\nfrom sentence_transformers import SentenceTransformer, util\n\n\n#This list the defines the different programm codes\ncode = [\"\"\"def sort_list(x):\n   return sorted(x)\"\"\",\n\"\"\"def count_above_threshold(elements, threshold=0):\n    counter = 0\n    for e in elements:\n        if e > threshold:\n            counter += 1\n    return counter\"\"\",\n\"\"\"def find_min_max(elements):\n    min_ele = 99999\n    max_ele = -99999\n    for e in elements:\n        if e < min_ele:\n            min_ele = e\n        if e > max_ele:\n            max_ele = e\n    return min_ele, max_ele\"\"\"]\n    \n\nmodel = SentenceTransformer(\"flax-sentence-embeddings/st-codesearch-distilroberta-base\")\n\n# Encode our code into the vector space\ncode_emb = model.encode(code, convert_to_tensor=True)\n\n# Interactive demo: Enter queries, and the method returns the best function from the \n# 3 functions we defined\nwhile True:\n    query = input(\"Query: \")\n    query_emb = model.encode(query, convert_to_tensor=True)\n    hits = util.semantic_search(query_emb, code_emb)[0]\n    top_hit = hits[0]\n\n    print(\"Cossim: {:.2f}\".format(top_hit['score']))\n    print(code[top_hit['corpus_id']])\n    print(\"\\n\\n\")\n```\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('flax-sentence-embeddings/st-codesearch-distilroberta-base')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n## Training\n\nThe model was trained with a DistilRoBERTa-base model for 10k training steps on the codesearch dataset with batch_size 256 and MultipleNegativesRankingLoss. \n\nIt is some preliminary model. It was neither tested nor was the trained quite sophisticated \n\n\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`MultiDatasetDataLoader.MultiDatasetDataLoader` of length 5371 with parameters:\n```\n{'batch_size': 256}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss` with parameters:\n  ```\n  {'scale': 20, 'similarity_fct': 'dot_score'}\n  ```\n\nParameters of the fit()-Method:\n```\n{\n    \"callback\": null,\n    \"epochs\": 1,\n    \"evaluation_steps\": 0,\n    \"evaluator\": \"NoneType\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 2e-05\n    },\n    \"scheduler\": \"warmupconstant\",\n    \"steps_per_epoch\": 10000,\n    \"warmup_steps\": 500,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: RobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Normalize()\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->", "qas": [{"id": "q1", "question": "What is the model architecture of flax-sentence-embeddings/st-codesearch-distilroberta-base?", "answers": [{"text": "roberta", "answer_start": 191, "answer_end": 197}]}, {"id": "q2", "question": "What is the model task of flax-sentence-embeddings/st-codesearch-distilroberta-base?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "flax-sentence-embeddings/stackoverflow_mpnet-base", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# stackoverflow_mpnet-base\n\nThis is a microsoft/mpnet-base model trained on 18,562,443 (title, body) pairs from StackOverflow. \n\nSentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used a pretrained [microsoft/mpnet-base]( model and trained it using Siamese Network setup and contrastive learning objective. 18,562,443 (title, body) pairs from StackOverflow was used as training data. For this model, mean pooling of hidden states were used as sentence embeddings. See data_config.json and train_script.py in this respository how the model was trained and which datasets have been used.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV]( \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs]( We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well\nas assistance from Google\u2019s Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence encoder for a search engine. Given an input sentence, it outputs a vector which captures \nthe sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.\n\n## How to use\n\nHere is how to use this model to get the features of a given text using [SentenceTransformers]( library:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/stackoverflow_mpnet-base')\ntext = \"Replace me by any question / answer you'd like.\"\ntext_embbedding = model.encode(text)\n# array([-0.01559514,  0.04046123,  0.1317083 ,  0.00085931,  0.04585106,\n#        -0.05607086,  0.0138078 ,  0.03569756,  0.01420381,  0.04266302 ...],\n#        dtype=float32)\n```\n\n# Training procedure\n\n## Pre-training \n\nWe use the pretrained [microsoft/mpnet-base]( Please refer to the model\ncard for more detailed information about the pre-training procedure.\n\n## Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n### Hyper parameters\n\nWe trained on model on a TPU v3-8. We train the model during 80k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository.\n\n### Training data\n\nWe used 18,562,443 (title, body) pairs from StackOverflow as training data.\n\n Paper                                    \n:----------------------------------------:\n - \n\n", "qas": [{"id": "q1", "question": "What is the model architecture of flax-sentence-embeddings/stackoverflow_mpnet-base?", "answers": [{"text": "mpnet", "answer_start": 132, "answer_end": 136}]}, {"id": "q2", "question": "What is the model task of flax-sentence-embeddings/stackoverflow_mpnet-base?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "google/roberta2roberta_L-24_cnn_daily_mail", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- cnn_dailymail\ntags:\n- summarization\n---\n\n# Roberta2Roberta_L-24_cnn_daily_mail EncoderDecoder model\n\nThe model was introduced in \n[this paper]( by Sascha Rothe, Shashi Narayan, Aliaksei Severyn and first released in [this repository]( \n\nThe model is an encoder-decoder model that was initialized on the `roberta-large` checkpoints for both the encoder \nand decoder and fine-tuned on summarization on the CNN / Dailymail dataset, which is linked above.\n\nDisclaimer: The model card has been written by the Hugging Face team.\n\n## How to use\n\nYou can use this model for summarization, *e.g.*\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_cnn_daily_mail\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/roberta2roberta_L-24_cnn_daily_mail\")\n\narticle = \"\"\"\t(The Hollywood Reporter)\"The Rocky Horror Picture\nShow\" is the latest musical getting the small-\nscreen treatment. Fox is developing a two-hour\nremake of the 1975 cult classic to be directed,\nexecutive-produced and choreographed by Kenneth\nOrtega (\"High School Musical\"). The project,\ntentatively titled \"The Rocky Horror Picture Show\nEvent,\" is casting-contingent. The special will be\nfilmed in advance and not air live, but few\ndetails beyond that are known. In addition to\nOrtega, Gail Berman and Lou Adler, who produced\nthe original film, are also attached as executive\nproducers. The special will be produced by Fox 21\nTelevision Studios, and Berman's The Jackal Group.\nThe special is timed to celebrate the 40th\nanniversary of the film, which has grossed more\nthan $112 million and still plays in theaters\nacross the country. TV premiere dates: The\ncomplete guide . This isn't the first stab at\nadapting \"The Rocky Horror Picture Show.\" In 2002,\nFox unveiled plans for an adaptation timed to the\n30th anniversary that never came to fruition. The\nfaces of pilot season 2015 . Fox's \"Glee\" covered\nseveral of the show's most popular songs for a\nSeason 2 episode and even released a special \"The\nRocky Horror Glee Show\" EP. There is no plan yet\nfor when the adaptation will air. Fox also has a\nlive musical production of \"Grease\", starring\nJulianne Hough and Vanessa Hudgens, scheduled to\nair on Jan. 31, 2016. Broadcast TV scorecard .\nFollowing in the footsteps of \"The Sound of Music\"\nand \"Peter Pan,\" NBC recently announced plans to\nair a live version of The Wiz later this year.\nOrtega's credits include \"Gilmore Girls,\" \"This Is\nIt\" and \"Hocus Pocus.\" He is repped by Paradigm\nand Hanson, Jacobson. \u00a92015 The Hollywood\nReporter. All rights reserved.\"\"\"\n\ninput_ids = tokenizer(article, return_tensors=\"pt\").input_ids\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n# should output\n# Fox is developing a two-hour remake of the 1975 cult classic. The special will be directed, executive-produced and choreographed by Kenneth Ortega. \n# The special is timed to celebrate the 40th anniversary of the film, which has grossed more than $112 million.\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of google/roberta2roberta_L-24_cnn_daily_mail?", "answers": [{"text": "encoder-decoder", "answer_start": 302, "answer_end": 316}]}, {"id": "q2", "question": "What is the model task of google/roberta2roberta_L-24_cnn_daily_mail?", "answers": [{"text": "summarization", "answer_start": 71, "answer_end": 83}]}]}]}, {"title": "google/t5-efficient-small-nl40", "paragraphs": [{"context": "---\nlanguage:\n- en\ndatasets:\n- c4\ntags:\n- deep-narrow\ninference: false\n\nlicense: apache-2.0\n---\n\n# T5-Efficient-SMALL-NL40 (Deep-Narrow version)\n\nT5-Efficient-SMALL-NL40 is a variation of [Google's original T5]( following the [T5 model architecture](\nIt is a *pretrained-only* checkpoint and was released with the\npaper **[Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers](\nby *Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler*.\n\nIn a nutshell, the paper indicates that a **Deep-Narrow** model architecture is favorable for **downstream** performance compared to other model architectures\nof similar parameter count.\n\nTo quote the paper:\n\n> We generally recommend a DeepNarrow strategy where the model\u2019s depth is preferentially increased\n> before considering any other forms of uniform scaling across other dimensions. This is largely due to\n> how much depth influences the Pareto-frontier as shown in earlier sections of the paper. Specifically, a\n> tall small (deep and narrow) model is generally more efficient compared to the base model. Likewise,\n> a tall base model might also generally more efficient compared to a large model. We generally find\n> that, regardless of size, even if absolute performance might increase as we continue to stack layers,\n> the relative gain of Pareto-efficiency diminishes as we increase the layers, converging at 32 to 36\n> layers. Finally, we note that our notion of efficiency here relates to any one compute dimension, i.e.,\n> params, FLOPs or throughput (speed). We report all three key efficiency metrics (number of params,\n> FLOPS and speed) and leave this decision to the practitioner to decide which compute dimension to\n> consider.\n\nTo be more precise, *model depth* is defined as the number of transformer blocks that are stacked sequentially.\nA sequence of word embeddings is therefore processed sequentially by each transformer block.\n\n## Details model architecture\n\nThis model checkpoint - **t5-efficient-small-nl40** - is of model type **Small** with the following variations:\n- **nl** is **40**\n\nIt has **310.25** million parameters and thus requires *ca.* **1240.99 MB** of memory in full precision (*fp32*)\n or  **620.5 MB** of memory in half precision (*fp16* or *bf16*).\n\nA summary of the *original* T5 model architectures can be seen here:\n\n nl (el/dl)  dm  nh \n ----  ----  ---- \n 4/4  256  4 \n 4/4  384  8 \n 6/6  512  8 \n 12/12  768  12 \n 24/24  1024  16 \n 24/24  1024  32 \n 24/24  1024  128 \n\nwhereas the following abbreviations are used:\n\n Definition |\n ---- |\n Number of transformer blocks (depth) |\n Dimension of embedding vector (output vector of transformers block) |\n Dimension of key/value projection matrix |\n Number of attention heads |\n Dimension of intermediate vector within transformer block (size of feed-forward projection matrix) | \n Number of transformer blocks in the encoder (encoder depth) | \n Number of transformer blocks in the decoder (decoder depth) | \n Signifies that attention heads are shared | \n Signifies that key-values projection matrices are tied | \n\nIf a model checkpoint has no specific, *el* or *dl* than both the number of encoder- and decoder layers correspond to *nl*.\n\n## Pre-Training\n\nThe checkpoint was pretrained on the [Colossal, Cleaned version of Common Crawl (C4)]( for 524288 steps using \nthe span-based masked language modeling (MLM) objective.\n\n## Fine-Tuning\n\n**Note**: This model is a **pretrained** checkpoint and has to be fine-tuned for practical usage.\nThe checkpoint was pretrained in English and is therefore only useful for English NLP tasks.\nYou can follow on of the following examples on how to fine-tune the model:\n\n*PyTorch*:\n\n- [Summarization](\n- [Question Answering](\n- [Text Classification]( - *Note*: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.\n\n*Tensorflow*:\n\n- [Summarization](\n- [Text Classification]( - *Note*: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.\n\n*JAX/Flax*:\n\n- [Summarization](\n- [Text Classification]( - *Note*: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.\n\n## Downstream Performance\n\nTODO: Add table if available\n\n## Computational Complexity\n\nTODO: Add table if available\n\n## More information\n\nWe strongly recommend the reader to go carefully through the original paper **[Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers]( to get a more nuanced understanding of this model checkpoint.\nAs explained in the following [issue]( checkpoints including the *sh* or *skv* \nmodel architecture variations have *not* been ported to Transformers as they are probably of limited practical usage and are lacking a more detailed description. Those checkpoints are kept [here]( as they might be ported potentially in the future.", "qas": [{"id": "q1", "question": "What is the model architecture of google/t5-efficient-small-nl40?", "answers": [{"text": "t5", "answer_start": 2065, "answer_end": 2066}]}]}]}, {"title": "google/t5-xxl-ssm-tqao", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- c4\n- wikipedia\n- trivia_qa\n\nlicense: apache-2.0\n---\n\n[Google's T5]( for **Closed Book Question Answering**.\n\nThe model was pre-trained using T5's denoising objective on [C4]( subsequently additionally pre-trained using [REALM]( salient span masking objective on [Wikipedia]( and finally fine-tuned on [Trivia QA (TQA)](\n\n**Note**: The model was fine-tuned on 90% of the train splits of [Trivia QA (TQA)]( for 20k steps and validated on the held-out 10% of the train split.\n\nOther community Checkpoints: [here](\n\nPaper: [How Much Knowledge Can You Pack\nInto the Parameters of a Language Model?](\n\nAuthors: *Adam Roberts, Colin Raffel, Noam Shazeer* \n\n\n## Results on Trivia QA - Test Set\n\n link \n---\n\n**\n\n## Usage\n\nThe model can be used as follows for **closed book question answering**:\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nt5_qa_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-xxl-ssm-tqao\")\nt5_tok = AutoTokenizer.from_pretrained(\"google/t5-xxl-ssm-tqao\")\n\ninput_ids = t5_tok(\"When was Franklin D. Roosevelt born?\", return_tensors=\"pt\").input_ids\ngen_output = t5_qa_model.generate(input_ids)[0]\n\nprint(t5_tok.decode(gen_output, skip_special_tokens=True))\n```\n\n## Abstract\n\nIt has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models at \n\n![model image](", "qas": [{"id": "q1", "question": "What is the model architecture of google/t5-xxl-ssm-tqao?", "answers": [{"text": "t5", "answer_start": 889, "answer_end": 890}]}]}]}, {"title": "google/tapas-base-masklm", "paragraphs": [{"context": "This model corresponds to **tapas_masklm_base_reset** of the [original repository](\n\nHere's how you can use it:\n\n```python\nfrom transformers import TapasTokenizer, TapasForMaskedLM\nimport pandas as pd\nimport torch\n\ntokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-masklm\")\nmodel = TapasForMaskedLM.from_pretrained(\"google/tapas-base-masklm\")\n\ndata = {'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"],\n        'Age': [\"56\", \"45\", \"59\"],\n        'Number of movies': [\"87\", \"53\", \"69\"]\n}\ntable = pd.DataFrame.from_dict(data)\nquery = \"How many movies has Leonardo [MASK] Caprio played in?\"\n\n# prepare inputs\ninputs = tokenizer(table=table, queries=query, padding=\"max_length\", return_tensors=\"pt\")\n\n# forward pass\noutputs = model(**inputs)\n\n# return top 5 values and predictions\nmasked_index = torch.nonzero(inputs.input_ids.squeeze() == tokenizer.mask_token_id, as_tuple=False)\nlogits = outputs.logits[0, masked_index.item(), :]\nprobs = logits.softmax(dim=0)\nvalues, predictions = probs.topk(5)\n\nfor value, pred in zip(values, predictions):\n  print(f\"{tokenizer.decode([pred])} with confidence {value}\")\n```", "qas": [{"id": "q1", "question": "What is the model architecture of google/tapas-base-masklm?", "answers": [{"text": "tapas", "answer_start": 28, "answer_end": 32}]}]}]}, {"title": "google/vit-large-patch16-384", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- image-classification\n- vision\ndatasets:\n- imagenet\n- imagenet-21k\n---\n\n# Vision Transformer (large-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale]( by Dosovitskiy et al. and first released in [this repository]( However, the weights were converted from the [timm repository]( by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, at a higher resolution of 384x384.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub]( to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = '\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-384')\nmodel = ViTForImageClassification.from_pretrained('google/vit-large-patch16-384')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nCurrently, both the feature extractor and model  support PyTorch. Tensorflow and JAX/FLAX are coming soon, and the API of ViTFeatureExtractor might change.\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k]( a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet]( a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here]( \n\nImages are resized/rescaled to the same resolution (224x224 during pre-training, 384x384 during fine-tuning) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of google/vit-large-patch16-384?", "answers": [{"text": "vit", "answer_start": 469, "answer_end": 471}]}, {"id": "q2", "question": "What is the model task of google/vit-large-patch16-384?", "answers": [{"text": "image-classification", "answer_start": 32, "answer_end": 51}]}]}]}, {"title": "google/vit-large-patch32-224-in21k", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- vision\ndatasets:\n- imagenet-21k\ninference: false\n---\n\n# Vision Transformer (large-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale]( by Dosovitskiy et al. and first released in [this repository]( However, the weights were converted from the [timm repository]( by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. \n\nImages are presented to the model as a sequence of fixed-size patches (resolution 32x32), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nNote that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub]( to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\nurl = '\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_state = outputs.last_hidden_state\n```\n\nCurrently, both the feature extractor and model  support PyTorch. Tensorflow and JAX/FLAX are coming soon, and the API of ViTFeatureExtractor might change.\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k]( a dataset consisting of 14 million images and 21k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here]( \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of google/vit-large-patch32-224-in21k?", "answers": [{"text": "vit", "answer_start": 363, "answer_end": 365}]}]}]}, {"title": "gorkemgoknar/gpt2-small-turkish", "paragraphs": [{"context": "---\nlanguage:\n- tr\nthumbnail:\ntags:\n- gpt2\n- turkish\n\nlicense: apache-2.0\ndatasets:\n- wikipedia-turkish\nmetrics:\n- perplexity\n- accuracy\n\nwidget:\n- text: Bu yaz\u0131y\u0131 bir bilgisayar yazd\u0131. Yazarken\n  context: ''\n- text: \u0130nternete kolay eri\u015fim sayesinde d\u00fcnya daha da k\u00fc\u00e7\u00fcld\u00fc. Bunun sonucunda\n  context: ''\n---\n\n# Turkish GPT2 Model Finetuned \n# T\u00fcrk\u00e7e GPT2 Modeli\n\n## Model description\n\nThis is a GPT2-Small English based model finetuned and additionaly trainied with Wikipedia Articles in Turkish as of 28-10-2020\n\nLive demo based on this work at : \n\nFine tuned writer on this model: \n\nWork has been done on Pierre Guillou tutorial as on this page.\n( \n\nCode is converted to work with Fastai 2.X .\n\nUsing Google Colab for training. \n\nAdditional tutorial and source will be in  in later stage.\n\nCurrent accuracy 33 %  , Perplexity : 51.88\n\nModels are available:\n\n* [gpt2-small-tuned-tr] (\n* [gpt2-small-turkish-writer] (\n\n## Intended uses & limitations\n\n#### How to use\n\n#### Install\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"gorkemgoknar/gpt2-small-turkish\")\nmodel = AutoModelWithLMHead.from_pretrained(\"gorkemgoknar/gpt2-small-turkish\")\n\n# Get sequence length max of 1024\ntokenizer.model_max_length=1024 \n\nmodel.eval()  # disable dropout (or leave in train mode to finetune)\n\n```\n\n#### Generate 1 word\n```python\n# input sequence\ntext = \"Bu yaz\u0131y\u0131 bilgisayar yazd\u0131.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# model output\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\nloss, logits = outputs[:2]\npredicted_index = torch.argmax(logits[0, -1, :]).item()\npredicted_text = tokenizer.decode([predicted_index])\n\n# results\nprint('input text:', text)\nprint('predicted text:', predicted_text)\n\n# input text: \n# predicted text:  \n\n```\n\n#### Generate Full Sequence\n```python\n# input sequence\ntext = \"Bu yaz\u0131y\u0131 bilgisayar yazd\u0131.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# model output using Top-k sampling text generation method\nsample_outputs = model.generate(inputs.input_ids,\n                                pad_token_id=50256,\n                                do_sample=True, \n                                max_length=50, # put the token number you want\n                                top_k=40,\n                                num_return_sequences=1)\n\n# generated sequence\nfor i, sample_output in enumerate(sample_outputs):\n    print(\">> Generated text {}\\\\\\\\\n\\\\\\\\\n{}\".format(i+1, tokenizer.decode(sample_output.tolist())))\n\n# >> Generated text\n#    \n\n```\n\n#### Limitations and bias\n\nThe training data used for this model come from Turkish Wikipedia. We know it contains a lot of unfiltered content from the internet, which is far from neutral. \n\n\n## Training data\n\nWikipedia Turkish article dump as of 28-10-2020\n\n## Training procedure\n\n\n## Eval results\n\ntrain_loss\\\\\\\\taccuracy\\\\\\\\ttime   |\n --------       ----------     ----- |\n4.777015\\\\\\\\t0.292547\\\\\\\\t2:42:05|\n4.509412\\\\\\\\t0.305574\\\\\\\\t1:09:38|\n4.169529\\\\\\\\t0.324908\\\\\\\\t1:07:45|\n4.293973\\\\\\\\t0.317211\\\\\\\\t1:07:02|\n4.049848\\\\\\\\t0.338347\\\\\\\\t1:05:53|\n\n#Epoch 0 on Tesla T4, others on V100\n\n```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of gorkemgoknar/gpt2-small-turkish?", "answers": [{"text": "gpt2", "answer_start": 38, "answer_end": 41}]}]}]}, {"title": "huggingartists/abba", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/abba\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">ABBA</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@abba</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from ABBA.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/abba\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on ABBA's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/abba')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/abba\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/abba\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/abba?", "answers": [{"text": "text-generation", "answer_start": 1652, "answer_end": 1666}]}]}]}, {"title": "huggingartists/arash", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/arash\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Arash</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@arash</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Arash.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/arash\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Arash's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/arash')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/arash\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/arash\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/arash?", "answers": [{"text": "text-generation", "answer_start": 1658, "answer_end": 1672}]}]}]}, {"title": "huggingartists/arctic-monkeys", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/arctic-monkeys\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Arctic Monkeys</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@arctic-monkeys</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Arctic Monkeys.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/arctic-monkeys\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Arctic Monkeys's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/arctic-monkeys')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/arctic-monkeys\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/arctic-monkeys\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/arctic-monkeys?", "answers": [{"text": "text-generation", "answer_start": 1712, "answer_end": 1726}]}]}]}, {"title": "huggingartists/ariana-grande", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/ariana-grande\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Ariana Grande</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@ariana-grande</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Ariana Grande.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/ariana-grande\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Ariana Grande's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/ariana-grande')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/ariana-grande\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/ariana-grande\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/ariana-grande?", "answers": [{"text": "text-generation", "answer_start": 1706, "answer_end": 1720}]}]}]}, {"title": "huggingartists/ariya", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/ariya\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">\u0410\u0440\u0438\u044f (Ariya)</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@ariya</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from \u0410\u0440\u0438\u044f (Ariya).\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/ariya\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on \u0410\u0440\u0438\u044f (Ariya)'s lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/ariya')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/ariya\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/ariya\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/ariya?", "answers": [{"text": "text-generation", "answer_start": 1679, "answer_end": 1693}]}]}]}, {"title": "huggingartists/armin-van-buuren", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/armin-van-buuren\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Armin van Buuren</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@armin-van-buuren</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Armin van Buuren.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/armin-van-buuren\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Armin van Buuren's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/armin-van-buuren')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/armin-van-buuren\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/armin-van-buuren\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/armin-van-buuren?", "answers": [{"text": "text-generation", "answer_start": 1724, "answer_end": 1738}]}]}]}, {"title": "huggingartists/as-i-lay-dying", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/as-i-lay-dying\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">As I Lay Dying</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@as-i-lay-dying</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from As I Lay Dying.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/as-i-lay-dying\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on As I Lay Dying's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/as-i-lay-dying')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/as-i-lay-dying\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/as-i-lay-dying\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/as-i-lay-dying?", "answers": [{"text": "text-generation", "answer_start": 1712, "answer_end": 1726}]}]}]}, {"title": "huggingartists/baklan", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/baklan\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">BAKLAN</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@baklan</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from BAKLAN.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/baklan\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on BAKLAN's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/baklan')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/baklan\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/baklan\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/baklan?", "answers": [{"text": "text-generation", "answer_start": 1664, "answer_end": 1678}]}]}]}, {"title": "huggingartists/platina", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/platina\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">\u041f\u043b\u0430\u0442\u0438\u043d\u0430 (Platina)</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@platina</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from \u041f\u043b\u0430\u0442\u0438\u043d\u0430 (Platina).\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/platina\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on \u041f\u043b\u0430\u0442\u0438\u043d\u0430 (Platina)'s lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/platina')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/platina\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/platina\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/platina?", "answers": [{"text": "text-generation", "answer_start": 1700, "answer_end": 1714}]}]}]}, {"title": "huggingartists/red-hot-chili-peppers", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/red-hot-chili-peppers\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Red Hot Chili Peppers</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@red-hot-chili-peppers</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Red Hot Chili Peppers.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/red-hot-chili-peppers\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Red Hot Chili Peppers's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/red-hot-chili-peppers')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/red-hot-chili-peppers\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/red-hot-chili-peppers\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/red-hot-chili-peppers?", "answers": [{"text": "text-generation", "answer_start": 1754, "answer_end": 1768}]}]}]}, {"title": "huggingartists/rex-orange-county", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/rex-orange-county\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Rex Orange County</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@rex-orange-county</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Rex Orange County.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/rex-orange-county\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Rex Orange County's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/rex-orange-county')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/rex-orange-county\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/rex-orange-county\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/rex-orange-county?", "answers": [{"text": "text-generation", "answer_start": 1730, "answer_end": 1744}]}]}]}, {"title": "huggingartists/rocket", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/rocket\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">ROCKET</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@rocket</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from ROCKET.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/rocket\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on ROCKET's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/rocket')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/rocket\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/rocket\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/rocket?", "answers": [{"text": "text-generation", "answer_start": 1664, "answer_end": 1678}]}]}]}, {"title": "huggingartists/sam-kim", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/sam-kim\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Sam Kim (\uc0d8\uae40)</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@sam-kim</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Sam Kim (\uc0d8\uae40).\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/sam-kim\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Sam Kim (\uc0d8\uae40)'s lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/sam-kim')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/sam-kim\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/sam-kim\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/sam-kim?", "answers": [{"text": "text-generation", "answer_start": 1685, "answer_end": 1699}]}]}]}, {"title": "huggingartists/sergei-letov", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/sergei-letov\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">\u0421\u0435\u0440\u0433\u0435\u0439 \u041b\u0435\u0442\u043e\u0432 (Sergei Letov)</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@sergei-letov</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from \u0421\u0435\u0440\u0433\u0435\u0439 \u041b\u0435\u0442\u043e\u0432 (Sergei Letov).\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/sergei-letov\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on \u0421\u0435\u0440\u0433\u0435\u0439 \u041b\u0435\u0442\u043e\u0432 (Sergei Letov)'s lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/sergei-letov')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/sergei-letov\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/sergei-letov\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/sergei-letov?", "answers": [{"text": "text-generation", "answer_start": 1745, "answer_end": 1759}]}]}]}, {"title": "huggingartists/shadowraze", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/shadowraze\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">\u200bshadowraze</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@shadowraze</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from \u200bshadowraze.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/shadowraze\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on \u200bshadowraze's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/shadowraze')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/shadowraze\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/shadowraze\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/shadowraze?", "answers": [{"text": "text-generation", "answer_start": 1691, "answer_end": 1705}]}]}]}, {"title": "huggingartists/skillet", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/skillet\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Skillet</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@skillet</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Skillet.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/skillet\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Skillet's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/skillet')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/skillet\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/skillet\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/skillet?", "answers": [{"text": "text-generation", "answer_start": 1670, "answer_end": 1684}]}]}]}, {"title": "huggingartists/slava-kpss", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/slava-kpss\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">\u0421\u043b\u0430\u0432\u0430 \u041a\u041f\u0421\u0421 (Slava KPSS)</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@slava-kpss</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from \u0421\u043b\u0430\u0432\u0430 \u041a\u041f\u0421\u0421 (Slava KPSS).\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/slava-kpss\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on \u0421\u043b\u0430\u0432\u0430 \u041a\u041f\u0421\u0421 (Slava KPSS)'s lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/slava-kpss')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/slava-kpss\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/slava-kpss\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/slava-kpss?", "answers": [{"text": "text-generation", "answer_start": 1727, "answer_end": 1741}]}]}]}, {"title": "huggingartists/slava-marlow", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/slava-marlow\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">SLAVA MARLOW</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@slava-marlow</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from SLAVA MARLOW.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/slava-marlow\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on SLAVA MARLOW's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/slava-marlow')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/slava-marlow\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/slava-marlow\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/slava-marlow?", "answers": [{"text": "text-generation", "answer_start": 1700, "answer_end": 1714}]}]}]}, {"title": "huggingartists/snoop-dogg", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/snoop-dogg\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Snoop Dogg</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@snoop-dogg</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Snoop Dogg.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/snoop-dogg\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Snoop Dogg's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/snoop-dogg')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/snoop-dogg\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/snoop-dogg\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/snoop-dogg?", "answers": [{"text": "text-generation", "answer_start": 1688, "answer_end": 1702}]}]}]}, {"title": "huggingartists/sqwore", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/sqwore\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Sqwore</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@sqwore</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Sqwore.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/sqwore\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Sqwore's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/sqwore')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/sqwore\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/sqwore\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/sqwore?", "answers": [{"text": "text-generation", "answer_start": 1664, "answer_end": 1678}]}]}]}, {"title": "huggingartists/sugar-ray", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/sugar-ray\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Sugar Ray</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@sugar-ray</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from Sugar Ray.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/sugar-ray\")\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/sugar-ray\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/sugar-ray\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on Sugar Ray's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/sugar-ray')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/sugar-ray?", "answers": [{"text": "text-generation", "answer_start": 1934, "answer_end": 1948}]}]}]}, {"title": "huggingartists/system-of-a-down", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- huggingartists/system-of-a-down\ntags:\n- huggingartists\n- lyrics\n- lm-head\n- causal-lm\nwidget:\n- text: \"I am\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 HuggingArtists Model \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">System of a Down</div>\n    <a href=\"\n    \t<div style=\"text-align: center; font-size: 14px;\">@system-of-a-down</div>\n    </a>\n</div>\n\nI was made with [huggingartists](\n\nCreate your own bot based on your favorite artist with [the demo](\n\n## How does it work?\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on lyrics from System of a Down.\n\nDataset is available [here](\nAnd can be used with:\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingartists/system-of-a-down\")\n```\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on System of a Down's lyrics.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingartists/system-of-a-down')\ngenerator(\"I am\", num_return_sequences=5)\n```\n\nOr with Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"huggingartists/system-of-a-down\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"huggingartists/system-of-a-down\")\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Aleksey Korshuk*\n\n[![Follow](\n\n[![Follow](\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingartists/system-of-a-down?", "answers": [{"text": "text-generation", "answer_start": 1724, "answer_end": 1738}]}]}]}, {"title": "huggingtweets/berniesanders-coffee__burger-sensanders", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI CYBORG \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Coffee Burger & Bernie Sanders & Bernie Sanders</div>\n    <div style=\"text-align: center; font-size: 14px;\">@berniesanders-coffee__burger-sensanders</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Coffee Burger & Bernie Sanders & Bernie Sanders.\n\n Coffee Burger  Bernie Sanders |\n ---  --- |\n 2471  3250 |\n 525  429 |\n 337  10 |\n 1609  2811 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @berniesanders-coffee__burger-sensanders's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/berniesanders-coffee__burger-sensanders')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/berniesanders-coffee__burger-sensanders?", "answers": [{"text": "text-generation", "answer_start": 2123, "answer_end": 2137}]}]}]}, {"title": "huggingtweets/berniesanders", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Bernie Sanders</div>\n    <div style=\"text-align: center; font-size: 14px;\">@berniesanders</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Bernie Sanders.\n\n Bernie Sanders |\n --- |\n 3250 |\n 387 |\n 7 |\n 2856 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @berniesanders's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/berniesanders')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/berniesanders?", "answers": [{"text": "text-generation", "answer_start": 1969, "answer_end": 1983}]}]}]}, {"title": "huggingtweets/bestmusiclyric-bygpt3", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI CYBORG \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Best Music Lyric & Wisdom_by_GPT3</div>\n    <div style=\"text-align: center; font-size: 14px;\">@bestmusiclyric-bygpt3</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Best Music Lyric & Wisdom_by_GPT3.\n\n Best Music Lyric \n --- \n 3248 \n 1092 \n 834 \n 1322 \n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @bestmusiclyric-bygpt3's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/bestmusiclyric-bygpt3')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/bestmusiclyric-bygpt3?", "answers": [{"text": "text-generation", "answer_start": 2020, "answer_end": 2034}]}]}]}, {"title": "huggingtweets/bladeefan91", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">sweetie p1e</div>\n    <div style=\"text-align: center; font-size: 14px;\">@bladeefan91</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from sweetie p1e.\n\n sweetie p1e |\n --- |\n 2249 |\n 351 |\n 547 |\n 1351 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @bladeefan91's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/bladeefan91')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/bladeefan91?", "answers": [{"text": "text-generation", "answer_start": 1958, "answer_end": 1972}]}]}]}, {"title": "huggingtweets/brennacgray", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">The Jackie Weaver of EdTech \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@brennacgray bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@brennacgray's tweets](\n\n Quantity |\n --- |\n 3229 |\n 500 |\n 301 |\n 2428 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @brennacgray's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/brennacgray')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/brennacgray?", "answers": [{"text": "text-generation", "answer_start": 1276, "answer_end": 1290}]}]}]}, {"title": "huggingtweets/cybercyberpop", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">CyberGoreAlice \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@cybercyberpop bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@cybercyberpop's tweets](\n\n Quantity |\n --- |\n 3223 |\n 870 |\n 925 |\n 1428 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @cybercyberpop's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/cybercyberpop')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/cybercyberpop?", "answers": [{"text": "text-generation", "answer_start": 1269, "answer_end": 1283}]}]}]}, {"title": "huggingtweets/d_greetest", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Greetest</div>\n    <div style=\"text-align: center; font-size: 14px;\">@d_greetest</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Greetest.\n\n Greetest |\n --- |\n 629 |\n 265 |\n 34 |\n 330 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @d_greetest's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/d_greetest')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/d_greetest?", "answers": [{"text": "text-generation", "answer_start": 1944, "answer_end": 1958}]}]}]}, {"title": "huggingtweets/daddykratos1", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Tweets by Kratos\ud83e\ude93</div>\n    <div style=\"text-align: center; font-size: 14px;\">@daddykratos1</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Tweets by Kratos\ud83e\ude93.\n\n Tweets by Kratos\ud83e\ude93 |\n --- |\n 626 |\n 14 |\n 52 |\n 560 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @daddykratos1's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/daddykratos1')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/daddykratos1?", "answers": [{"text": "text-generation", "answer_start": 1974, "answer_end": 1988}]}]}]}, {"title": "huggingtweets/daddyscumcock", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Big Daddy's Cum Cock</div>\n    <div style=\"text-align: center; font-size: 14px;\">@daddyscumcock</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Big Daddy's Cum Cock.\n\n Big Daddy's Cum Cock |\n --- |\n 449 |\n 85 |\n 41 |\n 323 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @daddyscumcock's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/daddyscumcock')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/daddyscumcock?", "answers": [{"text": "text-generation", "answer_start": 1985, "answer_end": 1999}]}]}]}, {"title": "huggingtweets/dadsaysjokes", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Dad Jokes \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@dadsaysjokes bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@dadsaysjokes's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3205</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>47</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>8</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>3150</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @dadsaysjokes's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/dadsaysjokes'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/dadsaysjokes?", "answers": [{"text": "text-generation", "answer_start": 2407, "answer_end": 2421}]}]}]}, {"title": "huggingtweets/daengerousk", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">K | Krimson Devils System\ud83d\udea9\ud83c\udff4 \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@daengerousk bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@daengerousk's tweets](\n\n Quantity |\n --- |\n 1980 |\n 1179 |\n 320 |\n 481 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @daengerousk's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/daengerousk')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/daengerousk?", "answers": [{"text": "text-generation", "answer_start": 1276, "answer_end": 1290}]}]}]}, {"title": "huggingtweets/daequaen", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Daequon G. (the g is for gaymer) \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@daequaen bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@daequaen's tweets](\n\n Quantity |\n --- |\n 1919 |\n 883 |\n 231 |\n 805 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @daequaen's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/daequaen')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/daequaen?", "answers": [{"text": "text-generation", "answer_start": 1271, "answer_end": 1285}]}]}]}, {"title": "huggingtweets/dailyartprompts", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Art Prompts</div>\n    <div style=\"text-align: center; font-size: 14px;\">@dailyartprompts</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Art Prompts.\n\n Art Prompts |\n --- |\n 726 |\n 16 |\n 1 |\n 709 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @dailyartprompts's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/dailyartprompts')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/dailyartprompts?", "answers": [{"text": "text-generation", "answer_start": 1961, "answer_end": 1975}]}]}]}, {"title": "huggingtweets/dailymicrofic", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Daily Micro Fiction \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@dailymicrofic bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@dailymicrofic's tweets](\n\n Quantity |\n --- |\n 296 |\n 5 |\n 47 |\n 244 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @dailymicrofic's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/dailymicrofic')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/dailymicrofic?", "answers": [{"text": "text-generation", "answer_start": 1269, "answer_end": 1283}]}]}]}, {"title": "huggingtweets/dakami", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Dan Kaminsky \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@dakami bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@dakami's tweets](\n\n Quantity |\n --- |\n 3250 |\n 133 |\n 395 |\n 2722 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @dakami's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/dakami')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/dakami?", "answers": [{"text": "text-generation", "answer_start": 1246, "answer_end": 1260}]}]}]}, {"title": "huggingtweets/dalailama", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Dalai Lama \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@dalailama bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@dalailama's tweets](\n\n Quantity |\n --- |\n 1664 |\n 0 |\n 1 |\n 1663 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @dalailama's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/dalailama')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/dalailama?", "answers": [{"text": "text-generation", "answer_start": 1249, "answer_end": 1263}]}]}]}, {"title": "huggingtweets/dallaswentdown-jwgrieve-shanselman", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI CYBORG \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Jack Grieve & Scott Hanselman & Marc Miller</div>\n    <div style=\"text-align: center; font-size: 14px;\">@dallaswentdown-jwgrieve-shanselman</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Jack Grieve & Scott Hanselman & Marc Miller.\n\n Jack Grieve  Marc Miller |\n ---  --- |\n 3241  204 |\n 408  11 |\n 325  16 |\n 2508  177 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @dallaswentdown-jwgrieve-shanselman's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/dallaswentdown-jwgrieve-shanselman')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/dallaswentdown-jwgrieve-shanselman?", "answers": [{"text": "text-generation", "answer_start": 2097, "answer_end": 2111}]}]}]}, {"title": "huggingtweets/daltonegreene", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Hugh Jazz \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@daltonegreene bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@daltonegreene's tweets](\n\n Quantity |\n --- |\n 3237 |\n 183 |\n 354 |\n 2700 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @daltonegreene's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/daltonegreene')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/daltonegreene?", "answers": [{"text": "text-generation", "answer_start": 1264, "answer_end": 1278}]}]}]}, {"title": "huggingtweets/daltonsakthi", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Dalton AR Sakthivadivel</div>\n    <div style=\"text-align: center; font-size: 14px;\">@daltonsakthi</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Dalton AR Sakthivadivel.\n\n Dalton AR Sakthivadivel |\n --- |\n 3220 |\n 1653 |\n 71 |\n 1496 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @daltonsakthi's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/daltonsakthi')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/daltonsakthi?", "answers": [{"text": "text-generation", "answer_start": 1996, "answer_end": 2010}]}]}]}, {"title": "huggingtweets/esyudkowsky", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Eliezer Yudkowsky</div>\n    <div style=\"text-align: center; font-size: 14px;\">@esyudkowsky</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Eliezer Yudkowsky.\n\n Eliezer Yudkowsky |\n --- |\n 3242 |\n 961 |\n 139 |\n 2142 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @esyudkowsky's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/esyudkowsky')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/esyudkowsky?", "answers": [{"text": "text-generation", "answer_start": 1976, "answer_end": 1990}]}]}]}, {"title": "huggingtweets/evandknox", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Evan Knox \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@evandknox bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@evandknox's tweets](\n\n Quantity |\n --- |\n 848 |\n 98 |\n 53 |\n 697 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @evandknox's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/evandknox')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/evandknox?", "answers": [{"text": "text-generation", "answer_start": 1248, "answer_end": 1262}]}]}]}, {"title": "huggingtweets/evetheism", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">D\u017crevelow\ud83e\udea4 (BANNED AGAIN) \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@evetheism bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@evetheism's tweets](\n\n Quantity |\n --- |\n 1393 |\n 136 |\n 386 |\n 871 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @evetheism's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/evetheism')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/evetheism?", "answers": [{"text": "text-generation", "answer_start": 1267, "answer_end": 1281}]}]}]}, {"title": "huggingtweets/evilbmcats", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Black Metal Cats \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@evilbmcats bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@evilbmcats's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>2201</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>2</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>240</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>1959</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @evilbmcats's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/evilbmcats'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                         -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/evilbmcats?", "answers": [{"text": "text-generation", "answer_start": 2409, "answer_end": 2423}]}]}]}, {"title": "huggingtweets/evolso", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Evan Olson \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@evolso bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@evolso's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>220</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>61</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>14</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>145</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @evolso's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/evolso'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                        -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/evolso?", "answers": [{"text": "text-generation", "answer_start": 2389, "answer_end": 2403}]}]}]}, {"title": "huggingtweets/existentialcoms", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Existential Comics \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@existentialcoms bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@existentialcoms's tweets](\n\n Quantity |\n --- |\n 3250 |\n 14 |\n 17 |\n 3219 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @existentialcoms's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/existentialcoms')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/existentialcoms?", "answers": [{"text": "text-generation", "answer_start": 1277, "answer_end": 1291}]}]}]}, {"title": "huggingtweets/ilike_birds", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Banon \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@ilike_birds bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@ilike_birds's tweets](\n\n Quantity |\n --- |\n 1017 |\n 39 |\n 337 |\n 641 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @ilike_birds's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/ilike_birds')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/ilike_birds?", "answers": [{"text": "text-generation", "answer_start": 1252, "answer_end": 1266}]}]}]}, {"title": "huggingtweets/incharmuese-sadsocrates-vvangone", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI CYBORG \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Charmeuse & Sad Socrates & Vincent Van Gone</div>\n    <div style=\"text-align: center; font-size: 14px;\">@incharmuese-sadsocrates-vvangone</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Charmeuse & Sad Socrates & Vincent Van Gone.\n\n Charmeuse  Vincent Van Gone |\n ---  --- |\n 3238  3233 |\n 1165  1054 |\n 248  266 |\n 1825  1913 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @incharmuese-sadsocrates-vvangone's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/incharmuese-sadsocrates-vvangone')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/incharmuese-sadsocrates-vvangone?", "answers": [{"text": "text-generation", "answer_start": 2102, "answer_end": 2116}]}]}]}, {"title": "huggingtweets/jasutherlandbks", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">J.A. Sutherland SciFi Books \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jasutherlandbks bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jasutherlandbks's tweets](\n\n Quantity |\n --- |\n 3192 |\n 952 |\n 169 |\n 2071 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jasutherlandbks's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jasutherlandbks')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jasutherlandbks?", "answers": [{"text": "text-generation", "answer_start": 1288, "answer_end": 1302}]}]}]}, {"title": "huggingtweets/jazzpomegranate", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Jasmine Persephone \u262d Black Podcast Revolution \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jazzpomegranate bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jazzpomegranate's tweets](\n\n Quantity |\n --- |\n 3208 |\n 184 |\n 720 |\n 2304 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jazzpomegranate's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jazzpomegranate')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jazzpomegranate?", "answers": [{"text": "text-generation", "answer_start": 1306, "answer_end": 1320}]}]}]}, {"title": "huggingtweets/jbmurray", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Jon Beasley-Murray \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jbmurray bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jbmurray's tweets](\n\n Quantity |\n --- |\n 2861 |\n 364 |\n 260 |\n 2237 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jbmurray's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jbmurray')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jbmurray?", "answers": [{"text": "text-generation", "answer_start": 1258, "answer_end": 1272}]}]}]}, {"title": "huggingtweets/jbpetersonquote", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Jordan Peterson Quotes \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jbpetersonquote bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jbpetersonquote's tweets](\n\n Quantity |\n --- |\n 1983 |\n 605 |\n 47 |\n 1331 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jbpetersonquote's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jbpetersonquote')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jbpetersonquote?", "answers": [{"text": "text-generation", "answer_start": 1282, "answer_end": 1296}]}]}]}, {"title": "huggingtweets/jcbdwsn", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">jacob \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jcbdwsn bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jcbdwsn's tweets](\n\n Quantity |\n --- |\n 3068 |\n 568 |\n 735 |\n 1765 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jcbdwsn's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jcbdwsn')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jcbdwsn?", "answers": [{"text": "text-generation", "answer_start": 1242, "answer_end": 1256}]}]}]}, {"title": "huggingtweets/jdcmedlock", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">James Medlock \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jdcmedlock bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jdcmedlock's tweets](\n\n Quantity |\n --- |\n 3249 |\n 186 |\n 598 |\n 2465 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jdcmedlock's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jdcmedlock')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jdcmedlock?", "answers": [{"text": "text-generation", "answer_start": 1259, "answer_end": 1273}]}]}]}, {"title": "huggingtweets/jeansingod", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">almond milk producer \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jeansingod bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jeansingod's tweets](\n\n Quantity |\n --- |\n 3229 |\n 121 |\n 928 |\n 2180 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jeansingod's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jeansingod')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jeansingod?", "answers": [{"text": "text-generation", "answer_start": 1266, "answer_end": 1280}]}]}]}, {"title": "huggingtweets/jeebustrump", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Bigger than Joe, Smaller than Corn Pop</div>\n    <div style=\"text-align: center; font-size: 14px;\">@jeebustrump</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Bigger than Joe, Smaller than Corn Pop.\n\n Bigger than Joe, Smaller than Corn Pop |\n --- |\n 3250 |\n 196 |\n 446 |\n 2608 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jeebustrump's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jeebustrump')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jeebustrump?", "answers": [{"text": "text-generation", "answer_start": 2039, "answer_end": 2053}]}]}]}, {"title": "huggingtweets/jeemstate", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">jeemers \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jeemstate bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jeemstate's tweets](\n\n Quantity |\n --- |\n 3243 |\n 220 |\n 400 |\n 2623 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jeemstate's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jeemstate')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jeemstate?", "answers": [{"text": "text-generation", "answer_start": 1250, "answer_end": 1264}]}]}]}, {"title": "huggingtweets/jeffdean", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Jeff Dean (@\ud83c\udfe1) \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@jeffdean bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jeffdean's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3222</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>926</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>153</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>2143</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jeffdean's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/jeffdean'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jeffdean?", "answers": [{"text": "text-generation", "answer_start": 2403, "answer_end": 2417}]}]}]}, {"title": "huggingtweets/jeffdeecee", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Jeff \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jeffdeecee bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jeffdeecee's tweets](\n\n Quantity |\n --- |\n 1918 |\n 424 |\n 318 |\n 1176 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jeffdeecee's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jeffdeecee')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jeffdeecee?", "answers": [{"text": "text-generation", "answer_start": 1250, "answer_end": 1264}]}]}]}, {"title": "huggingtweets/jematrics", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Jemma \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jematrics bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jematrics's tweets](\n\n Quantity |\n --- |\n 1860 |\n 131 |\n 437 |\n 1292 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jematrics's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jematrics')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jematrics?", "answers": [{"text": "text-generation", "answer_start": 1248, "answer_end": 1262}]}]}]}, {"title": "huggingtweets/jenslennartsson", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Jens \ud83e\uddf2 | Email Marketing</div>\n    <div style=\"text-align: center; font-size: 14px;\">@jenslennartsson</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Jens \ud83e\uddf2 | Email Marketing.\n\n Jens \ud83e\uddf2 \n --- |\n 3250 |\n 316 |\n 346 |\n 2588 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jenslennartsson's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jenslennartsson')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jenslennartsson?", "answers": [{"text": "text-generation", "answer_start": 1986, "answer_end": 2000}]}]}]}, {"title": "huggingtweets/jeremymmele", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Jeremy \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@jeremymmele bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@jeremymmele's tweets](\n\n Quantity |\n --- |\n 2670 |\n 596 |\n 135 |\n 1939 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @jeremymmele's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/jeremymmele')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/jeremymmele?", "answers": [{"text": "text-generation", "answer_start": 1255, "answer_end": 1269}]}]}]}, {"title": "huggingtweets/lily_dusk", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Lily Dusk \ud83c\udf80 | EN VTuber</div>\n    <div style=\"text-align: center; font-size: 14px;\">@lily_dusk</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Lily Dusk \ud83c\udf80 | EN VTuber.\n\n Lily Dusk \ud83c\udf80 \n --- |\n 3240 |\n 608 |\n 1143 |\n 1489 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lily_dusk's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lily_dusk')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lily_dusk?", "answers": [{"text": "text-generation", "answer_start": 1978, "answer_end": 1992}]}]}]}, {"title": "huggingtweets/lilyw12_", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Lily \ud83c\udff3\ufe0f\u200d\u26a7\ufe0f \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lilyw12_ bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lilyw12_'s tweets](\n\n Quantity |\n --- |\n 2974 |\n 94 |\n 693 |\n 2187 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lilyw12_'s tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lilyw12_')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lilyw12_?", "answers": [{"text": "text-generation", "answer_start": 1249, "answer_end": 1263}]}]}]}, {"title": "huggingtweets/lingtolls", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Josh Lester \ud83d\ude0e \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lingtolls bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lingtolls's tweets](\n\n Quantity |\n --- |\n 3244 |\n 866 |\n 290 |\n 2088 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lingtolls's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lingtolls')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lingtolls?", "answers": [{"text": "text-generation", "answer_start": 1256, "answer_end": 1270}]}]}]}, {"title": "huggingtweets/lionel_scott_", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">lionel scott \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lionel_scott_ bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lionel_scott_'s tweets](\n\n Quantity |\n --- |\n 1829 |\n 254 |\n 213 |\n 1362 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lionel_scott_'s tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lionel_scott_')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lionel_scott_?", "answers": [{"text": "text-generation", "answer_start": 1267, "answer_end": 1281}]}]}]}, {"title": "huggingtweets/liquidgoth", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">\u26a2 Jane \u26a2 \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@liquidgoth bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@liquidgoth's tweets](\n\n Quantity |\n --- |\n 3165 |\n 1232 |\n 424 |\n 1509 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @liquidgoth's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/liquidgoth')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/liquidgoth?", "answers": [{"text": "text-generation", "answer_start": 1255, "answer_end": 1269}]}]}]}, {"title": "huggingtweets/lisaannsimpson2", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Lisa Ann Simpson - Procrastinatrix \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lisaannsimpson2 bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lisaannsimpson2's tweets](\n\n Quantity |\n --- |\n 2661 |\n 16 |\n 150 |\n 2495 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lisaannsimpson2's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lisaannsimpson2')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lisaannsimpson2?", "answers": [{"text": "text-generation", "answer_start": 1294, "answer_end": 1308}]}]}]}, {"title": "huggingtweets/lisatomic5", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">lisatomic \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lisatomic5 bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lisatomic5's tweets](\n\n Quantity |\n --- |\n 3250 |\n 9 |\n 126 |\n 3115 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lisatomic5's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lisatomic5')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lisatomic5?", "answers": [{"text": "text-generation", "answer_start": 1253, "answer_end": 1267}]}]}]}, {"title": "huggingtweets/lithros", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Scott Hansen \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lithros bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lithros's tweets](\n\n Quantity |\n --- |\n 3246 |\n 279 |\n 505 |\n 2462 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lithros's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lithros')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lithros?", "answers": [{"text": "text-generation", "answer_start": 1249, "answer_end": 1263}]}]}]}, {"title": "huggingtweets/liv_garde", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Liv \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@liv_garde bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@liv_garde's tweets](\n\n Quantity |\n --- |\n 1974 |\n 70 |\n 81 |\n 1823 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @liv_garde's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/liv_garde')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/liv_garde?", "answers": [{"text": "text-generation", "answer_start": 1244, "answer_end": 1258}]}]}]}, {"title": "huggingtweets/liyrex_irl-mkleosb-vermontsmash", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI CYBORG \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">LNR  MkLeo & Vermont Smash Ultimate</div>\n    <div style=\"text-align: center; font-size: 14px;\">@liyrex_irl-mkleosb-vermontsmash</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from LNR  MkLeo & Vermont Smash Ultimate.\n\n LNR  T1  Vermont Smash Ultimate |\n ---  --- |\n 3203  205 |\n 1683  26 |\n 277  14 |\n 1243  165 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @liyrex_irl-mkleosb-vermontsmash's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/liyrex_irl-mkleosb-vermontsmash')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/liyrex_irl-mkleosb-vermontsmash?", "answers": [{"text": "text-generation", "answer_start": 2083, "answer_end": 2097}]}]}]}, {"title": "huggingtweets/lizasoberano", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Liza Soberano \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@lizasoberano bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lizasoberano's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3094</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>1102</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>308</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>1684</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lizasoberano's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/lizasoberano'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                                                                                               -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lizasoberano?", "answers": [{"text": "text-generation", "answer_start": 2415, "answer_end": 2429}]}]}]}, {"title": "huggingtweets/lizzo", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">ALL THE RUMORS ARE TRUE</div>\n    <div style=\"text-align: center; font-size: 14px;\">@lizzo</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from ALL THE RUMORS ARE TRUE.\n\n ALL THE RUMORS ARE TRUE |\n --- |\n 3095 |\n 1412 |\n 420 |\n 1263 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lizzo's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lizzo')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lizzo?", "answers": [{"text": "text-generation", "answer_start": 1983, "answer_end": 1997}]}]}]}, {"title": "huggingtweets/lloyd_devoid", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">noid \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lloyd_devoid bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lloyd_devoid's tweets](\n\n Quantity |\n --- |\n 3045 |\n 1182 |\n 177 |\n 1686 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lloyd_devoid's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lloyd_devoid')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lloyd_devoid?", "answers": [{"text": "text-generation", "answer_start": 1257, "answer_end": 1271}]}]}]}, {"title": "huggingtweets/lmgriffjohnson", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Griffin Johnson \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@lmgriffjohnson bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lmgriffjohnson's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3197</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>251</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>690</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>2256</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lmgriffjohnson's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/lmgriffjohnson'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                                                  -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lmgriffjohnson?", "answers": [{"text": "text-generation", "answer_start": 2422, "answer_end": 2436}]}]}]}, {"title": "huggingtweets/lnglggdsclst", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">ballad of big nothing \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lnglggdsclst bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lnglggdsclst's tweets](\n\n Quantity |\n --- |\n 181 |\n 19 |\n 13 |\n 149 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lnglggdsclst's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lnglggdsclst')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lnglggdsclst?", "answers": [{"text": "text-generation", "answer_start": 1269, "answer_end": 1283}]}]}]}, {"title": "huggingtweets/locosherman2", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Sevag \ud83c\udf10\u271d\ufe0f \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@locosherman2 bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@locosherman2's tweets](\n\n Quantity |\n --- |\n 3130 |\n 868 |\n 372 |\n 1890 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @locosherman2's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/locosherman2')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/locosherman2?", "answers": [{"text": "text-generation", "answer_start": 1261, "answer_end": 1275}]}]}]}, {"title": "huggingtweets/logicaldota2", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">adarsh \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@logicaldota2 bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@logicaldota2's tweets](\n\n Quantity |\n --- |\n 309 |\n 20 |\n 146 |\n 143 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @logicaldota2's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/logicaldota2')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/logicaldota2?", "answers": [{"text": "text-generation", "answer_start": 1255, "answer_end": 1269}]}]}]}, {"title": "huggingtweets/logo_daedalus", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">R.\u0421\u0430\u043c \ud83e\udd8b\ud83d\udc0f</div>\n    <div style=\"text-align: center; font-size: 14px;\">@logo_daedalus</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from R.\u0421\u0430\u043c \ud83e\udd8b\ud83d\udc0f.\n\n R.\u0421\u0430\u043c \ud83e\udd8b\ud83d\udc0f |\n --- |\n 3246 |\n 286 |\n 525 |\n 2435 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @logo_daedalus's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/logo_daedalus')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/logo_daedalus?", "answers": [{"text": "text-generation", "answer_start": 1953, "answer_end": 1967}]}]}]}, {"title": "huggingtweets/lol8ball", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Cone \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lol8ball bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lol8ball's tweets](\n\n Quantity |\n --- |\n 3219 |\n 1127 |\n 143 |\n 1949 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lol8ball's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lol8ball')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lol8ball?", "answers": [{"text": "text-generation", "answer_start": 1245, "answer_end": 1259}]}]}]}, {"title": "huggingtweets/lord_voldemort7", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">The Dark Lord \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@lord_voldemort7 bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lord_voldemort7's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3227</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>9</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>269</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>2949</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lord_voldemort7's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/lord_voldemort7'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lord_voldemort7?", "answers": [{"text": "text-generation", "answer_start": 2421, "answer_end": 2435}]}]}]}, {"title": "huggingtweets/louispotok", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Louis Potok \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@louispotok bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@louispotok's tweets](\n\n Quantity |\n --- |\n 3225 |\n 474 |\n 117 |\n 2634 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @louispotok's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/louispotok')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/louispotok?", "answers": [{"text": "text-generation", "answer_start": 1257, "answer_end": 1271}]}]}]}, {"title": "huggingtweets/love_alvays", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Love Quotes \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@love_alvays bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@love_alvays's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3246</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>0</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>1</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>3245</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @love_alvays's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/love_alvays'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                                                    -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/love_alvays?", "answers": [{"text": "text-generation", "answer_start": 2405, "answer_end": 2419}]}]}]}, {"title": "huggingtweets/loverachelle2", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">LoveRachelle2</div>\n    <div style=\"text-align: center; font-size: 14px;\">@loverachelle2</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from LoveRachelle2.\n\n LoveRachelle2 |\n --- |\n 1440 |\n 102 |\n 92 |\n 1246 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @loverachelle2's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/loverachelle2')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/loverachelle2?", "answers": [{"text": "text-generation", "answer_start": 1967, "answer_end": 1981}]}]}]}, {"title": "huggingtweets/lowqualitybot", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">lowqualitybot \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lowqualitybot bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lowqualitybot's tweets](\n\n Quantity |\n --- |\n 1803 |\n 8 |\n 195 |\n 1600 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lowqualitybot's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lowqualitybot')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lowqualitybot?", "answers": [{"text": "text-generation", "answer_start": 1266, "answer_end": 1280}]}]}]}, {"title": "huggingtweets/lp_lapresse", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">La Presse \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lp_lapresse bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lp_lapresse's tweets](\n\n Quantity |\n --- |\n 3250 |\n 0 |\n 11 |\n 3239 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lp_lapresse's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lp_lapresse')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lp_lapresse?", "answers": [{"text": "text-generation", "answer_start": 1255, "answer_end": 1269}]}]}]}, {"title": "huggingtweets/lrcssndr", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">lara \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@lrcssndr bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lrcssndr's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3121</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>1068</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>564</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>1489</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lrcssndr's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/lrcssndr'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lrcssndr?", "answers": [{"text": "text-generation", "answer_start": 2394, "answer_end": 2408}]}]}]}, {"title": "huggingtweets/ltwukwuk", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">lieutenant-wukwuk</div>\n    <div style=\"text-align: center; font-size: 14px;\">@ltwukwuk</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from lieutenant-wukwuk.\n\n lieutenant-wukwuk |\n --- |\n 3219 |\n 366 |\n 150 |\n 2703 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @ltwukwuk's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/ltwukwuk')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/ltwukwuk?", "answers": [{"text": "text-generation", "answer_start": 1970, "answer_end": 1984}]}]}]}, {"title": "huggingtweets/lukashasnoidea", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">lukas \ud83c\udff3\ufe0f\u200d\ud83c\udf08 \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lukashasnoidea bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lukashasnoidea's tweets](\n\n Quantity |\n --- |\n 1557 |\n 829 |\n 132 |\n 596 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lukashasnoidea's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lukashasnoidea')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lukashasnoidea?", "answers": [{"text": "text-generation", "answer_start": 1267, "answer_end": 1281}]}]}]}, {"title": "huggingtweets/lukasvalatka", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Lukas Valatka</div>\n    <div style=\"text-align: center; font-size: 14px;\">@lukasvalatka</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Lukas Valatka.\n\n Lukas Valatka |\n --- |\n 1155 |\n 42 |\n 49 |\n 1064 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lukasvalatka's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lukasvalatka')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lukasvalatka?", "answers": [{"text": "text-generation", "answer_start": 1964, "answer_end": 1978}]}]}]}, {"title": "huggingtweets/lumakiri", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Ayup, they called me Stars \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lumakiri bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lumakiri's tweets](\n\n Quantity |\n --- |\n 565 |\n 152 |\n 68 |\n 345 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lumakiri's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lumakiri')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lumakiri?", "answers": [{"text": "text-generation", "answer_start": 1263, "answer_end": 1277}]}]}]}, {"title": "huggingtweets/lumetroid", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">lito \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lumetroid bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lumetroid's tweets](\n\n Quantity |\n --- |\n 3038 |\n 1326 |\n 414 |\n 1298 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lumetroid's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lumetroid')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lumetroid?", "answers": [{"text": "text-generation", "answer_start": 1248, "answer_end": 1262}]}]}]}, {"title": "huggingtweets/luna_lun_a", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Luna \ud83c\udff3\ufe0f\u200d\u26a7\ufe0f #ACAB \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@luna_lun_a bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@luna_lun_a's tweets](\n\n Quantity |\n --- |\n 1606 |\n 654 |\n 485 |\n 467 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @luna_lun_a's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/luna_lun_a')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/luna_lun_a?", "answers": [{"text": "text-generation", "answer_start": 1261, "answer_end": 1275}]}]}]}, {"title": "huggingtweets/lunch_enjoyer", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Patrick \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lunch_enjoyer bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lunch_enjoyer's tweets](\n\n Quantity |\n --- |\n 3237 |\n 398 |\n 667 |\n 2172 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lunch_enjoyer's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lunch_enjoyer')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lunch_enjoyer?", "answers": [{"text": "text-generation", "answer_start": 1262, "answer_end": 1276}]}]}]}, {"title": "huggingtweets/lux_capital", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Lux Capital</div>\n    <div style=\"text-align: center; font-size: 14px;\">@lux_capital</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Lux Capital.\n\n Lux Capital |\n --- |\n 2329 |\n 597 |\n 22 |\n 1710 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lux_capital's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lux_capital')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lux_capital?", "answers": [{"text": "text-generation", "answer_start": 1957, "answer_end": 1971}]}]}]}, {"title": "huggingtweets/lynnbee01", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Lynnbee \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@lynnbee01 bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@lynnbee01's tweets](\n\n Quantity |\n --- |\n 3136 |\n 1145 |\n 243 |\n 1748 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lynnbee01's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lynnbee01')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lynnbee01?", "answers": [{"text": "text-generation", "answer_start": 1251, "answer_end": 1265}]}]}]}, {"title": "huggingtweets/matthewespinosa", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Matthew Espinosa \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@matthewespinosa bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@matthewespinosa's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3144</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>302</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>563</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>2279</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @matthewespinosa's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/matthewespinosa'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                                                            -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/matthewespinosa?", "answers": [{"text": "text-generation", "answer_start": 2426, "answer_end": 2440}]}]}]}, {"title": "huggingtweets/mattjope", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Matt Jope \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@mattjope bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mattjope's tweets](\n\n Quantity |\n --- |\n 827 |\n 104 |\n 95 |\n 628 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mattjope's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mattjope')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mattjope?", "answers": [{"text": "text-generation", "answer_start": 1246, "answer_end": 1260}]}]}]}, {"title": "huggingtweets/mdennedy", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Michelle Finneran Dennedy \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@mdennedy bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mdennedy's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3217</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>1383</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>327</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>1507</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mdennedy's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/mdennedy'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mdennedy?", "answers": [{"text": "text-generation", "answer_start": 2415, "answer_end": 2429}]}]}]}, {"title": "huggingtweets/midwaymedway", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Medway \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@midwaymedway bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@midwaymedway's tweets](\n\n Quantity |\n --- |\n 2852 |\n 584 |\n 273 |\n 1995 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @midwaymedway's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/midwaymedway')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/midwaymedway?", "answers": [{"text": "text-generation", "answer_start": 1258, "answer_end": 1272}]}]}]}, {"title": "huggingtweets/miild90", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">mild\ud83c\udf79 \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@miild90 bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@miild90's tweets](\n\n Quantity |\n --- |\n 909 |\n 48 |\n 183 |\n 678 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @miild90's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/miild90')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/miild90?", "answers": [{"text": "text-generation", "answer_start": 1239, "answer_end": 1253}]}]}]}, {"title": "huggingtweets/mike_massive", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">mike insane \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@mike_massive bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mike_massive's tweets](\n\n Quantity |\n --- |\n 434 |\n 67 |\n 73 |\n 294 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mike_massive's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mike_massive')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mike_massive?", "answers": [{"text": "text-generation", "answer_start": 1259, "answer_end": 1273}]}]}]}, {"title": "huggingtweets/mike_pence", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Mike Pence \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@mike_pence bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mike_pence's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>2498</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>1360</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>161</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>977</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mike_pence's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/mike_pence'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mike_pence?", "answers": [{"text": "text-generation", "answer_start": 2405, "answer_end": 2419}]}]}]}, {"title": "huggingtweets/mikekyismad", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Mikeky Mckekerson \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@mikekyismad bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mikekyismad's tweets](\n\n Quantity |\n --- |\n 576 |\n 11 |\n 198 |\n 367 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mikekyismad's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mikekyismad')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mikekyismad?", "answers": [{"text": "text-generation", "answer_start": 1263, "answer_end": 1277}]}]}]}, {"title": "huggingtweets/mikeyyshorts", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">mikey l-h-f-m (donathon creek)</div>\n    <div style=\"text-align: center; font-size: 14px;\">@mikeyyshorts</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from mikey l-h-f-m (donathon creek).\n\n mikey l-h-f-m (donathon creek) |\n --- |\n 1850 |\n 162 |\n 336 |\n 1352 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mikeyyshorts's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mikeyyshorts')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mikeyyshorts?", "answers": [{"text": "text-generation", "answer_start": 2017, "answer_end": 2031}]}]}]}, {"title": "huggingtweets/mikrodystopies", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Mikrodystopies \ud83e\udd16 \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@mikrodystopies bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mikrodystopies's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>1353</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>14</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>3</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>1336</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mikrodystopies's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/mikrodystopies'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                         -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mikrodystopies?", "answers": [{"text": "text-generation", "answer_start": 2420, "answer_end": 2434}]}]}]}, {"title": "huggingtweets/mild_lakes", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Mild \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@mild_lakes bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mild_lakes's tweets](\n\n Quantity |\n --- |\n 2207 |\n 517 |\n 601 |\n 1089 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mild_lakes's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mild_lakes')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mild_lakes?", "answers": [{"text": "text-generation", "answer_start": 1250, "answer_end": 1264}]}]}]}, {"title": "huggingtweets/milligram3d", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">im gay \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@milligram3d bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@milligram3d's tweets](\n\n Quantity |\n --- |\n 3102 |\n 514 |\n 267 |\n 2321 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @milligram3d's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/milligram3d')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/milligram3d?", "answers": [{"text": "text-generation", "answer_start": 1255, "answer_end": 1269}]}]}]}, {"title": "huggingtweets/mineplay512", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">MinePlay512</div>\n    <div style=\"text-align: center; font-size: 14px;\">@mineplay512</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from MinePlay512.\n\n MinePlay512 |\n --- |\n 3234 |\n 1107 |\n 404 |\n 1723 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mineplay512's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mineplay512')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mineplay512?", "answers": [{"text": "text-generation", "answer_start": 1959, "answer_end": 1973}]}]}]}, {"title": "huggingtweets/minidiscplus", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Diskette \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@minidiscplus bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@minidiscplus's tweets](\n\n Quantity |\n --- |\n 731 |\n 58 |\n 98 |\n 575 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @minidiscplus's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/minidiscplus')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/minidiscplus?", "answers": [{"text": "text-generation", "answer_start": 1256, "answer_end": 1270}]}]}]}, {"title": "huggingtweets/mrmeatscience", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Chet Humphries \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@mrmeatscience bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mrmeatscience's tweets](\n\n Quantity |\n --- |\n 1483 |\n 641 |\n 121 |\n 721 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mrmeatscience's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mrmeatscience')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mrmeatscience?", "answers": [{"text": "text-generation", "answer_start": 1268, "answer_end": 1282}]}]}]}, {"title": "huggingtweets/mrwheatley3", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Mr Wheatley</div>\n    <div style=\"text-align: center; font-size: 14px;\">@mrwheatley3</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Mr Wheatley.\n\n Mr Wheatley |\n --- |\n 730 |\n 0 |\n 290 |\n 440 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mrwheatley3's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mrwheatley3')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mrwheatley3?", "answers": [{"text": "text-generation", "answer_start": 1954, "answer_end": 1968}]}]}]}, {"title": "huggingtweets/mullbot_forever", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">extremely online bot</div>\n    <div style=\"text-align: center; font-size: 14px;\">@mullbot_forever</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from extremely online bot.\n\n extremely online bot |\n --- |\n 1432 |\n 0 |\n 22 |\n 1410 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mullbot_forever's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mullbot_forever')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mullbot_forever?", "answers": [{"text": "text-generation", "answer_start": 1990, "answer_end": 2004}]}]}]}, {"title": "huggingtweets/murderlinart", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">AJ \ud83c\udf40 \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@murderlinart bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@murderlinart's tweets](\n\n Quantity |\n --- |\n 3230 |\n 1141 |\n 544 |\n 1545 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @murderlinart's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/murderlinart')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/murderlinart?", "answers": [{"text": "text-generation", "answer_start": 1257, "answer_end": 1271}]}]}]}, {"title": "huggingtweets/musebiihi", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Muse Bihi Abdi \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@musebiihi bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@musebiihi's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>494</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>48</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>4</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>442</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @musebiihi's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/musebiihi'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/musebiihi?", "answers": [{"text": "text-generation", "answer_start": 2401, "answer_end": 2415}]}]}]}, {"title": "huggingtweets/musicalmushr00m", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">mushr00m</div>\n    <div style=\"text-align: center; font-size: 14px;\">@musicalmushr00m</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from mushr00m.\n\n mushr00m |\n --- |\n 161 |\n 50 |\n 33 |\n 78 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @musicalmushr00m's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/musicalmushr00m')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/musicalmushr00m?", "answers": [{"text": "text-generation", "answer_start": 1952, "answer_end": 1966}]}]}]}, {"title": "huggingtweets/musingsofyouth", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Autumn Youth</div>\n    <div style=\"text-align: center; font-size: 14px;\">@musingsofyouth</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Autumn Youth.\n\n Autumn Youth |\n --- |\n 3241 |\n 89 |\n 129 |\n 3023 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @musingsofyouth's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/musingsofyouth')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/musingsofyouth?", "answers": [{"text": "text-generation", "answer_start": 1966, "answer_end": 1980}]}]}]}, {"title": "huggingtweets/mutilumila", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">p a ' u l \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@mutilumila bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mutilumila's tweets](\n\n Quantity |\n --- |\n 3227 |\n 432 |\n 618 |\n 2177 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mutilumila's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mutilumila')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mutilumila?", "answers": [{"text": "text-generation", "answer_start": 1255, "answer_end": 1269}]}]}]}, {"title": "huggingtweets/mysticmaryy", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Mary \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@mysticmaryy bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@mysticmaryy's tweets](\n\n Quantity |\n --- |\n 3185 |\n 829 |\n 417 |\n 1939 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mysticmaryy's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mysticmaryy')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mysticmaryy?", "answers": [{"text": "text-generation", "answer_start": 1253, "answer_end": 1267}]}]}]}, {"title": "huggingtweets/najmc", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Najm Clayton \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@najmc bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@najmc's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3172</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>2115</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>170</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>887</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @najmc's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/najmc'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/najmc?", "answers": [{"text": "text-generation", "answer_start": 2392, "answer_end": 2406}]}]}]}, {"title": "huggingtweets/nancycartnite", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">luiz \ud83c\udf24\ud83d\udc3a \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@nancycartnite bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@nancycartnite's tweets](\n\n Quantity |\n --- |\n 3245 |\n 67 |\n 639 |\n 2539 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @nancycartnite's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/nancycartnite')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/nancycartnite?", "answers": [{"text": "text-generation", "answer_start": 1261, "answer_end": 1275}]}]}]}, {"title": "huggingtweets/nasa", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">NASA</div>\n    <div style=\"text-align: center; font-size: 14px;\">@nasa</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from NASA.\n\n NASA |\n --- |\n 3250 |\n 671 |\n 61 |\n 2518 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @nasa's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/nasa')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/nasa?", "answers": [{"text": "text-generation", "answer_start": 1922, "answer_end": 1936}]}]}]}, {"title": "huggingtweets/natashajaques", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Natasha Jaques \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@natashajaques bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@natashajaques's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>799</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>518</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>23</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>258</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @natashajaques's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/natashajaques'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/natashajaques?", "answers": [{"text": "text-generation", "answer_start": 2415, "answer_end": 2429}]}]}]}, {"title": "huggingtweets/nateritter-naval", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI CYBORG \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Nate Ritter & Naval</div>\n    <div style=\"text-align: center; font-size: 14px;\">@nateritter-naval</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Nate Ritter & Naval.\n\n Nate Ritter \n --- \n 3244 \n 401 \n 400 \n 2443 \n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @nateritter-naval's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/nateritter-naval')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/nateritter-naval?", "answers": [{"text": "text-generation", "answer_start": 1976, "answer_end": 1990}]}]}]}, {"title": "huggingtweets/natesilver538", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Nate Silver</div>\n    <div style=\"text-align: center; font-size: 14px;\">@natesilver538</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Nate Silver.\n\n Nate Silver |\n --- |\n 3250 |\n 413 |\n 43 |\n 2794 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @natesilver538's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/natesilver538')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/natesilver538?", "answers": [{"text": "text-generation", "answer_start": 1961, "answer_end": 1975}]}]}]}, {"title": "huggingtweets/nathanlawkc", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Nathan Law \u7f85\u51a0\u8070 \ud83d\ude37 \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@nathanlawkc bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@nathanlawkc's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>2786</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>996</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>463</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>1327</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @nathanlawkc's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/nathanlawkc'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/nathanlawkc?", "answers": [{"text": "text-generation", "answer_start": 2414, "answer_end": 2428}]}]}]}, {"title": "huggingtweets/nathanmarz", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Nathan Marz</div>\n    <div style=\"text-align: center; font-size: 14px;\">@nathanmarz</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Nathan Marz.\n\n Nathan Marz |\n --- |\n 3188 |\n 459 |\n 239 |\n 2490 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @nathanmarz's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/nathanmarz')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/nathanmarz?", "answers": [{"text": "text-generation", "answer_start": 1956, "answer_end": 1970}]}]}]}, {"title": "huggingtweets/onlinepete", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">im pete online</div>\n    <div style=\"text-align: center; font-size: 14px;\">@onlinepete</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from im pete online.\n\n im pete online |\n --- |\n 3190 |\n 94 |\n 1003 |\n 2093 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @onlinepete's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/onlinepete')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/onlinepete?", "answers": [{"text": "text-generation", "answer_start": 1965, "answer_end": 1979}]}]}]}, {"title": "huggingtweets/oohloo", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Dave Davies \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@oohloo bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@oohloo's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3249</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>128</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>285</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>2836</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @oohloo's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/oohloo'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                                                                        -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/oohloo?", "answers": [{"text": "text-generation", "answer_start": 2394, "answer_end": 2408}]}]}]}, {"title": "huggingtweets/ookinanami73", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">\u5927\u6a39\u4e03\u6d77\u30aa\u30aa\u30ad\u30ca\u30ca\u30df\u3000\u66f8\u7c4d\u300e\u5f01\u7406\u58eb\u306b\u304a\u4efb\u305b\u3042\u308c\u300f\uff5c\u300e\u30d3\u30b8\u30cd\u30b9\u30c4\u30fc\u30eb\u3068\u3057\u3066\u306e\u77e5\u7684\u8ca1\u7523\u300f\u767a\u58f2\u4e2d \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@ookinanami73 bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@ookinanami73's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3192</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>1202</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>1279</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>711</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @ookinanami73's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/ookinanami73'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                                                                                                     -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/ookinanami73?", "answers": [{"text": "text-generation", "answer_start": 2447, "answer_end": 2461}]}]}]}, {"title": "huggingtweets/oooolya", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">I ksenitemeni \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@oooolya bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@oooolya's tweets](\n\n Quantity |\n --- |\n 3155 |\n 2625 |\n 43 |\n 487 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @oooolya's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/oooolya')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/oooolya?", "answers": [{"text": "text-generation", "answer_start": 1249, "answer_end": 1263}]}]}]}, {"title": "huggingtweets/opalresplendent", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Opal, Effortlessly \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@opalresplendent bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@opalresplendent's tweets](\n\n Quantity |\n --- |\n 3237 |\n 406 |\n 499 |\n 2332 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @opalresplendent's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/opalresplendent')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/opalresplendent?", "answers": [{"text": "text-generation", "answer_start": 1279, "answer_end": 1293}]}]}]}, {"title": "huggingtweets/opolopso", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">caroline \u25cd\u2022\u1d17\u2022\u25cd \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@opolopso bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@opolopso's tweets](\n\n Quantity |\n --- |\n 1569 |\n 373 |\n 278 |\n 918 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @opolopso's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/opolopso')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/opolopso?", "answers": [{"text": "text-generation", "answer_start": 1253, "answer_end": 1267}]}]}]}, {"title": "huggingtweets/opossumzavod", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Leonid Bruhzhnev \ud83d\udea9\u2694\ud83d\udc51 \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@opossumzavod bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@opossumzavod's tweets](\n\n Quantity |\n --- |\n 3184 |\n 1034 |\n 242 |\n 1908 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @opossumzavod's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/opossumzavod')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/opossumzavod?", "answers": [{"text": "text-generation", "answer_start": 1273, "answer_end": 1287}]}]}]}, {"title": "huggingtweets/oprah", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Oprah Winfrey \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@oprah bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@oprah's tweets](\n\n Quantity |\n --- |\n 3226 |\n 436 |\n 179 |\n 2611 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @oprah's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/oprah')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/oprah?", "answers": [{"text": "text-generation", "answer_start": 1244, "answer_end": 1258}]}]}]}, {"title": "huggingtweets/ora_vt", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Ora \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@ora_vt bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@ora_vt's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>757</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>543</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>54</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>160</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @ora_vt's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/ora_vt'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/ora_vt?", "answers": [{"text": "text-generation", "answer_start": 2383, "answer_end": 2397}]}]}]}, {"title": "huggingtweets/oratorofvibes", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Bri \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@oratorofvibes bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@oratorofvibes's tweets](\n\n Quantity |\n --- |\n 949 |\n 190 |\n 115 |\n 644 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @oratorofvibes's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/oratorofvibes')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/oratorofvibes?", "answers": [{"text": "text-generation", "answer_start": 1256, "answer_end": 1270}]}]}]}, {"title": "huggingtweets/oreocamus", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">oweo \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@oreocamus bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@oreocamus's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>764</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>60</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>105</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>599</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @oreocamus's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/oreocamus'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/oreocamus?", "answers": [{"text": "text-generation", "answer_start": 2393, "answer_end": 2407}]}]}]}, {"title": "huggingtweets/uhaul_cares", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">U-Haul Cares \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@uhaul_cares bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@uhaul_cares's tweets](\n\n Quantity |\n --- |\n 3225 |\n 9 |\n 5 |\n 3211 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @uhaul_cares's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/uhaul_cares')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/uhaul_cares?", "answers": [{"text": "text-generation", "answer_start": 1257, "answer_end": 1271}]}]}]}, {"title": "huggingtweets/universal_lucas-void_vomicae", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI CYBORG \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">GWF HeyGirl & \u300a \ud835\ude9f o\u0336 \ud835\ude92 \ud835\ude8d \u300b</div>\n    <div style=\"text-align: center; font-size: 14px;\">@universal_lucas-void_vomicae</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from GWF HeyGirl & \u300a \ud835\ude9f o\u0336 \ud835\ude92 \ud835\ude8d \u300b.\n\n GWF HeyGirl \n --- \n 292 \n 46 \n 30 \n 216 \n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @universal_lucas-void_vomicae's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/universal_lucas-void_vomicae')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/universal_lucas-void_vomicae?", "answers": [{"text": "text-generation", "answer_start": 2010, "answer_end": 2024}]}]}]}, {"title": "huggingtweets/unkledell", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">freeuzi</div>\n    <div style=\"text-align: center; font-size: 14px;\">@unkledell</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from freeuzi.\n\n freeuzi |\n --- |\n 3220 |\n 138 |\n 1159 |\n 1923 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @unkledell's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/unkledell')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/unkledell?", "answers": [{"text": "text-generation", "answer_start": 1943, "answer_end": 1957}]}]}]}, {"title": "huggingtweets/unmoglich1", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">unmoglich</div>\n    <div style=\"text-align: center; font-size: 14px;\">@unmoglich1</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from unmoglich.\n\n unmoglich |\n --- |\n 1176 |\n 108 |\n 267 |\n 801 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @unmoglich1's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/unmoglich1')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/unmoglich1?", "answers": [{"text": "text-generation", "answer_start": 1949, "answer_end": 1963}]}]}]}, {"title": "huggingtweets/uppityducky", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Ducky \ud83d\udc24\u2728 VTUBER</div>\n    <div style=\"text-align: center; font-size: 14px;\">@uppityducky</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Ducky \ud83d\udc24\u2728 VTUBER.\n\n Ducky \ud83d\udc24\u2728 VTUBER |\n --- |\n 3234 |\n 541 |\n 414 |\n 2279 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @uppityducky's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/uppityducky')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/uppityducky?", "answers": [{"text": "text-generation", "answer_start": 1970, "answer_end": 1984}]}]}]}, {"title": "huggingtweets/urmomlolroasted", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">anna!!!!!</div>\n    <div style=\"text-align: center; font-size: 14px;\">@urmomlolroasted</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from anna!!!!!.\n\n anna!!!!! |\n --- |\n 3192 |\n 477 |\n 700 |\n 2015 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @urmomlolroasted's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/urmomlolroasted')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/urmomlolroasted?", "answers": [{"text": "text-generation", "answer_start": 1960, "answer_end": 1974}]}]}]}, {"title": "huggingtweets/urst0ff", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">death metal minivan \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@urst0ff bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@urst0ff's tweets](\n\n Quantity |\n --- |\n 1955 |\n 75 |\n 151 |\n 1729 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @urst0ff's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/urst0ff')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/urst0ff?", "answers": [{"text": "text-generation", "answer_start": 1255, "answer_end": 1269}]}]}]}, {"title": "huggingtweets/usethespacebar", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">\ud83e\udde0 Grant R. Vousden-Dishington \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@usethespacebar bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@usethespacebar's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3178</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>1289</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>91</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>1798</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @usethespacebar's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/usethespacebar'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                                                                                                     -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/usethespacebar?", "answers": [{"text": "text-generation", "answer_start": 2436, "answer_end": 2450}]}]}]}, {"title": "huggingtweets/uspto", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">USPTO \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@uspto bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@uspto's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>3211</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>530</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>8</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>2673</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @uspto's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/uspto'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file           -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/uspto?", "answers": [{"text": "text-generation", "answer_start": 2383, "answer_end": 2397}]}]}]}, {"title": "huggingtweets/uwusman", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">UwUsman el Pez | piss arc \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@uwusman bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@uwusman's tweets](\n\n Quantity |\n --- |\n 3241 |\n 576 |\n 629 |\n 2036 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @uwusman's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/uwusman')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/uwusman?", "answers": [{"text": "text-generation", "answer_start": 1262, "answer_end": 1276}]}]}]}, {"title": "huggingtweets/vanpelt", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<link rel=\"stylesheet\" href=\"\n\n<style>\n@media (prefers-color-scheme: dark) {\n  .prose { color: #E2E8F0 !important; }\n  .prose h2, .prose h3, .prose a, .prose thead { color: #F7FAFC !important; }\n}\n</style>\n\n<section class='prose'>\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Chris Van Pelt (CVP) \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px; color: #657786\">@vanpelt bot</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on [@vanpelt's tweets](\n\n<table style='border-width:0'>\n<thead style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #CBD5E0'>\n<th style='border-width:0'>Data</th>\n<th style='border-width:0'>Quantity</th>\n</tr>\n</thead>\n<tbody style='border-width:0'>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Tweets downloaded</td>\n<td style='border-width:0'>813</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Retweets</td>\n<td style='border-width:0'>87</td>\n</tr>\n<tr style='border-width:0 0 1px 0; border-color: #E2E8F0'>\n<td style='border-width:0'>Short tweets</td>\n<td style='border-width:0'>43</td>\n</tr>\n<tr style='border-width:0'>\n<td style='border-width:0'>Tweets kept</td>\n<td style='border-width:0'>683</td>\n</tr>\n</tbody>\n</table>\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @vanpelt's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## Intended uses & limitations\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n<pre><code><span style=\"color:#03A9F4\">from</span> transformers <span style=\"color:#03A9F4\">import</span> pipeline\ngenerator = pipeline(<span style=\"color:#FF9800\">'text-generation'</span>,\n                     model=<span style=\"color:#FF9800\">'huggingtweets/vanpelt'</span>)\ngenerator(<span style=\"color:#FF9800\">\"My dream is\"</span>, num_return_sequences=<span style=\"color:#8BC34A\">5</span>)</code></pre>\n\n\n### Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n</section>\n\n[![Follow](\n\n<section class='prose'>\nFor more details, visit the project repository.\n</section>\n\n[![GitHub stars](\n\n<!--- random size file                -->", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/vanpelt?", "answers": [{"text": "text-generation", "answer_start": 2402, "answer_end": 2416}]}]}]}, {"title": "ipuneetrathore/bert-base-cased-finetuned-finBERT", "paragraphs": [{"context": "## FinBERT\n\nCode for importing and using this model is available [here](\n", "qas": []}]}, {"title": "ironman123/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n#Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of ironman123/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "ishan/bert-base-uncased-mnli", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- pytorch\n- text-classification\ndatasets:\n- MNLI\n---\n\n# bert-base-uncased finetuned on MNLI\n\n## Model Details and Training Data\n\nWe used the pretrained model from [bert-base-uncased]( and finetuned it on [MultiNLI]( dataset. \n\nThe training parameters were kept the same as [Devlin et al., 2019]( (learning rate = 2e-5, training epochs = 3, max_sequence_len = 128 and batch_size = 32).\n\n## Evaluation Results\n\nThe evaluation results are mentioned in the table below.\n\n Accuracy |\n:---------:|\n 0.8456 |\n 0.8484 |\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of ishan/bert-base-uncased-mnli?", "answers": [{"text": "bert", "answer_start": 91, "answer_end": 94}]}, {"id": "q2", "question": "What is the model task of ishan/bert-base-uncased-mnli?", "answers": [{"text": "text-classification", "answer_start": 47, "answer_end": 65}]}]}]}, {"title": "ishan/distilbert-base-uncased-mnli", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- pytorch\n- text-classification\ndatasets:\n- MNLI\n---\n\n# distilbert-base-uncased finetuned on MNLI\n\n## Model Details and Training Data\n\nWe used the pretrained model from [distilbert-base-uncased]( and finetuned it on [MultiNLI]( dataset. \n\nThe training parameters were kept the same as [Devlin et al., 2019]( (learning rate = 2e-5, training epochs = 3, max_sequence_len = 128 and batch_size = 32).\n\n## Evaluation Results\n\nThe evaluation results are mentioned in the table below.\n\n Accuracy |\n:---------:|\n 0.8223 |\n 0.8216 |\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of ishan/distilbert-base-uncased-mnli?", "answers": [{"text": "distilbert", "answer_start": 91, "answer_end": 100}]}, {"id": "q2", "question": "What is the model task of ishan/distilbert-base-uncased-mnli?", "answers": [{"text": "text-classification", "answer_start": 47, "answer_end": 65}]}]}]}, {"title": "ishraaqparvez/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Hrry Potter DialoGPT Model\n", "qas": [{"id": "q2", "question": "What is the model task of ishraaqparvez/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "ixa-ehu/SciBERT-SQuAD-QuAC", "paragraphs": [{"context": "---\nlanguage: en\n---\n\n# SciBERT-SQuAD-QuAC\n\nThis is the [SciBERT language representation model]( fine tuned for Question Answering. SciBERT is a pre-trained language model based on BERT that has been trained on a large corpus of scientific text. When fine tuning for Question Answering we combined [SQuAD2.0]( and [QuAC]( datasets.\n\nIf using this model, please cite the following paper:\n```\n@inproceedings{otegi-etal-2020-automatic,\n    title = \"Automatic Evaluation vs. User Preference in Neural Textual {Q}uestion{A}nswering over {COVID}-19 Scientific Literature\",\n    author = \"Otegi, Arantxa  and\n      Campos, Jon Ander  and\n      Azkune, Gorka  and\n      Soroa, Aitor  and\n      Agirre, Eneko\",\n    booktitle = \"Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020\",\n    month = dec,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    doi = \"10.18653/v1/2020.nlpcovid19-2.15\",\n}\n```\n", "qas": []}]}, {"title": "ixa-ehu/berteus-base-cased", "paragraphs": [{"context": "---\nlanguage: eu\n---\n\n# BERTeus base cased\n\nThis is the Basque language pretrained model presented in [Give your Text Representation Models some Love: the Case for Basque]( This model has been trained on a Basque corpus comprising Basque crawled news articles from online newspapers and the Basque Wikipedia. The training corpus contains 224.6 million tokens, of which 35 million come from the Wikipedia.\n\nBERTeus has been tested on four different downstream tasks for Basque: part-of-speech (POS) tagging, named entity recognition (NER), sentiment analysis and topic classification; improving the state of the art for all tasks. See summary of results below:\n\n\n BERTeus  Previous SOTA |\n -------  ------------- |\n **76.77**    63.00 \t    |\n **78.10**    74.02 \t    |\n **97.76**    96.10 \t    |\n **87.06**    76.72 \t    |\n\n\nIf using this model, please cite the following paper:\n```\n@inproceedings{agerri2020give,\n  title={Give your Text Representation Models some Love: the Case for Basque},\n  author={Rodrigo Agerri and I{\\~n}aki San Vicente and Jon Ander Campos and Ander Barrena and Xabier Saralegi and Aitor Soroa and Eneko Agirre},\n  booktitle={Proceedings of the 12th International Conference on Language Resources and Evaluation},\n  year={2020}\n}\n```\n", "qas": []}]}, {"title": "izumi-lab/bert-small-japanese-fin", "paragraphs": [{"context": "---\n\nlanguage: ja\n\nlicense: cc-by-sa-4.0\n\ntags:\n\n- finance\n\nwidget:\n\n- text: \u6d41\u52d5[MASK]\u306f\u30011\u5104\u5186\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n\n---\n\n# BERT small Japanese finance\n\nThis is a [BERT]( model pretrained on texts in the Japanese language.\n\nThe codes for the pretraining are available at [retarfi/language-pretraining](\n\n## Model architecture\n\nThe model architecture is the same as BERT small in the [original ELECTRA paper]( 12 layers, 256 dimensions of hidden states, and 4 attention heads.\n\n## Training Data\n\nThe models are trained on Wikipedia corpus and financial corpus.\n\nThe Wikipedia corpus is generated from the Japanese Wikipedia dump file as of June 1, 2021. \n\nThe corpus file is 2.9GB, consisting of approximately 20M sentences.\n\nThe financial corpus consists of 2 corpora:\n\n- Summaries of financial results from October 9, 2012, to December 31, 2020\n- Securities reports from February 8, 2018, to December 31, 2020\n\nThe financial corpus file is 5.2GB, consisting of approximately 27M sentences.\n\n\n## Tokenization\n\nThe texts are first tokenized by MeCab with IPA dictionary and then split into subwords by the WordPiece algorithm.\n\nThe vocabulary size is 32768.\n\n## Training\n\nThe models are trained with the same configuration as BERT small in the [original ELECTRA paper]( 128 tokens per instance, 128 instances per batch, and 1.45M training steps.\n\n## Citation\n\n```\n@article{Suzuki-etal-2023-ipm,\n  title = {Constructing and analyzing domain-specific language model for financial text mining}\n  author = {Masahiro Suzuki and Hiroki Sakaji and Masanori Hirano and Kiyoshi Izumi},\n  journal = {Information Processing & Management},\n  volume = {60},\n  number = {2},\n  pages = {103194},\n  year = {2023},\n  doi = {10.1016/j.ipm.2022.103194}\n}\n```\n\n## Licenses\n\nThe pretrained models are distributed under the terms of the [Creative Commons Attribution-ShareAlike 4.0](\n\n## Acknowledgments\n\nThis work was supported by JSPS KAKENHI Grant Number JP21K12010.\n", "qas": []}]}, {"title": "izumi-lab/bert-small-japanese", "paragraphs": [{"context": "---\n\nlanguage: ja\n\nlicense: cc-by-sa-4.0\n\ndatasets:\n\n- wikipedia\n\nwidget:\n\n- text: \u6771\u4eac\u5927\u5b66\u3067[MASK]\u306e\u7814\u7a76\u3092\u3057\u3066\u3044\u307e\u3059\u3002 \n\n---\n\n# BERT small Japanese finance\n\nThis is a [BERT]( model pretrained on texts in the Japanese language.\n\nThe codes for the pretraining are available at [retarfi/language-pretraining](\n\n## Model architecture\n\nThe model architecture is the same as BERT small in the [original ELECTRA paper]( 12 layers, 256 dimensions of hidden states, and 4 attention heads.\n\n## Training Data\n\nThe models are trained on the Japanese version of Wikipedia.\n\nThe training corpus is generated from the Japanese version of Wikipedia, using Wikipedia dump file as of June 1, 2021. \n\nThe corpus file is 2.9GB, consisting of approximately 20M sentences.\n\n## Tokenization\n\nThe texts are first tokenized by MeCab with IPA dictionary and then split into subwords by the WordPiece algorithm.\n\nThe vocabulary size is 32768.\n\n## Training\n\nThe models are trained with the same configuration as BERT small in the [original ELECTRA paper]( 128 tokens per instance, 128 instances per batch, and 1.45M training steps.\n\n## Citation\n\n```\n@article{Suzuki-etal-2023-ipm,\n  title = {Constructing and analyzing domain-specific language model for financial text mining}\n  author = {Masahiro Suzuki and Hiroki Sakaji and Masanori Hirano and Kiyoshi Izumi},\n  journal = {Information Processing & Management},\n  volume = {60},\n  number = {2},\n  pages = {103194},\n  year = {2023},\n  doi = {10.1016/j.ipm.2022.103194}\n}\n```\n\n## Licenses\n\nThe pretrained models are distributed under the terms of the [Creative Commons Attribution-ShareAlike 4.0](\n\n## Acknowledgments\n\nThis work was supported by JSPS KAKENHI Grant Number JP21K12010.\n", "qas": []}]}, {"title": "izumi-lab/electra-base-japanese-generator", "paragraphs": [{"context": "---\n\nlanguage: ja\n\nlicense: cc-by-sa-4.0\n\ndatasets:\n\n- wikipedia\n\nwidget:\n\n- text: \u6771\u4eac\u5927\u5b66\u3067[MASK]\u306e\u7814\u7a76\u3092\u3057\u3066\u3044\u307e\u3059\u3002\n\n---\n\n# ELECTRA base Japanese generator\n\nThis is a [ELECTRA]( model pretrained on texts in the Japanese language.\n\nThe codes for the pretraining are available at [retarfi/language-pretraining](\n\n## Model architecture\n\nThe model architecture is the same as ELECTRA base in the [original ELECTRA implementation]( 12 layers, 256 dimensions of hidden states, and 4 attention heads.\n\n## Training Data\n\nThe models are trained on the Japanese version of Wikipedia.\n\nThe training corpus is generated from the Japanese version of Wikipedia, using Wikipedia dump file as of June 1, 2021. \n\nThe corpus file is 2.9GB, consisting of approximately 20M sentences.\n\n## Tokenization\n\nThe texts are first tokenized by MeCab with IPA dictionary and then split into subwords by the WordPiece algorithm.\n\nThe vocabulary size is 32768.\n\n## Training\n\nThe models are trained with the same configuration as ELECTRA base in the [original ELECTRA paper]( except size; 512 tokens per instance, 256 instances per batch, and 766k training steps.\n\nThe size of the generator is 1/3 of the size of the discriminator.\n\n## Citation\n\n```\n@article{Suzuki-etal-2023-ipm,\n  title = {Constructing and analyzing domain-specific language model for financial text mining}\n  author = {Masahiro Suzuki and Hiroki Sakaji and Masanori Hirano and Kiyoshi Izumi},\n  journal = {Information Processing & Management},\n  volume = {60},\n  number = {2},\n  pages = {103194},\n  year = {2023},\n  doi = {10.1016/j.ipm.2022.103194}\n}\n```\n\n## Licenses\n\nThe pretrained models are distributed under the terms of the [Creative Commons Attribution-ShareAlike 4.0](\n\n## Acknowledgments\n\nThis work was supported by JSPS KAKENHI Grant Number JP21K12010.\n", "qas": []}]}, {"title": "jaesun/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: distilbert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.51728018358102\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8815\n- Matthews Correlation: 0.5173\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5099          \n 2.0    0.5114          \n 3.0    0.6696          \n 4.0    0.7715          \n 5.0    0.8815          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.9.1\n- Datasets 1.14.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jaesun/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 125, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of jaesun/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 229, "answer_end": 247}]}]}]}, {"title": "jaimin/Gujarati-Model", "paragraphs": [{"context": "tokenizer = AutoTokenizer.from_pretrained(\"jaimin/Gujarati-Model\")\nmodel = AutoModel.from_pretrained(\"jaimin/Gujarati-Model\")", "qas": []}]}, {"title": "jaimin/plagiarism_checker", "paragraphs": [{"context": "\"hello\" \n", "qas": []}]}, {"title": "jaimin/wav2vec2-base-gujarati-demo", "paragraphs": [{"context": "---\nlanguage: Guj\ndatasets:\n- google\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 Guj by Jaimin\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Google\n      type: voice\n      args: guj\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 28.92\n---\n\n# wav2vec2-base-gujarati-demo\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( in Guj\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\n\nThe model can be used directly (without a language model) as follows:\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\ncommon_voice_train,common_voice_test = load_dataset('csv', data_files={'train': 'train.csv','test': 'test.csv'},error_bad_lines=False,encoding='utf-8',split=['train', 'test']).\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"jaimin/wav2vec2-base-gujarati-demo\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"jaimin/wav2vec2-base-gujarati-demo\")\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = common_voice_test.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n     logits = model(inputs.input_values).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][0].lower())\n```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on the {language} test data of Common Voice.\n\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\ncommon_voice_validation = load_dataset('csv', data_files={'test': 'validation.csv'},error_bad_lines=False,encoding='utf-8',split='test')\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"jaimin/wav2vec2-base-gujarati-demo\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"Amrrs/wav2vec2-base-gujarati-demo\")\nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = common_voice_validation.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\")).logits\n    \n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = common_voice_validation.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n```\n\n**Test Result**: 28.92 %\n\n\n## Training\n\nThe Google datasets were used for training.\n\nThe script used for training can be found [here](\n", "qas": [{"id": "q1", "question": "What is the model architecture of jaimin/wav2vec2-base-gujarati-demo?", "answers": [{"text": "wav2vec2", "answer_start": 429, "answer_end": 436}]}, {"id": "q2", "question": "What is the model task of jaimin/wav2vec2-base-gujarati-demo?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 53, "answer_end": 80}]}, {"id": "q3", "question": "What is the model category of jaimin/wav2vec2-base-gujarati-demo?", "answers": [{"text": "audio", "answer_start": 45, "answer_end": 49}]}]}]}, {"title": "jakelever/coronabert", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- coronavirus\n- covid\n- bionlp\ndatasets:\n- cord19\n- pubmed\nlicense: mit\nwidget:\n- text: \"Pre-existing T-cell immunity to SARS-CoV-2 in unexposed healthy controls in Ecuador, as detected with a COVID-19 Interferon-Gamma Release Assay.\"\n- text: \"Lifestyle and mental health disruptions during COVID-19.\"\n- text: \"More than 50 Long-term effects of COVID-19: a systematic review and meta-analysis\"\n---\n# CoronaCentral BERT Model for Topic / Article Type Classification\n\nThis is the topic / article type multi-label classification for the [CoronaCentral website]( This forms part of the pipeline for downloading and processing coronavirus literature described in the [corona-ml repo]( with available [step-by-step descriptions]( The method is described in the [preprint]( and detailed performance results can be found in the [machine learning details]( document.\n\nThis model was derived by fine-tuning the [microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract]( model on this coronavirus sequence (document) classification task.\n\n## Usage\n\nBelow are two Google Colab notebooks with example usage of this sequence classification model using HuggingFace transformers and KTrain.\n\n- [HuggingFace example on Google Colab](\n- [KTrain example on Google Colab](\n\n## Training Data\n\nThe model is trained on ~3200 manually-curated articles sampled at various stages during the coronavirus pandemic. The code for training is available in the [category\\_prediction]( directory of the main Github Repo. The data is available in the [annotated_documents.json.gz]( file.\n\n## Inputs and Outputs\n\nThe model takes in a tokenized title and abstract (combined into a single string and separated by a new line). The outputs are topics and article types, broadly called categories in the pipeline code. The types are listed below. Some others are managed by hand-coded rules described in the [step-by-step descriptions](\n\n### List of Article Types \n\n- Comment/Editorial\n- Meta-analysis\n- News\n- Review\n\n### List of Topics\n\n- Clinical Reports\n- Communication\n- Contact Tracing\n- Diagnostics\n- Drug Targets\n- Education\n- Effect on Medical Specialties\n- Forecasting & Modelling\n- Health Policy\n- Healthcare Workers\n- Imaging\n- Immunology\n- Inequality\n- Infection Reports\n- Long Haul\n- Medical Devices\n- Misinformation\n- Model Systems & Tools\n- Molecular Biology\n- Non-human\n- Non-medical\n- Pediatrics\n- Prevalence\n- Prevention\n- Psychology\n- Recommendations\n- Risk Factors\n- Surveillance\n- Therapeutics\n- Transmission\n- Vaccines\n", "qas": []}]}, {"title": "jakobwes/xlm_roberta_squad_v1.1", "paragraphs": [{"context": "XLM-RoBERTa base (`xlm-roberta-base`) finetuned on squad v1.1.\n\n**Training-specifications:**\n\n- training_epochs: 3.0\n- max_seq_length: 384\n- batch_size: 16\n- dataset_name: squad\n- doc_stride 128 \n\n\n**Train-results:**\n\n```\n{\n    \"epoch\": 3.0,\n    \"init_mem_cpu_alloc_delta\": 991453184,\n    \"init_mem_cpu_peaked_delta\": 0,\n    \"init_mem_gpu_alloc_delta\": 1109893120,\n    \"init_mem_gpu_peaked_delta\": 0,\n    \"train_mem_cpu_alloc_delta\": 14753792,\n    \"train_mem_cpu_peaked_delta\": 0,\n    \"train_mem_gpu_alloc_delta\": 3330195456,\n    \"train_mem_gpu_peaked_delta\": 8287144960,\n    \"train_runtime\": 11376.3034,\n    \"train_samples\": 89597,\n    \"train_samples_per_second\": 1.477\n}\n```\n\n**Eval-results:**\n\n```\n{\n    \"epoch\": 3.0,\n    \"eval_samples\": 10918,\n    \"exact_match\": 82.06244087038789,\n    \"f1\": 89.09539709124654\n}\n```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of jakobwes/xlm_roberta_squad_v1.1?", "answers": [{"text": "xlm-roberta", "answer_start": 19, "answer_end": 29}]}]}]}, {"title": "jalensmh/DialoGPT-medium-jalenbot", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# jalenbot DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of jalensmh/DialoGPT-medium-jalenbot?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "jalensmh/DialoGPT-small-exophoria", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# exophoria DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of jalensmh/DialoGPT-small-exophoria?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "jamarju/roberta-base-bne-squad-2.0-es", "paragraphs": [{"context": "---\nlanguage:\n- es\ndatasets:\n- squad_es\nwidget:\n- text: \"\u00bfQui\u00e9n era el duque en la batalla de Hastings?\"\n  context: \"La dinast\u00eda normanda tuvo un gran impacto pol\u00edtico, cultural y militar en la Europa medieval e incluso en el Cercano Oriente. Los normandos eran famosos por su esp\u00edritu marcial y, finalmente, por su piedad cristiana, convirti\u00e9ndose en exponentes de la ortodoxia cat\u00f3lica en la que se asimilaron. Adoptaron la lengua galorromance de la tierra franca que establecieron, siendo su dialecto conocido como franc\u00e9s normando, normando o normando, una lengua literaria importante. El ducado de Normand\u00eda, que formaron por tratado con la corona francesa, fue un gran feudo de la Francia medieval, y bajo Ricardo I de Normand\u00eda se forj\u00f3 en un principado cohesionado y formidable en la tenencia feudal. Los normandos se caracterizan tanto por su cultura, como por su singular arquitectura rom\u00e1nica y sus tradiciones musicales, y por sus importantes logros e innovaciones militares. Aventureros normandos fundaron el Reino de Sicilia bajo Roger II despu\u00e9s de conquistar el sur de Italia con los sarracenos y bizantinos, y una expedici\u00f3n en nombre de su duque, Guillermo el Conquistador, condujo a la conquista normanda de Inglaterra. La influencia cultural y militar normanda se extendi\u00f3 desde estos nuevos centros europeos a los estados cruzados del Cercano Oriente, donde su pr\u00edncipe Bohemundo I fund\u00f3 el Principado de Antioqu\u00eda en el Levante mediterr\u00e1neo, a Escocia y Gales en Gran Breta\u00f1a.\"\n---\n\nThis is the [BSC-TeMU/roberta-base-bne]( model ([source]( trained on the [squad_es v2.0.0]( dataset ([source](\n\nCurrent achievement: em=58.80, f1=67.40\n\nResults:\n\n```\n{\n    \"epoch\": 4.0,\n    \"eval_HasAns_exact\": 48.51551956815115,\n    \"eval_HasAns_f1\": 65.70745010262016,\n    \"eval_HasAns_total\": 5928,\n    \"eval_NoAns_exact\": 69.0893760539629,\n    \"eval_NoAns_f1\": 69.0893760539629,\n    \"eval_NoAns_total\": 5930,\n    \"eval_best_exact\": 58.804182830156854,\n    \"eval_best_exact_thresh\": 0.0,\n    \"eval_best_f1\": 67.39869828034618,\n    \"eval_best_f1_thresh\": 0.0,\n    \"eval_exact\": 58.804182830156854,\n    \"eval_f1\": 67.39869828034568,\n    \"eval_samples\": 12211,\n    \"eval_total\": 11858\n}\n```\n\nTraining script:\n\n```\npython -m torch.distributed.launch --nproc_per_node=3 ./run_qa.py \\\n    --model_name_or_path BSC-TeMU/roberta-base-bne \\\n    --dataset_name squad_es \\\n    --dataset_config_name v2.0.0 \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 4 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ./models/roberta-base-bne-squad-2.0-es/ \\\n    --per_device_eval_batch_size=8   \\\n    --per_device_train_batch_size=8   \\\n    --version_2_with_negative \\\n    --ddp_find_unused_parameters=False \\\n    --overwrite_output_dir \\\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of jamarju/roberta-base-bne-squad-2.0-es?", "answers": [{"text": "roberta", "answer_start": 1527, "answer_end": 1533}]}]}]}, {"title": "jamarju/roberta-large-bne-squad-2.0-es", "paragraphs": [{"context": "---\nlanguage:\n- es\ndatasets:\n- squad_es\nwidget:\n- text: \"\u00bfQui\u00e9n era el duque en la batalla de Hastings?\"\n  context: \"La dinast\u00eda normanda tuvo un gran impacto pol\u00edtico, cultural y militar en la Europa medieval e incluso en el Cercano Oriente. Los normandos eran famosos por su esp\u00edritu marcial y, finalmente, por su piedad cristiana, convirti\u00e9ndose en exponentes de la ortodoxia cat\u00f3lica en la que se asimilaron. Adoptaron la lengua galorromance de la tierra franca que establecieron, siendo su dialecto conocido como franc\u00e9s normando, normando o normando, una lengua literaria importante. El ducado de Normand\u00eda, que formaron por tratado con la corona francesa, fue un gran feudo de la Francia medieval, y bajo Ricardo I de Normand\u00eda se forj\u00f3 en un principado cohesionado y formidable en la tenencia feudal. Los normandos se caracterizan tanto por su cultura, como por su singular arquitectura rom\u00e1nica y sus tradiciones musicales, y por sus importantes logros e innovaciones militares. Aventureros normandos fundaron el Reino de Sicilia bajo Roger II despu\u00e9s de conquistar el sur de Italia con los sarracenos y bizantinos, y una expedici\u00f3n en nombre de su duque, Guillermo el Conquistador, condujo a la conquista normanda de Inglaterra. La influencia cultural y militar normanda se extendi\u00f3 desde estos nuevos centros europeos a los estados cruzados del Cercano Oriente, donde su pr\u00edncipe Bohemundo I fund\u00f3 el Principado de Antioqu\u00eda en el Levante mediterr\u00e1neo, a Escocia y Gales en Gran Breta\u00f1a.\"\n---\n\nThis is the [BSC-TeMU/roberta-large-bne]( model ([source]( trained on the [squad_es v2.0.0]( dataset ([source](\n\nCurrent achievement: em=60.21, f1=68.61\n\nResults:\n\n```\n{\n    \"epoch\": 4.0,\n    \"eval_HasAns_exact\": 48.44804318488529,\n    \"eval_HasAns_f1\": 65.24520506718169,\n    \"eval_HasAns_total\": 5928,\n    \"eval_NoAns_exact\": 71.97301854974705,\n    \"eval_NoAns_f1\": 71.97301854974705,\n    \"eval_NoAns_total\": 5930,\n    \"eval_best_exact\": 60.22094788328555,\n    \"eval_best_exact_thresh\": 0.0,\n    \"eval_best_f1\": 68.6181122987237,\n    \"eval_best_f1_thresh\": 0.0,\n    \"eval_exact\": 60.2125147579693,\n    \"eval_f1\": 68.60967917340695,\n    \"eval_samples\": 12203,\n    \"eval_total\": 11858\n}\n```\n\nTraining script:\n\n```\npython -m torch.distributed.launch --nproc_per_node=3 ./run_qa.py \\\n    --model_name_or_path BSC-TeMU/roberta-large-bne \\\n    --dataset_name squad_es \\\n    --dataset_config_name v2.0.0 \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 4 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ./models/roberta-large-bne-finetuned-squad-es/ \\\n    --per_device_eval_batch_size=24   \\\n    --per_device_train_batch_size=12   \\\n    --version_2_with_negative \\\n    --ddp_find_unused_parameters=False \\\n```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of jamarju/roberta-large-bne-squad-2.0-es?", "answers": [{"text": "roberta", "answer_start": 1527, "answer_end": 1533}]}]}]}, {"title": "jambo/marker-associations-snp-binary-base", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- marker-associations-snp-binary-base\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: marker-associations-snp-binary-base\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: marker-associations-snp-binary-base\n      type: marker-associations-snp-binary-base\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.9384057971014492\n    - name: Recall\n      type: recall\n      value: 0.9055944055944056\n    - name: F1\n      type: f1\n      value: 0.9217081850533808\n    - name: Accuracy\n      type: accuracy\n      value: 0.9107505070993914\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# marker-associations-snp-binary-base\n\nThis model is a fine-tuned version of [microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext]( on the marker-associations-snp-binary-base dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4027\n- Precision: 0.9384\n- Recall: 0.9056\n- F1: 0.9217\n- Accuracy: 0.9108\n- Auc: 0.9578\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 1\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 15\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy \n:-----::---------------::------::--------:\n 1.0    0.2776           0.9441  0.9067   \n 2.0    0.4380           0.9126  0.8986   \n 3.0    0.4027           0.9056  0.9108   \n 4.0    0.3547           0.8986  0.9108   \n 5.0    0.4465           0.9266  0.9047   \n 6.0    0.5770           0.9441  0.9047   \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.9.0+cu111\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of jambo/marker-associations-snp-binary-base?", "answers": [{"text": "text-classification", "answer_start": 264, "answer_end": 282}]}]}]}, {"title": "jhgan/ko-sbert-multitask", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\n# ko-sbert-multitask\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n<!--- Describe your model here -->\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"\uc548\ub155\ud558\uc138\uc694?\", \"\ud55c\uad6d\uc5b4 \ubb38\uc7a5 \uc784\ubca0\ub529\uc744 \uc704\ud55c \ubc84\ud2b8 \ubaa8\ub378\uc785\ub2c8\ub2e4.\"]\n\nmodel = SentenceTransformer('jhgan/ko-sbert-multitask')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sbert-multitask')\nmodel = AutoModel.from_pretrained('jhgan/ko-sbert-multitask')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Evaluation Results\n\nKorSTS, KorNLI \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc73c\ub85c \uba40\ud2f0 \ud0dc\uc2a4\ud06c \ud559\uc2b5\uc744 \uc9c4\ud589\ud55c \ud6c4 KorSTS \ud3c9\uac00 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud3c9\uac00\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.\n\n- Cosine Pearson: 84.13\n- Cosine Spearman: 84.71\n- Euclidean Pearson: 82.42\n- Euclidean Spearman: 82.66\n- Manhattan Pearson: 81.41\n- Manhattan Spearman: 81.69\n- Dot Pearson: 80.05\n- Dot Spearman: 79.69\n\n\n\n## Training\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`sentence_transformers.datasets.NoDuplicatesDataLoader.NoDuplicatesDataLoader` of length 8885 with parameters:\n```\n{'batch_size': 64}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.MultipleNegativesRankingLoss.MultipleNegativesRankingLoss` with parameters:\n  ```\n  {'scale': 20.0, 'similarity_fct': 'cos_sim'}\n  ```\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 719 with parameters:\n```\n{'batch_size': 8, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"epochs\": 5,\n    \"evaluation_steps\": 1000,\n    \"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 2e-05\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": null,\n    \"warmup_steps\": 360,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->\n- Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). Kornli and korsts: New benchmark datasets for korean natural language understanding. arXiv\npreprint arXiv:2004.03289\n- Reimers, Nils and Iryna Gurevych. \u201cSentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\u201d ArXiv abs/1908.10084 (2019)\n- Reimers, Nils and Iryna Gurevych. \u201cMaking Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation.\u201d EMNLP (2020).\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of jhgan/ko-sbert-multitask?", "answers": [{"text": "bert", "answer_start": 137, "answer_end": 140}]}, {"id": "q2", "question": "What is the model task of jhgan/ko-sbert-multitask?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "jhonparra18/wav2vec2-xls-r-300m-spanish-large-noLM", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- \"es\"\n- \"robust-speech-event\"\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-large-xls-r-300m-spanish-large\n  results: []\n \n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xls-r-300m-spanish-large\n\nThis model is a fine-tuned version of [tomascufaro/xls-r-es-test]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1431\n- Wer: 0.1197\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002\n- train_batch_size: 10\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 20\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 300\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.15   0.1795          \n 0.3    0.2000          \n 0.45   0.1985          \n 0.6    0.1901          \n 0.75   0.1968          \n 0.9    0.1873          \n 1.06   0.1840          \n 1.21   0.1904          \n 1.36   0.1827          \n 1.51   0.1788          \n 1.66   0.1755          \n 1.81   0.1795          \n 1.96   0.1762          \n 2.11   0.1721          \n 2.26   0.1735          \n 2.41   0.1708          \n 2.56   0.1644          \n 2.71   0.1638          \n 2.86   0.1582          \n 3.02   0.1607          \n 3.17   0.1559          \n 3.32   0.1521          \n 3.47   0.1534          \n 3.62   0.1485          \n 3.77   0.1498          \n 3.92   0.1463          \n 4.07   0.1483          \n 4.22   0.1498          \n 4.37   0.1461          \n 4.52   0.1444          \n 4.67   0.1434          \n 4.82   0.1424          \n 4.98   0.1431          \n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.2.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of jhonparra18/wav2vec2-xls-r-300m-spanish-large-noLM?", "answers": [{"text": "wav2vec2", "answer_start": 132, "answer_end": 139}]}]}]}, {"title": "jhu-clsp/roberta-large-eng-ara-128k", "paragraphs": [{"context": "---\nlanguage:\n- ar\n- en\n- multilingual\nlicense: mit\ntags:\n- bert\n- roberta\n- exbert\ndatasets:\n- arabic_billion_words\n- cc100\n- gigaword\n- oscar\n- wikipedia\n---\n\n# An English-Arabic Bilingual Encoder\n\n```\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/roberta-large-eng-ara-128k\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"jhu-clsp/roberta-large-eng-ara-128k\")\n```\n\n`roberta-large-eng-ara-128k` is an English\ufffdArabic bilingual encoders of 24-layer Transformers (d\\_model= 1024), the same size as XLM-R large. We use the same Common Crawl corpus as XLM-R for pretraining. Additionally, we also use English and Arabic Wikipedia, Arabic Gigaword (Parker et al., 2011), Arabic OSCAR (Ortiz Su\ufffdrez et al., 2020), Arabic News Corpus (El-Khair, 2016), and Arabic OSIAN (Zeroual et al.,2019). In total, we train with 9.2B words of Arabic text and 26.8B words of English text, more than either XLM-R (2.9B words/23.6B words) or GigaBERT v4 (Lan et al., 2020) (4.3B words/6.1B words). We build an English\ufffdArabic joint vocabulary using SentencePiece (Kudo and Richardson, 2018) with size of 128K. We additionally enforce coverage of all Arabic characters after normalization.\n\n## Pretraining Detail\n\nWe pretrain each encoder with a batch size of 2048 sequences and 512 sequence length for 250K steps from scratch roughly 1/24 the amount of pretraining compute of XLM-R. Training takes 8 RTX6000 GPUs roughly three weeks. We follow the pretraining recipe of RoBERTa (Liu et al., 2019) and XLM-R. We omit the next sentence prediction task and use a learning rate of 2e-4, Adam optimizer, and linear warmup of 10K steps then decay linearly to 0, multilingual sampling alpha of 0.3, and the fairseq (Ott et al., 2019) implementation.\n\n## Citation\n\nPlease cite this paper for reference:\n\n```bibtex\n@inproceedings{yarmohammadi-etal-2021-everything,\n    title = \"Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction\",\n    author = \"Yarmohammadi, Mahsa  and\n      Wu, Shijie  and\n      Marone, Marc  and\n      Xu, Haoran  and\n      Ebner, Seth  and\n      Qin, Guanghui  and\n      Chen, Yunmo and\n      Guo, Jialiang and\n      Harman, Craig  and\n      Murray, Kenton and\n      White, Aaron Steven  and\n      Dredze, Mark and\n      Van Durme, Benjamin\",\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    year = \"2021\",\n}\n```\n", "qas": []}]}, {"title": "jiho0304/curseELECTRA", "paragraphs": [{"context": "ElectraBERT tuned with korean-bad-speeches", "qas": []}]}, {"title": "jimregan/BERTreach-finetuned-ner", "paragraphs": [{"context": "---\nlicense: apache-2.0\nlanguage: ga\ntags:\n- generated_from_trainer\n- irish\ndatasets:\n- wikiann\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: BERTreach-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: wikiann\n      type: wikiann\n      args: ga\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.5200517464424321\n    - name: Recall\n      type: recall\n      value: 0.5667293233082706\n    - name: F1\n      type: f1\n      value: 0.5423881268270744\n    - name: Accuracy\n      type: accuracy\n      value: 0.8365605828220859\nwidget:\n- text: \"Saola\u00edodh P\u00e1draic \u00d3 Conaire i nGaillimh sa bhliain 1882.\"\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# BERTreach-finetuned-ner\n\nThis model is a fine-tuned version of [jimregan/BERTreach]( on the wikiann dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4944\n- Precision: 0.5201\n- Recall: 0.5667\n- F1: 0.5424\n- Accuracy: 0.8366\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.7249           0.3905  0.7584   |\n 2.0    0.5850           0.4948  0.8072   |\n 3.0    0.5192           0.5456  0.8288   |\n 4.0    0.5042           0.5592  0.8348   |\n 5.0    0.4944           0.5667  0.8366   |\n\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of jimregan/BERTreach-finetuned-ner?", "answers": [{"text": "token-classification", "answer_start": 253, "answer_end": 272}]}]}]}, {"title": "jiobiala24/wav2vec2-base-checkpoint-12", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-base-checkpoint-12\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-checkpoint-12\n\nThis model is a fine-tuned version of [jiobiala24/wav2vec2-base-checkpoint-11.1]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0795\n- Wer: 0.3452\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.64   0.5692          \n 3.28   0.6127          \n 4.93   0.6622          \n 6.57   0.6768          \n 8.21   0.7559          \n 9.85   0.7873          \n 11.49  0.8170          \n 13.14  0.8682          \n 14.78  0.8317          \n 16.42  0.9213          \n 18.06  0.9746          \n 19.7   1.0027          \n 21.35  1.0045          \n 22.99  0.9821          \n 24.63  1.0818          \n 26.27  1.0995          \n 27.91  1.0533          \n 29.56  1.0795          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.13.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jiobiala24/wav2vec2-base-checkpoint-12?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "jiobiala24/wav2vec2-base-checkpoint-14", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-base-checkpoint-14\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-checkpoint-14\n\nThis model is a fine-tuned version of [jiobiala24/wav2vec2-base-checkpoint-13]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.2822\n- Wer: 0.4068\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.59   0.7181          \n 3.17   0.7735          \n 4.76   0.8152          \n 6.35   0.8575          \n 7.94   0.9005          \n 9.52   0.9232          \n 11.11  0.9680          \n 12.7   1.0633          \n 14.29  1.0875          \n 15.87  1.0281          \n 17.46  1.2164          \n 19.05  1.1868          \n 20.63  1.1678          \n 22.22  1.2444          \n 23.81  1.2042          \n 25.4   1.3019          \n 26.98  1.2001          \n 28.57  1.2822          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.13.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jiobiala24/wav2vec2-base-checkpoint-14?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "jiobiala24/wav2vec2-base-checkpoint-2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-base-TPU-cv-fine-tune-2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-TPU-cv-fine-tune-2\n\nThis model is a fine-tuned version of [jiobiala24/wav2vec2-base-TPU-cv-fine-tune]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6051\n- Wer: 0.5484\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 6.45   1.2550          \n 12.9   1.4235          \n 19.35  1.5743          \n 25.8   1.6051          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.13.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jiobiala24/wav2vec2-base-checkpoint-2?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "jiobiala24/wav2vec2-base-checkpoint-4", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-base-checkpoint-4\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-checkpoint-4\n\nThis model is a fine-tuned version of [jiobiala24/wav2vec2-base-checkpoint-3]( on the common_voice dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.13.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jiobiala24/wav2vec2-base-checkpoint-4?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "jiobiala24/wav2vec2-base-checkpoint-5", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-base-checkpoint-5\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-checkpoint-5\n\nThis model is a fine-tuned version of [jiobiala24/wav2vec2-base-checkpoint-4]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9849\n- Wer: 0.3354\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.96   0.5749          \n 3.93   0.6212          \n 5.89   0.6280          \n 7.86   0.6517          \n 9.82   0.7115          \n 11.79  0.7687          \n 13.75  0.7785          \n 15.72  0.8115          \n 17.68  0.8290          \n 19.65  0.8517          \n 21.61  0.9370          \n 23.58  0.9157          \n 25.54  0.9673          \n 27.5   0.9804          \n 29.47  0.9849          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.13.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jiobiala24/wav2vec2-base-checkpoint-5?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "jogonba2/bart-JES-cnn_dailymail", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: bart-JES-cnn_dailymail\n  results:\n  - task:\n      name: Summarization\n      type: summarization\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 43.9753\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bart-JES-cnn_dailymail\n\nThis model is a fine-tuned version of [facebook/bart-large]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1452\n- Rouge1: 43.9753\n- Rouge2: 19.7191\n- Rougel: 33.6236\n- Rougelsum: 41.1683\n- Gen Len: 80.1767\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 6.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Rouge2  Rougelsum \n:-----::---------------::------::---------:\n 1.0    1.2080           3.3284  11.4022   \n 2.0    1.1615           3.363   11.5037   \n 3.0    1.1452           3.773   12.2359   \n 4.0    1.1670           3.7329  12.0617   \n 5.0    1.1667           3.7842  12.1643   \n 6.0    1.1997           3.778   12.1332   \n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.7.1+cu110\n- Datasets 1.11.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jogonba2/bart-JES-cnn_dailymail?", "answers": [{"text": "bart", "answer_start": 93, "answer_end": 96}]}]}]}, {"title": "jonatasgrosman/wav2vec2-xls-r-1b-polish", "paragraphs": [{"context": "---\nlanguage:\n- pl\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- hf-asr-leaderboard\n- mozilla-foundation/common_voice_8_0\n- pl\n- robust-speech-event\ndatasets:\n- mozilla-foundation/common_voice_8_0\nmodel-index:\n- name: XLS-R Wav2Vec2 Polish by Jonatas Grosman\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 8\n      type: mozilla-foundation/common_voice_8_0\n      args: pl\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 11.01\n    - name: Test CER\n      type: cer\n      value: 2.55\n    - name: Test WER (+LM)\n      type: wer\n      value: 7.32\n    - name: Test CER (+LM)\n      type: cer\n      value: 1.95\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: pl\n    metrics:\n    - name: Dev WER\n      type: wer\n      value: 26.31\n    - name: Dev CER\n      type: cer\n      value: 13.85\n    - name: Dev WER (+LM)\n      type: wer\n      value: 20.33\n    - name: Dev CER (+LM)\n      type: cer\n      value: 13.0\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Test Data\n      type: speech-recognition-community-v2/eval_data\n      args: pl\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 22.77\n---\n\n# Fine-tuned XLS-R 1B model for speech recognition in Polish\n\nFine-tuned [facebook/wav2vec2-xls-r-1b]( on Polish using the train and validation splits of [Common Voice 8.0]( [Multilingual LibriSpeech]( and [Voxpopuli](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned by the [HuggingSound]( tool, and thanks to the GPU credits generously given by the [OVHcloud]( :)\n\n## Usage\n\nUsing the [HuggingSound]( library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-xls-r-1b-polish\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"pl\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-xls-r-1b-polish\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n```\n\n## Evaluation Commands\n\n1. To evaluate on `mozilla-foundation/common_voice_8_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-xls-r-1b-polish --dataset mozilla-foundation/common_voice_8_0 --config pl --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-xls-r-1b-polish --dataset speech-recognition-community-v2/dev_data --config pl --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr-1b-polish,\n  title={Fine-tuned {XLS-R} 1{B} model for speech recognition in {P}olish},\n  author={Grosman, Jonatas},\n  howpublished={\\url{\n  year={2022}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of jonatasgrosman/wav2vec2-xls-r-1b-polish?", "answers": [{"text": "wav2vec2", "answer_start": 1559, "answer_end": 1566}]}, {"id": "q2", "question": "What is the model task of jonatasgrosman/wav2vec2-xls-r-1b-polish?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}, {"id": "q3", "question": "What is the model category of jonatasgrosman/wav2vec2-xls-r-1b-polish?", "answers": [{"text": "audio", "answer_start": 2082, "answer_end": 2086}]}]}]}, {"title": "jonatasgrosman/wav2vec2-xls-r-1b-russian", "paragraphs": [{"context": "---\nlanguage:\n- ru\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- hf-asr-leaderboard\n- mozilla-foundation/common_voice_8_0\n- robust-speech-event\n- ru\ndatasets:\n- mozilla-foundation/common_voice_8_0\nmodel-index:\n- name: XLS-R Wav2Vec2 Russian by Jonatas Grosman\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 8\n      type: mozilla-foundation/common_voice_8_0\n      args: ru\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 9.82\n    - name: Test CER\n      type: cer\n      value: 2.3\n    - name: Test WER (+LM)\n      type: wer\n      value: 7.08\n    - name: Test CER (+LM)\n      type: cer\n      value: 1.87\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: ru\n    metrics:\n    - name: Dev WER\n      type: wer\n      value: 23.96\n    - name: Dev CER\n      type: cer\n      value: 8.88\n    - name: Dev WER (+LM)\n      type: wer\n      value: 15.88\n    - name: Dev CER (+LM)\n      type: cer\n      value: 7.42\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Test Data\n      type: speech-recognition-community-v2/eval_data\n      args: ru\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 14.23\n---\n\n# Fine-tuned XLS-R 1B model for speech recognition in Russian\n\nFine-tuned [facebook/wav2vec2-xls-r-1b]( on Russian using the train and validation splits of [Common Voice 8.0]( [Golos]( and [Multilingual TEDx](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned by the [HuggingSound]( tool, and thanks to the GPU credits generously given by the [OVHcloud]( :)\n\n## Usage\n\nUsing the [HuggingSound]( library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-xls-r-1b-russian\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ru\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-xls-r-1b-russian\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n```\n\n## Evaluation Commands\n\n1. To evaluate on `mozilla-foundation/common_voice_8_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-xls-r-1b-russian --dataset mozilla-foundation/common_voice_8_0 --config ru --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-xls-r-1b-russian --dataset speech-recognition-community-v2/dev_data --config ru --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr-1b-russian,\n  title={Fine-tuned {XLS-R} 1{B} model for speech recognition in {R}ussian},\n  author={Grosman, Jonatas},\n  howpublished={\\url{\n  year={2022}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of jonatasgrosman/wav2vec2-xls-r-1b-russian?", "answers": [{"text": "wav2vec2", "answer_start": 1558, "answer_end": 1565}]}, {"id": "q2", "question": "What is the model task of jonatasgrosman/wav2vec2-xls-r-1b-russian?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}, {"id": "q3", "question": "What is the model category of jonatasgrosman/wav2vec2-xls-r-1b-russian?", "answers": [{"text": "audio", "answer_start": 2072, "answer_end": 2076}]}]}]}, {"title": "jonatasgrosman/wav2vec2-xls-r-1b-spanish", "paragraphs": [{"context": "---\nlanguage:\n- es\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- es\n- hf-asr-leaderboard\n- mozilla-foundation/common_voice_8_0\n- robust-speech-event\ndatasets:\n- mozilla-foundation/common_voice_8_0\nmodel-index:\n- name: XLS-R Wav2Vec2 Spanish by Jonatas Grosman\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 8\n      type: mozilla-foundation/common_voice_8_0\n      args: es\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 9.97\n    - name: Test CER\n      type: cer\n      value: 2.85\n    - name: Test WER (+LM)\n      type: wer\n      value: 6.74\n    - name: Test CER (+LM)\n      type: cer\n      value: 2.24\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: es\n    metrics:\n    - name: Dev WER\n      type: wer\n      value: 24.79\n    - name: Dev CER\n      type: cer\n      value: 9.7\n    - name: Dev WER (+LM)\n      type: wer\n      value: 16.37\n    - name: Dev CER (+LM)\n      type: cer\n      value: 8.84\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Test Data\n      type: speech-recognition-community-v2/eval_data\n      args: es\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 16.67\n---\n\n# Fine-tuned XLS-R 1B model for speech recognition in Spanish\n\nFine-tuned [facebook/wav2vec2-xls-r-1b]( on Spanish using the train and validation splits of [Common Voice 8.0]( [MediaSpeech]( [Multilingual TEDx]( [Multilingual LibriSpeech]( and [Voxpopuli](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned by the [HuggingSound]( tool, and thanks to the GPU credits generously given by the [OVHcloud]( :)\n\n## Usage\n\nUsing the [HuggingSound]( library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-xls-r-1b-spanish\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"es\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-xls-r-1b-spanish\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n```\n\n## Evaluation Commands\n\n1. To evaluate on `mozilla-foundation/common_voice_8_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-xls-r-1b-spanish --dataset mozilla-foundation/common_voice_8_0 --config es --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-xls-r-1b-spanish --dataset speech-recognition-community-v2/dev_data --config es --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr-1b-spanish,\n  title={Fine-tuned {XLS-R} 1{B} model for speech recognition in {S}panish},\n  author={Grosman, Jonatas},\n  howpublished={\\url{\n  year={2022}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of jonatasgrosman/wav2vec2-xls-r-1b-spanish?", "answers": [{"text": "wav2vec2", "answer_start": 1558, "answer_end": 1565}]}, {"id": "q2", "question": "What is the model task of jonatasgrosman/wav2vec2-xls-r-1b-spanish?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}, {"id": "q3", "question": "What is the model category of jonatasgrosman/wav2vec2-xls-r-1b-spanish?", "answers": [{"text": "audio", "answer_start": 2119, "answer_end": 2123}]}]}]}, {"title": "jonc/distilbert-base-uncased-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotion\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: default\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.923\n    - name: F1\n      type: f1\n      value: 0.9230733583303665\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2159\n- Accuracy: 0.923\n- F1: 0.9231\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.3134           0.9051 |\n 2.0    0.2159           0.9231 |\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jonc/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 121, "answer_end": 130}]}, {"id": "q2", "question": "What is the model task of jonc/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "text-classification", "answer_start": 228, "answer_end": 246}]}]}]}, {"title": "jonfd/convbert-base-igc-is", "paragraphs": [{"context": "---\nlanguage:\n- is\nlicense: cc-by-4.0\ndatasets:\n- igc\n---\n\n# Icelandic ConvBERT-Base\nThis model was pretrained on the [Icelandic Gigaword Corpus]( which contains approximately 1.69B tokens, using default settings. The model uses a WordPiece tokenizer with a vocabulary size of 32,105.\n\n# Acknowledgments\nThis research was supported with Cloud TPUs from Google's TPU Research Cloud (TRC).\n\nThis project was funded by the Language Technology Programme for Icelandic 2019-2023. The programme, which is managed and coordinated by [Almannar\u00f3mur]( is funded by the Icelandic Ministry of Education, Science and Culture.", "qas": []}]}, {"title": "jonfd/convbert-small-igc-is", "paragraphs": [{"context": "---\nlanguage:\n- is\nlicense: cc-by-4.0\ndatasets:\n- igc\n---\n\n# Icelandic ConvBERT-Small\nThis model was pretrained on the [Icelandic Gigaword Corpus]( which contains approximately 1.69B tokens, using default settings. The model uses a Unigram tokenizer with a vocabulary size of 96,000.\n\n# Acknowledgments\nThis research was supported with Cloud TPUs from Google's TPU Research Cloud (TRC).\n\nThis project was funded by the Language Technology Programme for Icelandic 2019-2023. The programme, which is managed and coordinated by [Almannar\u00f3mur]( is funded by the Icelandic Ministry of Education, Science and Culture.", "qas": []}]}, {"title": "joniponi/bert-finetuned-sem_eval-english", "paragraphs": [{"context": "---\nEpoch \tTraining Loss \tValidation Loss \tF1 \tRoc Auc \tAccuracy\n1 \t0.115400 \t0.099458 \t0.888763 \t0.920410 \t0.731760\n2 \t0.070400 \t0.080343 \t0.911700 \t0.943234 \t0.781116", "qas": []}]}, {"title": "jonx18/DialoGPT-small-Creed-Odyssey", "paragraphs": [{"context": "# Summary\nThe app was conceived with the idea of recreating and generate new dialogs for existing games.\nIn order to generate a dataset for training the steps followed were:\n1. Download from [Assassins Creed Fandom Wiki]( from the category \"Memories relived using the Animus HR-8.5\".\n2. Keep only text elements from XML.\n3. Keep only the dialog section.\n4. Parse wikimarkup with [wikitextparser](\n5. Clean description of dialog's context.\nDue to the small size of the dataset obtained, a transfer learning approach was considered based on a pretrained [\"Dialog GPT\" model](", "qas": []}]}, {"title": "junnyu/electra_small_generator", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- pytorch\n- electra\n- masked-lm\nlicense: mit\ndatasets:\n- openwebtext\n---\n# \u4e00\u3001 \u4e2a\u4eba\u5728openwebtext\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5f97\u5230\u7684electra-small\u6a21\u578b\n\n# \u4e8c\u3001 \u590d\u73b0\u7ed3\u679c(dev dataset)\nCoLAMRPCQQPQNLIAvg.|\n---------------|\n56.887.488.387.980.36|\n 55.82 87.089.2887.5080.30|\n\n# \u4e09\u3001 \u8bad\u7ec3\u7ec6\u8282\n- \u6570\u636e\u96c6 openwebtext\n- \u8bad\u7ec3batch_size 256\n- \u5b66\u4e60\u7387lr  5e-4\n- \u6700\u5927\u53e5\u5b50\u957f\u5ea6max_seqlen  128\n- \u8bad\u7ec3total step  62.5W\n- GPU RTX3090\n- \u8bad\u7ec3\u65f6\u95f4\u603b\u5171\u8017\u8d392.5\u5929\n\n# \u56db\u3001 \u4f7f\u7528\n```python\nfrom transformers import pipeline\nfill_mask = pipeline(\n\t\"fill-mask\",\n\tmodel=\"junnyu/electra_small_generator\",\n\ttokenizer=\"junnyu/electra_small_generator\"\n)\nprint(\n\tfill_mask(\"HuggingFace is creating a [MASK] that the community uses to solve NLP tasks.\")\n)\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/electra_small_generator?", "answers": [{"text": "electra", "answer_start": 47, "answer_end": 53}]}, {"id": "q2", "question": "What is the model task of junnyu/electra_small_generator?", "answers": [{"text": "fill-mask", "answer_start": 479, "answer_end": 487}]}]}]}, {"title": "junnyu/roformer_base_wwm_cluecorpussmall", "paragraphs": [{"context": "---\nlanguage: zh\ntags:\n- roformer\n- pytorch\n- tf2.0\n- paddlepaddle\nwidget:\n- text: \"\u4eca\u5929[MASK]\u5f88\u597d\uff0c\u6211\u60f3\u53bb\u516c\u56ed\u73a9\uff01\"\n---\n## \u4ecb\u7ecd\nPretrained model on 13G Chinese corpus(clue corpus small). Masked language modeling(MLM) and sentence order prediction(SOP) are used as training task.\n\u572813g\u7684clue corpus small\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u4e86`Whole Mask LM` \u548c `SOP` \u4efb\u52a1\n\n\u8bad\u7ec3\u903b\u8f91\u53c2\u8003\u4e86\u8fd9\u91cc\u3002\n\n## \u8bad\u7ec3\u7ec6\u8282\uff1a\n- paddlepaddle+paddlenlp\n- V100 x 4\n- batch size 256\n- max_seq_len 512 \n- max_lr 0.0001\n- min_lr 0.00001\n- weight_decay 0.01\n- grad_clip 1.0\n- \u603b\u5171\u8bad\u7ec3\u7684\u53e5\u5b50```128*30w + 256*15w + 256*14.5w + 256*46.5w + 256*17w = 27648w```\n- \u7ea6\u7b49\u4e8e512 batch size, 100w\u6b65\u6761\u4ef6\u4e0b\u768454%\n\n\u6700\u7ec8loss\uff1a\n```python\n[2022-02-05 16:05:59,067] [    INFO] - global step 170100, loss: 2.651634932, lm_loss: 2.603405, sop_loss: 0.048229, speed: 1.06 steps/s, ips: 271.68 seqs/s, learning rate: 6.66465e-05, loss_scaling: 137438.96875, num_good_steps: 356, num_bad_steps: 0\n[2022-02-05 16:07:28,227] [    INFO] - global step 170200, loss: 2.822231531, lm_loss: 2.662831, sop_loss: 0.159401, speed: 1.12 steps/s, ips: 287.13 seqs/s, learning rate: 6.66263e-05, loss_scaling: 137438.96875, num_good_steps: 59, num_bad_steps: 0\n[2022-02-05 16:08:57,346] [    INFO] - global step 170300, loss: 2.710968971, lm_loss: 2.673646, sop_loss: 0.037323, speed: 1.12 steps/s, ips: 287.26 seqs/s, learning rate: 6.66061e-05, loss_scaling: 137438.96875, num_good_steps: 159, num_bad_steps: 0\n[2022-02-05 16:10:26,698] [    INFO] - global step 170400, loss: 2.867662907, lm_loss: 2.619032, sop_loss: 0.248631, speed: 1.12 steps/s, ips: 286.51 seqs/s, learning rate: 6.65859e-05, loss_scaling: 137438.96875, num_good_steps: 259, num_bad_steps: 0\n[2022-02-05 16:11:55,714] [    INFO] - global step 170500, loss: 3.158756495, lm_loss: 2.953678, sop_loss: 0.205079, speed: 1.12 steps/s, ips: 287.59 seqs/s, learning rate: 6.65657e-05, loss_scaling: 137438.96875, num_good_steps: 359, num_bad_steps: 0\n[2022-02-05 16:13:24,869] [    INFO] - global step 170600, loss: 2.860815048, lm_loss: 2.754750, sop_loss: 0.106064, speed: 1.12 steps/s, ips: 287.14 seqs/s, learning rate: 6.65455e-05, loss_scaling: 137438.96875, num_good_steps: 33, num_bad_steps: 0\n```\n### tf\u7248\u672c \n\n\n### pytorch\u7248\u672c+tf2.0\u7248\u672c\n\n\n## pytorch\u4f7f\u7528\n```python\nimport torch\nfrom transformers import RoFormerForMaskedLM, BertTokenizer\n\ntext = \"\u4eca\u5929[MASK]\u5f88\u597d\uff0c\u6211[MASK]\u53bb\u516c\u56ed\u73a9\u3002\"\ntokenizer = BertTokenizer.from_pretrained(\"junnyu/roformer_base_wwm_cluecorpussmall\")\npt_model = RoFormerForMaskedLM.from_pretrained(\"junnyu/roformer_base_wwm_cluecorpussmall\")\npt_inputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    pt_outputs = pt_model(**pt_inputs).logits[0]\npt_outputs_sentence = \"pytorch: \"\nfor i, id in enumerate(tokenizer.encode(text)):\n    if id == tokenizer.mask_token_id:\n        tokens = tokenizer.convert_ids_to_tokens(pt_outputs[i].topk(k=5)[1])\n        pt_outputs_sentence += \"[\" + \"\".join(tokens) + \"]\"\n    else:\n        pt_outputs_sentence += \"\".join(\n            tokenizer.convert_ids_to_tokens([id], skip_special_tokens=True))\nprint(pt_outputs_sentence)\n# pytorch: \u4eca\u5929[\u5929\u4eba\u6c14\u9633\u96e8]\u5f88\u597d\uff0c\u6211[\u60f3\u5c31\u8981\u4e5f\u8fd8]\u53bb\u516c\u56ed\u73a9\u3002\n\n```\n\n## \u5f15\u7528\n\nBibtex\uff1a\n\n```tex\n@misc{su2021roformer,\n      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, \n      author={Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n      year={2021},\n      eprint={2104.09864},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/roformer_base_wwm_cluecorpussmall?", "answers": [{"text": "roformer", "answer_start": 25, "answer_end": 32}]}]}]}, {"title": "junnyu/roformer_chinese_sim_char_base", "paragraphs": [{"context": "---\nlanguage: zh\ntags:\n- roformer\n- pytorch\n- tf2.0\ninference: False\n---\n# \u5b89\u88c5\n- pip install roformer==0.4.3\n\n# \u4f7f\u7528\n```python\nimport torch\nimport numpy as np\nfrom roformer import RoFormerForCausalLM, RoFormerConfig\nfrom transformers import BertTokenizer\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\npretrained_model = \"junnyu/roformer_chinese_sim_char_base\"\ntokenizer = BertTokenizer.from_pretrained(pretrained_model)\nconfig = RoFormerConfig.from_pretrained(pretrained_model)\nconfig.is_decoder = True\nconfig.eos_token_id = tokenizer.sep_token_id\nconfig.pooler_activation = \"linear\"\nmodel = RoFormerForCausalLM.from_pretrained(pretrained_model, config=config)\nmodel.to(device)\nmodel.eval()\n\ndef gen_synonyms(text, n=100, k=20):\n    ''''\u542b\u4e49\uff1a \u4ea7\u751fsent\u7684n\u4e2a\u76f8\u4f3c\u53e5\uff0c\u7136\u540e\u8fd4\u56de\u6700\u76f8\u4f3c\u7684k\u4e2a\u3002\n    \u505a\u6cd5\uff1a\u7528seq2seq\u751f\u6210\uff0c\u5e76\u7528encoder\u7b97\u76f8\u4f3c\u5ea6\u5e76\u6392\u5e8f\u3002\n    '''\n    # \u5bfb\u627e\u6240\u6709\u76f8\u4f3c\u7684\u53e5\u5b50\n    r = []\n    inputs1 = tokenizer(text, return_tensors=\"pt\")\n    for _ in range(n):\n        inputs1.to(device)\n        output = tokenizer.batch_decode(model.generate(**inputs1, top_p=0.95, do_sample=True, max_length=128), skip_special_tokens=True)[0].replace(\" \",\"\").replace(text, \"\") # \u53bb\u9664\u7a7a\u683c\uff0c\u53bb\u9664\u539f\u59cbtext\u6587\u672c\u3002\n        r.append(output)\n    \n    # \u5bf9\u76f8\u4f3c\u7684\u53e5\u5b50\u8fdb\u884c\u6392\u5e8f\n    r = [i for i in set(r) if i != text and len(i) > 0]\n    r = [text] + r\n    inputs2 = tokenizer(r, padding=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        inputs2.to(device)\n        outputs = model(**inputs2)\n        Z = outputs.pooler_output.cpu().numpy()\n    Z /= (Z**2).sum(axis=1, keepdims=True)**0.5\n    argsort = np.dot(Z[1:], -Z[0]).argsort()\n    \n    return [r[i + 1] for i in argsort[:k]]\n\nout = gen_synonyms(\"\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u597d\uff1f\")\nprint(out)\n# ['\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u597d\uff1f',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6bd4\u8f83\u597d\u3002',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6700\u597d\uff1f',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6bd4\u8f83\u597d',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u90a3\u4e2a\u6bd4\u8f83\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u66f4\u597d\uff1f',\n#  '\u6df1\u5733\u4e0e\u5e7f\u5dde\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\uff0c\u54ea\u4e2a\u6bd4\u8f83\u597d',\n#  '\u5e7f\u5dde\u4e0e\u6df1\u5733\u6bd4\u8f83\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u91cc\u6bd4\u8f83\u597d',\n#  '\u6df1\u5733\u8fd8\u662f\u5e7f\u5dde\u6bd4\u8f83\u597d\uff1f',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u5730\u65b9\u597d\u4e00\u4e9b\uff1f',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d\uff1f',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d\u5462\uff1f',\n#  '\u5e7f\u5dde\u4e0e\u6df1\u5733\u54ea\u4e2a\u5730\u65b9\u597d\u70b9\uff1f',\n#  '\u6df1\u5733\u597d\u8fd8\u662f\u5e7f\u5dde\u597d',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u57ce\u5e02\u597d\uff1f']\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/roformer_chinese_sim_char_base?", "answers": [{"text": "roformer", "answer_start": 25, "answer_end": 32}]}]}]}, {"title": "junnyu/roformer_chinese_sim_char_ft_base", "paragraphs": [{"context": "---\nlanguage: zh\ntags:\n- roformer\n- pytorch\n- tf2.0\ninference: False\n---\n# \u5b89\u88c5\n- pip install roformer==0.4.3\n\n# \u4f7f\u7528\n```python\nimport torch\nimport numpy as np\nfrom roformer import RoFormerForCausalLM, RoFormerConfig\nfrom transformers import BertTokenizer\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\npretrained_model = \"junnyu/roformer_chinese_sim_char_base\"\ntokenizer = BertTokenizer.from_pretrained(pretrained_model)\nconfig = RoFormerConfig.from_pretrained(pretrained_model)\nconfig.is_decoder = True\nconfig.eos_token_id = tokenizer.sep_token_id\nconfig.pooler_activation = \"linear\"\nmodel = RoFormerForCausalLM.from_pretrained(pretrained_model, config=config)\nmodel.to(device)\nmodel.eval()\n\ndef gen_synonyms(text, n=100, k=20):\n    ''''\u542b\u4e49\uff1a \u4ea7\u751fsent\u7684n\u4e2a\u76f8\u4f3c\u53e5\uff0c\u7136\u540e\u8fd4\u56de\u6700\u76f8\u4f3c\u7684k\u4e2a\u3002\n    \u505a\u6cd5\uff1a\u7528seq2seq\u751f\u6210\uff0c\u5e76\u7528encoder\u7b97\u76f8\u4f3c\u5ea6\u5e76\u6392\u5e8f\u3002\n    '''\n    # \u5bfb\u627e\u6240\u6709\u76f8\u4f3c\u7684\u53e5\u5b50\n    r = []\n    inputs1 = tokenizer(text, return_tensors=\"pt\")\n    for _ in range(n):\n        inputs1.to(device)\n        output = tokenizer.batch_decode(model.generate(**inputs1, top_p=0.95, do_sample=True, max_length=128), skip_special_tokens=True)[0].replace(\" \",\"\").replace(text, \"\") # \u53bb\u9664\u7a7a\u683c\uff0c\u53bb\u9664\u539f\u59cbtext\u6587\u672c\u3002\n        r.append(output)\n    \n    # \u5bf9\u76f8\u4f3c\u7684\u53e5\u5b50\u8fdb\u884c\u6392\u5e8f\n    r = [i for i in set(r) if i != text and len(i) > 0]\n    r = [text] + r\n    inputs2 = tokenizer(r, padding=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        inputs2.to(device)\n        outputs = model(**inputs2)\n        Z = outputs.pooler_output.cpu().numpy()\n    Z /= (Z**2).sum(axis=1, keepdims=True)**0.5\n    argsort = np.dot(Z[1:], -Z[0]).argsort()\n    \n    return [r[i + 1] for i in argsort[:k]]\n\nout = gen_synonyms(\"\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u597d\uff1f\")\nprint(out)\n# ['\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u597d\uff1f',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6bd4\u8f83\u597d\u3002',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6700\u597d\uff1f',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6bd4\u8f83\u597d',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u90a3\u4e2a\u6bd4\u8f83\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u66f4\u597d\uff1f',\n#  '\u6df1\u5733\u4e0e\u5e7f\u5dde\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\uff0c\u54ea\u4e2a\u6bd4\u8f83\u597d',\n#  '\u5e7f\u5dde\u4e0e\u6df1\u5733\u6bd4\u8f83\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u91cc\u6bd4\u8f83\u597d',\n#  '\u6df1\u5733\u8fd8\u662f\u5e7f\u5dde\u6bd4\u8f83\u597d\uff1f',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u5730\u65b9\u597d\u4e00\u4e9b\uff1f',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d\uff1f',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d\u5462\uff1f',\n#  '\u5e7f\u5dde\u4e0e\u6df1\u5733\u54ea\u4e2a\u5730\u65b9\u597d\u70b9\uff1f',\n#  '\u6df1\u5733\u597d\u8fd8\u662f\u5e7f\u5dde\u597d',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u57ce\u5e02\u597d\uff1f']\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/roformer_chinese_sim_char_ft_base?", "answers": [{"text": "roformer", "answer_start": 25, "answer_end": 32}]}]}]}, {"title": "junnyu/roformer_chinese_sim_char_ft_small", "paragraphs": [{"context": "---\nlanguage: zh\ntags:\n- roformer\n- pytorch\n- tf2.0\ninference: False\n---\n# \u5b89\u88c5\n- pip install roformer==0.4.3\n\n# \u4f7f\u7528\n```python\nimport torch\nimport numpy as np\nfrom roformer import RoFormerForCausalLM, RoFormerConfig\nfrom transformers import BertTokenizer\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\npretrained_model = \"junnyu/roformer_chinese_sim_char_base\"\ntokenizer = BertTokenizer.from_pretrained(pretrained_model)\nconfig = RoFormerConfig.from_pretrained(pretrained_model)\nconfig.is_decoder = True\nconfig.eos_token_id = tokenizer.sep_token_id\nconfig.pooler_activation = \"linear\"\nmodel = RoFormerForCausalLM.from_pretrained(pretrained_model, config=config)\nmodel.to(device)\nmodel.eval()\n\ndef gen_synonyms(text, n=100, k=20):\n    ''''\u542b\u4e49\uff1a \u4ea7\u751fsent\u7684n\u4e2a\u76f8\u4f3c\u53e5\uff0c\u7136\u540e\u8fd4\u56de\u6700\u76f8\u4f3c\u7684k\u4e2a\u3002\n    \u505a\u6cd5\uff1a\u7528seq2seq\u751f\u6210\uff0c\u5e76\u7528encoder\u7b97\u76f8\u4f3c\u5ea6\u5e76\u6392\u5e8f\u3002\n    '''\n    # \u5bfb\u627e\u6240\u6709\u76f8\u4f3c\u7684\u53e5\u5b50\n    r = []\n    inputs1 = tokenizer(text, return_tensors=\"pt\")\n    for _ in range(n):\n        inputs1.to(device)\n        output = tokenizer.batch_decode(model.generate(**inputs1, top_p=0.95, do_sample=True, max_length=128), skip_special_tokens=True)[0].replace(\" \",\"\").replace(text, \"\") # \u53bb\u9664\u7a7a\u683c\uff0c\u53bb\u9664\u539f\u59cbtext\u6587\u672c\u3002\n        r.append(output)\n    \n    # \u5bf9\u76f8\u4f3c\u7684\u53e5\u5b50\u8fdb\u884c\u6392\u5e8f\n    r = [i for i in set(r) if i != text and len(i) > 0]\n    r = [text] + r\n    inputs2 = tokenizer(r, padding=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        inputs2.to(device)\n        outputs = model(**inputs2)\n        Z = outputs.pooler_output.cpu().numpy()\n    Z /= (Z**2).sum(axis=1, keepdims=True)**0.5\n    argsort = np.dot(Z[1:], -Z[0]).argsort()\n    \n    return [r[i + 1] for i in argsort[:k]]\n\nout = gen_synonyms(\"\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u597d\uff1f\")\nprint(out)\n# ['\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u597d\uff1f',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6bd4\u8f83\u597d\u3002',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6700\u597d\uff1f',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6bd4\u8f83\u597d',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u90a3\u4e2a\u6bd4\u8f83\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u66f4\u597d\uff1f',\n#  '\u6df1\u5733\u4e0e\u5e7f\u5dde\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\uff0c\u54ea\u4e2a\u6bd4\u8f83\u597d',\n#  '\u5e7f\u5dde\u4e0e\u6df1\u5733\u6bd4\u8f83\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u91cc\u6bd4\u8f83\u597d',\n#  '\u6df1\u5733\u8fd8\u662f\u5e7f\u5dde\u6bd4\u8f83\u597d\uff1f',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u5730\u65b9\u597d\u4e00\u4e9b\uff1f',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d\uff1f',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d\u5462\uff1f',\n#  '\u5e7f\u5dde\u4e0e\u6df1\u5733\u54ea\u4e2a\u5730\u65b9\u597d\u70b9\uff1f',\n#  '\u6df1\u5733\u597d\u8fd8\u662f\u5e7f\u5dde\u597d',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u57ce\u5e02\u597d\uff1f']\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/roformer_chinese_sim_char_ft_small?", "answers": [{"text": "roformer", "answer_start": 25, "answer_end": 32}]}]}]}, {"title": "junnyu/roformer_chinese_sim_char_small", "paragraphs": [{"context": "---\nlanguage: zh\ntags:\n- roformer\n- pytorch\n- tf2.0\ninference: False\n---\n# \u5b89\u88c5\n- pip install roformer==0.4.3\n\n# \u4f7f\u7528\n```python\nimport torch\nimport numpy as np\nfrom roformer import RoFormerForCausalLM, RoFormerConfig\nfrom transformers import BertTokenizer\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\npretrained_model = \"junnyu/roformer_chinese_sim_char_base\"\ntokenizer = BertTokenizer.from_pretrained(pretrained_model)\nconfig = RoFormerConfig.from_pretrained(pretrained_model)\nconfig.is_decoder = True\nconfig.eos_token_id = tokenizer.sep_token_id\nconfig.pooler_activation = \"linear\"\nmodel = RoFormerForCausalLM.from_pretrained(pretrained_model, config=config)\nmodel.to(device)\nmodel.eval()\n\ndef gen_synonyms(text, n=100, k=20):\n    ''''\u542b\u4e49\uff1a \u4ea7\u751fsent\u7684n\u4e2a\u76f8\u4f3c\u53e5\uff0c\u7136\u540e\u8fd4\u56de\u6700\u76f8\u4f3c\u7684k\u4e2a\u3002\n    \u505a\u6cd5\uff1a\u7528seq2seq\u751f\u6210\uff0c\u5e76\u7528encoder\u7b97\u76f8\u4f3c\u5ea6\u5e76\u6392\u5e8f\u3002\n    '''\n    # \u5bfb\u627e\u6240\u6709\u76f8\u4f3c\u7684\u53e5\u5b50\n    r = []\n    inputs1 = tokenizer(text, return_tensors=\"pt\")\n    for _ in range(n):\n        inputs1.to(device)\n        output = tokenizer.batch_decode(model.generate(**inputs1, top_p=0.95, do_sample=True, max_length=128), skip_special_tokens=True)[0].replace(\" \",\"\").replace(text, \"\") # \u53bb\u9664\u7a7a\u683c\uff0c\u53bb\u9664\u539f\u59cbtext\u6587\u672c\u3002\n        r.append(output)\n    \n    # \u5bf9\u76f8\u4f3c\u7684\u53e5\u5b50\u8fdb\u884c\u6392\u5e8f\n    r = [i for i in set(r) if i != text and len(i) > 0]\n    r = [text] + r\n    inputs2 = tokenizer(r, padding=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        inputs2.to(device)\n        outputs = model(**inputs2)\n        Z = outputs.pooler_output.cpu().numpy()\n    Z /= (Z**2).sum(axis=1, keepdims=True)**0.5\n    argsort = np.dot(Z[1:], -Z[0]).argsort()\n    \n    return [r[i + 1] for i in argsort[:k]]\n\nout = gen_synonyms(\"\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u597d\uff1f\")\nprint(out)\n# ['\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u597d\uff1f',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6bd4\u8f83\u597d\u3002',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6700\u597d\uff1f',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u6bd4\u8f83\u597d',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u90a3\u4e2a\u6bd4\u8f83\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u4e2a\u66f4\u597d\uff1f',\n#  '\u6df1\u5733\u4e0e\u5e7f\u5dde\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\uff0c\u54ea\u4e2a\u6bd4\u8f83\u597d',\n#  '\u5e7f\u5dde\u4e0e\u6df1\u5733\u6bd4\u8f83\u54ea\u4e2a\u597d',\n#  '\u6df1\u5733\u548c\u5e7f\u5dde\u54ea\u91cc\u6bd4\u8f83\u597d',\n#  '\u6df1\u5733\u8fd8\u662f\u5e7f\u5dde\u6bd4\u8f83\u597d\uff1f',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u5730\u65b9\u597d\u4e00\u4e9b\uff1f',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d\uff1f',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d\u5462\uff1f',\n#  '\u5e7f\u5dde\u4e0e\u6df1\u5733\u54ea\u4e2a\u5730\u65b9\u597d\u70b9\uff1f',\n#  '\u6df1\u5733\u597d\u8fd8\u662f\u5e7f\u5dde\u597d',\n#  '\u5e7f\u5dde\u597d\u8fd8\u662f\u6df1\u5733\u597d',\n#  '\u5e7f\u5dde\u548c\u6df1\u5733\u54ea\u4e2a\u57ce\u5e02\u597d\uff1f']\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/roformer_chinese_sim_char_small?", "answers": [{"text": "roformer", "answer_start": 25, "answer_end": 32}]}]}]}, {"title": "junnyu/roformer_small_discriminator", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- pytorch\n- electra\n- roformer\n- rotary position embedding\nlicense: mit\ndatasets:\n- openwebtext\n---\n# \u4e00\u3001 \u4e2a\u4eba\u5728openwebtext\u6570\u636e\u96c6\u4e0a\u6dfb\u52a0rotary-position-embedding\uff0c\u8bad\u7ec3\u5f97\u5230\u7684electra-small\u6a21\u578b\n\n# \u4e8c\u3001 \u590d\u73b0\u7ed3\u679c(dev dataset)\nCoLAMRPCQQPQNLIAvg.|\n---------------|\n56.887.488.387.980.36|\n55.7687.389.6188.8580.31|\n\n# \u4e09\u3001 \u8bad\u7ec3\u7ec6\u8282\n- \u6570\u636e\u96c6 openwebtext\n- \u8bad\u7ec3batch_size 256\n- \u5b66\u4e60\u7387lr  5e-4\n- \u6700\u5927\u53e5\u5b50\u957f\u5ea6max_seqlen  128\n- \u8bad\u7ec3total step  50W\n- GPU RTX3090\n- \u8bad\u7ec3\u65f6\u95f4\u603b\u5171\u8017\u8d3955h\n\n# \u56db\u3001wandb\u65e5\u5fd7 \n- [**\u9884\u8bad\u7ec3\u65e5\u5fd7**](\n- [**GLUE\u5fae\u8c03\u65e5\u5fd7**](\n\n\n# \u4e94\u3001 \u4f7f\u7528\n```python\nimport torch\nfrom transformers import ElectraTokenizer,RoFormerModel\ntokenizer = ElectraTokenizer.from_pretrained(\"junnyu/roformer_small_discriminator\")\nmodel = RoFormerModel.from_pretrained(\"junnyu/roformer_small_discriminator\")\ninputs = tokenizer(\"Beijing is the capital of China.\", return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n    print(outputs[0].shape)\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/roformer_small_discriminator?", "answers": [{"text": "roformer", "answer_start": 57, "answer_end": 64}]}]}]}, {"title": "junnyu/roformer_small_generator", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- pytorch\n- electra\n- masked-lm\n- rotary position embedding\nwidget:\n- text: Paris is the [MASK] of France.\nlicense: mit\ndatasets:\n- openwebtext\n---\n# \u4e00\u3001 \u4e2a\u4eba\u5728openwebtext\u6570\u636e\u96c6\u4e0a\u6dfb\u52a0rotary-position-embedding\uff0c\u8bad\u7ec3\u5f97\u5230\u7684electra-small\u6a21\u578b\n\n# \u4e8c\u3001 \u590d\u73b0\u7ed3\u679c(dev dataset)\nCoLAMRPCQQPQNLIAvg.|\n---------------|\n56.887.488.387.980.36|\n55.7687.389.6188.8580.31|\n\n# \u4e09\u3001 \u8bad\u7ec3\u7ec6\u8282\n- \u6570\u636e\u96c6 openwebtext\n- \u8bad\u7ec3batch_size 256\n- \u5b66\u4e60\u7387lr  5e-4\n- \u6700\u5927\u53e5\u5b50\u957f\u5ea6max_seqlen  128\n- \u8bad\u7ec3total step  50W\n- GPU RTX3090\n- \u8bad\u7ec3\u65f6\u95f4\u603b\u5171\u8017\u8d3955h\n\n# \u56db\u3001wandb\u65e5\u5fd7 \n- [**\u9884\u8bad\u7ec3\u65e5\u5fd7**](\n- [**GLUE\u5fae\u8c03\u65e5\u5fd7**](\n\n# \u4e94\u3001 \u4f7f\u7528\n```python\nimport torch\nfrom transformers import ElectraTokenizer,RoFormerForMaskedLM\n\ntext = \"Beijing is the capital of [MASK].\"\ntokenizer = ElectraTokenizer.from_pretrained(\"junnyu/roformer_small_generator\")\npt_model = RoFormerForMaskedLM.from_pretrained(\n    \"junnyu/roformer_small_generator\")\npt_inputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    pt_outputs = pt_model(**pt_inputs).logits[0]\npt_outputs_sentence = \"pytorch: \"\nfor i, id in enumerate(tokenizer.encode(text)):\n    if id == tokenizer.mask_token_id:\n        tokens = tokenizer.convert_ids_to_tokens(pt_outputs[i].topk(k=5)[1])\n        pt_outputs_sentence += \"[\" + \"\".join(tokens) + \"]\"\n    else:\n        pt_outputs_sentence += \"\".join(\n            tokenizer.convert_ids_to_tokens([id], skip_special_tokens=True))+\" \"\nprint(pt_outputs_sentence)\n# pytorch:  beijing is the capital of [chinabeijingtaiwanindiashanghai].  \n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/roformer_small_generator?", "answers": [{"text": "roformer", "answer_start": 738, "answer_end": 745}]}]}]}, {"title": "junnyu/wobert_chinese_plus_base", "paragraphs": [{"context": "---\nlanguage: zh\ntags:\n- wobert\ninference: False\n---\n## \u4ecb\u7ecd\n### tf\u7248\u672c \n\n### pytorch\u7248\u672c \n\n\n## \u5b89\u88c5(\u4e3b\u8981\u4e3a\u4e86\u5b89\u88c5WoBertTokenizer)\n```bash\npip install git+\n```\n\n## \u4f7f\u7528\n```python\nimport torch\nfrom transformers import BertForMaskedLM as WoBertForMaskedLM\nfrom wobert import WoBertTokenizer\n\npretrained_model_or_path_list = [\n    \"junnyu/wobert_chinese_plus_base\", \"junnyu/wobert_chinese_base\"\n]\nfor path in pretrained_model_or_path_list:\n    text = \"\u4eca\u5929[MASK]\u5f88\u597d\uff0c\u6211[MASK]\u53bb\u516c\u56ed\u73a9\u3002\"\n    tokenizer = WoBertTokenizer.from_pretrained(path)\n    model = WoBertForMaskedLM.from_pretrained(path)\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs).logits[0]\n    outputs_sentence = \"\"\n    for i, id in enumerate(tokenizer.encode(text)):\n        if id == tokenizer.mask_token_id:\n            tokens = tokenizer.convert_ids_to_tokens(outputs[i].topk(k=5)[1])\n            outputs_sentence += \"[\" + \"\".join(tokens) + \"]\"\n        else:\n            outputs_sentence += \"\".join(\n                tokenizer.convert_ids_to_tokens([id],\n                                                skip_special_tokens=True))\n    print(outputs_sentence)\n# RoFormer    \u4eca\u5929[\u5929\u6c14\u5929\u5fc3\u60c5\u9633\u5149\u7a7a\u6c14]\u5f88\u597d\uff0c\u6211[\u60f3\u8981\u6253\u7b97\u51c6\u5907\u559c\u6b22]\u53bb\u516c\u56ed\u73a9\u3002\n# PLUS WoBERT \u4eca\u5929[\u5929\u6c14\u9633\u5149\u5929\u5fc3\u60c5\u7a7a\u6c14]\u5f88\u597d\uff0c\u6211[\u60f3\u8981\u6253\u7b97\u51c6\u5907\u5c31]\u53bb\u516c\u56ed\u73a9\u3002\n# WoBERT      \u4eca\u5929[\u5929\u6c14\u9633\u5149\u5929\u5fc3\u60c5\u7a7a\u6c14]\u5f88\u597d\uff0c\u6211[\u60f3\u8981\u5c31\u51c6\u5907\u4e5f]\u53bb\u516c\u56ed\u73a9\u3002\n```\n## \u5f15\u7528\n\nBibtex\uff1a\n\n```tex\n@techreport{zhuiyiwobert,\n  title={WoBERT: Word-based Chinese BERT model - ZhuiyiAI},\n  author={Jianlin Su},\n  year={2020},\n  url=\"\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/wobert_chinese_plus_base?", "answers": [{"text": "bert", "answer_start": 27, "answer_end": 30}]}]}]}, {"title": "kSaluja/autonlp-tele_red_data_model-585716433", "paragraphs": [{"context": "---\ntags: autonlp\nlanguage: en\nwidget:\n- text: \"I love AutoNLP \ud83e\udd17\"\ndatasets:\n- kSaluja/autonlp-data-tele_red_data_model\nco2_eq_emissions: 2.379476355147211\n---\n\n# Model Trained Using AutoNLP\n\n- Problem type: Entity Extraction\n- Model ID: 585716433\n- CO2 Emissions (in grams): 2.379476355147211\n\n## Validation Metrics\n\n- Loss: 0.15210922062397003\n- Accuracy: 0.9724770642201835\n- Precision: 0.950836820083682\n- Recall: 0.9625838333921638\n- F1: 0.9566742676723382\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoNLP\"}' \n```\n\nOr Python API:\n\n```\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"kSaluja/autonlp-tele_red_data_model-585716433\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"kSaluja/autonlp-tele_red_data_model-585716433\", use_auth_token=True)\n\ninputs = tokenizer(\"I love AutoNLP\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n```", "qas": []}]}, {"title": "kangnichaluo/mnli-2", "paragraphs": [{"context": "learning rate: 3e-5  \ntraining epochs: 3  \nbatch size: 64  \nseed: 0  \nmodel: bert-base-uncased  \ntrained on MNLI which is converted into two-way nli classification (predict entailment or not-entailment class)", "qas": [{"id": "q1", "question": "What is the model architecture of kangnichaluo/mnli-2?", "answers": [{"text": "bert", "answer_start": 77, "answer_end": 80}]}]}]}, {"title": "kangnichaluo/mnli-3", "paragraphs": [{"context": "learning rate: 2e-5  \ntraining epochs: 3  \nbatch size: 64  \nseed: 13    \nmodel: bert-base-uncased  \ntrained on MNLI which is converted into two-way nli classification (predict entailment or not-entailment class)", "qas": [{"id": "q1", "question": "What is the model architecture of kangnichaluo/mnli-3?", "answers": [{"text": "bert", "answer_start": 80, "answer_end": 83}]}]}]}, {"title": "kangnichaluo/mnli-4", "paragraphs": [{"context": "learning rate: 2e-5  \ntraining epochs: 3  \nbatch size: 64  \nseed: 87    \nmodel: bert-base-uncased  \ntrained on MNLI which is converted into two-way nli classification (predict entailment or not-entailment class)", "qas": [{"id": "q1", "question": "What is the model architecture of kangnichaluo/mnli-4?", "answers": [{"text": "bert", "answer_start": 80, "answer_end": 83}]}]}]}, {"title": "kangnichaluo/mnli-5", "paragraphs": [{"context": "learning rate: 2e-5  \ntraining epochs: 3  \nbatch size: 64  \nseed: 111    \nmodel: bert-base-uncased  \ntrained on MNLI which is converted into two-way nli classification (predict entailment or not-entailment class)", "qas": [{"id": "q1", "question": "What is the model architecture of kangnichaluo/mnli-5?", "answers": [{"text": "bert", "answer_start": 81, "answer_end": 84}]}]}]}, {"title": "kangnichaluo/mnli-cb", "paragraphs": [{"context": "learning rate: 3e-5  \ntraining epochs: 5  \nbatch size: 8  \nseed: 42  \nmodel: bert-base-uncased  \nThe model is pretrained on MNLI (we use kangnichaluo/mnli-2 directly) and then finetuned on CB which is converted into two-way nli classification (predict entailment or not-entailment class)", "qas": [{"id": "q1", "question": "What is the model architecture of kangnichaluo/mnli-cb?", "answers": [{"text": "bert", "answer_start": 77, "answer_end": 80}]}]}]}, {"title": "kapilchauhan/bert-base-uncased-CoLA-finetuned-cola", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: bert-base-uncased-CoLA-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5755298089385917\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-uncased-CoLA-finetuned-cola\n\nThis model is a fine-tuned version of [textattack/bert-base-uncased-CoLA]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8318\n- Matthews Correlation: 0.5755\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5742          \n 2.0    0.7226          \n 3.0    0.8318          \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of kapilchauhan/bert-base-uncased-CoLA-finetuned-cola?", "answers": [{"text": "bert", "answer_start": 105, "answer_end": 108}]}, {"id": "q2", "question": "What is the model task of kapilchauhan/bert-base-uncased-CoLA-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 208, "answer_end": 226}]}]}]}, {"title": "kapilchauhan/distilbert-base-uncased-CoLA-finetuned-cola", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: distilbert-base-uncased-CoLA-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5689051637185746\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-CoLA-finetuned-cola\n\nThis model is a fine-tuned version of [textattack/distilbert-base-uncased-CoLA]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6996\n- Matthews Correlation: 0.5689\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.6061          \n 2.0    0.5808          \n 3.0    0.6996          \n 4.0    0.8249          \n 5.0    0.8714          \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of kapilchauhan/distilbert-base-uncased-CoLA-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 105, "answer_end": 114}]}, {"id": "q2", "question": "What is the model task of kapilchauhan/distilbert-base-uncased-CoLA-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 214, "answer_end": 232}]}]}]}, {"title": "kapilchauhan/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: distilbert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5135743708561838\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7696\n- Matthews Correlation: 0.5136\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.4948          \n 2.0    0.5135          \n 3.0    0.6303          \n 4.0    0.7696          \n 5.0    0.8774          \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of kapilchauhan/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 125, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of kapilchauhan/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 229, "answer_end": 247}]}]}]}, {"title": "lg/ghpy_20k", "paragraphs": [{"context": "**This model is provided with no guarantees whatsoever; use at your own risk.**\n\nThis is a Neo2.7B model fine tuned on github data scraped by an EleutherAI member (filtered for python-only) for 20k steps. A better code model is coming soon\u2122 (hopefully, maybe); this model was created mostly as a test of infrastructure code.", "qas": []}]}, {"title": "lgris/WavLM-large-CORAA-pt", "paragraphs": [{"context": "---\nlanguage:\n- pt\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- pt\nmodel-index:\n- name: WavLM-large-CORAA-pt\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# WavLM-large-CORAA-pt\n\nThis model is a fine-tuned version of [microsoft/wavlm-large]( on [CORAA dataset](\nIt achieves the following results on the evaluation set:\n- Loss: 0.6144\n- Wer: 0.3840\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- training_steps: 40000\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.04   1.9230          \n 0.08   1.3733          \n 0.13   1.1992          \n 0.17   1.1289          \n 0.21   1.0357          \n 0.25   1.0216          \n 0.29   0.9338          \n 0.33   0.9149          \n 0.38   0.8885          \n 0.42   0.8678          \n 0.46   0.8349          \n 0.5    0.8230          \n 0.54   0.8245          \n 0.59   0.7802          \n 0.63   0.7650          \n 0.67   0.7665          \n 0.71   0.7568          \n 0.75   0.7403          \n 0.8    0.7219          \n 0.84   0.7180          \n 0.88   0.7017          \n 0.92   0.6992          \n 0.96   0.7021          \n 1.0    0.6892          \n 1.05   0.6940          \n 1.09   0.6767          \n 1.13   0.6734          \n 1.17   0.6650          \n 1.21   0.6559          \n 1.26   0.6536          \n 1.3    0.6537          \n 1.34   0.6462          \n 1.38   0.6381          \n 1.42   0.6266          \n 1.46   0.6280          \n 1.51   0.6248          \n 1.55   0.6275          \n 1.59   0.6199          \n 1.63   0.6180          \n 1.67   0.6144          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- Pytorch 1.10.1+cu102\n- Datasets 1.17.1.dev0\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/WavLM-large-CORAA-pt?", "answers": [{"text": "wavlm", "answer_start": 397, "answer_end": 401}]}]}]}, {"title": "lgris/base_10k_8khz_pt", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# Wav2vec 2.0 for Portuguese in 8kHz\n\nThis is a fine-tuned model from [facebook/wav2vec2-base-10k-voxpopuli](\n\nDatasets used to fine-tune the model:\nCETUC: contains approximately 145 hours of Brazilian Portuguese speech distributed among 50 male and 50 female speakers, each pronouncing approximately 1,000 phonetically balanced sentences selected from the CETEN-Folha corpus.\nCommon Voice 7.0: is a project proposed by Mozilla Foundation with the goal to create a wide open dataset in different languages. In this project, volunteers donate and validate speech using the oficial site.\nLapsbm: \"Falabrasil - UFPA\" is a dataset used by the Fala Brasil group to benchmark ASR systems in Brazilian Portuguese. Contains 35 speakers (10 females), each one pronouncing 20 unique sentences, totalling 700 utterances in Brazilian Portuguese. The audios were recorded in 22.05 kHz without environment control.\nMultilingual Librispeech (MLS): a massive dataset available in many languages. The MLS is based on audiobook recordings in public domain like LibriVox. The dataset contains a total of 6k hours of transcribed data in many languages. The set in Portuguese used in this work (mostly Brazilian variant) has approximately 284 hours of speech, obtained from 55 audiobooks read by 62 speakers.\nMultilingual TEDx: a collection of audio recordings from TEDx talks in 8 source languages. The Portuguese set (mostly Brazilian Portuguese variant) contains 164 hours of transcribed speech.\nSidney (SID): contains 5,777 utterances recorded by 72 speakers (20 women) from 17 to 59 years old with fields such as place of birth, age, gender, education, and occupation;\nVoxForge: is a project with the goal to build open datasets for acoustic models. The corpus contains approximately 100 speakers and 4,130 utterances of Brazilian Portuguese, with sample rates varying from 16kHz to 44.1kHz\nVoxPopuli", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/base_10k_8khz_pt?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/base_10k_8khz_pt?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/base_10k_8khz_pt?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp-cetuc100-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# cetuc100-xlsr: Wav2vec 2.0 with CETUC Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the [CETUC]( dataset. This dataset contains approximately 145 hours of Brazilian Portuguese speech distributed among 50 male and 50 female speakers, each pronouncing approximately 1,000 phonetically balanced sentences selected from the [CETEN-Folha]( corpus.\n\nIn this notebook the model is tested against other available Brazilian Portuguese datasets. \n\n  Train   Test |\n-------:------:|\n  94h     5.4h |\n          9.5h |\n          0.1h |\n          3.7h |\n          1.8h |\n          1.0h |\n          0.1h |\n         21.6h |\n\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n 0.446  0.089  1.172  0.902  \n0.339  0.076  1.188  0.801 \n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/cetuc100-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.44677581829220825\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.8561919899139065\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.08955808080808081\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.9670008790979718\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 1.1723738343632861\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.929976436317539\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.9020183982683985\n\n\n### Tests with LM\n\n\n```python\n# !find -type f -name \"*.wav\" -delete\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.3396346663354827\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.7341013242719512\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.07612373737373737\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.960908940243212\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 1.188118540533579\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 1.2271077178339618\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.800196158008658\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp-cetuc100-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp-cetuc100-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp-cetuc100-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp-commonvoice10-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# commonvoice10-xlsr: Wav2vec 2.0 with Common Voice Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the [Common Voice 7.0]( dataset.\n\nIn this notebook the model is tested against other available Brazilian Portuguese datasets. \n\n  Train   Test |\n-------:------:|\n          5.4h |\n  37.8h   9.5h |\n          0.1h |\n          3.7h |\n          1.8h |\n          1.0h |\n          0.1h |\n         21.6h |\n\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n 0.133  0.165  0.247  0.251  \n 0.060  0.088  0.181  0.227 \n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/commonvoice10-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.13291846056190185\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.18909733896486755\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.1655429292929293\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.1894711228284466\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.2471983709551264\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.4739658565194102\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.2510294913419914\n\n\n### Tests with LM\n\n\n```python\n# !find -type f -name \"*.wav\" -delete\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.060609303416680915\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.11758415681158373\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.08815340909090909\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.1359966791836458\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.1818429601530829\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.39469326522731385\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.22779897186147183\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp-commonvoice10-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp-commonvoice10-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp-commonvoice10-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp-commonvoice100-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# commonvoice100-xlsr: Wav2vec 2.0 with Common Voice Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the [Common Voice 7.0]( dataset.\n\nIn this notebook the model is tested against other available Brazilian Portuguese datasets. \n\n  Train   Test |\n-------:------:|\n          5.4h |\n  37.8h   9.5h |\n          0.1h |\n          3.7h |\n          1.8h |\n          1.0h |\n          0.1h |\n         21.6h |\n\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n0.088  0.121  0.177  0.145 \n0.057  0.076  0.146  0.130 \n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/commonvoice100-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.08868880057404624\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.12601035333655114\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.12149621212121209\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.173594387890256\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.1775290775992294\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.4245704568241374\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.14541801948051947\n\n\n### Tests with LM\n\n\n```python\n# !find -type f -name \"*.wav\" -delete\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.05764220069547976\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.09569130510737103\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.07688131313131312\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.13814768877494732\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.14652459944499036\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.38196090002435623\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.13054112554112554\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp-commonvoice100-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp-commonvoice100-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp-commonvoice100-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp-lapsbm1-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# lapsbm1-xlsr: Wav2vec 2.0 with LaPSBM Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the [LaPS BM]( dataset.\n\nIn this notebook the model is tested against other available Brazilian Portuguese datasets. \n\n  Train   Test |\n-------:------:|\n          5.4h |\n          9.5h |\n  0.8h    0.1h |\n          3.7h |\n          1.8h |\n          1.0h |\n          0.1h |\n         21.6h |\n\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n 0.111  0.145  0.562  0.469  \n 0.061  0.089  0.452  0.381 \n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/lapsbm1-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.11147816967489037\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.41880890234535906\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.1451893939393939\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.29958960206171104\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.5626767414610376\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.5807549973642049\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.4693479437229436\n\n\n### Tests with LM\n\n\n```python\n# !find -type f -name \"*.wav\" -delete\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.06157628194513477\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.3051714756833442\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.0893623737373737\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.20062044237806004\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.4522665618175908\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.5256707813182246\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.38106331168831165\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp-lapsbm1-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp-lapsbm1-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp-lapsbm1-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp-mls100-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# mls100-xlsr: Wav2vec 2.0 with MLS Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the [Multilingual Librispeech in Portuguese (MLS)]( dataset.\n\nIn this notebook the model is tested against other available Brazilian Portuguese datasets. \n\n  Train   Test |\n-------:------:|\n          5.4h |\n          9.5h |\n          0.1h |\n  161h    3.7h |\n          1.8h |\n          1.0h |\n          0.1h |\n  161h   21.6h |\n\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n 0.192  0.162  0.268  0.268 \n 0.087  0.077  0.245  0.218 \n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/mls100-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n\n```python\n%cd bp_dataset/\n```\n\n    /content/bp_dataset\n\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.192586382955233\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.2604333640312866\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.16259469696969692\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.16343014413283674\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.2682880375992515\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.49252836581485837\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.2686972402597403\n\n\n### Tests with LM\n\n\n```python\n!rm -rf ~/.cache\n%cd /content/\n# !gdown --id '1d13Onxy9ubmJZORZ8FO2vnsnl36QMiUc' # trained with wikipedia;\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n%cd bp_dataset/\n```\n\n    /content/bp_dataset\n\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.0878818926974661\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.173303354010221\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.07691919191919189\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.12624377042839321\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.24545473435776916\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.4156272215612955\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.21832386363636366\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp-mls100-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp-mls100-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp-mls100-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp-sid10-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# sid10-xlsr: Wav2vec 2.0 with Sidney Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the [Sidney]( dataset.\n\nIn this notebook the model is tested against other available Brazilian Portuguese datasets. \n\n  Train   Test |\n-------:------:|\n          5.4h |\n          9.5h |\n          0.1h |\n          3.7h |\n          1.8h |\n   7.2h   1.0h |\n          0.1h |\n    7.2h 21.6h |\n\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n0.186  0.207  0.124  0.472  \n0.096  0.115  0.101  0.348 \n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/sid10-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.18623689076557778\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.3279775395502392\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.20780303030303032\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.5056711598536057\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.1247776617710105\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.8350609256842175\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.47242153679653687\n\n\n### Tests with LM\n\n\n```python\n# !find -type f -name \"*.wav\" -delete\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.09677271347353278\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.22363215674470321\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.1154924242424242\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.4322369152606427\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.10080313085145765\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.7911789829264236\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.34786255411255407\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp-sid10-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp-sid10-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp-sid10-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp-tedx100-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# tedx100-xlsr: Wav2vec 2.0 with TEDx Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the [TEDx multilingual in Portuguese]( dataset.\n\nIn this notebook the model is tested against other available Brazilian Portuguese datasets. \n\n  Train   Test |\n-------:------:|\n          5.4h |\n          9.5h |\n          0.1h |\n          3.7h |\n  148.8h  1.8h |\n          1.0h |\n          0.1h |\n148.8h   21.6h |\n\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n0.138  0.169  0.794  0.395  \n0.123  0.171  0.982  0.395 \n\n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/tedx100-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.13846663354859937\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.36960721735520236\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.16941287878787875\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.16586103382107384\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.7943364822145216\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.22221476803982182\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.39486066017315996\n\n\n### Tests with LM\n\n\n```python\n# !find -type f -name \"*.wav\" -delete\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.12338749517028079\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.4146185693398481\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.17142676767676762\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.15212081808962674\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.982518441309493\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.21567860841157235\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.3952218614718614\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp-tedx100-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp-tedx100-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp-tedx100-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp-voxforge1-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# voxforge1-xlsr: Wav2vec 2.0 with VoxForge Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the [VoxForge]( dataset.\n\nIn this notebook the model is tested against other available Brazilian Portuguese datasets. \n\n  Train   Test |\n-------:------:|\n          5.4h |\n          9.5h |\n          0.1h |\n          3.7h |\n          1.8h |\n          1.0h |\n   3.9h   0.1h |\n   3.9h  21.6h |\n\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n 0.468  0.503  0.717  0.561  \n 0.322  0.356  0.586  0.428 \n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/voxforge1-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.4684840205331983\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.6080167359840954\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.5037468434343434\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.505595213971485\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.7177723323755854\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.7309431974873112\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.5613906926406929\n\n\n### Tests with LM\n\n\n```python\n# !find -type f -name \"*.wav\" -delete\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.32184971297675896\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.4707820098981609\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.356227904040404\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.3786376653384398\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.5864959640811857\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.6368727228726417\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.4279924242424241\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp-voxforge1-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp-voxforge1-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp-voxforge1-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp400-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\n- hf-asr-leaderboard\nmodel-index:\n- name: bp400-xlsr\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 7.0\n      type: mozilla-foundation/common_voice_7_0\n      args: pt\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 14.0\nlicense: apache-2.0\n---\n\n# bp400-xlsr: Wav2vec 2.0 with Brazilian Portuguese (BP) Dataset\n\n**Paper:** \n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the following datasets:\n\n- [CETUC]( contains approximately 145 hours of Brazilian Portuguese speech distributed among 50 male and 50 female speakers, each pronouncing approximately 1,000 phonetically balanced sentences selected from the [CETEN-Folha]( corpus.\n- [Common Voice 7.0](  is a project proposed by Mozilla Foundation with the goal to create a wide open dataset in different languages. In this project, volunteers donate and validate speech using the [oficial site]( \n- [Lapsbm]( \"Falabrasil - UFPA\" is a dataset used by the Fala Brasil group to benchmark ASR systems in Brazilian Portuguese. Contains 35 speakers (10 females), each one pronouncing 20 unique sentences, totalling  700 utterances in Brazilian Portuguese. The audios were recorded in 22.05 kHz without environment control.\n- [Multilingual Librispeech (MLS)]( a massive dataset available in many languages. The MLS is based on audiobook recordings in public domain like [LibriVox]( The dataset contains a total of 6k hours of transcribed data in many languages. The set in Portuguese [used in this work]( (mostly Brazilian variant) has approximately 284 hours of speech, obtained from 55 audiobooks read by 62 speakers.\n- [Multilingual TEDx]( a collection of audio recordings from TEDx talks in 8 source languages. The Portuguese set (mostly Brazilian Portuguese variant) contains 164 hours of transcribed speech. \n- [Sidney]( (SID): contains 5,777 utterances recorded by 72 speakers (20 women) from 17 to 59 years old with fields such as place of birth, age, gender, education, and occupation;\n- [VoxForge]( is a project with the goal to build open datasets for acoustic models. The corpus contains approximately 100 speakers and 4,130 utterances of Brazilian Portuguese, with sample rates varying from 16kHz to 44.1kHz.\n\nThese datasets were combined to build a larger Brazilian Portuguese dataset. All data was used for training except Common Voice dev/test sets, that were used for validation/test respectively. We also made test sets for all the gathered datasets.\n\n  Train   Test |\n-------:------:|\n  93.9h   5.4h |\n  37.6h   9.5h |\n   0.8h   0.1h |\n 161.0h   3.7h |\n 144.2h   1.8h |\n   5.0h   1.0h |\n   2.8h   0.1h |\n 437.2h  21.6h |\n\nThe original model was fine-tuned using [fairseq]( This notebook uses a converted version of the original one. The link to the original fairseq model is available [here](\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n          0.052           0.074           0.121           0.118 \n  0.033   0.046           0.112           0.123 \n **0.030**  0.043  0.118  **0.117** \n 0.033   0.043  **0.111**  0.123 \n 0.032  **0.036**           0.115           0.125 \n\n#### Transcription examples\n\n Transcription                                                                                                |\n--------------------------------------------------------------------------------------------------------------|\n algu\u00e9m sabe a que horas **come\u00e7o** jantar |\n**lilacovas** ainda n\u00e3o sabe o que vai fazer no fundo|\n**quet\u00e1** um pouco **deste** bom **ispaguete**|\n**rongkong** **en** **cantones** significa porto perfumado|\nvamos **rackar** esse problema|\napenas **ha** poucos metros **\u00e1** uma esta\u00e7\u00e3o de \u00f4nibus|\n**relampagotrev\u00e3o** sempre andam juntos|\n\n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/bp400-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.05159104708285062\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.14031426198658084\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.07432133838383838\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.11678793514817509\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.12152357273433984\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.24666815906766504\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.11873106060606062\n\n\n### Tests with LM\n\n\n```python\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n### Cetuc\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.030266462438593742\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.09577710237417715\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.043617424242424235\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.10642133314350002\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.11839021001747055\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.22929952467810416\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.11716314935064935\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp400-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp400-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp400-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp500-base100k_voxpopuli", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\nlicense: apache-2.0\n---\n\n# bp500-base100k_voxpopuli: Wav2vec 2.0 with Brazilian Portuguese (BP) Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the following datasets:\n\n- [CETUC]( contains approximately 145 hours of Brazilian Portuguese speech distributed among 50 male and 50 female speakers, each pronouncing approximately 1,000 phonetically balanced sentences selected from the [CETEN-Folha]( corpus.\n- [Common Voice 7.0](  is a project proposed by Mozilla Foundation with the goal to create a wide open dataset in different languages. In this project, volunteers donate and validate speech using the [oficial site]( \n- [Lapsbm]( \"Falabrasil - UFPA\" is a dataset used by the Fala Brasil group to benchmark ASR systems in Brazilian Portuguese. Contains 35 speakers (10 females), each one pronouncing 20 unique sentences, totalling  700 utterances in Brazilian Portuguese. The audios were recorded in 22.05 kHz without environment control.\n- [Multilingual Librispeech (MLS)]( a massive dataset available in many languages. The MLS is based on audiobook recordings in public domain like [LibriVox]( The dataset contains a total of 6k hours of transcribed data in many languages. The set in Portuguese [used in this work]( (mostly Brazilian variant) has approximately 284 hours of speech, obtained from 55 audiobooks read by 62 speakers.\n- [Multilingual TEDx]( a collection of audio recordings from TEDx talks in 8 source languages. The Portuguese set (mostly Brazilian Portuguese variant) contains 164 hours of transcribed speech. \n- [Sidney]( (SID): contains 5,777 utterances recorded by 72 speakers (20 women) from 17 to 59 years old with fields such as place of birth, age, gender, education, and occupation;\n- [VoxForge]( is a project with the goal to build open datasets for acoustic models. The corpus contains approximately 100 speakers and 4,130 utterances of Brazilian Portuguese, with sample rates varying from 16kHz to 44.1kHz.\n\nThese datasets were combined to build a larger Brazilian Portuguese dataset. All data was used for training except Common Voice dev/test sets, that were used for validation/test respectively. We also made test sets for all the gathered datasets.\n\n  Train   Test |\n-------:------:|\n 94.0h    5.4h |\n 37.8h    9.5h |\n 0.8h     0.1h |\n 161.0h   3.7h |\n 148.9h   1.8h |\n 7.2h     1.0h |\n 3.9h     0.1h |\n 453.6h  21.6h |\n\nThe original model was fine-tuned using [fairseq]( This notebook uses a converted version of the original one. The link to the original fairseq model is available [here](\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n 0.142  0.052  0.102  0.048 \n 0.099  0.047  0.115  0.127 \n\n#### Transcription examples\n\n Transcription                                                                                                |\n--------------------------------------------------------------------------------------------------------------|\n**qualo** **est\u00e1** **gramedele**|\no **capit\u00e3l** foi **exposo** do ex\u00e9rcito porque era doido|\ntamb\u00e9m **porque** n\u00e3o|\nn\u00e3o existe tempo como *o* presente|\neu pulei para salvar **haquel**|\naugusto **cesa** **passoesmarinho**|\n\n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/bp500-base100k_voxpopuli\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n\n```python\n%cd bp_dataset \n```\n\n    /content/bp_dataset\n\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.1419179499917191\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.20079950312040154\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.052780934343434324\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.22413887199364113\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.1019041538671034\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.31711268778273327\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.04826433982683982\n\n\n### Tests with LM\n\n\n```python\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n### Cetuc\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.099518615112877\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.1488912889506362\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.047080176767676764\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.19220291966887196\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.11535498771650306\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.3707890073539895\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.12682088744588746\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp500-base100k_voxpopuli?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp500-base100k_voxpopuli?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp500-base100k_voxpopuli?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp500-base10k_voxpopuli", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\n- hf-asr-leaderboard\nmodel-index:\n- name: bp500-base10k_voxpopuli\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice \n      type: common_voice\n      args: pt\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 24.9\nlicense: apache-2.0\n---\n\n# bp500-base10k_voxpopuli: Wav2vec 2.0 with Brazilian Portuguese (BP) Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the following datasets:\n\n- [CETUC]( contains approximately 145 hours of Brazilian Portuguese speech distributed among 50 male and 50 female speakers, each pronouncing approximately 1,000 phonetically balanced sentences selected from the [CETEN-Folha]( corpus.\n- [Common Voice 7.0](  is a project proposed by Mozilla Foundation with the goal to create a wide open dataset in different languages. In this project, volunteers donate and validate speech using the [oficial site]( \n- [Lapsbm]( \"Falabrasil - UFPA\" is a dataset used by the Fala Brasil group to benchmark ASR systems in Brazilian Portuguese. Contains 35 speakers (10 females), each one pronouncing 20 unique sentences, totalling  700 utterances in Brazilian Portuguese. The audios were recorded in 22.05 kHz without environment control.\n- [Multilingual Librispeech (MLS)]( a massive dataset available in many languages. The MLS is based on audiobook recordings in public domain like [LibriVox]( The dataset contains a total of 6k hours of transcribed data in many languages. The set in Portuguese [used in this work]( (mostly Brazilian variant) has approximately 284 hours of speech, obtained from 55 audiobooks read by 62 speakers.\n- [Multilingual TEDx]( a collection of audio recordings from TEDx talks in 8 source languages. The Portuguese set (mostly Brazilian Portuguese variant) contains 164 hours of transcribed speech. \n- [Sidney]( (SID): contains 5,777 utterances recorded by 72 speakers (20 women) from 17 to 59 years old with fields such as place of birth, age, gender, education, and occupation;\n- [VoxForge]( is a project with the goal to build open datasets for acoustic models. The corpus contains approximately 100 speakers and 4,130 utterances of Brazilian Portuguese, with sample rates varying from 16kHz to 44.1kHz.\n\nThese datasets were combined to build a larger Brazilian Portuguese dataset. All data was used for training except Common Voice dev/test sets, that were used for validation/test respectively. We also made test sets for all the gathered datasets.\n\n  Train   Test |\n-------:------:|\n 94.0h    5.4h |\n 37.8h    9.5h |\n 0.8h     0.1h |\n 161.0h   3.7h |\n 148.9h   1.8h |\n 7.2h     1.0h |\n 3.9h     0.1h |\n 453.6h  21.6h |\n\nThe original model was fine-tuned using [fairseq]( This notebook uses a converted version of the original one. The link to the original fairseq model is available [here](\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n 0.120  0.039  0.169  0.116 \n  0.074  0.032  0.181  0.111 \n\n#### Transcription examples\n\n Transcription                                                                                                |\n--------------------------------------------------------------------------------------------------------------|\nsuco **de\u00fava** e \u00e1gua **mistur\u00e3o** bem|\n**cupa** do dinheiro|\neu **omo** **sh\u00faters cofedete** \u00e9 meu favorito|\nvoc\u00ea pode explicar *por* que isso **ontece**|\nno futuro voc\u00ea desejar\u00e1 **a** ter come\u00e7ado a investir hoje|\n\n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/bp500-base10k_voxpopuli\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n\n```python\n%cd bp_dataset \n```\n\n    /content/bp_dataset\n\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.12096759949218888\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.24977003159495725\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.039769570707070705\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.2269637077788063\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.1691680138494731\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.34908555859018014\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.11649350649350651\n\n\n### Tests with LM\n\n\n```python\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n### Cetuc\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.07499558425787961\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.17442648452610307\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.032774621212121206\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.18213620321569274\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.18102544972868206\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.3491402028105601\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.11189529220779222\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp500-base10k_voxpopuli?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp500-base10k_voxpopuli?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp500-base10k_voxpopuli?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/bp500-xlsr", "paragraphs": [{"context": "---\nlanguage: pt\ndatasets:\n- common_voice \n- mls\n- cetuc\n- lapsbm\n- voxforge\n- tedx\n- sid\nmetrics:\n- wer\ntags:\n- audio\n- speech\n- wav2vec2\n- pt\n- portuguese-speech-corpus\n- automatic-speech-recognition\n- speech\n- PyTorch\n- hf-asr-leaderboard\nmodel-index:\n- name: bp400-xlsr\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice \n      type: common_voice\n      args: pt\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 13.6\nlicense: apache-2.0\n---\n\n# bp500-xlsr: Wav2vec 2.0 with Brazilian Portuguese (BP) Dataset\n\nThis is a the demonstration of a fine-tuned Wav2vec model for Brazilian Portuguese using the following datasets:\n\n- [CETUC]( contains approximately 145 hours of Brazilian Portuguese speech distributed among 50 male and 50 female speakers, each pronouncing approximately 1,000 phonetically balanced sentences selected from the [CETEN-Folha]( corpus;\n- [Common Voice 7.0](  is a project proposed by Mozilla Foundation with the goal to create a wide open dataset in different languages. In this project, volunteers donate and validate speech using the [oficial site](\n- [Lapsbm]( \"Falabrasil - UFPA\" is a dataset used by the Fala Brasil group to benchmark ASR systems in Brazilian Portuguese. Contains 35 speakers (10 females), each one pronouncing 20 unique sentences, totalling  700 utterances in Brazilian Portuguese. The audios were recorded in 22.05 kHz without environment control;\n- [Multilingual Librispeech (MLS)]( a massive dataset available in many languages. The MLS is based on audiobook recordings in public domain like [LibriVox]( The dataset contains a total of 6k hours of transcribed data in many languages. The set in Portuguese [used in this work]( (mostly Brazilian variant) has approximately 284 hours of speech, obtained from 55 audiobooks read by 62 speakers;\n- [VoxForge]( is a project with the goal to build open datasets for acoustic models. The corpus contains approximately 100 speakers and 4,130 utterances of Brazilian Portuguese, with sample rates varying from 16kHz to 44.1kHz.\n\nThese datasets were combined to build a larger Brazilian Portuguese dataset. All data was used for training except Common Voice dev/test sets, that were used for validation/test respectively. We also made test sets for all the gathered datasets.\n\n  Train   Test |\n-------:------:|\n  93.9h   5.4h |\n  37.6h   9.5h |\n   0.8h   0.1h |\n 161.0h   3.7h |\n 144.2h   1.8h |\n   5.0h   1.0h |\n   2.8h   0.1h |\n 437.2h  21.6h |\n\nThe original model was fine-tuned using [fairseq]( This notebook uses a converted version of the original one. The link to the original fairseq model is available [here](\n\n#### Summary\n\n CETUC           LaPS            SID             VF             \n---------------------------------------------------------------\n 0.051  0.032  0.095  0.082 \n 0.032  0.022  0.125  0.065 \n\n#### Transcription examples\n\n Transcription                                                                                                |\n--------------------------------------------------------------------------------------------------------------|\nn\u00e3o h\u00e1 um **dearamento** de mediadores independente das federa\u00e7\u00f5es e das **agrebia\u00e7\u00f5es**|\n**masque** bodega|\na cortina abriu o **ch\u00f4** come\u00e7ou|\n**busote avinhoa** **passadeiro**|\n**stou** estou maravilhada est\u00e1 tudo pronto|\n\n\n## Demonstration\n\n\n```python\nMODEL_NAME = \"lgris/bp500-xlsr\" \n```\n\n### Imports and dependencies\n\n\n```python\n%%capture\n!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f \n!pip install datasets\n!pip install jiwer\n!pip install transformers\n!pip install soundfile\n!pip install pyctcdecode\n!pip install \n```\n\n\n```python\nimport jiwer\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2Processor,\n)\nfrom pyctcdecode import build_ctcdecoder\nimport torch\nimport re\nimport sys\n```\n\n### Helpers\n\n\n```python\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\;\\:\\\"]'  # noqa: W605\n\ndef map_to_array(batch):\n    speech, _ = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = speech.squeeze(0).numpy() \n    batch[\"sampling_rate\"] = 16_000 \n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower().replace(\"\u2019\", \"'\")\n    batch[\"target\"] = batch[\"sentence\"]\n    return batch\n```\n\n\n```python\ndef calc_metrics(truths, hypos):\n    wers = []\n    mers = []\n    wils = []\n    for t, h in zip(truths, hypos):\n        try:\n            wers.append(jiwer.wer(t, h))\n            mers.append(jiwer.mer(t, h))\n            wils.append(jiwer.wil(t, h))\n        except: # Empty string?\n            pass\n    wer = sum(wers)/len(wers)\n    mer = sum(mers)/len(mers)\n    wil = sum(wils)/len(wils)\n    return wer, mer, wil\n```\n\n\n```python\ndef load_data(dataset):\n    data_files = {'test': f'{dataset}/test.csv'}\n    dataset = load_dataset('csv', data_files=data_files)[\"test\"]\n    return dataset.map(map_to_array)\n```\n\n### Model\n\n\n```python\nclass STT:\n\n    def __init__(self, \n                 model_name, \n                 device='cuda' if torch.cuda.is_available() else 'cpu', \n                 lm=None):\n        self.model_name = model_name\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.vocab_dict = self.processor.tokenizer.get_vocab()\n        self.sorted_dict = {\n            k.lower(): v for k, v in sorted(self.vocab_dict.items(), \n                                            key=lambda item: item[1])\n        }\n        self.device = device\n        self.lm = lm\n        if self.lm:            \n            self.lm_decoder = build_ctcdecoder(\n                list(self.sorted_dict.keys()),\n                self.lm\n            )\n\n    def batch_predict(self, batch):\n        features = self.processor(batch[\"speech\"], \n                                  sampling_rate=batch[\"sampling_rate\"][0], \n                                  padding=True, \n                                  return_tensors=\"pt\")\n        input_values = features.input_values.to(self.device)\n        attention_mask = features.attention_mask.to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n        if self.lm:\n            logits = logits.cpu().numpy()\n            batch[\"predicted\"] = []\n            for sample_logits in logits:\n                batch[\"predicted\"].append(self.lm_decoder.decode(sample_logits))\n        else:\n            pred_ids = torch.argmax(logits, dim=-1)\n            batch[\"predicted\"] = self.processor.batch_decode(pred_ids)\n        return batch\n```\n\n### Download datasets\n\n\n```python\n%%capture\n!gdown --id 1HFECzIizf-bmkQRLiQD0QVqcGtOG5upI\n!mkdir bp_dataset\n!unzip bp_dataset -d bp_dataset/\n```\n\n\n```python\n%cd bp_dataset \n```\n\n    /content/bp_dataset\n\n\n### Tests \n\n\n```python\nstt = STT(MODEL_NAME)\n```\n\n#### CETUC\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.05159097808687998\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.13659981509705973\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.03196969696969697\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.1178481066463896\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.09544588416964224\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.24868046340420813\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.08246076839826841\n\n\n### Tests with LM\n\n\n```python\n!rm -rf ~/.cache\n!gdown --id 1GJIKseP5ZkTbllQVgOL98R4yYAcIySFP  # trained with wikipedia\nstt = STT(MODEL_NAME, lm='pt-BR-wiki.word.4-gram.arpa')\n# !gdown --id 1dLFldy7eguPtyJj5OAlI4Emnx0BpFywg  # trained with bp\n# stt = STT(MODEL_NAME, lm='pt-BR.word.4-gram.arpa')\n```\n\n### Cetuc\n\n\n```python\nds = load_data('cetuc_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CETUC WER:\", wer)\n```\n    CETUC WER: 0.03222801788375573\n\n\n#### Common Voice\n\n\n```python\nds = load_data('commonvoice_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"CV WER:\", wer)\n```\n    CV WER: 0.09713866021093655\n\n\n#### LaPS\n\n\n```python\nds = load_data('lapsbm_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Laps WER:\", wer)\n```\n    Laps WER: 0.022310606060606065\n\n\n#### MLS\n\n\n```python\nds = load_data('mls_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"MLS WER:\", wer)\n```\n    MLS WER: 0.11408590958696524\n\n\n#### SID\n\n\n```python\nds = load_data('sid_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"Sid WER:\", wer)\n```\n    Sid WER: 0.12502797252979136\n\n\n#### TEDx\n\n\n```python\nds = load_data('tedx_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"TEDx WER:\", wer)\n```\n    TEDx WER: 0.24603179403904793\n\n\n#### VoxForge\n\n\n```python\nds = load_data('voxforge_dataset')\nresult = ds.map(stt.batch_predict, batched=True, batch_size=8) \nwer, mer, wil = calc_metrics(result[\"sentence\"], result[\"predicted\"])\nprint(\"VoxForge WER:\", wer)\n```\n    VoxForge WER: 0.06542207792207791\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of lgris/bp500-xlsr?", "answers": [{"text": "wav2vec2", "answer_start": 130, "answer_end": 137}]}, {"id": "q2", "question": "What is the model task of lgris/bp500-xlsr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 173, "answer_end": 200}]}, {"id": "q3", "question": "What is the model category of lgris/bp500-xlsr?", "answers": [{"text": "audio", "answer_start": 113, "answer_end": 117}]}]}]}, {"title": "lgris/distilxlsr_bp_12-16", "paragraphs": [{"context": "---\nlanguage: pt\ntags:\n- speech\nlicense: apache-2.0\n---\n\n# DistilXLSR-53 for BP\n[DistilXLSR-53 for BP: DistilHuBERT applied to Wav2vec XLSR-53 for Brazilian Portuguese](\n\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog]( for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n\n**Note 2**: The XLSR-53 model was distilled using [Brazilian Portuguese Datasets]( for test purposes. The dataset is quite small to perform such task (the performance might not be so good as the [original work](\n\n\n**Abstract**\n\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\n\n# Usage\nSee [this blog]( for more information on how to fine-tune the model. \n", "qas": []}]}, {"title": "manandey/wav2vec2-large-xlsr-breton", "paragraphs": [{"context": "---\nlanguage: br\ndatasets:\n- common_voice\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 Breton by Manan Dey\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice br\n      type: common_voice\n      args: br\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 54.04\n---\n\n# Wav2Vec2-Large-XLSR-53-Breton\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( in Breton using the [Common Voice](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\n\nThe model can be used directly (without a language model) as follows:\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\ntest_dataset = load_dataset(\"common_voice\", \"br\", split=\"test[:2%]\").\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"manandey/wav2vec2-large-xlsr-breton\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"manandey/wav2vec2-large-xlsr-breton\")\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on the {language} test data of Common Voice.\n\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\ntest_dataset = load_dataset(\"common_voice\", \"br\", split=\"test\")\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"manandey/wav2vec2-large-xlsr-breton\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"manandey/wav2vec2-large-xlsr-breton\")\nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\\u201d\\\ufffd\\\u2019\\\u2013\\(\\)\\/\\\u00ab\\\u00bb\\\u00bd\\\u2026]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n```\n\n**Test Result**: 54.04%\n\n\n## Training\n\nThe Common Voice `train`, `validation` datasets were used for training.\n", "qas": [{"id": "q1", "question": "What is the model architecture of manandey/wav2vec2-large-xlsr-breton?", "answers": [{"text": "wav2vec2", "answer_start": 507, "answer_end": 514}]}, {"id": "q2", "question": "What is the model task of manandey/wav2vec2-large-xlsr-breton?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 58, "answer_end": 85}]}, {"id": "q3", "question": "What is the model category of manandey/wav2vec2-large-xlsr-breton?", "answers": [{"text": "audio", "answer_start": 50, "answer_end": 54}]}]}]}, {"title": "manandey/wav2vec2-large-xlsr-estonian", "paragraphs": [{"context": "---\nlanguage: et\ndatasets:\n- common_voice\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 Estonian by Manan Dey\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice et\n      type: common_voice\n      args: et\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 37.36\n---\n\n# Wav2Vec2-Large-XLSR-53-Estonian\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( in Estonian using the [Common Voice](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\n\nThe model can be used directly (without a language model) as follows:\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\ntest_dataset = load_dataset(\"common_voice\", \"et\", split=\"test[:2%]\").\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"manandey/wav2vec2-large-xlsr-estonian\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"manandey/wav2vec2-large-xlsr-estonian\")\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on the {language} test data of Common Voice.\n\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\ntest_dataset = load_dataset(\"common_voice\", \"et\", split=\"test\")\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"manandey/wav2vec2-large-xlsr-estonian\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"manandey/wav2vec2-large-xlsr-estonian\")\nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\\u201d\\\ufffd\\|\\\u0964\\\u2013\\\u2019\\']'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n```\n\n**Test Result**: 37.36%\n\n\n## Training\n\nThe Common Voice `train`, `validation` datasets were used for training.\n", "qas": [{"id": "q1", "question": "What is the model architecture of manandey/wav2vec2-large-xlsr-estonian?", "answers": [{"text": "wav2vec2", "answer_start": 511, "answer_end": 518}]}, {"id": "q2", "question": "What is the model task of manandey/wav2vec2-large-xlsr-estonian?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 58, "answer_end": 85}]}, {"id": "q3", "question": "What is the model category of manandey/wav2vec2-large-xlsr-estonian?", "answers": [{"text": "audio", "answer_start": 50, "answer_end": 54}]}]}]}, {"title": "manandey/wav2vec2-large-xlsr-mongolian", "paragraphs": [{"context": "---\nlanguage: mn\ndatasets:\n- common_voice\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 Mongolian by Manan Dey\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice mn\n      type: common_voice\n      args: mn\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 43.08\n---\n\n# Wav2Vec2-Large-XLSR-53-Mongolian\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( in Mongolian using the [Common Voice](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\n## Usage\n\nThe model can be used directly (without a language model) as follows:\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\ntest_dataset = load_dataset(\"common_voice\", \"mn\", split=\"test[:2%]\").\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"manandey/wav2vec2-large-xlsr-mongolian\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"manandey/wav2vec2-large-xlsr-mongolian\")\n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n  speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n  batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n  return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n  logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on the {language} test data of Common Voice.\n\n\n```python\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\ntest_dataset = load_dataset(\"common_voice\", \"mn\", split=\"test\")\nwer = load_metric(\"wer\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"manandey/wav2vec2-large-xlsr-mongolian\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"manandey/wav2vec2-large-xlsr-mongolian\")\nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\\u201d\\\ufffd\\\u2019\\\u2013\\(\\)]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n```\n\n**Test Result**: 43.08%\n\n\n## Training\n\nThe Common Voice `train`, `validation` datasets were used for training.\n", "qas": [{"id": "q1", "question": "What is the model architecture of manandey/wav2vec2-large-xlsr-mongolian?", "answers": [{"text": "wav2vec2", "answer_start": 513, "answer_end": 520}]}, {"id": "q2", "question": "What is the model task of manandey/wav2vec2-large-xlsr-mongolian?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 58, "answer_end": 85}]}, {"id": "q3", "question": "What is the model category of manandey/wav2vec2-large-xlsr-mongolian?", "answers": [{"text": "audio", "answer_start": 50, "answer_end": 54}]}]}]}, {"title": "mariagrandury/roberta-base-finetuned-sms-spam-detection", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- sms_spam\nmetrics:\n- accuracy\nmodel-index:\n- name: roberta-base-finetuned-sms-spam-detection\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: sms_spam\n      type: sms_spam\n      args: plain_text\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.998\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-base-finetuned-sms-spam-detection\n\nThis model is a fine-tuned version of [roberta-base]( on the sms_spam dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0133\n- Accuracy: 0.998\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.0156          \n 2.0    0.0133          \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of mariagrandury/roberta-base-finetuned-sms-spam-detection?", "answers": [{"text": "roberta", "answer_start": 110, "answer_end": 116}]}, {"id": "q2", "question": "What is the model task of mariagrandury/roberta-base-finetuned-sms-spam-detection?", "answers": [{"text": "text-classification", "answer_start": 217, "answer_end": 235}]}]}]}, {"title": "microsoft/deberta-v2-xlarge-mnli", "paragraphs": [{"context": "---\nlanguage: en\ntags: \n- deberta\n- deberta-mnli\ntasks: mnli\nthumbnail: \nlicense: mit\nwidget:\n- text: \"[CLS] I love you. [SEP] I like you. [SEP]\"\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository]( for more details and updates.\n\nThis the DeBERTa V2 xlarge model fine-tuned with MNLI task, 24 layers, 1536 hidden size. Total parameters 900M.\n\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n SQuAD 1.1  MNLI-m/mm    QNLI  RTE     QQP   \n---------------------------------------------\n F1/EM      Acc          Acc   Acc    Acc/F1 \n 90.9/84.1  86.6/-       92.3  70.4    91.3/- \n 94.6/88.9  90.2/-       93.9  86.6    92.2/- \n 95.1/89.7  90.8/-       94.9  85.9    92.3/- \n 95.5/90.1  91.3/91.195.3 91.0 92.3/- \n -/-   91.5/91.2 -  93.1    -    \n91.7/91.6 95.8**93.9**92.3/89.8\n **93.1/94.9****93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI]( [DeBERTa-XLarge-MNLI]( [DeBERTa-V2-XLarge-MNLI]( [DeBERTa-V2-XXLarge-MNLI]( The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers]( you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of microsoft/deberta-v2-xlarge-mnli?", "answers": [{"text": "deberta-v2", "answer_start": 1811, "answer_end": 1820}]}, {"id": "q2", "question": "What is the model task of microsoft/deberta-v2-xlarge-mnli?", "answers": [{"text": "text-classification", "answer_start": 1669, "answer_end": 1687}]}]}]}, {"title": "microsoft/deberta-v2-xlarge", "paragraphs": [{"context": "---\nlanguage: en\ntags: \n- deberta\n- fill-mask\nthumbnail: \nlicense: mit\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data.\n\nPlease check the [official repository]( for more details and updates.\n\nThis is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n SQuAD 1.1  MNLI-m/mm    QNLI  RTE     QQP   \n---------------------------------------------\n F1/EM      Acc          Acc   Acc    Acc/F1 \n 90.9/84.1  86.6/-       92.3  70.4    91.3/- \n 94.6/88.9  90.2/-       93.9  86.6    92.2/- \n 95.1/89.7  90.8/-       94.9  85.9    92.3/- \n 95.5/90.1  91.3/91.195.3 91.0 92.3/- \n -/-   91.5/91.2 -  93.1    -    \n91.7/91.6 95.8**93.9**92.3/89.8\n **93.1/94.9****93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI]( [DeBERTa-XLarge-MNLI]( [DeBERTa-V2-XLarge-MNLI]( [DeBERTa-V2-XXLarge-MNLI]( The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers]( you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of microsoft/deberta-v2-xlarge?", "answers": [{"text": "deberta-v2", "answer_start": 1761, "answer_end": 1770}]}, {"id": "q2", "question": "What is the model task of microsoft/deberta-v2-xlarge?", "answers": [{"text": "fill-mask", "answer_start": 36, "answer_end": 44}]}]}]}, {"title": "microsoft/deberta-v2-xxlarge-mnli", "paragraphs": [{"context": "---\nlanguage: en\ntags: \n- deberta\n- deberta-mnli\ntasks: mnli\nthumbnail: \nlicense: mit\nwidget:\n- text: \"[CLS] I love you. [SEP] I like you. [SEP]\"\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data.  \n\nPlease check the [official repository]( for more details and updates.\n\nThis the DeBERTa V2 XXLarge model fine-tuned with MNLI task, 48 layers, 1536 hidden size. Total parameters 1.5B.\n\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n SQuAD 1.1  MNLI-m/mm    QNLI  RTE     QQP   \n---------------------------------------------\n F1/EM      Acc          Acc   Acc    Acc/F1 \n 90.9/84.1  86.6/-       92.3  70.4    91.3/- \n 94.6/88.9  90.2/-       93.9  86.6    92.2/- \n 95.1/89.7  90.8/-       94.9  85.9    92.3/- \n 95.5/90.1  91.3/91.195.3 91.0 92.3/- \n -/-   91.5/91.2 -  93.1    -    \n91.7/91.6 95.8**93.9**92.3/89.8\n **93.1/94.9****93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI]( [DeBERTa-XLarge-MNLI]( [DeBERTa-V2-XLarge-MNLI]( [DeBERTa-V2-XXLarge-MNLI]( The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers]( we recommand using **deepspeed** as it's faster and saves memory.\n\nRun with `Deepspeed`,\n\n```bash\npip install datasets\npip install deepspeed\n\n# Download the deepspeed config file\nwget  -O ds_config.json\n\nexport TASK_NAME=rte\noutput_dir=\"ds_results\"\nnum_gpus=8\nbatch_size=4\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\\\n  run_glue.py \\\\\n  --model_name_or_path microsoft/deberta-v2-xxlarge-mnli \\\\\n  --task_name $TASK_NAME \\\\\n  --do_train \\\\\n  --do_eval \\\\\n  --max_seq_length 256 \\\\\n  --per_device_train_batch_size ${batch_size} \\\\\n  --learning_rate 3e-6 \\\\\n  --num_train_epochs 3 \\\\\n  --output_dir $output_dir \\\\\n  --overwrite_output_dir \\\\\n  --logging_steps 10 \\\\\n  --logging_dir $output_dir \\\\\n  --deepspeed ds_config.json\n```\n\nYou can also run with `--sharded_ddp`\n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=rte\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge-mnli   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 256   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of microsoft/deberta-v2-xxlarge-mnli?", "answers": [{"text": "deberta-v2", "answer_start": 1986, "answer_end": 1995}]}, {"id": "q2", "question": "What is the model task of microsoft/deberta-v2-xxlarge-mnli?", "answers": [{"text": "text-classification", "answer_start": 2419, "answer_end": 2437}]}]}]}, {"title": "microsoft/deberta-v3-large", "paragraphs": [{"context": "---\nlanguage: en\ntags: \n  - deberta\n  - deberta-v3\n  - fill-mask\nthumbnail: \nlicense: mit\n---\n\n## DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nIn [DeBERTa V3]( we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our [paper](\n\nPlease check the [official repository]( for more implementation details and updates.\n\nThe DeBERTa V3 large model comes with 24 layers and a hidden size of 1024. It has 304M backbone parameters  with a vocabulary containing 128K tokens which introduces 131M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\n\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 2.0 and MNLI tasks.\n\nVocabulary(K) SQuAD 2.0(F1/EM) \n---------------------\n50      89.4/86.5 \n32      90.6/87.9 \n50      90.7/88.0 \n128  **91.5/89.0**\n\n\n#### Fine-tuning with HF transformers\n\n```bash\n#!/bin/bash\n\ncd transformers/examples/pytorch/text-classification/\n\npip install datasets\nexport TASK_NAME=mnli\n\noutput_dir=\"ds_results\"\n\nnum_gpus=8\n\nbatch_size=8\n\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\n  run_glue.py \\\n  --model_name_or_path microsoft/deberta-v3-large \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --evaluation_strategy steps \\\n  --max_seq_length 256 \\\n  --warmup_steps 50 \\\n  --per_device_train_batch_size ${batch_size} \\\n  --learning_rate 6e-6 \\\n  --num_train_epochs 2 \\\n  --output_dir $output_dir \\\n  --overwrite_output_dir \\\n  --logging_steps 1000 \\\n  --logging_dir $output_dir\n\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following papers:\n\n``` latex\n@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={\n}\n```\n", "qas": [{"id": "q2", "question": "What is the model task of microsoft/deberta-v3-large?", "answers": [{"text": "fill-mask", "answer_start": 55, "answer_end": 63}]}]}]}, {"title": "microsoft/deberta-v3-small", "paragraphs": [{"context": "---\nlanguage: en\ntags: \n  - deberta\n  - deberta-v3\n  - fill-mask\nthumbnail: \nlicense: mit\n---\n\n## DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nIn [DeBERTa V3]( we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our [paper](\n\nPlease check the [official repository]( for more implementation details and updates.\n\nThe DeBERTa V3 small model comes with 6 layers and a hidden size of 768. It has **44M** backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\n\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 2.0 and MNLI tasks.\n\nVocabulary(K) SQuAD 2.0(F1/EM) \n---------------------\n50      83.7/80.5 \n32      -/80.2    \n30     -/80.5    \n50       86.2/83.1\n128  91.5/89.0  \n128 88.4/85.4 \n128 **82.8/80.4** \n128 -/-       \n\n\n#### Fine-tuning with HF transformers\n\n```bash\n#!/bin/bash\n\ncd transformers/examples/pytorch/text-classification/\n\npip install datasets\nexport TASK_NAME=mnli\n\noutput_dir=\"ds_results\"\n\nnum_gpus=8\n\nbatch_size=8\n\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\n  run_glue.py \\\n  --model_name_or_path microsoft/deberta-v3-small \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --evaluation_strategy steps \\\n  --max_seq_length 256 \\\n  --warmup_steps 1500 \\\n  --per_device_train_batch_size ${batch_size} \\\n  --learning_rate 4.5e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir $output_dir \\\n  --overwrite_output_dir \\\n  --logging_steps 1000 \\\n  --logging_dir $output_dir\n\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following papers:\n\n``` latex\n@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={\n}\n```\n", "qas": [{"id": "q2", "question": "What is the model task of microsoft/deberta-v3-small?", "answers": [{"text": "fill-mask", "answer_start": 55, "answer_end": 63}]}]}]}, {"title": "microsoft/deberta-v3-xsmall", "paragraphs": [{"context": "---\nlanguage: en\ntags: \n  - deberta\n  - deberta-v3\n  - fill-mask\nthumbnail: \nlicense: mit\n---\n\n## DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nIn [DeBERTa V3]( we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our [paper](\n\nPlease check the [official repository]( for more implementation details and updates.\n\nThe DeBERTa V3 xsmall model comes with 12 layers and a hidden size of 384. It has only **22M** backbone parameters  with a vocabulary containing 128K tokens which introduces 48M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\n\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 2.0 and MNLI tasks.\n\nVocabulary(K) SQuAD 2.0(F1/EM) \n---------------------\n50      83.7/80.5 \n32      -/80.2    \n30     -/80.5    \n50       86.2/83.1\n128 91.5/89.0 \n128 88.4/85.4 \n128 82.8/80.4 \n128 **84.8/82.0** \n128 -/-       \n\n\n[#30     -         ]::\n\n#### Fine-tuning with HF transformers\n\n```bash\n#!/bin/bash\n\ncd transformers/examples/pytorch/text-classification/\n\npip install datasets\nexport TASK_NAME=mnli\n\noutput_dir=\"ds_results\"\n\nnum_gpus=8\n\nbatch_size=8\n\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\n  run_glue.py \\\n  --model_name_or_path microsoft/deberta-v3-xsmall \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --evaluation_strategy steps \\\n  --max_seq_length 256 \\\n  --warmup_steps 1000 \\\n  --per_device_train_batch_size ${batch_size} \\\n  --learning_rate 4.5e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir $output_dir \\\n  --overwrite_output_dir \\\n  --logging_steps 1000 \\\n  --logging_dir $output_dir\n\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following papers:\n\n``` latex\n@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={\n}\n```\n", "qas": [{"id": "q2", "question": "What is the model task of microsoft/deberta-v3-xsmall?", "answers": [{"text": "fill-mask", "answer_start": 55, "answer_end": 63}]}]}]}, {"title": "microsoft/deberta-xlarge", "paragraphs": [{"context": "---\nlanguage: en\ntags: \n- deberta-v1\n- fill-mask\nthumbnail: \nlicense: mit\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository]( for more details and updates.\n\nThis the DeBERTa XLarge model with 48 layers, 1024 hidden size. Total parameters 750M.\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n SQuAD 1.1  MNLI-m/mm    QNLI  RTE     QQP   \n---------------------------------------------\n F1/EM      Acc          Acc   Acc    Acc/F1 \n 90.9/84.1  86.6/-       92.3  70.4    91.3/- \n 94.6/88.9  90.2/-       93.9  86.6    92.2/- \n 95.1/89.7  90.8/-       94.9  85.9    92.3/- \n 95.5/90.1  91.3/91.195.3 91.0 92.3/- \n -/-   91.5/91.2 -  93.1    -    \n91.7/91.6 95.8**93.9**92.3/89.8\n **93.1/94.9****93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI]( [DeBERTa-XLarge-MNLI]( [DeBERTa-V2-XLarge-MNLI]( [DeBERTa-V2-XXLarge-MNLI]( The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers]( you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of microsoft/deberta-xlarge?", "answers": [{"text": "deberta", "answer_start": 26, "answer_end": 32}]}, {"id": "q2", "question": "What is the model task of microsoft/deberta-xlarge?", "answers": [{"text": "fill-mask", "answer_start": 39, "answer_end": 47}]}]}]}, {"title": "microsoft/graphcodebert-base", "paragraphs": [{"context": "## GraphCodeBERT model\n\nGraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages. \n\nMore details can be found in the [paper]( by Guo et. al.\n\n**Disclaimer:** The team releasing BERT did not write a model card for this model so this model card has been written by the Hugging Face community members.", "qas": []}]}, {"title": "microsoft/infoxlm-base", "paragraphs": [{"context": "# InfoXLM\n\n**InfoXLM** (NAACL 2021, [paper]( [repo]( [model]( InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training.\n\n**MD5**\n\n```\nb9d214025837250ede2f69c9385f812c  config.json\nbd6b1f392293f0cd9cd829c02971ecd9  pytorch_model.bin\nbf25eb5120ad92ef5c7d8596b5dc4046  sentencepiece.bpe.model\needbd60a7268b9fc45981b849664f747  tokenizer.json\n```\n\n**BibTeX**\n\n```\n@inproceedings{chi-etal-2021-infoxlm,\n  title = \"{I}nfo{XLM}: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training\",\n  author={Chi, Zewen and Dong, Li and Wei, Furu and Yang, Nan and Singhal, Saksham and Wang, Wenhui and Song, Xia and Mao, Xian-Ling and Huang, Heyan and Zhou, Ming},\n  booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n  month = jun,\n  year = \"2021\",\n  address = \"Online\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"\n  doi = \"10.18653/v1/2021.naacl-main.280\",\n  pages = \"3576--3588\",}\n```", "qas": []}]}, {"title": "microsoft/mdeberta-v3-base", "paragraphs": [{"context": "---\nlanguage: \n- multilingual\n- en \n- ar \n- bg \n- de \n- el \n- es \n- fr \n- hi \n- ru \n- sw \n- th \n- tr \n- ur \n- vi \n- zh\ntags: \n  - deberta\n  - deberta-v3\n  - mdeberta\n  - fill-mask\nthumbnail: \nlicense: mit\n---\n\n## DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nIn [DeBERTa V3]( we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our [paper](\n\nPlease check the [official repository]( for more implementation details and updates.\n\nmDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.\nThe mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R.\n\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on XNLI with zero-shot cross-lingual transfer setting, i.e. training with English data only, test on other languages.\n\navg   fr de   bg  tr   vi    zh  sw   \n ----------   --   --   --    --  --  \n76.2 79.778.7 79.6 74.2 76.5 76.7 66.5\n**79.8**+/-0.2**82.6****82.7** **82.4** **79.5** **78.1** **79.5** **73.9**\n\n#### Fine-tuning with HF transformers\n\n```bash\n#!/bin/bash\n\ncd transformers/examples/pytorch/text-classification/\n\npip install datasets\n\noutput_dir=\"ds_results\"\n\nnum_gpus=8\n\nbatch_size=4\n\npython -m torch.distributed.launch --nproc_per_node=${num_gpus} \\\n  run_xnli.py \\\n  --model_name_or_path microsoft/mdeberta-v3-base \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --train_language en \\\n  --language en \\\n  --evaluation_strategy steps \\\n  --max_seq_length 256 \\\n  --warmup_steps 3000 \\\n  --per_device_train_batch_size ${batch_size} \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 6 \\\n  --output_dir $output_dir \\\n  --overwrite_output_dir \\\n  --logging_steps 1000 \\\n  --logging_dir $output_dir\n\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following papers:\n\n``` latex\n@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={\n}\n```\n", "qas": [{"id": "q2", "question": "What is the model task of microsoft/mdeberta-v3-base?", "answers": [{"text": "fill-mask", "answer_start": 170, "answer_end": 178}]}]}]}, {"title": "microsoft/prophetnet-large-uncased-squad-qg", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- squad\n---\n\n## \nprophetnet-large-uncased-squad-qg\nFine-tuned weights(converted from [original fairseq version repo]( for [ProphetNet]( on question generation \nSQuAD 1.1.  \nProphetNet is a new pre-trained language model for sequence-to-sequence learning with a novel self-supervised objective called future n-gram prediction.  \nProphetNet is able to predict more future tokens with a n-stream decoder. The original implementation is Fairseq version at [github repo](   \n\n### Usage\n```\nfrom transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration, ProphetNetConfig\n\nmodel = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased-squad-qg')\ntokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased-squad-qg')\n\nFACT_TO_GENERATE_QUESTION_FROM = \"\"Bill Gates [SEP] Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975.\"\n\ninputs = tokenizer([FACT_TO_GENERATE_QUESTION_FROM], return_tensors='pt')\n\n# Generate Summary\nquestion_ids = model.generate(inputs['input_ids'], num_beams=5, early_stopping=True)\ntokenizer.batch_decode(question_ids, skip_special_tokens=True)\n\n# should give: 'along with paul allen, who founded microsoft?'\n```\n### Citation\n```bibtex\n@article{yan2020prophetnet,\n  title={Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training},\n  author={Yan, Yu and Qi, Weizhen and Gong, Yeyun and Liu, Dayiheng and Duan, Nan and Chen, Jiusheng and Zhang, Ruofei and Zhou, Ming},\n  journal={arXiv preprint arXiv:2001.04063},\n  year={2020}\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of microsoft/prophetnet-large-uncased-squad-qg?", "answers": [{"text": "prophetnet", "answer_start": 44, "answer_end": 53}]}]}]}, {"title": "mrm8488/bert-spanish-cased-finetuned-pos", "paragraphs": [{"context": "---\nlanguage: es\nthumbnail: \ntags:\n- POS\n- Spanish\nwidget:\n- text: \"Mis amigos y yo estamos pensando en viajar a Londres este verano.\"\n---\n\n\n# Spanish BERT (BETO) + POS\n\nThis model is a fine-tuned on Spanish [CONLL CORPORA]( version of the Spanish BERT cased [(BETO)]( for **POS** (Part of Speech tagging) downstream task.\n\n## Details of the downstream task (POS) - Dataset\n\n- [Dataset:  CONLL Corpora ES]( with data augmentation techniques\n\nI preprocessed the dataset and split it as train / dev (80/20)\n\n # Examples |\n ----- |\n 340 K |\n 50 K |\n\n\n- [Fine-tune on NER script provided by Huggingface](\n\n- **60** Labels covered:\n\n```\nAO, AQ, CC, CS, DA, DD, DE, DI, DN, DP, DT, Faa, Fat, Fc, Fd, Fe, Fg, Fh, Fia, Fit, Fp, Fpa, Fpt, Fs, Ft, Fx, Fz, I, NC, NP, P0, PD, PI, PN, PP, PR, PT, PX, RG, RN, SP, VAI, VAM, VAN, VAP, VAS, VMG, VMI, VMM, VMN, VMP, VMS, VSG, VSI, VSM, VSN, VSP, VSS, Y and Z\n```\n\n\n## Metrics on evaluation set:\n\n  # score  |\n :-------: |\n **90.06**  \n **89.46** | \n **90.67** |                                    \n\n## Model in action\n\nFast usage with **pipelines**:\n\n```python\nfrom transformers import pipeline\n\nnlp_pos = pipeline(\n    \"ner\",\n    model=\"mrm8488/bert-spanish-cased-finetuned-pos\",\n    tokenizer=(\n        'mrm8488/bert-spanish-cased-finetuned-pos',  \n        {\"use_fast\": False}\n))\n\n\ntext = 'Mis amigos est\u00e1n pensando en viajar a Londres este verano'\n\nnlp_pos(text)\n\n#Output:\n'''\n[{'entity': 'NC', 'score': 0.7792173624038696, 'word': '[CLS]'},\n {'entity': 'DP', 'score': 0.9996283650398254, 'word': 'Mis'},\n {'entity': 'NC', 'score': 0.9999253749847412, 'word': 'amigos'},\n {'entity': 'VMI', 'score': 0.9998560547828674, 'word': 'est\u00e1n'},\n {'entity': 'VMG', 'score': 0.9992249011993408, 'word': 'pensando'},\n {'entity': 'SP', 'score': 0.9999602437019348, 'word': 'en'},\n {'entity': 'VMN', 'score': 0.9998666048049927, 'word': 'viajar'},\n {'entity': 'SP', 'score': 0.9999545216560364, 'word': 'a'},\n {'entity': 'VMN', 'score': 0.8722310662269592, 'word': 'Londres'},\n {'entity': 'DD', 'score': 0.9995203614234924, 'word': 'este'},\n {'entity': 'NC', 'score': 0.9999248385429382, 'word': 'verano'},\n {'entity': 'NC', 'score': 0.8802427649497986, 'word': '[SEP]'}]\n '''\n```\n![model in action](\n\n16 POS tags version also available [here](\n\n\n> Created by [Manuel Romero/@mrm8488](\n\n> Made with <span style=\"color: #e25555;\">&hearts;</span> in Spain\n", "qas": [{"id": "q1", "question": "What is the model architecture of mrm8488/bert-spanish-cased-finetuned-pos?", "answers": [{"text": "bert", "answer_start": 1181, "answer_end": 1184}]}]}]}, {"title": "mrm8488/codebert-base-finetuned-stackoverflow-ner", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- \nwidget:\n- text: \"I want to create a table and ListView or ArrayList for Android or javascript in Windows 10\"\n\nlicense: mit\n---\n\n# Codebert (base) fine-tuned this [dataset]( for NER\n\n\n## Eval metrics\n\neval_accuracy_score = 0.9430622955139325\n\neval_precision = 0.6047440699126092\n\neval_recall = 0.6100755667506297\n\neval_f1 = 0.607398119122257\n", "qas": []}]}, {"title": "mrm8488/convbert-base-spanish", "paragraphs": [{"context": "---\nlanguage: es\ndatasets:\n- large_spanish_corpus\nlicense: mit\n---\n\n# ConvBERT base pre-trained on large_spanish_corpus\n\nThe ConvBERT architecture is presented in the [\"ConvBERT: Improving BERT with Span-based Dynamic Convolution\"]( paper.\n\n## Metrics on evaluation set\n\n```\ndisc_accuracy = 0.9488542\ndisc_auc = 0.8833056\ndisc_loss = 0.15933733\ndisc_precision = 0.79224133\ndisc_recall = 0.27443287\nglobal_step = 1000000\nloss = 9.658503\nmasked_lm_accuracy = 0.6177698\nmasked_lm_loss = 1.7050561\nsampled_masked_lm_accuracy = 0.5379228\n```\n\n## Usage\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\nmodel_name = \"mrm8488/convbert-base-spanish\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n```\n\n> Created by [Manuel Romero/@mrm8488]( with the support of [Narrativa](\n\n> Made with <span style=\"color: #e25555;\">&hearts;</span> in Spain", "qas": [{"id": "q1", "question": "What is the model architecture of mrm8488/convbert-base-spanish?", "answers": [{"text": "convbert", "answer_start": 630, "answer_end": 637}]}]}]}, {"title": "mrm8488/deberta-v3-small-finetuned-cola", "paragraphs": [{"context": "---\nlanguage:\n- en\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nwidget:\n- text: They represented seriously to the dean Mary as a genuine linguist.\nmodel-index:\n- name: deberta-v3-small\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: GLUE COLA\n      type: glue\n      args: cola\n    metrics:\n    - type: matthews_correlation\n      value: 0.6333205721749096\n      name: Matthews Correlation\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: cola\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.8494726749760306\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjJjOTM0MTEzMzBlZWJlMWYwNzgzZmI3M2NiZWVjMDQ5ZDA1MWY0NGY3NjU1NTlmZWE3N2JjZWEzODE0ZTNkNSIsInZlcnNpb24iOjF9.Kt-3jnDTp3-Te5zMHVgG_5hpB5UMCkAMP7fmjx46QDWJfFHpyRgBlf-qz_fw5saFPAQ5G6QNq3bjEJ6mY2lhAw\n    - type: precision\n      value: 0.8455882352941176\n      name: Precision\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODAxMzNkZGEwNGNmYjk4NWRhZDk4OWE4MzA5Y2NiNjQyNTdkOWRmYjU0ZjY0YzQzYmE4ZmI3MjQ4OTk4OWIwNCIsInZlcnNpb24iOjF9.YBFnePtD5-HX15aST39xpPLroFYBgqEn5iLyVaClh62j0M7HQbB8aaGEbgaTIUIr-qz12gVfIQ7UZZIHxby_BQ\n    - type: recall\n      value: 0.957004160887656\n      name: Recall\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjRjMTVhN2E4YjNlOWY2MWRhODZiM2FhZDVjNzYwMjIyNWUyYTMxMWFlZjkwNzVhYjNmMjQxYjk2MTFmMzYyYiIsInZlcnNpb24iOjF9.40GYlU9Do74Y_gLmbIKR2WM8okz5fm-QUwJAsoIyM1UtQ71lKd-FV5Yr9CdAh3fyQYa3SMYe6tm9OByNMMw_AA\n    - type: auc\n      value: 0.9167413271767129\n      name: AUC\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzVjYmMyZDkyMzM0ZTQ1MTk0ZmY4MWUwZmIxMGRlOWMyMjJmNDRiZGNkMGZlZDZmY2I5OWI2NDYzMGQ2YzhiNSIsInZlcnNpb24iOjF9.setZF_g9x-aknFXM1k0NxrOWMJcmpNi6z7QlyfL0i6fTPJOj6SbKJ1WQb3J1zTuabgx9cOc5xgHtBH3IA7fkDQ\n    - type: f1\n      value: 0.8978529603122967\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmQ1NmNiMDhmNTU2Y2UxMzU0ODRmYmZmZTFkYjI4MzczMWUwYWQ4OTk2NGJlY2MzNmViYTA4MTRkODJhMTU1MyIsInZlcnNpb24iOjF9.GUIRxsYKgjYK63JS2rd9vCLHHmCiB4H68Xo5GxMaITfyzcUcdNc6l62njmQGrOoUidlTt1F7DzGP2Cu_Gz8HDg\n    - type: loss\n      value: 0.4050811529159546\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjBjNjg0OTFjOTc5Mzc2MWQ1ZDIyYmM5MmIzZDVlY2JjYzBlZjMyN2IwOWU4YzNlMDcwZmM0NTMxYjExY2I0MiIsInZlcnNpb24iOjF9.xayLZc97iUW0zNqG65TiW9BXoqzV-tqF8g9qGCYQ1ZGuSDSjLlK7Y4og7-wqPEiME8JtNyVxl6-ZcWnF1t8cDg\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# DeBERTa-v3-small fine-tuned on CoLA\n\nThis model is a fine-tuned version of [microsoft/deberta-v3-small]( on the GLUE COLA dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4051\n- Matthews Correlation: 0.6333\n\n## Model description\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository]( for more details and updates.\n\nIn [DeBERTa V3]( we replaced the MLM objective with the RTD(Replaced Token Detection) objective introduced by ELECTRA for pre-training, as well as some innovations to be introduced in our upcoming paper. Compared to DeBERTa-V2,  our V3 version significantly improves the model performance in downstream tasks.  You can find a simple introduction about the model from the appendix A11 in our original [paper](  but we will provide more details in a separate write-up.\n\nThe DeBERTa V3 small model comes with 6 layers and a hidden size of 768. Its total parameter number is 143M since we use a vocabulary containing 128K tokens which introduce 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\n\nThe Corpus of Linguistic Acceptability (CoLA) in its full form consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version provided here contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set.\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5.0\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.4051          \n 2.0    0.4455          \n 3.0    0.5755          \n 4.0    0.7188          \n 5.0    0.8047          \n\n\n### Framework versions\n\n- Transformers 4.13.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of mrm8488/deberta-v3-small-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 266, "answer_end": 284}]}]}]}, {"title": "mrm8488/deberta-v3-small-finetuned-mnli", "paragraphs": [{"context": "---\nlanguage:\n- en\nlicense: mit\ntags:\n- generated_from_trainer\n- deberta-v3\ndatasets:\n- glue\nmetrics:\n- accuracy\nmodel-index:\n- name: ds_results\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: GLUE MNLI\n      type: glue\n      args: mnli\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.874593165174939\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# DeBERTa v3 (small) fine-tuned on MNLI\n\nThis model is a fine-tuned version of [microsoft/deberta-v3-small]( on the GLUE MNLI dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4985\n- Accuracy: 0.8746\n\n## Model description\n\n[DeBERTa]( improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \nPlease check the [official repository]( for more details and updates.\nIn [DeBERTa V3]( we replaced the MLM objective with the RTD(Replaced Token Detection) objective introduced by ELECTRA for pre-training, as well as some innovations to be introduced in our upcoming paper. Compared to DeBERTa-V2,  our V3 version significantly improves the model performance in downstream tasks.  You can find a simple introduction about the model from the appendix A11 in our original [paper](  but we will provide more details in a separate write-up.\nThe DeBERTa V3 small model comes with 6 layers and a hidden size of 768. Its total parameter number is 143M since we use a vocabulary containing 128K tokens which introduce 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nThe Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. The authors of the benchmark use the standard test set, for which they obtained private labels from the RTE authors, and evaluate on both the matched (in-domain) and mismatched (cross-domain) section. They also uses and recommend the SNLI corpus as 550k examples of auxiliary training data.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.04   0.5241          \n 0.08   0.4629          \n 0.12   0.4704          \n 0.16   0.4383          \n 0.2    0.4652          \n 0.24   0.4234          \n 0.29   0.4825          \n 0.33   0.3985          \n 0.37   0.4084          \n 0.41   0.3850          \n 0.45   0.3855          \n 0.49   0.4122          \n 0.53   0.4009          \n 0.57   0.4136          \n 0.61   0.3869          \n 0.65   0.3911          \n 0.69   0.3880          \n 0.73   0.3907          \n 0.77   0.3686          \n 0.81   0.4008          \n 0.86   0.3716          \n 0.9    0.3594          \n 0.94   0.3595          \n 0.98   0.3577          \n 1.02   0.4080          \n 1.06   0.3919          \n 1.1    0.4346          \n 1.14   0.4105          \n 1.18   0.4133          \n 1.22   0.4062          \n 1.26   0.3972          \n 1.3    0.3713          \n 1.34   0.3717          \n 1.39   0.3826          \n 1.43   0.4155          \n 1.47   0.4224          \n 1.51   0.3832          \n 1.55   0.4179          \n 1.59   0.3969          \n 1.63   0.4000          \n 1.67   0.3853          \n 1.71   0.3924          \n 1.75   0.4177          \n 1.79   0.3877          \n 1.83   0.3961          \n 1.87   0.3791          \n 1.91   0.3877          \n 1.96   0.4022          \n 2.0    0.3837          \n 2.04   0.4644          \n 2.08   0.4919          \n 2.12   0.4870          \n 2.16   0.5038          \n 2.2    0.5232          \n 2.24   0.5192          \n 2.28   0.5215          \n 2.32   0.4604          \n 2.36   0.5162          \n 2.4    0.4886          \n 2.44   0.4921          \n 2.49   0.4917          \n 2.53   0.4789          \n 2.57   0.4997          \n 2.61   0.4748          \n 2.65   0.4840          \n 2.69   0.4889          \n 2.73   0.4820          \n 2.77   0.4857          \n 2.81   0.4836          \n 2.85   0.4859          \n 2.89   0.4853          \n 2.93   0.4946          \n 2.97   0.4851          \n\n\n### Framework versions\n\n- Transformers 4.13.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of mrm8488/deberta-v3-small-finetuned-mnli?", "answers": [{"text": "text-classification", "answer_start": 210, "answer_end": 228}]}]}]}, {"title": "mrm8488/deberta-v3-small-finetuned-sst2", "paragraphs": [{"context": "---\nlanguage:\n- en\nlicense: mit\ntags:\n- generated_from_trainer\n- deberta-v3\ndatasets:\n- glue\nmetrics:\n- accuracy\nmodel-index:\n- name: deberta-v3-small\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: GLUE SST2\n      type: glue\n      args: sst2\n    metrics:\n    - type: accuracy\n      value: 0.9403669724770642\n      name: Accuracy\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: sst2\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.9403669724770642\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2MyOTE4ZTk0YzUyNGFkMGVjNTk4MDBlZGRlZjgzOGIzYWY0YjExMmZmMDZkYjFmOTlkYmM2ZDEwYjMxM2JkOCIsInZlcnNpb24iOjF9.Ks2vdjAFUe0isZp4F-OFK9HzvPqeU3mJEG_XJfOvkTdm9DyaefT9x78sof8i_EbIync5Ao7NOC4STCTQIUvgBw\n    - type: precision\n      value: 0.9375\n      name: Precision\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzNiZTEwNGNlZWUwZjMxYmRjNWU0ZGQ1Njg1M2MwNTQ3YWEwN2JlNDk4OWQ4MzNkMmNhOGUwMzA0YWU3ZWZjMiIsInZlcnNpb24iOjF9.p5Gbs680U45zHoWH9YgRLmOxINR4emvc2yNe9Kt3-y_WyyCd6CAAK9ht-IyGJ7GSO5WQny-ISngJFtyFt5NqDQ\n    - type: recall\n      value: 0.9459459459459459\n      name: Recall\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjk2MmJjMDZlZDUzM2QzMWZhMzMxNWRkYjJlYzA3MjUwMThiYWMwNmQzODE1MTMxNTdkNWVmMDhhNzJjMjg3MyIsInZlcnNpb24iOjF9.Jeu6tyhXQxMykqqFH0V-IXvyTrxAsgnYByYCOJgfj86957G5LiGdfQzDtTuGkt0XcoenXhPuueT8m5tsuJyLBA\n    - type: auc\n      value: 0.9804217184474193\n      name: AUC\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2Q5MWU1MGMzMjEwNzY4MDkzN2Q5ZjM5MTQ2MDc5YTRkZTNmNTk2YTdhODI1ZGJlOTlkNTQ2M2Q4YTUxN2Y3OSIsInZlcnNpb24iOjF9.INkDvQhg2jfD7WEE4qHJazPYo10O4Ffc5AZz5vI8fmN01rK3sXzzydvmrmTMzYSSmLhn9sc1-ZkoWbcv81oqBA\n    - type: f1\n      value: 0.9417040358744394\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYWRhNjljZjk0NjY1ZjU1ZjU2ZmM5ODk1YTVkMTI0ZGY4MjI1OTFlZWJkZWMyMGYxY2I1MzRjODBkNGVlMzJkZSIsInZlcnNpb24iOjF9.kQ547NVFUxeE4vNiGzGsCvMxR1MCJTChX44ds27qQ4Rj2m1UuD2C9TLTuiu8KMvq1mH1io978dJEpOCHYq6KCQ\n    - type: loss\n      value: 0.21338027715682983\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2YyYmVhNzgxMzMyNjJiNzZkYjE1YWM5Y2ZmMTlkNjQ5MThhYjIxNTE5MmE3Y2E0ODllODMyYjAzYWI3ZWRlMSIsInZlcnNpb24iOjF9.ad9rLnOeJZbRi_QQKEBpNNBp_Bt5SHf39ZeWQOZxp7tAK9dc0OK8XOqtihoXcAWDahwuoGiiYtcFNtvueaX6DA\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# DeBERTa v3 (small) fine-tuned on SST2\n\nThis model is a fine-tuned version of [microsoft/deberta-v3-small]( on the GLUE SST2 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2134\n- Accuracy: 0.9404\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5.0\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.2134          \n 2.0    0.2362          \n 3.0    0.3187          \n 4.0    0.3039          \n 5.0    0.3521          \n\n\n### Framework versions\n\n- Transformers 4.13.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of mrm8488/deberta-v3-small-finetuned-sst2?", "answers": [{"text": "text-classification", "answer_start": 184, "answer_end": 202}]}]}]}, {"title": "nateraw/vit-base-beans-demo-v2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- image-classification\n- other-image-classification\n- generated_from_trainer\ndatasets:\n- beans\nmetrics:\n- accuracy\nmodel-index:\n- name: vit-base-beans-demo-v2\n  results:\n  - task:\n      name: Image Classification\n      type: image-classification\n    dataset:\n      name: beans\n      type: beans\n      args: default\n    metrics:\n      - name: Accuracy\n        type: accuracy\n        value: 1.0\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# vit-base-beans-demo-v2\n\nThis model is a fine-tuned version of [google/vit-base-patch16-224-in21k]( on the beans dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0099\n- Accuracy: 1.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.54   0.0562          \n 3.08   0.0124          \n 4.62   0.0099          \n\n\n### Framework versions\n\n- Transformers 4.10.0.dev0\n- Pytorch 1.9.0+cu102\n- Datasets 1.11.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of nateraw/vit-base-beans-demo-v2?", "answers": [{"text": "vit", "answer_start": 166, "answer_end": 168}]}, {"id": "q2", "question": "What is the model task of nateraw/vit-base-beans-demo-v2?", "answers": [{"text": "image-classification", "answer_start": 32, "answer_end": 51}]}]}]}, {"title": "ncats/EpiExtract4GARD-v1", "paragraphs": [{"context": "## Model description\n**EpiExtract4GARD** is a fine-tuned [BioBERT-base-cased]( model that is ready to use for **Named Entity Recognition** of locations (LOC), epidemiologic types (EPI), and epidemiologic rates (STAT). This model was fine-tuned on [EpiSet4NER]( for epidemiological information from rare disease abstracts. See dataset documentation for details on the weakly supervised teaching methods and dataset biases and limitations. See [EpiExtract4GARD on GitHub]( for details on the entire pipeline. \n\n#### How to use\nYou can use this model with the Hosted inference API to the right with this [test sentence]( \"27 patients have been diagnosed with PKU in Iceland since 1947. Incidence 1972-2008 is 1/8400 living births.\"\n\nSee code below for use with Transformers *pipeline* for NER.:\n~~~\nfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(\"ncats/EpiExtract4GARD\")\ntokenizer = AutoTokenizer.from_pretrained(\"ncats/EpiExtract4GARD\")\nNER_pipeline = pipeline('ner', model=model, tokenizer=tokenizer,aggregation_strategy='simple')\n\nsample = \"The live-birth prevalence of mucopolysaccharidoses in Estonia. Previous studies on the prevalence of mucopolysaccharidoses (MPS) in different populations have shown considerable variations. There are, however, few data with regard to the prevalence of MPSs in Fenno-Ugric populations or in north-eastern Europe, except for a report about Scandinavian countries. A retrospective epidemiological study of MPSs in Estonia was undertaken, and live-birth prevalence of MPS patients born between 1985 and 2006 was estimated. The live-birth prevalence for all MPS subtypes was found to be 4.05 per 100,000 live births, which is consistent with most other European studies. MPS II had the highest calculated incidence, with 2.16 per 100,000 live births (4.2 per 100,000 male live births), forming 53% of all diagnosed MPS cases, and was twice as high as in other studied European populations. The second most common subtype was MPS IIIA, with a live-birth prevalence of 1.62 in 100,000 live births. With 0.27 out of 100,000 live births, MPS VI had the third-highest live-birth prevalence. No cases of MPS I were diagnosed in Estonia, making the prevalence of MPS I in Estonia much lower than in other European populations. MPSs are the third most frequent inborn error of metabolism in Estonia after phenylketonuria and galactosemia.\"\nsample2 = \"Early Diagnosis of Classic Homocystinuria in Kuwait through Newborn Screening: A 6-Year Experience. Kuwait is a small Arabian Gulf country with a high rate of consanguinity and where a national newborn screening program was expanded in October 2014 to include a wide range of endocrine and metabolic disorders. A retrospective study conducted between January 2015 and December 2020 revealed a total of 304,086 newborns have been screened in Kuwait. Six newborns were diagnosed with classic homocystinuria with an incidence of 1:50,000, which is not as high as in Qatar but higher than the global incidence. Molecular testing for five of them has revealed three previously reported pathogenic variants in the <i>CBS</i> gene, c.969G>A, p.(Trp323Ter); c.982G>A, p.(Asp328Asn); and the Qatari founder variant c.1006C>T, p.(Arg336Cys). This is the first study to review the screening of newborns in Kuwait for classic homocystinuria, starting with the detection of elevated blood methionine and providing a follow-up strategy for positive results, including plasma total homocysteine and amino acid analyses. Further, we have demonstrated an increase in the specificity of the current newborn screening test for classic homocystinuria by including the methionine to phenylalanine ratio along with the elevated methionine blood levels in first-tier testing. Here, we provide evidence that the newborn screening in Kuwait has led to the early detection of classic homocystinuria cases and enabled the affected individuals to lead active and productive lives.\"\n#Sample 1 is from: Krabbi K, Joost K, Zordania R, Talvik I, Rein R, Huijmans JG, Verheijen FV, \u00d5unap K. The live-birth prevalence of mucopolysaccharidoses in Estonia. Genet Test Mol Biomarkers. 2012 Aug;16(8):846-9. doi: 10.1089/gtmb.2011.0307. Epub 2012 Apr 5. PMID: 22480138; PMCID: PMC3422553.\n#Sample 2 is from: Alsharhan H, Ahmed AA, Ali NM, Alahmad A, Albash B, Elshafie RM, Alkanderi S, Elkazzaz UM, Cyril PX, Abdelrahman RM, Elmonairy AA, Ibrahim SM, Elfeky YME, Sadik DI, Al-Enezi SD, Salloum AM, Girish Y, Al-Ali M, Ramadan DG, Alsafi R, Al-Rushood M, Bastaki L. Early Diagnosis of Classic Homocystinuria in Kuwait through Newborn Screening: A 6-Year Experience. Int J Neonatal Screen. 2021 Aug 17;7(3):56. doi: 10.3390/ijns7030056. PMID: 34449519; PMCID: PMC8395821.\n\nNER_pipeline(sample)\nNER_pipeline(sample2)\n~~~\nOr if you download [*classify_abs.py*]( [*extract_abs.py*]( and [*gard-id-name-synonyms.json*]( from GitHub then you can test with this [*additional* code](\n\n~~~\nimport pandas as pd\nimport extract_abs\nimport classify_abs\npd.set_option('display.max_colwidth', None)\n\nNER_pipeline = extract_abs.init_NER_pipeline()\nGARD_dict, max_length = extract_abs.load_GARD_diseases()\nnlp, nlpSci, nlpSci2, classify_model, classify_tokenizer = classify_abs.init_classify_model()\n\n\ndef search(term,num_results = 50):\n    return extract_abs.search_term_extraction(term, num_results, NER_pipeline, GARD_dict, max_length,nlp, nlpSci, nlpSci2, classify_model, classify_tokenizer)\n    \na = search(7058)\na\n\nb = search('Santos Mateus Leal syndrome')\nb\n\nc = search('Fellman syndrome')\nc\n\n\nd = search('GARD:0009941')\nd\n\ne = search('Homocystinuria')\ne\n~~~\n\n#### Limitations and bias\n## Training data\nIt was trained on [EpiSet4NER]( See dataset documentation for details on the weakly supervised teaching methods and dataset biases and limitations. The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\n\nAbbreviation|Description\n---------|--------------\nO        |Outside of a named entity\nB-LOC    | Beginning of a location \nI-LOC    | Inside of a location\nB-EPI    | Beginning of an epidemiologic type (e.g. \"incidence\", \"prevalence\", \"occurrence\") \nI-EPI    | Epidemiologic type that is not the beginning token. \nB-STAT   | Beginning of an epidemiologic rate\nI-STAT   | Inside of an epidemiologic rate\n\n### EpiSet Statistics\n\nBeyond any limitations due to the EpiSet4NER dataset, this model is limited in numeracy due to BERT-based model's use of subword embeddings, which is crucial for epidemiologic rate identification and limits the entity-level results. Additionally, more recent weakly supervised learning techniques could be used to improve the performance of the model without improving the underlying dataset.\n\n## Training procedure\nThis model was trained on a [AWS EC2 p3.2xlarge]( which utilized a single Tesla V100 GPU, with these hyperparameters:\n4 epochs of training (AdamW weight decay = 0.05) with a batch size of 16. Maximum sequence length = 192. Model was fed one sentence at a time. Full config [here](\n\n## Hold-out validation  results\nmetric| entity-level result\n-|-\nf1 | 83.8\nprecision | 83.2\nrecall | 84.5\n\n## Test results\n Evaluation Level  Precision    F1  |\n:----------------::---------::-----:|\n   Entity-Level      0.556    0.605 |\n                     0.661    0.678 |\n                     0.854    0.882 |\n                     0.143    0.173 |\n    Token-Level      0.811    0.759 |\n                     0.949    0.833 |\n                      0.9     0.908 |\n                     0.724    0.677 |\n\nThanks to [@William Kariampuzha]( at Axle Informatics/NCATS for contributing this model.", "qas": []}]}, {"title": "Intel/dpt-large-ade", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-segmentation\ndatasets:\n- scene_parse_150\nwidget:\n- src: \n  example_title: Tiger\n- src: \n  example_title: Teapot\n- src: \n  example_title: Palace\n---\n\n# DPT (large-sized model) fine-tuned on ADE20k\n\nDense Prediction Transformer (DPT) model trained on ADE20k for semantic segmentation. It was introduced in the paper [Vision Transformers for Dense Prediction]( by Ranftl et al. and first released in [this repository]( \n\nDisclaimer: The team releasing DPT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nDPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for semantic segmentation.\n\n![model image](\n\n## Intended uses & limitations\n\nYou can use the raw model for semantic segmentation. See the [model hub]( to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import DPTFeatureExtractor, DPTForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nurl = \"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-large-ade\")\nmodel = DPTForSemanticSegmentation.from_pretrained(\"Intel/dpt-large-ade\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n```\n\nFor more code examples, we refer to the [documentation](\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-2103-13413,\n  author    = {Ren{\\'{e}} Ranftl and\n               Alexey Bochkovskiy and\n               Vladlen Koltun},\n  title     = {Vision Transformers for Dense Prediction},\n  journal   = {CoRR},\n  volume    = {abs/2103.13413},\n  year      = {2021},\n  url       = {\n  eprinttype = {arXiv},\n  eprint    = {2103.13413},\n  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},\n  biburl    = {\n  bibsource = {dblp computer science bibliography, \n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of Intel/dpt-large-ade?", "answers": [{"text": "dpt", "answer_start": 1218, "answer_end": 1220}]}, {"id": "q2", "question": "What is the model task of Intel/dpt-large-ade?", "answers": [{"text": "image-segmentation", "answer_start": 41, "answer_end": 58}]}]}]}, {"title": "nielsr/nt5-small-rc1", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\ndatasets:\n- drop\n---\n\n# NT5, a T5 model trained to perform numerical reasoning\n\nT5-small model pre-trained on 3 million (partly synthetic) texts and fine-tuned on [DROP]( It was introduced in the paper [NT5?! Training T5 to Perform Numerical Reasoning]( by Yang et al. and first released in [this repository]( As the original implementation was in Tensorflow 2, I've converted the weigths to PyTorch. This model corresponds to RC Experiment 1 (see the paper), their best performing model.\n\nDisclaimer: The team releasing NT5 did not write a model card for this model so this model card has been written by me.\n\n## Model description\n\nThe NT5 model is a T5 model, in other words, an encoder-decoder Transformer. In order to encourage numerical reasoning, the model was further pre-trained on three datasets designed to strengthen skills necessary for numerical reasoning over text (NRoT) and general reading comprehension before being fine-tuned on the Discrete Reasoning over Text (DROP) dataset.\n\n## Intended uses & limitations\n\nYou can use the model for numerical reasoning over text. \n\n### How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ncontext = \"\"\"Saint Jean de Br\u00e9beuf was a French Jesuit missionary who\ntravelled to New France in 1625. There he worked primarily with the Huron\nfor the rest of his life, except for a few years in France from 1629 to\n1633. He learned their language and culture, writing extensively about\neach to aid other missionaries. In 1649, Br\u00b4ebeuf and another missionary\nwere captured when an Iroquois raid took over a Huron village . Together\nwith Huron captives, the missionaries were ritually tortured and killed\non March 16, 1649. Br\u00b4ebeuf was beatified in 1925 and among eight Jesuit\nmissionaries canonized as saints in the Roman Catholic Church in 1930.\"\"\"\n\nquestion = \"How many years did Saint Jean de Br\u00e9beuf stay in New France \nbefore he went back to France for a few years?\"\n\ntokenizer = T5Tokenizer.from_pretrained(\"nielsr/nt5-small-rc1\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"nielsr/nt5-small-rc1\")\n\n# encode context & question\ninput_text = f\"answer_me: {question} context: {context}\"\nencoded_query = tokenizer(\n                    input_text, \n                    return_tensors='pt', \n                    padding='max_length', \n                    truncation=True, \n                    max_length=512)\n\n# generate answer\ngenerated_answer = model.generate(input_ids=encoded_query[\"input_ids\"], \n                                  attention_mask=encoded_query[\"attention_mask\"], \n                                  max_length=54)\n\ndecoded_answer = tokenizer.decode(generated_answer.numpy()[0])\nprint(\"T5 Answer: \", decoded_answer)\nT5 Answer: 4\n```\n\n## Evaluation results\n\nThis model achieves an F1 score of 0.7031 and exact match of 0.6687 on the development set of DROP. \n\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{yang2021nt5,\n      title={NT5?! Training T5 to Perform Numerical Reasoning}, \n      author={Peng-Jian Yang and Ying Ting Chen and Yuechan Chen and Daniel Cer},\n      year={2021},\n      eprint={2104.07307},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1903-00161,\n  author    = {Dheeru Dua and\n               Yizhong Wang and\n               Pradeep Dasigi and\n               Gabriel Stanovsky and\n               Sameer Singh and\n               Matt Gardner},\n  title     = {{DROP:} {A} Reading Comprehension Benchmark Requiring Discrete Reasoning\n               Over Paragraphs},\n  journal   = {CoRR},\n  volume    = {abs/1903.00161},\n  year      = {2019},\n  url       = {\n  archivePrefix = {arXiv},\n  eprint    = {1903.00161},\n  timestamp = {Wed, 03 Jul 2019 07:17:04 +0200},\n  biburl    = {\n  bibsource = {dblp computer science bibliography, \n}\na service of Schloss Dagstuhl - Leibniz Center for Informatics\\\\\\\\thomebrowsesearchabout\n\n```", "qas": [{"id": "q1", "question": "What is the model architecture of nielsr/nt5-small-rc1?", "answers": [{"text": "t5", "answer_start": 2066, "answer_end": 2067}]}]}]}, {"title": "niksmer/RoBERTa-RILE", "paragraphs": [{"context": "---\nlicense: mit\nmetrics:\n- accuracy\n- precision\n- recall\nmodel-index:\n- name: RoBERTa-RILE\n  results: []\nwidget: \n- text: \"Russia must end the war.\"\n- text: \"Democratic institutions must be supported.\"\n- text: \"The state must fight political corruption.\"\n- text: \"Our energy economy must be nationalised.\"\n- text: \"We must increase social spending.\"\n\n---\n\n\n# RoBERTa-RILE\n\nThis model is a fine-tuned version of [roberta-base]( on data from the [Manifesto Project](\n\n\n## Model description\n\nThis model was trained on 115,943 manually annotated sentences to classify text into one of three political categories: \"neutral\", \"left\", \"right\".\n\n\n## Intended uses & limitations\n\nThe model output reproduces the limitations of the dataset in terms of country coverage, time span, domain definitions and potential biases of the annotators - as any supervised machine learning model would. Applying the model to other types of data (other types of texts, countries etc.) will reduce performance.\n\n```python\nfrom transformers import pipeline\nimport pandas as pd\nclassifier = pipeline(\n    task=\"text-classification\",\n    model=\"niksmer/RoBERTa-RILE\")\n# Load text data you want to classify\ntext = pd.read_csv(\"example.csv\")[\"text_you_want_to_classify\"].to_list()\n# Inference\noutput = classifier(text)\n# Print output\npd.DataFrame(output).head()\n```\n\n## Training and evaluation data\n\n## Training and evaluation data\n\nRoBERTa-RILE was trained on the English-speaking subset of the [Manifesto Project Dataset (MPDS2021a)]( The model was trained on 115,943 sentences from 163 political manifestos in 7 English-speaking countries (Australia, Canada, Ireland, New Zealand, South Africa, United Kingdom, United States). The manifestos were published between 1992 - 2020. \n\n Count manifestos  Time span        |\n--------------------------------------|\n 18                2010-2016          |\n 23                2007-2016          |\n 14                2004-2008 & 2015   |\n 46                1993-2017          |\n 29                1994-2019          |\n 9                 1992   & 2004-2020 |\n 34                1997-2019          |\n\nCanadian manifestos between 2004 and 2008 are used as test data.\n\nThe Manifesto Project mannually annotates individual sentences from political party manifestos in over 50 main categories - see the [codebook]( for the exact definitions of each categorie. It has created a valid left-right-scale, the rile-index, to aaggregate manifesto in a standardized, onde-dimensional political space from left to right based on saliency-theory.\nRoBERTa-RILE classifies texts based on the rile index.\n\n### Tain data\n\nTrain data was slightly imbalanced.\n\n Description \n--------------\n neutral       \n left       \n right       \n\nOverall count: 115,943\n\n### Validation data\n\nThe validation was created by chance.\n\n Description \n--------------\n neutral       \n left       \n right       \n\nOverall count: 20,461\n\n### Test data\n\nThe test dataset contains ten canadian manifestos between 2004 and 2008.\n\n Description \n--------------\n neutral       \n left       \n right       \n\nOverall count: 8,330\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n```\ntraining_args = TrainingArguments(\n    warmup_ratio=0.05,\n    weight_decay=0.1, \n    learning_rate=1e-05,\n    fp16 = True,\n    evaluation_strategy=\"epoch\",\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    save_strategy=\"no\",\n    logging_dir='logs',   \n    logging_strategy= 'steps',     \n    logging_steps=10,\n    push_to_hub=True,\n    hub_strategy=\"end\")\n```\n\n### Training results\n\n Epoch  Validation Loss  F1-micro  F1-weighted  Recall |\n:-----::---------------::--------::-----------::------:|\n 1.0    0.6827           0.7120    0.7126       0.7120 |\n 2.0    0.6618           0.7281    0.7281       0.7281 |\n 3.0    0.6657           0.7309    0.7295       0.7309 |\n 4.0    0.6654           0.7346    0.7345       0.7346 |\n 5.0    0.6757           0.7350    0.7347       0.7350 |\n\n### Validation evaluation\n\n Micro F1-Score  Weighted F1-Score |\n-----------------------------------|\n  0.74            0.73             |\n\n### Test evaluation\n\n Micro F1-Score  Weighted F1-Score |\n-----------------------------------|\n 0.69            0.69              |\n\n### Evaluation per category\n\n Validation F1-Score \n---------------------\n 0.77                \n 0.73                \n 0.67                \n\n### Evaluation based on saliency theory\n\nSaliency theory is a theory to analyse politial text data. In sum, parties tend to write about policies in which they think that they are seen as competent.\nVoters tend to assign advantages in policy competence in line to the assumed ideology of parties. Therefore you can analyze the share of policies parties tend to write about in their manifestos to analyze the party ideology.\n\nThe Manifesto Project presented for such an analysis the rile-index. For a quick overview, check [this](\n\nIn the following plot, the predicted and original rile-indices are shown per manifesto in the test dataset. Overall the pearson correlation between the predicted and original rile-indices is 0.95. As alternative, you can use [ManiBERT](\n\n![image](english_robertarile_manifesto.png)\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.8.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of niksmer/RoBERTa-RILE?", "answers": [{"text": "roberta", "answer_start": 413, "answer_end": 419}]}, {"id": "q2", "question": "What is the model task of niksmer/RoBERTa-RILE?", "answers": [{"text": "text-classification", "answer_start": 1084, "answer_end": 1102}]}]}]}, {"title": "nimrah/wav2vec2-large-xls-r-300m-hindi-colab", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-large-xls-r-300m-hindi-colab\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xls-r-300m-hindi-colab\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the common_voice dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.13.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of nimrah/wav2vec2-large-xls-r-300m-hindi-colab?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "nlp4good/psych-search", "paragraphs": [{"context": "---\nlanguage:\n- en\ntags:\n- mental-health\nlicense: apache-2.0\ndatasets:\n- PubMed\n---\n# Psych-Search\nPsych-Search is a work in progress to bring cutting edge NLP to mental health practitioners. The model detailed here serves as a foundation for traditional classification models as well as NLU models for a Psych-Search application. The goal of the Psych-Search Application is to use a combination of traditional text classification models to expand the scope of the MESH taxonomy with the inclusion of relevant categories for mental health pracitioners designing suicide prevention programs for adolescent communities within the United States, as well as the automatic extraction and standardization of entities such as risk factors and protective factors.\n\nOur first expansion efforts to the MESH taxonomy include categories:\n- Prevention Strategies\n- Protective Factors\n\nWe are actively looking for partners on this work and would love to hear from you! Please ping us at nlp4good@gmail.com. \n\n## Model description\n\nThis model is an extension of [allenai/scibert_scivocab_uncased]( Continued pretraining was done using SciBERT as the base model using abstract text only from Pyschology and Psychiatry PubMed research. Training was done on approximately 3.5 million papers for 10 epochs and evaluated on a task similar to BioASQ Task A.\n\n## Intended uses & limitations\n\n#### How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\nmname = \"nlp4good/psych-search\"\ntokenizer = AutoTokenizer.from_pretrained(mname)\nmodel = AutoModel.from_pretrained(mname)\n```\n\n### Limitations and bias\n\nThis model was trained on all PubMed abstracts categorized under [Psychology and Psychiatry]( As of March 1, this corresponds to approximately 3.2 million papers that contains abstract text. Of these 3.2 million papers, relevant sparse mental health categories were back translated to increase the representation of certain mental health categories.\n\nThere are several limitation with this dataset including large discrepancies in the number of papers associated with [Sexual and Gender Minorities]( The training data consisted of the following breakdown across gender groups:\n\nFemale  Sexual and Gender Minorities\n-----------------\n1,896,301  4,529\n\nSimilar discrepancies are present within [Ethnic Groups]( as defined within the MESH taxonomy:\n\n Arabs  Hispanic Americans  Indians, North American  Indigenous Peoples \n------------------------------------------------------------------------\n 2,437  18,893              5,657                    174                \n\nThese discrepancies can have a significant impact on information retrieval systems, downstream  machine learning models, and other forms of NLP that leverage these pretrained models.\n## Training data\n\nThis model was trained on all PubMed abstracts categorized under [Psychology and Psychiatry]( As of March 1, this corresponds to approximately 3.2 million papers that contains abstract text. Of these 3.2 million papers, relevant sparse categories were back translated from english to french and from french to english to increase the representation of sparser mental health categories. This included backtranslating the following papers with the following categories:\n- Depressive Disorder\n- Risk Factors\n- Mental Disorders\n- Child, Preschool\n- Mental Health\n\nIn aggregate, this process added 557,980 additional papers to our training data. \n\n\n## Training procedure\nContinued pretraining was done on Psychology and Psychiatry PubMed papers for 10 epochs. Default parameters were used with the exception of gradient accumulation steps which was set at 4, with a per device train batch size of 32. 2 x Nvidia 3090's were used in the development of this model. \n\n\n## Evaluation results\nTo evaluate the effectiveness of psych-search within the mental health domain, an evaluation task was constructed by finetuning psych-search for a task similar to [BioASQ Task A]( Here we perform large scale biomedical indexing using the MESH taxonomy associated with each paper underneath Psychology and Psychiatry. The evaluation metric is the micro F1 score across all second level descriptors within Psychology and Psychiatry. This corresponds to 38 different MESH categories used during evaluation.\n\nbert-base-uncased    Psych-Search\n-----------------\n0.7348   0.7415\n\n## Next Steps\nIf you are interested in continuing to build on this work or have other ideas on how we can build on others work, please let us know! We can be reached at nlp4good@gmail.com. Our goal is to bring state of the art NLP capabilities to underserved areas of research, with mental health being our top priority.", "qas": [{"id": "q1", "question": "What is the model architecture of nlp4good/psych-search?", "answers": [{"text": "bert", "answer_start": 1059, "answer_end": 1062}]}]}]}, {"title": "nlpaueb/sec-bert-num", "paragraphs": [{"context": "---\nlanguage: en\npipeline_tag: fill-mask\nlicense: cc-by-sa-4.0\nthumbnail: \ntags:\n- finance\n- financial\nwidget:\n- text: \"Total net sales decreased [MASK]% or $[NUM] billion during [NUM] compared to [NUM].\"\n- text: \"Total net sales decreased [NUM]% or $[MASK] billion during [NUM] compared to [NUM].\"\n- text: \"Total net sales decreased [NUM]% or $[NUM] billion during [MASK] compared to [NUM].\"\n- text: \"During [MASK], the Company repurchased $[NUM] billion of its common stock and paid dividend equivalents of $[NUM] billion.\"\n- text: \"During 2019, the Company repurchased $[MASK] billion of its common stock and paid dividend equivalents of $[NUM] billion.\"\n---\n\n# SEC-BERT\n\n<img align=\"center\" src=\" alt=\"sec-bert-logo\" width=\"400\"/>\n\n<div style=\"text-align: justify\">\n\nSEC-BERT is a family of BERT models for the financial domain, intended to assist financial NLP research and FinTech applications.\nSEC-BERT consists of the following models:\n* [**SEC-BERT-BASE**]( Same architecture as BERT-BASE trained on financial documents.\n* **SEC-BERT-NUM** (this model): Same as SEC-BERT-BASE but we replace every number token with a [NUM] pseudo-token handling all numeric expressions in a uniform manner, disallowing their fragmentation).\n* [**SEC-BERT-SHAPE**]( Same as SEC-BERT-BASE but we replace numbers with pseudo-tokens that represent the number\u2019s shape, so numeric expressions (of known shapes) are no longer fragmented, e.g., '53.2' becomes '[XX.X]' and '40,200.5' becomes '[XX,XXX.X]'.\n</div>\n\n## Pre-training corpus\n\nThe model was pre-trained on 260,773 10-K filings from 1993-2019, publicly available at <a href=\" Securities and Exchange Commission (SEC)</a>\n\n## Pre-training details\n\n<div style=\"text-align: justify\">\n\n* We created a new vocabulary of 30k subwords by training a [BertWordPieceTokenizer]( from scratch on the pre-training corpus.\n* We trained BERT using the official code provided in [Google BERT's GitHub repository](\n* We then used [Hugging Face]( [Transformers]( conversion script to convert the TF checkpoint in the desired format in order to be able to load the model in two lines of code for both PyTorch and TF2 users.\n* We release a model similar to the English BERT-BASE model (12-layer, 768-hidden, 12-heads, 110M parameters).\n* We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4.\n* We were able to use a single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TRC)]( while also utilizing [GCP research credits]( Huge thanks to both Google programs for supporting us!\n</div>\n\n## Load Pretrained Model\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/sec-bert-num\")\nmodel = AutoModel.from_pretrained(\"nlpaueb/sec-bert-num\")\n```\n\n## Pre-process Text\n\n<div style=\"text-align: justify\">\n\nTo use SEC-BERT-NUM, you have to pre-process texts replacing every numerical token with [NUM] pseudo-token.\nBelow there is an example of how you can pre-process a simple sentence. This approach is quite simple; feel free to modify it as you see fit.\n</div>\n\n```python\nimport re\nimport spacy\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/sec-bert-num\")\nspacy_tokenizer = spacy.load(\"en_core_web_sm\")\n\nsentence = \"Total net sales decreased 2% or $5.4 billion during 2019 compared to 2018.\"\n\ndef sec_bert_num_preprocess(text):\n    tokens = [t.text for t in spacy_tokenizer(text)]\n\n    processed_text = []\n    for token in tokens:\n        if re.fullmatch(r\"(\\d+[\\d,.]*)|([,.]\\d+)\", token):\n            processed_text.append('[NUM]')\n        else:\n            processed_text.append(token)\n            \n    return ' '.join(processed_text)\n        \ntokenized_sentence = tokenizer.tokenize(sec_bert_num_preprocess(sentence))\nprint(tokenized_sentence)\n\"\"\"\n['total', 'net', 'sales', 'decreased', '[NUM]', '%', 'or', '$', '[NUM]', 'billion', 'during', '[NUM]', 'compared', 'to', '[NUM]', '.']\n\"\"\"\n```\n\n## Using SEC-BERT variants as Language Models\n\n Masked Token |\n ------------ |\n decreased\n\n Predictions (Probability)  |\n ------------ |\n increased (0.221), were (0.131), are (0.103), rose (0.075), of (0.058)\n increased (0.678), decreased (0.282), declined (0.017), grew (0.016), rose (0.004)\n increased (0.753), decreased (0.211), grew (0.019), declined (0.010), rose (0.006)\n increased (0.747), decreased (0.214), grew (0.021), declined (0.013), rose (0.002)\n\n\n Masked Token |\n ------------ |\n billion\n\n Predictions (Probability)  |\n ------------ |\n billion (0.841), million (0.097), trillion (0.028), ##m (0.015), ##bn (0.006)\n million (0.972), billion (0.028), millions (0.000), ##million (0.000), m (0.000)\n million (0.974), billion (0.012), , (0.010), thousand (0.003), m (0.000)\n million (0.978), billion (0.021), % (0.000), , (0.000), millions (0.000)\n\n\n Masked Token |\n ------------ |\n 2\n\n Predictions (Probability)  |\n ------------ |\n 20 (0.031), 10 (0.030), 6 (0.029), 4 (0.027), 30 (0.027)\n 13 (0.045), 12 (0.040), 11 (0.040), 14 (0.035), 10 (0.035)\n [NUM] (1.000), one (0.000), five (0.000), three (0.000), seven (0.000)\n [XX] (0.316), [XX.X] (0.253), [X.X] (0.237), [X] (0.188), [X.XX] (0.002)\n\n\n Masked Token |\n ------------ |\n %\n\n Predictions (Probability)  |\n ------------ |\n % (0.795), percent (0.174), ##fold (0.009), billion (0.004), times (0.004)\n % (0.924), percent (0.076), points (0.000), , (0.000), times (0.000)\n % (0.882), percent (0.118), million (0.000), units (0.000), bps (0.000)\n % (0.961), percent (0.039), bps (0.000), , (0.000), bcf (0.000)\n\n\n Masked Token |\n ------------ |\n 5.4\n\n Predictions (Probability)  |\n ------------ |\n 1 (0.074), 4 (0.045), 3 (0.044), 2 (0.037), 5 (0.034)\n 1 (0.218), 2 (0.136), 3 (0.078), 4 (0.066), 5 (0.048)\n [NUM] (1.000), l (0.000), 1 (0.000), - (0.000), 30 (0.000)\n [X.X] (0.787), [X.XX] (0.095), [XX.X] (0.049), [X.XXX] (0.046), [X] (0.013)\n\n\n Masked Token |\n ------------ |\n 2019\n\n Predictions (Probability)  |\n ------------ |\n 2017 (0.485), 2018 (0.169), 2016 (0.164), 2015 (0.070), 2014 (0.022)\n 2019 (0.990), 2017 (0.007), 2018 (0.003), 2020 (0.000), 2015 (0.000)\n [NUM] (1.000), as (0.000), fiscal (0.000), year (0.000), when (0.000)\n [XXXX] (1.000), as (0.000), year (0.000), periods (0.000), , (0.000)\n\n\n Masked Token |\n ------------ |\n 2018\n\n Predictions (Probability)  |\n ------------ |\n 2017 (0.100), 2016 (0.097), above (0.054), inflation (0.050), previously (0.037)\n 2018 (0.999), 2019 (0.000), 2017 (0.000), 2016 (0.000), 2014 (0.000)\n [NUM] (1.000), year (0.000), last (0.000), sales (0.000), fiscal (0.000)\n [XXXX] (1.000), year (0.000), sales (0.000), prior (0.000), years (0.000)\n\n\n Masked Token |\n ------------ |\n repurchased\n\n Predictions (Probability)  |\n ------------ |\n held (0.229), sold (0.192), acquired (0.172), owned (0.052), traded (0.033)\n repurchased (0.913), issued (0.036), purchased (0.029), redeemed (0.010), sold (0.003)\n repurchased (0.917), purchased (0.054), reacquired (0.013), issued (0.005), acquired (0.003)\n repurchased (0.902), purchased (0.068), issued (0.010), reacquired (0.008), redeemed (0.006)\n\n\n Masked Token |\n ------------ |\n stock\n\n Predictions (Probability)  |\n ------------ |\n stock (0.835), assets (0.039), equity (0.025), debt (0.021), bonds (0.017)\n stock (0.857), shares (0.135), equity (0.004), units (0.002), securities (0.000)\n stock (0.842), shares (0.157), equity (0.000), securities (0.000), units (0.000)\n stock (0.888), shares (0.109), equity (0.001), securities (0.001), stocks (0.000)\n\n\n Masked Token |\n ------------ |\n dividend\n\n Predictions (Probability)  |\n ------------ |\n cash (0.276), net (0.128), annual (0.083), the (0.040), debt (0.027)\n dividend (0.890), cash (0.018), dividends (0.016), share (0.013), tax (0.010)\n dividend (0.735), cash (0.115), share (0.087), tax (0.025), stock (0.013)\n dividend (0.655), cash (0.248), dividends (0.042), share (0.019), out (0.003)\n\n\n Masked Token |\n ------------ |\n equivalents\n\n Predictions (Probability)  |\n ------------ |\n revenue (0.085), earnings (0.078), rates (0.065), amounts (0.064), proceeds (0.062)\n payments (0.790), distributions (0.087), equivalents (0.068), cash (0.013), amounts (0.004)\n payments (0.845), equivalents (0.097), distributions (0.024), increases (0.005), dividends (0.004)\n payments (0.784), equivalents (0.093), distributions (0.043), dividends (0.015), requirements (0.009)\n\n## Publication\n\n<div style=\"text-align: justify\">\n\nIf you use this model cite the following article:<br> \n[**FiNER: Financial Numeric Entity Recognition for XBRL Tagging**](\nLefteris Loukas, Manos Fergadiotis, Ilias Chalkidis, Eirini Spyropoulou, Prodromos Malakasiotis, Ion Androutsopoulos and George Paliouras<br>\nIn the Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022) (Long Papers), Dublin, Republic of Ireland, May 22 - 27, 2022\n</div>\n\n```\n@inproceedings{loukas-etal-2022-finer,\n    title = {FiNER: Financial Numeric Entity Recognition for XBRL Tagging},\n    author = {Loukas, Lefteris and\n      Fergadiotis, Manos and\n      Chalkidis, Ilias and\n      Spyropoulou, Eirini and\n      Malakasiotis, Prodromos and\n      Androutsopoulos, Ion and\n      Paliouras George},\n    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022)},\n    publisher = {Association for Computational Linguistics},\n    location = {Dublin, Republic of Ireland},\n    year = {2022},\n    url = {\n}\n```\n\n## About Us\n\n<div style=\"text-align: justify\">\n\n[AUEB's Natural Language Processing Group]( develops algorithms, models, and systems that allow computers to process and generate natural language texts.\n\nThe group's current research interests include:\n* question answering systems for databases, ontologies, document collections, and the Web, especially biomedical question answering,\n* natural language generation from databases and ontologies, especially Semantic Web ontologies,\ntext classification, including filtering spam and abusive content,\n* information extraction and opinion mining, including legal text analytics and sentiment analysis,\n* natural language processing tools for Greek, for example parsers and named-entity recognizers,\nmachine learning in natural language processing, especially deep learning.\n\nThe group is part of the Information Processing Laboratory of the Department of Informatics of the Athens University of Economics and Business.\n</div>\n\n[Manos Fergadiotis]( on behalf of [AUEB's Natural Language Processing Group](", "qas": [{"id": "q1", "question": "What is the model architecture of nlpaueb/sec-bert-num?", "answers": [{"text": "bert", "answer_start": 710, "answer_end": 713}]}, {"id": "q2", "question": "What is the model task of nlpaueb/sec-bert-num?", "answers": [{"text": "fill-mask", "answer_start": 31, "answer_end": 39}]}]}]}, {"title": "nlptown/bert-base-multilingual-uncased-sentiment", "paragraphs": [{"context": "---\nlanguage:\n- en\n- nl\n- de\n- fr\n- it\n- es\n\nlicense: mit\n---\n\n# bert-base-multilingual-uncased-sentiment\n\nThis a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\n\nThis model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above, or for further finetuning on related sentiment analysis tasks.\n\n## Training data\n\nHere is the number of product reviews we used for finetuning the model: \n\n Number of reviews |\n ----------------- |\n 150k           |\n 80k            |\n 137k           |\n 140k           |\n 72k            |\n 50k            |\n\n## Accuracy\n\nThe finetuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:\n\n- Accuracy (exact) is the exact match on the number of stars.\n- Accuracy (off-by-1) is the percentage of reviews where the number of stars the model predicts differs by a maximum of 1 from the number given by the human reviewer. \n\n\n Accuracy (exact) \n ---------------------- \n 67%                 | 95%\n 57%                 | 93%\n 61%                 | 94%\n 59%                 | 94%\n 59%                 | 95%\n 58%                 | 95%\n\n## Contact \n\nIn addition to this model, [NLP Town]( offers custom, monolingual sentiment models for many languages and an improved multilingual model through [RapidAPI]( \n\nFeel free to contact us for questions, feedback and/or requests for similar models.", "qas": [{"id": "q1", "question": "What is the model architecture of nlptown/bert-base-multilingual-uncased-sentiment?", "answers": [{"text": "bert", "answer_start": 65, "answer_end": 68}]}]}]}, {"title": "nntadotzip/bert-base-cased-IUChatbot-ontologyDts", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bert-base-cased-IUChatbot-ontologyDts\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-cased-IUChatbot-ontologyDts\n\nThis model is a fine-tuned version of [bert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2446\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.2686          |\n 2.0    0.2535          |\n 3.0    0.2446          |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of nntadotzip/bert-base-cased-IUChatbot-ontologyDts?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "nntadotzip/xlnet-base-cased-IUChatbot-ontologyDts-BertPretrainedTokenizerFast", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: xlnet-base-cased-IUChatbot-ontologyDts-BertPretrainedTokenizerFast\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlnet-base-cased-IUChatbot-ontologyDts-BertPretrainedTokenizerFast\n\nThis model is a fine-tuned version of [xlnet-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3489\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.4695          |\n 2.0    0.3361          |\n 3.0    0.3489          |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of nntadotzip/xlnet-base-cased-IUChatbot-ontologyDts-BertPretrainedTokenizerFast?", "answers": [{"text": "xlnet", "answer_start": 69, "answer_end": 73}]}]}]}, {"title": "nntadotzip/xlnet-base-cased-IUChatbot-ontologyDts-localParams", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: xlnet-base-cased-IUChatbot-ontologyDts-localParams\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlnet-base-cased-IUChatbot-ontologyDts-localParams\n\nThis model is a fine-tuned version of [xlnet-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0238\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.0657          |\n 2.0    0.0237          |\n 3.0    0.0238          |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of nntadotzip/xlnet-base-cased-IUChatbot-ontologyDts-localParams?", "answers": [{"text": "xlnet", "answer_start": 69, "answer_end": 73}]}]}]}, {"title": "nntadotzip/xlnet-base-cased-IUChatbot-ontologyDts", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: xlnet-base-cased-IUChatbot-ontologyDts\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlnet-base-cased-IUChatbot-ontologyDts\n\nThis model is a fine-tuned version of [xlnet-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4965\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.5005          |\n 2.0    0.4488          |\n 3.0    0.4965          |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of nntadotzip/xlnet-base-cased-IUChatbot-ontologyDts?", "answers": [{"text": "xlnet", "answer_start": 69, "answer_end": 73}]}]}]}, {"title": "noah-ai/mt5-base-question-generation-vi", "paragraphs": [{"context": "## Model description\nThis model is a sequence-to-sequence question generator that takes an answer and context as an input and generates a question as an output. It is based on a pre-trained mt5-base by [Google]( model.\n\n## Training data\nThe model was fine-tuned on [XQuAD](\n\n## Example usage\n```python\nfrom transformers import MT5ForConditionalGeneration, AutoTokenizer\nimport torch\n\nmodel = MT5ForConditionalGeneration.from_pretrained(\"noah-ai/mt5-base-question-generation-vi\")\ntokenizer = AutoTokenizer.from_pretrained(\"noah-ai/mt5-base-question-generation-vi\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Content used to create a set of questions\ncontext = '''Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh (c\u00f2n g\u1ecdi l\u00e0 S\u00e0i G\u00f2n) t\u00ean g\u1ecdi c\u0169 tr\u01b0\u1edbc 1975 l\u00e0 S\u00e0i G\u00f2n hay S\u00e0i G\u00f2n-Gia \u0110\u1ecbnh l\u00e0 th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t \u1edf Vi\u1ec7t Nam v\u1ec1 d\u00e2n s\u1ed1 v\u00e0 quy m\u00f4 \u0111\u00f4 th\u1ecb h\u00f3a. \u0110\u00e2y c\u00f2n l\u00e0 trung t\u00e2m kinh t\u1ebf, ch\u00ednh tr\u1ecb, v\u0103n h\u00f3a v\u00e0 gi\u00e1o d\u1ee5c t\u1ea1i Vi\u1ec7t Nam. Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh l\u00e0 th\u00e0nh ph\u1ed1 tr\u1ef1c thu\u1ed9c trung \u01b0\u01a1ng thu\u1ed9c lo\u1ea1i \u0111\u00f4 th\u1ecb \u0111\u1eb7c bi\u1ec7t c\u1ee7a Vi\u1ec7t Nam c\u00f9ng v\u1edbi th\u1ee7 \u0111\u00f4 H\u00e0 N\u1ed9i.N\u1eb1m trong v\u00f9ng chuy\u1ec3n ti\u1ebfp gi\u1eefa \u0110\u00f4ng Nam B\u1ed9 v\u00e0 T\u00e2y Nam B\u1ed9, th\u00e0nh ph\u1ed1 n\u00e0y hi\u1ec7n c\u00f3 16 qu\u1eadn, 1 th\u00e0nh ph\u1ed1 v\u00e0 5 huy\u1ec7n, t\u1ed5ng di\u1ec7n t\u00edch 2.061 km\u00b2. Theo k\u1ebft qu\u1ea3 \u0111i\u1ec1u tra d\u00e2n s\u1ed1 ch\u00ednh th\u1ee9c v\u00e0o th\u1eddi \u0111i\u1ec3m ng\u00e0y m\u1ed9t th\u00e1ng 4 n\u0103m 2009 th\u00ec d\u00e2n s\u1ed1 th\u00e0nh ph\u1ed1 l\u00e0 7.162.864 ng\u01b0\u1eddi (chi\u1ebfm 8,34% d\u00e2n s\u1ed1 Vi\u1ec7t Nam), m\u1eadt \u0111\u1ed9 d\u00e2n s\u1ed1 trung b\u00ecnh 3.419 ng\u01b0\u1eddi/km\u00b2. \u0110\u1ebfn n\u0103m 2019, d\u00e2n s\u1ed1 th\u00e0nh ph\u1ed1 t\u0103ng l\u00ean 8.993.082 ng\u01b0\u1eddi v\u00e0 c\u0169ng l\u00e0 n\u01a1i c\u00f3 m\u1eadt \u0111\u1ed9 d\u00e2n s\u1ed1 cao nh\u1ea5t Vi\u1ec7t Nam. Tuy nhi\u00ean, n\u1ebfu t\u00ednh nh\u1eefng ng\u01b0\u1eddi c\u01b0 tr\u00fa kh\u00f4ng \u0111\u0103ng k\u00fd h\u1ed9 kh\u1ea9u th\u00ec d\u00e2n s\u1ed1 th\u1ef1c t\u1ebf c\u1ee7a th\u00e0nh ph\u1ed1 n\u00e0y n\u0103m 2018 l\u00e0 g\u1ea7n 14 tri\u1ec7u ng\u01b0\u1eddi.'''\n\nencoding = tokenizer.encode_plus(context, return_tensors=\"pt\")\n\ninput_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n\noutput = model.generate(input_ids=input_ids, attention_mask=attention_masks, max_length=256)\n\nquestion =  tokenizer.decode(output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n\nquestion\n#question: Th\u00e0nh ph\u1ed1 h\u1ed3 ch\u00ed minh c\u00f3 bao nhi\u00eau qu\u1eadn?\n```\n\n> Created by [Duong Thanh Nguyen](", "qas": [{"id": "q1", "question": "What is the model architecture of noah-ai/mt5-base-question-generation-vi?", "answers": [{"text": "mt5", "answer_start": 190, "answer_end": 192}]}]}]}, {"title": "noahjadallah/cause-effect-detection", "paragraphs": [{"context": "---\nwidget:\n- text: \"If a user signs up, he will receive a confirmation email.\"\n---\n\n# Cause-Effect Detection for Software Requirements Based on Token Classification with BERT\n\nThis model uses BERT to detect cause and effect from a single sentence. The focus of this model is the domain of software requirements engineering, however, it can also be used for other domains.\n\nThe model outputs one of the following 5 labels for each token:\n\nOther\n\nB-Cause\n\nI-Cause\n\nB-Effect\n\nI-Effect\n\nThe source code can be found here: ", "qas": []}]}, {"title": "nonamenlp/thai_new_gen_from_kw", "paragraphs": [{"context": "# Generate News in Thai language by keywords.\n\nMODEL_NAME = 'nonamenlp/news_gen' \n\nTOKENIZER_NAME = \"nonamenlp/news_gen\"\n\ntrained_model = MT5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n\ntokenizer = T5Tokenizer.from_pretrained(TOKENIZER_NAME)", "qas": []}]}, {"title": "noobed/DialoGPT-small-astley", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# astley talks", "qas": [{"id": "q2", "question": "What is the model task of noobed/DialoGPT-small-astley?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "norie4/DialoGPT-small-kyutebot", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# mingbot DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of norie4/DialoGPT-small-kyutebot?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "nostalgebraist/nostalgebraist-autoresponder-2_7b", "paragraphs": [{"context": "", "qas": []}]}, {"title": "not7even/DialoGPT-small-7evenpool", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# 7evenpool DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of not7even/DialoGPT-small-7evenpool?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "nouamanetazi/cover-letter-t5-base", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- t5-base\nmodel-index:\n- name: cover-letter-t5-base\n  results: []\nwidget:\n- text: \"coverletter name: Nouamane Tazi job: Machine Learning Engineer at HuggingFace background: Master's student in AI at the University of Paris Saclay experiences: I participated in the Digital Tech Year program, developing three minimal valuable products for three companies in a 7-week constraint. I also spent 1 year as a machine learning engineer for Flashbrand where I mainly worked on their chatbot . And I recently completed the HuggingFace course, where I built an amazing huggingface space. I am a strong team player.\"\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# cover-letter-t5-base\n\nThis model is a fine-tuned version of [t5-base]( on cover letter samples scraped from Indeed and JobHero.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 0.0001\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n- mixed_precision_training: Native AMP\n\n### Training results\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of nouamanetazi/cover-letter-t5-base?", "answers": [{"text": "t5", "answer_start": 70, "answer_end": 71}]}]}]}, {"title": "novakat/nerkor-cars-onpp-hubert", "paragraphs": [{"context": "---\n\nlanguage: \n  - hu\ntags:\n- token-classification\nlicense: gpl\nmetrics:\n- F1\nwidget:\n- text: \"A j\u00f3t\u00e9konys\u00e1gi szervezet \u00e1ltal id\u00e9zett Forbes-adatok szerint a vil\u00e1g t\u00edz leggazdagabb embere: Elon Musk (Tesla, SpaceX), Jeff Bezos (Amazon, Blue Origin), Bernard Arnault \u00e9s csal\u00e1dja (LVMH, azaz Louis Vuitton \u00e9s Mo\u00ebt Hennessy), Bill Gates (Microsoft), Larry Ellison (Oracle), Larry Page (Google), Sergey Brin (Google), Mark Zuckerberg (Facebook), Steve Ballmer (Microsoft) \u00e9s Warren Buffett (befektet\u0151).\nMik\u00f6zben vagyonuk egy\u00fcttesen 700 milli\u00e1rdr\u00f3l m\u00e1sf\u00e9l ezer milli\u00e1rd doll\u00e1rra n\u0151tt 2020 m\u00e1rciusa \u00e9s 2021 novembere k\u00f6z\u00f6tt, jelent\u0151s elt\u00e9r\u00e9sek vannak k\u00f6z\u00f6tt\u00fck: Musk vagyona t\u00f6bb mint 1000 sz\u00e1zal\u00e9kos, m\u00edg Gates\u00e9 szer\u00e9nyebb, 30 sz\u00e1zal\u00e9kos n\u00f6veked\u00e9st mutatott.\"\ninference:\n  parameters:\n    aggregation_strategy: \"first\"\n\n---\n\n# Hungarian named entity recognition model with OntoNotes5 + more entity types\n\n  - Pretrained model used: SZTAKI-HLT/hubert-base-cc \n  - Finetuned on NerKor+CARS-ONPP Corpus\n  \t\n## Limitations\n\n- max_seq_length = 448\n\n## Training data\n\nThe underlying corpus, [NerKor+CARS-OntoNotes++]( was derived from [NYTK-NerKor]( a Hungarian gold standard named entity annotated corpus containing about 1 million tokens. \nIt includes a small addition of 12k tokens of text (individual sentences) concerning motor vehicles (cars, buses, motorcycles) from the news archive of [hvg.hu](hvg.hu).\nWhile the annotation in NYTK-NerKor followed the CoNLL2002 labelling standard with just four NE categories (`PER`, `LOC`, `MISC`, `ORG`), this version of the corpus features over 30 entity types, including all entity types used in the [OntoNotes 5.0] English NER annotation.\nThe new annotation elaborates on subtypes of the `LOC` and `MISC` entity types, and includes annotation for non-names like times and dates, quantities, languages and nationalities or religious or political groups. The annotation was elaborated with further entity subtypes not present in the Ontonotes 5 annotation (see below).\n\n## Tags derived from the OntoNotes 5.0 annotation\n\nNames are annotated according to the following set of types:\n\n | \n---------|\n = PERSON People, including fictional |\n = FACILITY Buildings, airports, highways, bridges, etc. |\n = ORGANIZATION Companies, agencies, institutions, etc. |\n Geopolitical entites: countries, cities, states |\n = LOCATION Non-GPE locations, mountain ranges, bodies of water |\n = PRODUCT Vehicles, weapons, foods, etc. (Not services) |\n Named hurricanes, battles, wars, sports events, etc. |\n Titles of books, songs, etc. |\n Named documents made into laws  |\n\nThe following are also annotated in a style similar to names:\n\n | \n---------|\n Nationalities or religious or political groups |\n Any named language |\n Absolute or relative dates or periods |\n Times smaller than a day |\n Percentage (including \"%\") |\n Monetary values, including unit |\n Measurements, as of weight or distance |\n \"first\", \"second\" |\n Numerals that do not fall under another type |\n\n## Additional tags (not in OntoNotes 5)\nFurther subtypes of names of type `MISC`:\n\n |\n-|\n Awards and prizes |\n Cars and other motor vehicles |\n Media outlets, TV channels, news portals|\n Social media platforms|\n Projects and initiatives |\n Unresolved subtypes of MISC entities |\n Organization-like unresolved subtypes of MISC entities |\n\nFurther non-name entities:\n\n |\n-|\nTime duration\nAge\n Identifier\n\n### If you use this model, please cite:\n\n```bibtex\n@inproceedings{novak-novak-2022-nerkor,\n    title = \"{N}er{K}or+{C}ars-{O}nto{N}otes++\",\n    author = \"Nov{\\'a}k, Attila  and\n      Nov{\\'a}k, Borb{\\'a}la\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"\n    pages = \"1907--1916\",\n    abstract = \"In this paper, we present an upgraded version of the Hungarian NYTK-NerKor named entity corpus, which contains about twice as many annotated spans and 7 times as many distinct entity types as the original version. We used an extended version of the OntoNotes 5 annotation scheme including time and numerical expressions. NerKor is the newest and biggest NER corpus for Hungarian containing diverse domains. We applied cross-lingual transfer of NER models trained for other languages based on multilingual contextual language models to preannotate the corpus. We corrected the annotation semi-automatically and manually. Zero-shot preannotation was very effective with about 0.82 F1 score for the best model. We also added a 12000-token subcorpus on cars and other motor vehicles. We trained and release a transformer-based NER tagger for Hungarian using the annotation in the new corpus version, which provides similar performance to an identical model trained on the original version of the corpus.\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of novakat/nerkor-cars-onpp-hubert?", "answers": [{"text": "bert", "answer_start": 940, "answer_end": 943}]}, {"id": "q2", "question": "What is the model task of novakat/nerkor-cars-onpp-hubert?", "answers": [{"text": "token-classification", "answer_start": 31, "answer_end": 50}]}]}]}, {"title": "novakat/nerkor-hubert", "paragraphs": [{"context": "---\n\nlanguage: \n  - hu\ntags:\n- token-classification\nlicense: gpl\nmetrics:\n- F1\nwidget:\n- text: \"A j\u00f3t\u00e9konys\u00e1gi szervezet \u00e1ltal id\u00e9zett Forbes-adatok szerint a vil\u00e1g t\u00edz leggazdagabb embere: Elon Musk (Tesla, SpaceX), Jeff Bezos (Amazon, Blue Origin), Bernard Arnault \u00e9s csal\u00e1dja (LVMH, azaz Louis Vuitton \u00e9s Mo\u00ebt Hennessy), Bill Gates (Microsoft), Larry Ellison (Oracle), Larry Page (Google), Sergey Brin (Google), Mark Zuckerberg (Facebook), Steve Ballmer (Microsoft) \u00e9s Warren Buffett (befektet\u0151).\nMik\u00f6zben vagyonuk egy\u00fcttesen 700 milli\u00e1rdr\u00f3l m\u00e1sf\u00e9l ezer milli\u00e1rd doll\u00e1rra n\u0151tt 2020 m\u00e1rciusa \u00e9s 2021 novembere k\u00f6z\u00f6tt, jelent\u0151s elt\u00e9r\u00e9sek vannak k\u00f6z\u00f6tt\u00fck: Musk vagyona t\u00f6bb mint 1000 sz\u00e1zal\u00e9kos, m\u00edg Gates\u00e9 szer\u00e9nyebb, 30 sz\u00e1zal\u00e9kos n\u00f6veked\u00e9st mutatott.\"\ninference:\n  parameters:\n    aggregation_strategy: \"first\"\n\n---\n\n# Hungarian named entity recognition model with four entity types: PER ORG LOC MISC\n\n  - Pretrained model used: SZTAKI-HLT/hubert-base-cc \n  - Finetuned on NYTK-NerKor Corpus\n  \t\n## Limitations\n\n- max_seq_length = 448\n\n## See [ for a much more elaborate Hungarian named entity model.\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of novakat/nerkor-hubert?", "answers": [{"text": "bert", "answer_start": 945, "answer_end": 948}]}, {"id": "q2", "question": "What is the model task of novakat/nerkor-hubert?", "answers": [{"text": "token-classification", "answer_start": 31, "answer_end": 50}]}]}]}, {"title": "nyu-mll/roberta-base-100M-2", "paragraphs": [{"context": "# RoBERTa Pretrained on Smaller Datasets\n\nWe pretrain RoBERTa on smaller datasets (1M, 10M, 100M, 1B tokens). We release 3 models with lowest perplexities for each pretraining data size out of 25 runs (or 10 in the case of 1B tokens). The pretraining data reproduces that of BERT: We combine English Wikipedia and a reproduction of BookCorpus using texts from smashwords in a ratio of approximately 3:1.\n\n### Hyperparameters and Validation Perplexity\n\nThe hyperparameters and validation perplexities corresponding to each model are as follows:\n\n Training Size  Max Steps  Validation Perplexity |\n-------------------------------------------------|\n 1B             100K       3.93                  |\n 1B             31K        4.25                  |\n 1B             31K        3.84                  |\n 100M           100K       4.99                  |\n 100M           31K        4.61                  |\n 100M           31K        5.02                  |\n 10M            10K        11.31                 |\n 10M            10K        10.78                 |\n 10M            31K        11.58                 |\n 1M             100K       153.38                |\n 1M             10K        134.18                |\n 1M             31K        139.39                |\n\nThe hyperparameters corresponding to model sizes mentioned above are as follows:\n\n L   HS   P    |\n---------------|\n 12  768  125M |\n 6   512  45M  |\n\n(AH = number of attention heads; HS = hidden size; FFN = feedforward network dimension; P = number of parameters.)\n\nFor other hyperparameters, we select:\n- Peak Learning rate: 5e-4\n- Warmup Steps: 6% of max steps\n- Dropout: 0.1\n\n[link-roberta-med-small-1M-1]: \n[link-roberta-med-small-1M-2]: \n[link-roberta-med-small-1M-3]: \n[link-roberta-base-10M-1]: \n[link-roberta-base-10M-2]: \n[link-roberta-base-10M-3]: \n[link-roberta-base-100M-1]: \n[link-roberta-base-100M-2]: \n[link-roberta-base-100M-3]: \n[link-roberta-base-1B-1]: \n[link-roberta-base-1B-2]: \n[link-roberta-base-1B-3]: \n", "qas": [{"id": "q1", "question": "What is the model architecture of nyu-mll/roberta-base-100M-2?", "answers": [{"text": "roberta", "answer_start": 1646, "answer_end": 1652}]}]}]}, {"title": "omoekan/opus-tatoeba-eng-yor", "paragraphs": [{"context": "## OPUS Tatoeba English-Yoruba\n\nThis model was obtained by running the script convert_marian_to_pytorch.py with the flag -m eng-yor. The original models were trained by J\u00f6rg Tiedemann using the MarianNMT library. See all available MarianMTModel models on the profile of the Helsinki NLP group.\n\n\n---\n - tags: translation\n - source language: English\n - target language: Yoruba\n \n - dataset: opus+bt\n -model: transformer-align\n -pre-processing: normalization + SentencePiece (spm12k,spm12k)\n -download original weights: [opus+bt-2021-04-10.zip](\n -test set translations: [opus+bt-2021-04-10.test.txt](\n -test set scores: [opus+bt-2021-04-10.eval.txt](\n\n -Benchmarks\n BLEU\n :---\n 13.0\n \n ---", "qas": [{"id": "q1", "question": "What is the model architecture of omoekan/opus-tatoeba-eng-yor?", "answers": [{"text": "marian", "answer_start": 86, "answer_end": 91}]}]}]}, {"title": "onlplab/alephbert-base", "paragraphs": [{"context": "---\nlanguage:\n- he \ntags:\n- language model\nlicense: apache-2.0\ndatasets:\n- oscar\n- wikipedia\n- twitter \n---\n\n# AlephBERT\n\n## Hebrew Language Model\n\nState-of-the-art language model for Hebrew.\nBased on Google's BERT architecture [(Devlin et al. 2018)](\n\n#### How to use\n\n```python\nfrom transformers import BertModel, BertTokenizerFast\n\nalephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\nalephbert = BertModel.from_pretrained('onlplab/alephbert-base')\n\n# if not finetuning - disable dropout\nalephbert.eval()\n```\n\n## Training data\n1. OSCAR [(Ortiz, 2019)]( Hebrew section (10 GB text, 20 million sentences).\n2. Hebrew dump of [Wikipedia]( (650 MB text, 3 million sentences).\n3. Hebrew Tweets collected from the Twitter sample stream (7 GB text, 70 million sentences).\n\n## Training procedure\n\nTrained on a DGX machine (8 V100 GPUs) using the standard huggingface training procedure.\n\nSince the larger part of our training data is based on tweets we decided to start by optimizing using Masked Language Model loss only.\n\nTo optimize training time we split the data into 4 sections based on max number of tokens:\n\n1. num tokens < 32 (70M sentences)\n2. 32 <= num tokens < 64 (12M sentences)\n3. 64 <= num tokens < 128 (10M sentences)\n4. 128 <= num tokens < 512 (1.5M sentences)\n\nEach section was first trained for 5 epochs with an initial learning rate set to 1e-4. Then each section was trained for another 5 epochs with an initial learning rate set to 1e-5, for a total of 10 epochs.\n\nTotal training time was 8 days.\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of onlplab/alephbert-base?", "answers": [{"text": "bert", "answer_start": 340, "answer_end": 343}]}]}]}, {"title": "ontocord/mt5-fix-asr-vietnamese", "paragraphs": [{"context": "---\nlanguage: vi\ndatasets:\n- common_voice\n- FOSD: \nmetrics:\n- wer\ntags:\n- language-modeling\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: MT5 Fix Asr Vietnamese by Ontocord\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice vi\n      type: common_voice\n      args: vi\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 25.207182\n---\n\n# Ontocord/mt5-fix-asr-vietnamese\nFine-tuned mt5 to correct output of an ASR model trained on [facebook/wav2vec2-large-xlsr-53]( which was trained on Vietnamese using the [Common Voice]( and [FOSD](\n\n## Usage\nThe model can be used directly by submitting vietnamese asr text, but is is best to use with the ontocord/wav2vec2-large-xlsr-vietnamese model.\n\n```\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, pipelines\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntest_dataset = load_dataset(\"common_voice\", \"vi\", split=\"test[:2%]\") \nprocessor = Wav2Vec2Processor.from_pretrained(\"ontocord/wav2vec2-large-xlsr-53-vietnamese\") \nmodel = Wav2Vec2ForCTC.from_pretrained(\"ontocord/wav2vec2-large-xlsr-53-vietnamese\").to(device) \nmt5 = pipelines.pipeline(\"text2text-generation\",\"ontocord/mt5-fix-asr-vietnamese\", device=0 if device == \"cuda\" else -1)\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n\tspeech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n\tbatch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n\treturn batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\n\tlogits = model(inputs.input_values.to(device), attention_mask=inputs.attention_mask.to(device)).logits\npredicted_ids = torch.argmax(logits, dim=-1)\nprint(\"Prediction:\", [aHash['generated_text'] for aHash in mt5(processor.batch_decode(predicted_ids), max_length=100)])\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n\n## Evaluation\nThe model can be evaluated as follows on the Vietnamese test data of Common Voice.\n```\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, pipelines\nimport re\ntest_dataset = load_dataset(\"common_voice\", \"vi\", split=\"test\")\nwer = load_metric(\"wer\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"ontocord/wav2vec2-large-xlsr-vietnamese\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"ontocord/wav2vec2-large-xlsr-vietnamese\").to(device)\nmt5 = pipelines.pipeline(\"text2text-generation\",\"ontocord/mt5-fix-asr-vietnamese\", device=0 if device == \"cuda\" else -1)\nchars_to_ignore_regex = '[\\\\\\+\\@\\\u01c0\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\\u201d\\\ufffd]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n  batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n  speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n  batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n  return batch\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# you may also want to use the  decode_string from \ndef evaluate(batch):\n  inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n  with torch.no_grad():\n    logits = model(inputs.input_values.to(device), attention_mask=inputs.attention_mask.to(device)).logits\n    pred_ids = torch.argmax(logits, dim=-1)\n    max_length = int(pred_ids.size()[1])\n    txt = [aHash['generated_text'].strip() for aHash in mt5(processor.batch_decode(pred_ids), max_length=max_length)]\n    batch[\"pred_strings\"] = txt\n  return batch\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n```\n**Test Result**: 25.207182\n## Training\nThe Common Voice train, validation, and FPT datasets were used for training.\nThe script used for training can be found here # TODO", "qas": [{"id": "q1", "question": "What is the model architecture of ontocord/mt5-fix-asr-vietnamese?", "answers": [{"text": "mt5", "answer_start": 513, "answer_end": 515}]}, {"id": "q2", "question": "What is the model task of ontocord/mt5-fix-asr-vietnamese?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 102, "answer_end": 129}]}, {"id": "q3", "question": "What is the model category of ontocord/mt5-fix-asr-vietnamese?", "answers": [{"text": "audio", "answer_start": 94, "answer_end": 98}]}]}]}, {"title": "ontocord/wav2vec2-large-xlsr-vietnamese", "paragraphs": [{"context": "---\nlanguage: vi\ndatasets:\n- common_voice\n- FOSD: \nmetrics:\n- wer\ntags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 Vietnamese by Ontocord\n  results:\n  - task: \n      name: Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice vi\n      type: common_voice\n      args: vi\n    metrics:\n       - name: Test WER\n         type: wer\n         value: 42.403315\n---\n# Ontocord/Wav2Vec2-Large-XLSR-53-Vietnamese\nFine-tuned [facebook/wav2vec2-large-xlsr-53]( on Vietnamese using the [Common Voice]( [FOSD](\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n## Usage\nThe model can be used directly (without a language model) as follows:\n\n```\nimport torch\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\ntest_dataset = load_dataset(\"common_voice\", \"vi\", split=\"test[:2%]\") \n\nprocessor = Wav2Vec2Processor.from_pretrained(\"ontocord/wav2vec2-large-xlsr-53-vietnamese\") \nmodel = Wav2Vec2ForCTC.from_pretrained(\"ontocord/wav2vec2-large-xlsr-53-vietnamese\") \n\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n\tspeech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n\tbatch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n\treturn batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n\tlogits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\n\nprint(\"Prediction:\", processor.batch_decode(predicted_ids))\nprint(\"Reference:\", test_dataset[\"sentence\"][:2])\n```\n## Evaluation\nThe model can be evaluated as follows on the Vietnamese test data of Common Voice.\n```\n\nimport torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport re\n\n\ntest_dataset = load_dataset(\"common_voice\", \"vi\", split=\"test\")\nwer = load_metric(\"wer\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"ontocord/wav2vec2-large-xlsr-vietnamese\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"ontocord/wav2vec2-large-xlsr-vietnamese\")\nmodel.to(\"cuda\")\n\nchars_to_ignore_regex = '[\\\\\\+\\@\\\u01c0\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\\u201d\\\ufffd]'\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\n\n\n# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_fn(batch):\n  batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n  speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n  batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n  return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# you may also want to use the  decode_string from \ndef evaluate(batch):\n  inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n  with torch.no_grad():\n    logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n  pred_ids = torch.argmax(logits, dim=-1)\n  batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n  return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\nprint(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n\n```\n**Test Result**: 42.403315\n## Training\nThe Common Voice train, validation, and FPT datasets were used for training.\n\nThe script used for training can be found here # TODO\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of ontocord/wav2vec2-large-xlsr-vietnamese?", "answers": [{"text": "wav2vec2", "answer_start": 549, "answer_end": 556}]}, {"id": "q2", "question": "What is the model task of ontocord/wav2vec2-large-xlsr-vietnamese?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 82, "answer_end": 109}]}, {"id": "q3", "question": "What is the model category of ontocord/wav2vec2-large-xlsr-vietnamese?", "answers": [{"text": "audio", "answer_start": 74, "answer_end": 78}]}]}]}, {"title": "oo/distilbert-base-uncased-finetuned-squad", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmodel-index:\n- name: distilbert-base-uncased-finetuned-squad\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-squad\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the squad dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of oo/distilbert-base-uncased-finetuned-squad?", "answers": [{"text": "distilbert", "answer_start": 94, "answer_end": 103}]}]}]}, {"title": "oododo/DialoGPT-small-elon", "paragraphs": [{"context": "---\ntags: \n- conversational\n---\n\n# Elon Musk DialogGPT Model", "qas": [{"id": "q2", "question": "What is the model task of oododo/DialoGPT-small-elon?", "answers": [{"text": "conversational", "answer_start": 13, "answer_end": 26}]}]}]}, {"title": "openai/clip-vit-base-patch16", "paragraphs": [{"context": "---\ntags:\n- vision\nwidget:\n- src: \n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n# Model Card: CLIP\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](\n\n\n## Model Details\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n\n### Model Date\nJanuary 2021\n\n\n### Model Type\nThe base model uses a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n- [Blog Post](\n- [CLIP Paper](\n\n\n### Use with Transformers\n```python3\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\nurl = \"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n\n#### Primary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n\n### Out-of-Scope Use Cases\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n## Data\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M]( A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n\n### Data Mission Statement\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n## Performance and Limitations\n\n\n### Performance\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n\n## Limitations\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n\n### Bias and Fairness\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface]( into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n## Feedback\n\n\n### Where to send questions or comments about the model\nPlease use [this Google Form](\n", "qas": [{"id": "q1", "question": "What is the model architecture of openai/clip-vit-base-patch16?", "answers": [{"text": "clip", "answer_start": 1424, "answer_end": 1427}]}]}]}, {"title": "openai/clip-vit-base-patch32", "paragraphs": [{"context": "---\ntags:\n- vision\nwidget:\n- src: \n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](\n- [CLIP Paper](\n\n\n### Use with Transformers\n\n```python3\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nurl = \"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M]( A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface]( into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](", "qas": [{"id": "q1", "question": "What is the model architecture of openai/clip-vit-base-patch32?", "answers": [{"text": "clip", "answer_start": 1426, "answer_end": 1429}]}]}]}, {"title": "openai/clip-vit-large-patch14", "paragraphs": [{"context": "---\ntags:\n- vision\nwidget:\n- src: \n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](\n- [CLIP Paper](\n\n\n### Use with Transformers\n\n```python\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nurl = \"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M]( A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface]( into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](", "qas": [{"id": "q1", "question": "What is the model architecture of openai/clip-vit-large-patch14?", "answers": [{"text": "clip", "answer_start": 1429, "answer_end": 1432}]}]}]}, {"title": "opensource/extract_names", "paragraphs": [{"context": "---\nlanguage: multilingual\n\ntags:\n- Extract Names\n\nlicense: apache-2.0\n---\n\n\n## Extract names in any language.\n\n", "qas": []}]}, {"title": "orisuchy/Descriptive_Classifier", "paragraphs": [{"context": "---\nlicense: afl-3.0\nlanguage: \"he\"\ntags:\n- Text Classification\nwidget:\n- text: \"\u05d4\u05d9\u05e2\u05e8 \u05d4\u05e9\u05d7\u05d5\u05e8 \u05d5\u05d4\u05d2\u05d3\u05d5\u05dc\"\n- text: \"\u05d5\u05d0\u05d6 \u05d4\u05d5\u05d0 \u05d4\u05dc\u05da \u05dc\u05d8\u05d9\u05d9\u05dc \u05d1\u05ea\u05d5\u05da \u05d4\u05d9\u05e2\u05e8 \u05d4\u05e9\u05d7\u05d5\u05e8 \u05d5\u05d4\u05d2\u05d3\u05d5\u05dc\"\ndatasets:\n- orisuchy/Descriptive_Sentences_He\nmetrics:\n- accuracy\n- f1\n\n---\n# **Descriptive Sentences Classifier**\n\nBased on [AlephBERT]( model.\n\n# **Metrics**\n[accuracy]( 0.813953488372093\n</br>\n[f1]( 0.8181818181818182\n## How to Use the model:\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='orisuchy/Descriptive_Classifier', return_all_scores=True)\noutputs = classifier(\"\u05de\u05e1\u05d5\u05d5\u05d2 \u05d7\u05ea\u05d9\u05da \u05d1\u05de\u05d9\u05d5\u05d7\u05d3\")\nprint(outputs)\n\n\"\"\"\nOutput:\n[[\n{'label': 'Descriptive', 'score': 0.999764621257782},\n{'label': 'Not Descriptive', 'score': 0.00023541577684227377}]]\n\"\"\"\n```\n#### Or, if you want only the final class:\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='orisuchy/Descriptive_Classifier')\noutput = classifier(\"\u05d4\u05dc\u05db\u05ea\u05d9 \u05d0\u05dc\u05d9\u05d5 \u05d4\u05d1\u05d9\u05ea\u05d4 \u05d5\u05d7\u05d9\u05db\u05d9\u05ea\u05d9\")\nprint(output)\n\n\"\"\"\nOutput:\n[{'label': 'Not Descriptive', 'score': 0.999901533126831}]\n\"\"\"\n```\nCreated by Daniel Smotritsky & Ori Suchy\n<br>\n[GitHub](\n<iframe src=\" style=\"border:none;height:1024px;width:100%\">\n\n\n", "qas": [{"id": "q2", "question": "What is the model task of orisuchy/Descriptive_Classifier?", "answers": [{"text": "text-classification", "answer_start": 464, "answer_end": 482}]}]}]}, {"title": "orri/IceBERT-finetuned-ner", "paragraphs": [{"context": "---\nlicense: gpl-3.0\ntags:\n- generated_from_trainer\ndatasets:\n- mim_gold_ner\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nwidget:\n- text: Systurnar Gu\u00f0r\u00fan og Monique \u00e1tu einar \u00e1 McDonalds og horf\u00f0u \u00e1 St\u00f6\u00f0 2, \u00fear glitti \u00ed Bruce Willis leika \u00ed Die Hard 2.\nmodel-index:\n- name: IceBERT-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: mim_gold_ner\n      type: mim_gold_ner\n      args: mim-gold-ner\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.89397115028973\n    - name: Recall\n      type: recall\n      value: 0.8664117576771418\n    - name: F1\n      type: f1\n      value: 0.8799757281553399\n    - name: Accuracy\n      type: accuracy\n      value: 0.9854156499755994\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# IceBERT-finetuned-ner\n\nThis model is a fine-tuned version of [vesteinn/IceBERT]( on the mim_gold_ner dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0802\n- Precision: 0.8940\n- Recall: 0.8664\n- F1: 0.8800\n- Accuracy: 0.9854\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0779           0.8504  0.9831   |\n 2.0    0.0784           0.8585  0.9839   |\n 3.0    0.0802           0.8664  0.9854   |\n\n\n### Framework versions\n\n- Transformers 4.11.1\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of orri/IceBERT-finetuned-ner?", "answers": [{"text": "token-classification", "answer_start": 364, "answer_end": 383}]}]}]}, {"title": "orri/XLMR-ENIS-finetuned-ner", "paragraphs": [{"context": "---\nlicense: agpl-3.0\ntags:\n- generated_from_trainer\ndatasets:\n- mim_gold_ner\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: XLMR-ENIS-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: mim_gold_ner\n      type: mim_gold_ner\n      args: mim-gold-ner\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.8714268909540054\n    - name: Recall\n      type: recall\n      value: 0.842296759522456\n    - name: F1\n      type: f1\n      value: 0.8566142460684552\n    - name: Accuracy\n      type: accuracy\n      value: 0.9827189115812273\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# XLMR-ENIS-finetuned-ner\n\nThis model is a fine-tuned version of [vesteinn/XLMR-ENIS]( on the mim_gold_ner dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0955\n- Precision: 0.8714\n- Recall: 0.8423\n- F1: 0.8566\n- Accuracy: 0.9827\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0939           0.8205  0.9804   |\n 2.0    0.0917           0.8299  0.9819   |\n 3.0    0.0955           0.8423  0.9827   |\n\n\n### Framework versions\n\n- Transformers 4.11.1\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of orri/XLMR-ENIS-finetuned-ner?", "answers": [{"text": "token-classification", "answer_start": 235, "answer_end": 254}]}]}]}, {"title": "orzhan/bart-transcription-aggregation", "paragraphs": [{"context": "---\nlanguage: ru\n---\n\nBART model fine-tuned to aggregate crowd-sourced transcriptions.\nRepository: [GitHub](", "qas": []}]}, {"title": "orzhan/rugpt3-simplify-large", "paragraphs": [{"context": "Text simplification model for Russian. Fine-tuned ruGPT3-large\n\n---\nlanguage: ru\n", "qas": []}]}, {"title": "orzhan/t5-long-extract", "paragraphs": [{"context": "T5-small model fine-tuned for extractive summarization on long documents. \n\nRepository: [GitHub](", "qas": []}]}, {"title": "osama7/t5-summarization-multinews", "paragraphs": [{"context": "This is a t5-base model trained on the multi_news dataset for abstraction summarization", "qas": [{"id": "q1", "question": "What is the model architecture of osama7/t5-summarization-multinews?", "answers": [{"text": "t5", "answer_start": 10, "answer_end": 11}]}]}]}, {"title": "osanseviero/asr-with-transformers-wav2vec2", "paragraphs": [{"context": "---\nbenchmark: superb\nlibrary_name: superb\nlanguage: en\ndatasets:\n- librispeech_asr\ntags:\n- audio\n- automatic-speech-recognition\n- superb\nlicense: apache-2.0\nwidget:\n- example_title: Librispeech sample 1\n  src: \n- example_title: Librispeech sample 2\n  src: \n---\n\n# Fork of Wav2Vec2-Base-960h\n\n[Facebook's Wav2Vec2](\n\nThe base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model\nmake sure that your speech input is also sampled at 16Khz.\n\n[Paper](\n\nAuthors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli\n\n**Abstract**\n\nWe show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.\n\nThe original model can be found under \n\n\n# Usage\n\nTo transcribe audio files the model can be used as a standalone acoustic model as follows:\n\n```python\n from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n from datasets import load_dataset\n import soundfile as sf\n import torch\n \n # load model and tokenizer\n tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n \n # define function to read in sound file\n def map_to_array(batch):\n     speech, _ = sf.read(batch[\"file\"])\n     batch[\"speech\"] = speech\n     return batch\n     \n # load dummy dataset and read soundfiles\n ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n ds = ds.map(map_to_array)\n \n # tokenize\n input_values = tokenizer(ds[\"speech\"][:2], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n \n # retrieve logits\n logits = model(input_values).logits\n \n # take argmax and decode\n predicted_ids = torch.argmax(logits, dim=-1)\n transcription = tokenizer.batch_decode(predicted_ids)\n ```\n \n ## Evaluation\n \n This code snippet shows how to evaluate **facebook/wav2vec2-base-960h** on LibriSpeech's \"clean\" and \"other\" test data.\n \n```python\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\nimport soundfile as sf\nimport torch\nfrom jiwer import wer\n\n\nlibrispeech_eval = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(\"cuda\")\ntokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\ndef map_to_array(batch):\n    speech, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech\n    return batch\n\nlibrispeech_eval = librispeech_eval.map(map_to_array)\n\ndef map_to_pred(batch):\n    input_values = tokenizer(batch[\"speech\"], return_tensors=\"pt\", padding=\"longest\").input_values\n    with torch.no_grad():\n        logits = model(input_values.to(\"cuda\")).logits\n\n    predicted_ids = torch.argmax(logits, dim=-1)\n    transcription = tokenizer.batch_decode(predicted_ids)\n    batch[\"transcription\"] = transcription\n    return batch\n\nresult = librispeech_eval.map(map_to_pred, batched=True, batch_size=1, remove_columns=[\"speech\"])\n\nprint(\"WER:\", wer(result[\"text\"], result[\"transcription\"]))\n```\n\n*Result (WER)*:\n\n \"other\" |\n---|\n 8.6 |", "qas": [{"id": "q1", "question": "What is the model architecture of osanseviero/asr-with-transformers-wav2vec2?", "answers": [{"text": "wav2vec2", "answer_start": 1836, "answer_end": 1843}]}, {"id": "q2", "question": "What is the model task of osanseviero/asr-with-transformers-wav2vec2?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 100, "answer_end": 127}]}, {"id": "q3", "question": "What is the model category of osanseviero/asr-with-transformers-wav2vec2?", "answers": [{"text": "audio", "answer_start": 92, "answer_end": 96}]}]}]}, {"title": "osanseviero/clip-st", "paragraphs": [{"context": "---\ntags:\n- sentence-transformers\n- feature-extraction\n---\n\n# TODO: Name of Model\n\nTODO: Description\n\n## Model Description\nTODO: Add relevant content\n\n(0) Base Transformer Type: DistilBertModel\n\n(1) Pooling mean\n\n(2) Dense 768x512\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes more convenient when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\"]\n\nmodel = SentenceTransformer(TODO)\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## TODO: Training Procedure\n\n## TODO: Evaluation Results\n\n## TODO: Citing & Authors\n", "qas": [{"id": "q2", "question": "What is the model task of osanseviero/clip-st?", "answers": [{"text": "feature-extraction", "answer_start": 36, "answer_end": 53}]}]}]}, {"title": "osanseviero/dalle-mini-fork", "paragraphs": [{"context": "---\nlibrary_name: generic\nlanguage:\n- en\npipeline_tag: text-to-image\n---\n\n## Fork of DALL\u00b7E mini - Generate images from text\n\nFor the original repo, head to ", "qas": [{"id": "q2", "question": "What is the model task of osanseviero/dalle-mini-fork?", "answers": [{"text": "text-to-image", "answer_start": 55, "answer_end": 67}]}]}]}, {"title": "osanseviero/distilbert-base-uncased-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotion\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: default\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9225\n    - name: F1\n      type: f1\n      value: 0.92271004914086\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2251\n- Accuracy: 0.9225\n- F1: 0.9227\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.3288           0.8979 |\n 2.0    0.2251           0.9227 |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1\n- Datasets 1.18.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of osanseviero/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 121, "answer_end": 130}]}, {"id": "q2", "question": "What is the model task of osanseviero/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "text-classification", "answer_start": 228, "answer_end": 246}]}]}]}, {"title": "osanseviero/full-sentence-distillroberta2", "paragraphs": [{"context": "---\ntags:\n- sentence-transformers\n- sentence-similarity\n---\n\n## Testing Sentence Transformer", "qas": [{"id": "q2", "question": "What is the model task of osanseviero/full-sentence-distillroberta2?", "answers": [{"text": "sentence-similarity", "answer_start": 36, "answer_end": 54}]}]}]}, {"title": "osanseviero/full-sentence-distillroberta3", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- causal-lm\nlicense:\n- cc-by-sa-4.0\n---\n\n# TODO: Name of Model\n\nTODO: Description\n\n## Model Description\nTODO: Add relevant content\n\n(0) Base Transformer Type: RobertaModel\n\n(1) Pooling mean\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes more convenient when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\"]\n\nmodel = SentenceTransformer(TODO)\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n## Usage (HuggingFace Transformers)\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n#\u00a0The next step is optional if you want your own pooling function.\n# Max Pooling - Take the max value over time for every dimension. \ndef max_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n    max_over_time = torch.max(token_embeddings, 1)[0]\n    return max_over_time\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(TODO)\nmodel = AutoModel.from_pretrained(TODO)\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt'))\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = max_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## TODO: Training Procedure\n\n## TODO: Evaluation Results\n\n## TODO: Citing & Authors\n", "qas": [{"id": "q2", "question": "What is the model task of osanseviero/full-sentence-distillroberta3?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "osanseviero/hot_dog_or_sandwich", "paragraphs": [{"context": "---\ntags:\n- image-classification\n- pytorch\n- huggingpics\nmetrics:\n- accuracy\n\nmodel-index:\n- name: hot_dog_or_sandwich\n  results:\n  - task:\n      name: Image Classification\n      type: image-classification\n    metrics:\n      - name: Accuracy\n        type: accuracy\n        value: 0.8541666865348816\n---\n\n# hot_dog_or_sandwich\n\n\nAutogenerated by HuggingPics\ud83e\udd17\ud83d\uddbc\ufe0f\n\nCreate your own image classifier for **anything** by running [the demo on Google Colab](\n\nReport any issues with the demo at the [github repo](\n\n\n## Example Images\n\n\n#### hot dog\n\n![hot dog](images/hot_dog.jpg)\n\n#### sandwich\n\n![sandwich](images/sandwich.jpg)", "qas": [{"id": "q2", "question": "What is the model task of osanseviero/hot_dog_or_sandwich?", "answers": [{"text": "image-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "osanseviero/hugging-geese", "paragraphs": [{"context": "---\ntags:\n- image-classification\n- pytorch\n- huggingpics\nmetrics:\n- accuracy\n\nmodel-index:\n- name: hugging-geese\n  results:\n  - task:\n      name: Image Classification\n      type: image-classification\n    metrics:\n      - name: Accuracy\n        type: accuracy\n        value: 0.9642857313156128\n---\n\n# hugging-geese\n\n\nAutogenerated by HuggingPics\ud83e\udd17\ud83d\uddbc\ufe0f\n\nCreate your own image classifier for **anything** by running [the demo on Google Colab](\n\nReport any issues with the demo at the [github repo](\n\n\n## Example Images\n\n\n#### dog\n\n![dog](images/dog.jpg)\n\n#### duck\n\n![duck](images/duck.jpg)\n\n#### goose\n\n![goose](images/goose.jpg)\n\n#### pigeon\n\n![pigeon](images/pigeon.jpg)\n\n#### swan\n\n![swan](images/swan.jpg)", "qas": [{"id": "q2", "question": "What is the model task of osanseviero/hugging-geese?", "answers": [{"text": "image-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "osanseviero/llama-alpaca-guanaco-vicuna", "paragraphs": [{"context": "---\ntags:\n- image-classification\n- pytorch\n- huggingpics\n- llama-leaderboard\nmetrics:\n- accuracy\n\nmodel-index:\n- name: llamastic\n  results:\n  - task:\n      name: Image Classification\n      type: image-classification\n    metrics:\n      - name: Accuracy\n        type: accuracy\n        value: 0.39772728085517883\n---\n\n# Llamastics\n\n\n\nAutogenerated by HuggingPics\ud83e\udd17\ud83d\uddbc\ufe0f\n\nCreate your own image classifier for **anything** by running [the demo](\n\nReport any issues with the demo at the [github repo](\n\n\n## Example Images\n\n\n#### alpaca\n\n![alpaca](images/alpaca.jpg)\n\n#### guanaco\n\n![guanaco](images/guanaco.jpg)\n\n#### llama\n\n![llama](images/llama.jpg)\n\n#### vicuna\n\n![vicuna](images/vicuna.jpg)", "qas": [{"id": "q2", "question": "What is the model task of osanseviero/llama-alpaca-guanaco-vicuna?", "answers": [{"text": "image-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "paintingpeter/distilbert-base-uncased-finetuned-clinc", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- clinc_oos\nmetrics:\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased-finetuned-clinc\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: clinc_oos\n      type: clinc_oos\n      args: plus\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9174193548387096\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-clinc\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the clinc_oos dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7713\n- Accuracy: 0.9174\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 48\n- eval_batch_size: 48\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    3.2831          \n 2.0    1.8739          \n 3.0    1.1525          \n 4.0    0.8569          \n 5.0    0.7713          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of paintingpeter/distilbert-base-uncased-finetuned-clinc?", "answers": [{"text": "distilbert", "answer_start": 118, "answer_end": 127}]}, {"id": "q2", "question": "What is the model task of paintingpeter/distilbert-base-uncased-finetuned-clinc?", "answers": [{"text": "text-classification", "answer_start": 223, "answer_end": 241}]}]}]}, {"title": "pediberto/autonlp-testing-504313966", "paragraphs": [{"context": "---\ntags: autonlp\nlanguage: unk\nwidget:\n- text: \"I love AutoNLP \ud83e\udd17\"\ndatasets:\n- pediberto/autonlp-data-testing\nco2_eq_emissions: 12.994518654810642\n---\n\n# Model Trained Using AutoNLP\n\n- Problem type: Binary Classification\n- Model ID: 504313966\n- CO2 Emissions (in grams): 12.994518654810642\n\n## Validation Metrics\n\n- Loss: 0.19673296809196472\n- Accuracy: 0.9398032027783138\n- Precision: 0.9133115705476967\n- Recall: 0.9718255499807025\n- AUC: 0.985316873222122\n- F1: 0.9416604338070308\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoNLP\"}' \n```\n\nOr Python API:\n\n```\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"pediberto/autonlp-testing-504313966\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"pediberto/autonlp-testing-504313966\", use_auth_token=True)\n\ninputs = tokenizer(\"I love AutoNLP\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n```", "qas": []}]}, {"title": "persiannlp/mt5-base-parsinlu-arc-comqa-obqa-multiple-choice", "paragraphs": [{"context": "---\nlanguage:\n- fa\n- multilingual\nthumbnail: \ntags:\n- multiple-choice\n- mt5\n- persian\n- farsi\nlicense: cc-by-nc-sa-4.0\ndatasets:\n- parsinlu\n- commonsenseqa\n- arc\n- openbookqa\nmetrics:\n- accuracy\n---\n\n# Multiple-Choice Question Answering (\u0645\u062f\u0644 \u0628\u0631\u0627\u06cc \u067e\u0627\u0633\u062e \u0628\u0647 \u0633\u0648\u0627\u0644\u0627\u062a \u0686\u0647\u0627\u0631 \u062c\u0648\u0627\u0628\u06cc)\n\nThis is a mT5-based model for multiple-choice question answering. \nHere is an example of how you can run this model: \n\n```python \nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer\n\nmodel_size = \"base\"\nmodel_name = f\"persiannlp/mt5-{model_size}-parsinlu-arc-comqa-obqa-multiple-choice\"\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\n\n\ndef run_model(input_string, **generator_args):\n    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n    res = model.generate(input_ids, **generator_args)\n    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n    print(output)\n    return output\n\n\nrun_model(\"\u0648\u0633\u06cc\u0639 \u062a\u0631\u06cc\u0646 \u06a9\u0634\u0648\u0631 \u062c\u0647\u0627\u0646 \u06a9\u062f\u0627\u0645 \u0627\u0633\u062a\u061f <sep> \u0622\u0645\u0631\u06cc\u06a9\u0627 <sep> \u06a9\u0627\u0646\u0627\u062f\u0627 <sep> \u0631\u0648\u0633\u06cc\u0647 <sep> \u0686\u06cc\u0646\")\nrun_model(\"\u0637\u0627\u0645\u0639 \u06cc\u0639\u0646\u06cc \u061f <sep> \u0622\u0632\u0645\u0646\u062f <sep> \u062e\u0648\u0634 \u0634\u0627\u0646\u0633 <sep> \u0645\u062d\u062a\u0627\u062c <sep> \u0645\u0637\u0645\u0626\u0646\")\nrun_model(\n    \"\u0632\u0645\u06cc\u0646\u06cc \u0628\u0647 \u06f3\u06f1 \u0642\u0637\u0639\u0647 \u0645\u062a\u0633\u0627\u0648\u06cc \u0645\u0641\u0631\u0648\u0636 \u0634\u062f\u0647 \u0627\u0633\u062a \u0648 \u0647\u0631 \u0631\u0648\u0632 \u0645\u0633\u0627\u062d\u062a \u0622\u0645\u0627\u062f\u0647 \u0634\u062f\u0647 \u0628\u0631\u0627\u06cc \u0627\u062d\u062f\u0627\u062b\u060c \u062f\u0648 \u0628\u0631\u0627\u0628\u0631 \u0645\u0633\u0627\u062d\u062a \u0631\u0648\u0632 \u0642\u0628\u0644 \u0627\u0633\u062a.\u0627\u06af\u0631 \u067e\u0633 \u0627\u0632 (\u06f5 \u0631\u0648\u0632) \u062a\u0645\u0627\u0645 \u0632\u0645\u06cc\u0646 \u0622\u0645\u0627\u062f\u0647 \u0634\u062f\u0647 \u0628\u0627\u0634\u062f\u060c \u062f\u0631 \u0686\u0647 \u0631\u0648\u0632\u06cc \u06cc\u06a9 \u0642\u0637\u0639\u0647 \u0632\u0645\u06cc\u0646 \u0622\u0645\u0627\u062f\u0647 \u0634\u062f\u0647 <sep> \u0631\u0648\u0632 \u0627\u0648\u0644 <sep> \u0631\u0648\u0632 \u062f\u0648\u0645 <sep> \u0631\u0648\u0632 \u0633\u0648\u0645 <sep> \u0647\u06cc\u0686\u06a9\u062f\u0627\u0645\")\n\n```\n\n\nFor more details, visit this page:  \n", "qas": [{"id": "q1", "question": "What is the model architecture of persiannlp/mt5-base-parsinlu-arc-comqa-obqa-multiple-choice?", "answers": [{"text": "t5", "answer_start": 73, "answer_end": 74}]}]}]}, {"title": "persiannlp/mt5-base-parsinlu-sentiment-analysis", "paragraphs": [{"context": "---\nlanguage:\n- fa\n- multilingual\nthumbnail: \ntags:\n- sentiment\n- sentiment-analysis\n- mt5\n- persian\n- farsi\nlicense: cc-by-nc-sa-4.0\ndatasets:\n- parsinlu\nmetrics:\n- accuracy\n---\n\n# Sentiment Analysis (\u0622\u0646\u0627\u0644\u06cc\u0632 \u0627\u062d\u0633\u0627\u0633\u0627\u062a)\n\nThis is a mT5 model for sentiment analysis.\nHere is an example of how you can run this model: \n\n```python \nimport torch\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer\nimport numpy as np\n\nmodel_name_or_path = \"persiannlp/mt5-base-parsinlu-sentiment-analysis\"\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\n\n\ndef model_predict(text_a, text_b):\n    features = tokenizer( [(text_a, text_b)], padding=\"max_length\", truncation=True, return_tensors='pt')\n    output = model(**features)\n    logits = output[0]\n    probs = torch.nn.functional.softmax(logits, dim=1).tolist()\n    idx = np.argmax(np.array(probs))\n    print(labels[idx], probs)\n\n\ndef run_model(context, query, **generator_args):\n    input_ids = tokenizer.encode(context + \"<sep>\" + query, return_tensors=\"pt\")\n    res = model.generate(input_ids, **generator_args)\n    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n    print(output)\n    return output\n\n\nrun_model(\n    \"\u06cc\u06a9 \u0641\u06cc\u0644\u0645 \u0636\u0639\u06cc\u0641 \u0628\u06cc \u0645\u062d\u062a\u0648\u0627 \u0628\u062f\u0648\u0646 \u0641\u06cc\u0644\u0645\u0646\u0627\u0645\u0647 . \u0634\u0648\u062e\u06cc \u0647\u0627\u06cc \u0633\u062e\u06cc\u0641 .\",\n    \"\u0646\u0638\u0631 \u0634\u0645\u0627 \u062f\u0631 \u0645\u0648\u0631\u062f \u062f\u0627\u0633\u062a\u0627\u0646\u060c \u0641\u06cc\u0644\u0645\u0646\u0627\u0645\u0647\u060c \u062f\u06cc\u0627\u0644\u0648\u06af \u0647\u0627 \u0648 \u0645\u0648\u0636\u0648\u0639 \u0641\u06cc\u0644\u0645  \u0644\u0648\u0646\u0647 \u0632\u0646\u0628\u0648\u0631 \u0686\u06cc\u0633\u062a\u061f\"\n)\n\nrun_model(\n    \"\u0641\u06cc\u0644\u0645 \u062a\u0627 \u0648\u0633\u0637 \u0641\u06cc\u0644\u0645 \u06cc\u0639\u0646\u06cc \u062f\u0642\u06cc\u0642\u0627 \u062a\u0627 \u062c\u0627\u06cc\u06cc \u06a9\u0647 \u0645\u0639\u0644\u0648\u0645 \u0645\u06cc\u0634\u0647 \u0628\u0686\u0647 \u0647\u0627\u06cc \u0627\u0645\u0644\u0634\u06cc \u062f\u0646\u0628\u0627\u0644 \u0631\u0636\u0627\u0646 \u062e\u06cc\u0644\u06cc \u062e\u0648\u0628 \u0648 \u062c\u0630\u0627\u0628 \u067e\u06cc\u0634 \u0645\u06cc\u0631\u0647 \u0648\u0644\u06cc \u062f\u0642\u06cc\u0642\u0627 \u0627\u0632 \u0647\u0645\u0648\u0646\u062c\u0627\u0634 \u0633\u06a9\u062a\u0647 \u0645\u06cc\u0632\u0646\u0647 \u0648 \u062e\u0644\u0627\u0635...\",\n    \"\u0646\u0638\u0631 \u0634\u0645\u0627 \u0628\u0647 \u0635\u0648\u0631\u062a \u06a9\u0644\u06cc \u062f\u0631 \u0645\u0648\u0631\u062f \u0641\u06cc\u0644\u0645  \u0698\u0646 \u062e\u0648\u06a9 \u0686\u06cc\u0633\u062a\u061f\"\n)\nrun_model(\n    \"\u0627\u0635\u0644\u0627 \u0628\u0647 \u0647\u06cc\u0686 \u0639\u0646\u0648\u0627\u0646 \u0639\u0644\u0627\u0642\u0647 \u0646\u062f\u0627\u0634\u062a\u0645 \u0627\u062c\u0631\u0627\u06cc \u0645\u06cc \u0633\u06cc \u0633\u06cc \u067e\u06cc \u0646\u0634\u0633\u062a\u0647 \u0645\u06cc\u0645\u06cc\u0631\u062f \u0631\u0648\u06cc \u067e\u0631\u062f\u0647 \u0633\u06cc\u0646\u0645\u0627 \u0628\u0628\u06cc\u0646\u0645  \u062f\u06cc\u0627\u0644\u0648\u06af \u0647\u0627\u06cc \u062a\u06a9\u0631\u0627\u0631\u06cc   \u0647\u0644\u06cc\u06a9\u0648\u067e\u062a\u0631  \u0645\u0627\u0634\u06cc\u0646  \u0622\u0644\u0646\u062f\u0644\u0648\u0646  \u0644\u0626\u0648\u0646  \u067e\u0627\u067e\u06cc\u0648\u0646  \u0622\u062e\u0647 \u0686\u0631\u0627\u0627\u0627\u0627\u0627\u0627\u0627\u0627\u0627\u0627\u0627\u0627\u0627\u0627\u0627   \u0647\u0645\u0648\u0646 \u062d\u0633\u06cc \u06a9\u0647 \u062a\u0648\u06cc \u062a\u0627\u0644\u0627\u0631 \u0648\u062d\u062f\u062a \u0628\u0639\u062f \u0627\u0632 \u0646\u06cc\u0645 \u0633\u0627\u0639\u062a \u0628\u0647 \u0633\u0631\u0645 \u0627\u0648\u0645\u062f \u0627\u0645\u0634\u0628 \u062a\u0648\u06cc \u0633\u0627\u0644\u0646 \u0633\u06cc\u0646\u0645\u0627 \u062a\u062c\u0631\u0628\u0647 \u06a9\u0631\u062f\u0645 \u060c\u062d\u0633 \u06af\u0631\u06cc\u0632 \u0627\u0632 \u0633\u0627\u0644\u0646.......\u2066 \u2066(\u30ce\u0ca0\u76ca\u0ca0)\u30ce\u2069 \",\n    \" \u0646\u0638\u0631 \u0634\u0645\u0627 \u062f\u0631 \u0645\u0648\u0631\u062f \u0635\u062f\u0627\u06af\u0630\u0627\u0631\u06cc \u0648 \u062c\u0644\u0648\u0647 \u0647\u0627\u06cc \u0635\u0648\u062a\u06cc \u0641\u06cc\u0644\u0645  \u0645\u0633\u062e\u0631\u0647\u200c\u0628\u0627\u0632 \u0686\u06cc\u0633\u062a\u061f\"\n)\n\nrun_model(\n    \" \u06af\u0648\u0644 \u0646\u062e\u0648\u0631\u06cc\u062f \u0627\u06cc\u0646 \u0631\u0646\u06af\u0627\u0631\u0646\u06af \u0645\u06cc\u0646\u0648 \u0646\u06cc\u0633\u062a \u0628\u0631\u0627\u06cc \u0634\u0631\u06a9\u062a \u06af\u0631\u062c\u06cc\u0647 \u0648 \u0645\u062a\u0627\u0633\u0641\u0627\u0646\u0647 \u0627\u06cc\u0646 \u0645\u062d\u0635\u0648\u0644\u0634 \u0627\u0635\u0644\u0627 \u0645\u0632\u0647 \u0631\u0646\u06af\u0627\u0631\u0646\u06af\u06cc \u06a9\u0647 \u0627\u0646\u062a\u0638\u0627\u0631 \u062f\u0627\u0631\u06cc\u062f \u0631\u0648 \u0646\u0645\u06cc\u062f\u0647 \",\n    \" \u0646\u0638\u0631 \u0634\u0645\u0627 \u062f\u0631 \u0645\u0648\u0631\u062f \u0639\u0637\u0631\u060c \u0628\u0648\u060c \u0648 \u0637\u0639\u0645 \u0627\u06cc\u0646 \u0628\u06cc\u0633\u06a9\u0648\u06cc\u062a \u0648 \u0648\u06cc\u0641\u0631 \u0686\u06cc\u0633\u062a\u061f\"\n)\n\nrun_model(\n    \"\u062f\u0631 \u0645\u0642\u0627\u06cc\u0633\u0647 \u0628\u0627 \u0633\u0627\u06cc\u0631 \u0628\u0631\u0646\u062f\u0647\u0627\u06cc \u0645\u0648\u062c\u0648\u062f \u062f\u0631 \u0628\u0627\u0632\u0627\u0631 \u0628\u0627 \u062a\u0648\u062c\u0647 \u0628\u0647 \u062d\u0631\u0627\u062c\u06cc \u06a9\u0647 \u062f\u0627\u0634\u062a \u0627\u0631\u0632\u0627\u0646\u062a\u0631 \u0628\",\n    \" \u0634\u0645\u0627 \u062f\u0631 \u0645\u0648\u0631\u062f \u0642\u06cc\u0645\u062a \u0648 \u0627\u0631\u0632\u0634 \u062e\u0631\u06cc\u062f \u0627\u06cc\u0646 \u062d\u0628\u0648\u0628\u0627\u062a \u0648 \u0633\u0648\u06cc\u0627 \u0686\u06cc\u0633\u062a\u061f\"\n)\n\nrun_model(\n    \"\u0645\u0646 \u067e\u0633\u0631\u0645 \u0639\u0627\u0634\u0642 \u0627\u06cc\u0646\u0627\u0633 \u0648\u0644\u06cc \u062f\u06cc\u06af\u0647 \u0628\u0647 \u062e\u0627\u0637\u0631 \u062d\u0641\u0638 \u0645\u062d\u06cc\u0637 \u0632\u06cc\u0633\u062a \u0641\u0642\u0637 \u0632\u0645\u0627\u0646\u0647\u0627\u06cc\u06cc \u06a9\u0647 \u0645\u062c\u0628\u0648\u0631 \u0628\u0627\u0634\u0645 \u0634\u06cc\u0631 \u062f\u0648\u0646\u0647 \u0627\u06cc \u0645\u06cc\u062e\u0631\u0645 \u0648 \u0633\u0639\u06cc \u0645\u06cc\u06a9\u0646\u0645 \u062f\u06cc\u06af\u0647 \u06a9\u0645\u062a\u0631 \u0634\u06cc\u0631 \u0628\u0627 \u0628\u0633\u062a\u0647 \u0628\u0646\u062f\u06cc \u062a\u062a\u0631\u0627\u067e\u06a9 \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u06a9\u0646\u0645 \",\n    \"\u0646\u0638\u0631 \u0634\u0645\u0627 \u0628\u0647 \u0635\u0648\u0631\u062a \u06a9\u0644\u06cc \u062f\u0631 \u0645\u0648\u0631\u062f \u0627\u06cc\u0646 \u0634\u06cc\u0631 \u0686\u06cc\u0633\u062a\u061f\"\n)\n```\n\n\nFor more details, visit this page:  \n", "qas": [{"id": "q1", "question": "What is the model architecture of persiannlp/mt5-base-parsinlu-sentiment-analysis?", "answers": [{"text": "mt5", "answer_start": 87, "answer_end": 89}]}]}]}, {"title": "persiannlp/mt5-large-parsinlu-arc-comqa-obqa-multiple-choice", "paragraphs": [{"context": "---\nlanguage:\n- fa\n- multilingual\nthumbnail: \ntags:\n- multiple-choice\n- mt5\n- persian\n- farsi\nlicense: cc-by-nc-sa-4.0\ndatasets:\n- parsinlu\n- commonsenseqa\n- arc\n- openbookqa\nmetrics:\n- accuracy\n---\n\n# Multiple-Choice Question Answering (\u0645\u062f\u0644 \u0628\u0631\u0627\u06cc \u067e\u0627\u0633\u062e \u0628\u0647 \u0633\u0648\u0627\u0644\u0627\u062a \u0686\u0647\u0627\u0631 \u062c\u0648\u0627\u0628\u06cc)\n\nThis is a mT5-based model for multiple-choice question answering. \nHere is an example of how you can run this model: \n\n```python \nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer\n\nmodel_size = \"large\"\nmodel_name = f\"persiannlp/mt5-{model_size}-parsinlu-arc-comqa-obqa-multiple-choice\"\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\n\n\ndef run_model(input_string, **generator_args):\n    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n    res = model.generate(input_ids, **generator_args)\n    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n    print(output)\n    return output\n\n\nrun_model(\"\u0648\u0633\u06cc\u0639 \u062a\u0631\u06cc\u0646 \u06a9\u0634\u0648\u0631 \u062c\u0647\u0627\u0646 \u06a9\u062f\u0627\u0645 \u0627\u0633\u062a\u061f <sep> \u0622\u0645\u0631\u06cc\u06a9\u0627 <sep> \u06a9\u0627\u0646\u0627\u062f\u0627 <sep> \u0631\u0648\u0633\u06cc\u0647 <sep> \u0686\u06cc\u0646\")\nrun_model(\"\u0637\u0627\u0645\u0639 \u06cc\u0639\u0646\u06cc \u061f <sep> \u0622\u0632\u0645\u0646\u062f <sep> \u062e\u0648\u0634 \u0634\u0627\u0646\u0633 <sep> \u0645\u062d\u062a\u0627\u062c <sep> \u0645\u0637\u0645\u0626\u0646\")\nrun_model(\n    \"\u0632\u0645\u06cc\u0646\u06cc \u0628\u0647 \u06f3\u06f1 \u0642\u0637\u0639\u0647 \u0645\u062a\u0633\u0627\u0648\u06cc \u0645\u0641\u0631\u0648\u0636 \u0634\u062f\u0647 \u0627\u0633\u062a \u0648 \u0647\u0631 \u0631\u0648\u0632 \u0645\u0633\u0627\u062d\u062a \u0622\u0645\u0627\u062f\u0647 \u0634\u062f\u0647 \u0628\u0631\u0627\u06cc \u0627\u062d\u062f\u0627\u062b\u060c \u062f\u0648 \u0628\u0631\u0627\u0628\u0631 \u0645\u0633\u0627\u062d\u062a \u0631\u0648\u0632 \u0642\u0628\u0644 \u0627\u0633\u062a.\u0627\u06af\u0631 \u067e\u0633 \u0627\u0632 (\u06f5 \u0631\u0648\u0632) \u062a\u0645\u0627\u0645 \u0632\u0645\u06cc\u0646 \u0622\u0645\u0627\u062f\u0647 \u0634\u062f\u0647 \u0628\u0627\u0634\u062f\u060c \u062f\u0631 \u0686\u0647 \u0631\u0648\u0632\u06cc \u06cc\u06a9 \u0642\u0637\u0639\u0647 \u0632\u0645\u06cc\u0646 \u0622\u0645\u0627\u062f\u0647 \u0634\u062f\u0647 <sep> \u0631\u0648\u0632 \u0627\u0648\u0644 <sep> \u0631\u0648\u0632 \u062f\u0648\u0645 <sep> \u0631\u0648\u0632 \u0633\u0648\u0645 <sep> \u0647\u06cc\u0686\u06a9\u062f\u0627\u0645\")\n\n```\n\n\nFor more details, visit this page:  \n", "qas": [{"id": "q1", "question": "What is the model architecture of persiannlp/mt5-large-parsinlu-arc-comqa-obqa-multiple-choice?", "answers": [{"text": "t5", "answer_start": 73, "answer_end": 74}]}]}]}, {"title": "philschmid/tf-distilbart-cnn-12-6-tradetheevent", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: philschmid/tf-distilbart-cnn-12-6-tradetheevent\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# philschmid/tf-distilbart-cnn-12-6-tradetheevent\n\nThis model is a fine-tuned version of [philschmid/tf-distilbart-cnn-12-6]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.6894\n- Validation Loss: 1.7245\n- Epoch: 4\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'inner_optimizer': {'class_name': 'AdamWeightDecay', 'config': {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 5.6e-05, 'decay_steps': 161440, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}}, 'dynamic': True, 'initial_scale': 32768.0, 'dynamic_growth_steps': 2000}\n- training_precision: mixed_float16\n\n### Training results\n\n Validation Loss \n:---------------:\n 1.5957          \n 1.5577          \n 1.6059          \n 1.6695          \n 1.7245          \n\n\n### Framework versions\n\n- Transformers 4.16.0.dev0\n- TensorFlow 2.7.0\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of philschmid/tf-distilbart-cnn-12-6-tradetheevent?", "answers": [{"text": "bart", "answer_start": 103, "answer_end": 106}]}]}]}, {"title": "philschmid/tf-distilbart-cnn-12-6", "paragraphs": [{"context": "---\nlanguage: en\ntags:\n- summarization\nlicense: apache-2.0\ndatasets:\n- cnn_dailymail\n- xsum\nthumbnail: \n---\n\n# This is an Tensorflow fork of [sshleifer/distilbart-cnn-12-6](\n\n### Usage\n\nThis checkpoint should be loaded into `BartForConditionalGeneration.from_pretrained`. See the [BART docs]( for more information.\n\n### Metrics for DistilBART models\n\n   MM Params    Speedup    Rouge-L |\n------------:----------:----------:|\n         222       2.54      33.37 |\n         230       1.73      35.73 |\n         255       2.16      36.39 |\n         268       1.68      36.61 |\n         406       1         36.50 |\n         306       1.68      36.99 |\n         406       1         30.63 |\n         255       1.78      30.00 |\n         306       1.24      30.59 |\n         230       2.09      29.70 |", "qas": [{"id": "q1", "question": "What is the model architecture of philschmid/tf-distilbart-cnn-12-6?", "answers": [{"text": "bart", "answer_start": 158, "answer_end": 161}]}, {"id": "q2", "question": "What is the model task of philschmid/tf-distilbart-cnn-12-6?", "answers": [{"text": "summarization", "answer_start": 25, "answer_end": 37}]}]}]}, {"title": "philschmid/tiny-distilbert-classification", "paragraphs": [{"context": "# Test model\n\n> ## This model is used to run tests for the Hugging Face DLCs ", "qas": []}]}, {"title": "phiyodr/bart-large-finetuned-squad2", "paragraphs": [{"context": "---\nlanguage: en\ntags:\n- pytorch\n- question-answering\ndatasets:\n- squad2\nmetrics:\n- exact\n- f1\nwidget:\n- text: \"What discipline did Winkelmann create?\"\n  context: \"Johann Joachim Winckelmann was a German art historian and archaeologist. He was a pioneering Hellenist who first articulated the difference between Greek, Greco-Roman and Roman art. The prophet and founding hero of modern archaeology, Winckelmann was one of the founders of scientific archaeology and first applied the categories of style on a large, systematic basis to the history of art.\"\n---\n\n# roberta-large-finetuned-squad2\n\n## Model description\n\nThis model is based on [facebook/bart-large]( and was finetuned on [SQuAD2.0]( The corresponding papers you can found [here (model)]( and [here (data)](\n\n\n## How to use\n\n```python\nfrom transformers.pipelines import pipeline\n\nmodel_name = \"phiyodr/bart-large-finetuned-squad2\"\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\ninputs = {\n    'question': 'What discipline did Winkelmann create?',\n    'context': 'Johann Joachim Winckelmann was a German art historian and archaeologist. He was a pioneering Hellenist who first articulated the difference between Greek, Greco-Roman and Roman art. \"The prophet and founding hero of modern archaeology\", Winckelmann was one of the founders of scientific archaeology and first applied the categories of style on a large, systematic basis to the history of art. '\n}\nnlp(inputs)\n```\n\n\n\n## Training procedure\n\n```\n{\n\t\"base_model\": \"facebook/bart-large\",\n\t\"do_lower_case\": True,\n\t\"learning_rate\": 3e-5,\n\t\"num_train_epochs\": 4,\n\t\"max_seq_length\": 384,\n\t\"doc_stride\": 128,\n\t\"max_query_length\": 64,\n\t\"batch_size\": 96 \n}\n```\n\n## Eval results\n\n- Data: [dev-v2.0.json](\n- Script: [evaluate-v2.0.py]( (original script from [here](\n\n```\n{\n  \"exact\": 81.96748926134929,\n  \"f1\": 85.93825235371045,\n  \"total\": 11873,\n  \"HasAns_exact\": 78.71120107962213,\n  \"HasAns_f1\": 86.6641144054667,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 85.21446593776282,\n  \"NoAns_f1\": 85.21446593776282,\n  \"NoAns_total\": 5945\n}\n\n```\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of phiyodr/bart-large-finetuned-squad2?", "answers": [{"text": "bart", "answer_start": 650, "answer_end": 653}]}, {"id": "q2", "question": "What is the model task of phiyodr/bart-large-finetuned-squad2?", "answers": [{"text": "question-answering", "answer_start": 35, "answer_end": 52}]}]}]}, {"title": "phiyodr/bert-large-finetuned-squad2", "paragraphs": [{"context": "---\nlanguage: en\ntags:\n- pytorch\n- question-answering\ndatasets:\n- squad2\nmetrics:\n- exact\n- f1\nwidget:\n- text: \"What discipline did Winkelmann create?\"\n  context: \"Johann Joachim Winckelmann was a German art historian and archaeologist. He was a pioneering Hellenist who first articulated the difference between Greek, Greco-Roman and Roman art. The prophet and founding hero of modern archaeology, Winckelmann was one of the founders of scientific archaeology and first applied the categories of style on a large, systematic basis to the history of art.\"\n---\n\n# bert-large-finetuned-squad2\n\n## Model description\n\nThis model is based on **[bert-large-uncased]( and was finetuned on **[SQuAD2.0]( The corresponding papers you can found [here (model)]( and [here (data)](\n\n\n## How to use\n\n```python\nfrom transformers.pipelines import pipeline\n\nmodel_name = \"phiyodr/bert-large-finetuned-squad2\"\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\ninputs = {\n    'question': 'What discipline did Winkelmann create?',\n    'context': 'Johann Joachim Winckelmann was a German art historian and archaeologist. He was a pioneering Hellenist who first articulated the difference between Greek, Greco-Roman and Roman art. \"The prophet and founding hero of modern archaeology\", Winckelmann was one of the founders of scientific archaeology and first applied the categories of style on a large, systematic basis to the history of art. '\n}\nnlp(inputs)\n```\n\n\n\n## Training procedure\n\n```\n{\n\t\"base_model\": \"bert-large-uncased\",\n\t\"do_lower_case\": True,\n\t\"learning_rate\": 3e-5,\n\t\"num_train_epochs\": 4,\n\t\"max_seq_length\": 384,\n\t\"doc_stride\": 128,\n\t\"max_query_length\": 64,\n\t\"batch_size\": 96 \n}\n```\n\n## Eval results\n\n- Data: [dev-v2.0.json](\n- Script: [evaluate-v2.0.py]( (original script from [here](\n\n```\n{\n  \"exact\": 76.22336393497852,\n  \"f1\": 79.72527570261339,\n  \"total\": 11873,\n  \"HasAns_exact\": 76.19770580296895,\n  \"HasAns_f1\": 83.21157193271408,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 76.24894869638352,\n  \"NoAns_f1\": 76.24894869638352,\n  \"NoAns_total\": 5945\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of phiyodr/bert-large-finetuned-squad2?", "answers": [{"text": "bert", "answer_start": 563, "answer_end": 566}]}, {"id": "q2", "question": "What is the model task of phiyodr/bert-large-finetuned-squad2?", "answers": [{"text": "question-answering", "answer_start": 35, "answer_end": 52}]}]}]}, {"title": "phongdtd/fb-vindata-vi-large", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- phongdtd/VinDataVLSP\n- generated_from_trainer\nmodel-index:\n- name: fb-vindata-vi-large\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# fb-vindata-vi-large\n\nThis model is a fine-tuned version of [facebook/wav2vec2-large-xlsr-53]( on the PHONGDTD/VINDATAVLSP - NA dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 2\n- total_train_batch_size: 8\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 200\n- num_epochs: 40.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of phongdtd/fb-vindata-vi-large?", "answers": [{"text": "wav2vec2", "answer_start": 428, "answer_end": 435}]}, {"id": "q2", "question": "What is the model task of phongdtd/fb-vindata-vi-large?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 32, "answer_end": 59}]}]}]}, {"title": "phongdtd/fb-youtube-vi-large", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- phongdtd/youtube_casual_audio\n- generated_from_trainer\nmodel-index:\n- name: fb-youtube-vi-large\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# fb-youtube-vi-large\n\nThis model is a fine-tuned version of [facebook/wav2vec2-large-xlsr-53]( on the PHONGDTD/YOUTUBE_CASUAL_AUDIO - NA dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 2\n- total_train_batch_size: 8\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 200\n- num_epochs: 25.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of phongdtd/fb-youtube-vi-large?", "answers": [{"text": "wav2vec2", "answer_start": 437, "answer_end": 444}]}, {"id": "q2", "question": "What is the model task of phongdtd/fb-youtube-vi-large?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 32, "answer_end": 59}]}, {"id": "q3", "question": "What is the model category of phongdtd/fb-youtube-vi-large?", "answers": [{"text": "audio", "answer_start": 87, "answer_end": 91}]}]}]}, {"title": "phongdtd/wavLM-VLSP-vi", "paragraphs": [{"context": "---\ntags:\n- automatic-speech-recognition\n- phongdtd/VinDataVLSP\n- generated_from_trainer\nmodel-index:\n- name: wavLM-VLSP-vi\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wavLM-VLSP-vi\n\nThis model is a fine-tuned version of [microsoft/wavlm-base-plus]( on the PHONGDTD/VINDATAVLSP - NA dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 45.8892\n- Wer: 0.9999\n- Cer: 0.9973\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 4\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 2\n- total_train_batch_size: 8\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 50.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Cer    |\n:-----::---------------::------:|\n 9.41   3.4480           0.9974 |\n 18.81  3.4514           0.9974 |\n 28.22  3.8732           0.9974 |\n 37.62  22.5457          0.9973 |\n 47.03  45.8892          0.9973 |\n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of phongdtd/wavLM-VLSP-vi?", "answers": [{"text": "wavlm", "answer_start": 397, "answer_end": 401}]}, {"id": "q2", "question": "What is the model task of phongdtd/wavLM-VLSP-vi?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 12, "answer_end": 39}]}]}]}, {"title": "phongdtd/wavlm-vindata-demo-dist", "paragraphs": [{"context": "---\ntags:\n- automatic-speech-recognition\n- phongdtd/VinDataVLSP\n- generated_from_trainer\ndatasets:\n- vin_data_vlsp\nmodel-index:\n- name: wavlm-vindata-demo-dist\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wavlm-vindata-demo-dist\n\nThis model is a fine-tuned version of [microsoft/wavlm-base]( on the PHONGDTD/VINDATAVLSP - NA dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 3.4439\n- Wer: 1.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 1\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 2\n- total_train_batch_size: 2\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 15.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.01   3.8768          \n 0.01   3.4611          \n 0.02   3.4557          \n 0.03   3.4567          \n 0.04   3.4631          \n 0.04   3.4651          \n 0.05   3.4917          \n 0.06   3.4680          \n 0.07   3.4518          \n 0.07   3.4506          \n 0.08   3.4474          \n 0.09   3.4684          \n 0.09   3.4465          \n 0.1    3.4723          \n 0.11   3.4732          \n 0.12   3.4416          \n 0.12   3.4481          \n 0.13   3.4570          \n 0.14   3.4448          \n 0.14   3.4416          \n 0.15   3.4455          \n 0.16   3.4447          \n 0.17   3.4512          \n 0.17   3.4484          \n 0.18   3.4435          \n 0.19   3.4530          \n 0.2    3.4466          \n 0.2    3.4463          \n 0.21   3.4418          \n 0.22   3.4447          \n 0.22   3.4715          \n 0.23   3.4437          \n 0.24   3.4910          \n 0.25   3.4574          \n 0.25   3.4607          \n 0.26   3.4421          \n 0.27   3.4481          \n 0.28   3.4411          \n 0.28   3.4422          \n 0.29   3.4659          \n 0.3    3.4519          \n 0.3    3.4827          \n 0.31   3.4632          \n 0.32   3.4480          \n 0.33   3.4404          \n 0.33   3.4633          \n 0.34   3.4439          \n 0.35   3.4587          \n 0.35   3.4520          \n 0.36   3.4450          \n 0.37   3.4577          \n 0.38   3.4443          \n 0.38   3.4505          \n 0.39   3.4418          \n 0.4    3.4425          \n 0.41   3.4581          \n 0.41   3.4404          \n 0.42   3.4596          \n 0.43   3.4401          \n 0.43   3.4413          \n 0.44   3.4452          \n 0.45   3.4481          \n 0.46   3.4420          \n 0.46   3.4494          \n 0.47   3.4477          \n 0.48   3.4382          \n 0.49   3.4580          \n 0.49   3.4767          \n 0.5    3.4476          \n 0.51   3.4557          \n 0.51   3.4438          \n 0.52   3.4445          \n 0.53   3.4463          \n 0.54   3.4482          \n 0.54   3.4422          \n 0.55   3.4505          \n 0.56   3.4461          \n 0.56   3.4511          \n 0.57   3.4389          \n 0.58   3.4563          \n 0.59   3.4601          \n 0.59   3.4439          \n 0.6    3.4444          \n 0.61   3.4629          \n 0.62   3.4389          \n 0.62   3.4427          \n 0.63   3.4521          \n 0.64   3.4489          \n 0.64   3.4478          \n 0.65   3.4510          \n 0.66   3.4411          \n 0.67   3.4416          \n 0.67   3.4643          \n 0.68   3.4587          \n 0.69   3.4799          \n 0.69   3.4490          \n 0.7    3.4532          \n 0.71   3.4427          \n 0.72   3.4492          \n 0.72   3.4497          \n 0.73   3.4476          \n 0.74   3.4539          \n 0.75   3.4547          \n 0.75   3.4663          \n 0.76   3.4401          \n 0.77   3.4404          \n 0.77   3.4426          \n 0.78   3.4439          \n 0.79   3.4446          \n 0.8    3.4456          \n 0.8    3.4388          \n 0.81   3.4418          \n 0.82   3.4392          \n 0.83   3.4494          \n 0.83   3.4572          \n 0.84   3.4377          \n 0.85   3.4533          \n 0.85   3.4600          \n 0.86   3.4673          \n 0.87   3.4407          \n 0.88   3.4427          \n 0.88   3.4394          \n 0.89   3.4407          \n 0.9    3.4415          \n 0.9    3.4454          \n 0.91   3.4379          \n 0.92   3.4423          \n 0.93   3.4635          \n 0.93   3.4411          \n 0.94   3.4396          \n 0.95   3.4458          \n 0.96   3.4398          \n 0.96   3.4514          \n 0.97   3.4437          \n 0.98   3.4623          \n 0.98   3.4512          \n 0.99   3.4493          \n 1.0    3.4597          \n 1.01   3.4813          \n 1.01   3.4510          \n 1.02   3.4389          \n 1.03   3.4519          \n 1.04   3.4399          \n 1.04   3.4378          \n 1.05   3.4476          \n 1.06   3.4646          \n 1.06   3.4520          \n 1.07   3.4575          \n 1.08   3.4443          \n 1.09   3.4434          \n 1.09   3.4448          \n 1.1    3.4560          \n 1.11   3.4405          \n 1.11   3.4408          \n 1.12   3.4395          \n 1.13   3.4488          \n 1.14   3.4482          \n 1.14   3.4621          \n 1.15   3.4433          \n 1.16   3.4434          \n 1.17   3.4542          \n 1.17   3.4388          \n 1.18   3.4577          \n 1.19   3.4510          \n 1.19   3.4434          \n 1.2    3.4410          \n 1.21   3.4507          \n 1.22   3.4718          \n 1.22   3.4439          \n 1.23   3.4471          \n 1.24   3.4435          \n 1.25   3.4432          \n 1.25   3.4472          \n 1.26   3.4388          \n 1.27   3.4444          \n 1.27   3.4438          \n 1.28   3.4406          \n 1.29   3.4573          \n 1.3    3.4580          \n 1.3    3.4455          \n 1.31   3.4606          \n 1.32   3.4378          \n 1.32   3.4432          \n 1.33   3.4458          \n 1.34   3.4617          \n 1.35   3.4549          \n 1.35   3.4557          \n 1.36   3.4462          \n 1.37   3.4606          \n 1.38   3.4458          \n 1.38   3.4712          \n 1.39   3.4483          \n 1.4    3.4455          \n 1.4    3.4379          \n 1.41   3.4477          \n 1.42   3.4478          \n 1.43   3.4492          \n 1.43   3.4441          \n 1.44   3.4385          \n 1.45   3.4437          \n 1.46   3.4644          \n 1.46   3.4529          \n 1.47   3.4524          \n 1.48   3.4551          \n 1.48   3.4433          \n 1.49   3.4448          \n 1.5    3.4590          \n 1.51   3.4720          \n 1.51   3.4461          \n 1.52   3.4541          \n 1.53   3.4556          \n 1.53   3.4438          \n 1.54   3.4422          \n 1.55   3.4637          \n 1.56   3.4435          \n 1.56   3.4434          \n 1.57   3.4675          \n 1.58   3.4565          \n 1.59   3.4538          \n 1.59   3.4492          \n 1.6    3.4381          \n 1.61   3.4558          \n 1.61   3.4484          \n 1.62   3.4574          \n 1.63   3.4498          \n 1.64   3.4384          \n 1.64   3.4503          \n 1.65   3.4578          \n 1.66   3.4563          \n 1.67   3.4564          \n 1.67   3.4490          \n 1.68   3.4403          \n 1.69   3.4409          \n 1.69   3.4534          \n 1.7    3.4486          \n 1.71   3.4413          \n 1.72   3.4694          \n 1.72   3.4444          \n 1.73   3.4505          \n 1.74   3.4712          \n 1.74   3.4552          \n 1.75   3.4511          \n 1.76   3.4368          \n 1.77   3.4517          \n 1.77   3.4475          \n 1.78   3.4413          \n 1.79   3.4482          \n 1.8    3.4473          \n 1.8    3.4435          \n 1.81   3.4439          \n 1.82   3.4382          \n 1.82   3.4396          \n 1.83   3.4577          \n 1.84   3.4458          \n 1.85   3.4436          \n 1.85   3.4435          \n 1.86   3.4428          \n 1.87   3.4638          \n 1.88   3.4450          \n 1.88   3.4616          \n 1.89   3.4391          \n 1.9    3.4375          \n 1.9    3.4533          \n 1.91   3.4538          \n 1.92   3.4460          \n 1.93   3.4443          \n 1.93   3.4392          \n 1.94   3.4609          \n 1.95   3.4388          \n 1.95   3.4434          \n 1.96   3.4653          \n 1.97   3.4465          \n 1.98   3.4535          \n 1.98   3.4442          \n 1.99   3.4491          \n 2.0    3.4477          \n 2.01   3.4540          \n 2.01   3.4602          \n 2.02   3.4578          \n 2.03   3.4494          \n 2.03   3.4570          \n 2.04   3.4504          \n 2.05   3.4399          \n 2.06   3.4464          \n 2.06   3.4473          \n 2.07   3.4484          \n 2.08   3.4413          \n 2.08   3.4411          \n 2.09   3.4406          \n 2.1    3.4461          \n 2.11   3.4409          \n 2.11   3.4556          \n 2.12   3.4545          \n 2.13   3.4410          \n 2.14   3.4370          \n 2.14   3.4407          \n 2.15   3.4390          \n 2.16   3.4386          \n 2.16   3.4600          \n 2.17   3.4492          \n 2.18   3.4406          \n 2.19   3.4461          \n 2.19   3.4478          \n 2.2    3.4482          \n 2.21   3.4424          \n 2.22   3.4432          \n 2.22   3.4453          \n 2.23   3.4457          \n 2.24   3.4605          \n 2.24   3.4700          \n 2.25   3.4558          \n 2.26   3.4396          \n 2.27   3.4411          \n 2.27   3.4473          \n 2.28   3.4400          \n 2.29   3.4469          \n 2.29   3.4499          \n 2.3    3.4388          \n 2.31   3.4503          \n 2.32   3.4429          \n 2.32   3.4617          \n 2.33   3.4487          \n 2.34   3.4678          \n 2.35   3.4441          \n 2.35   3.4433          \n 2.36   3.4514          \n 2.37   3.4520          \n 2.37   3.4544          \n 2.38   3.4444          \n 2.39   3.4450          \n 2.4    3.4454          \n 2.4    3.4541          \n 2.41   3.4386          \n 2.42   3.4518          \n 2.43   3.4554          \n 2.43   3.4412          \n 2.44   3.4434          \n 2.45   3.4393          \n 2.45   3.4375          \n 2.46   3.4402          \n 2.47   3.4389          \n 2.48   3.4416          \n 2.48   3.4571          \n 2.49   3.4560          \n 2.5    3.4482          \n 2.5    3.4640          \n 2.51   3.4538          \n 2.52   3.4555          \n 2.53   3.4524          \n 2.53   3.4448          \n 2.54   3.4503          \n 2.55   3.4538          \n 2.56   3.4443          \n 2.56   3.4391          \n 2.57   3.4411          \n 2.58   3.4488          \n 2.58   3.4400          \n 2.59   3.4390          \n 2.6    3.4455          \n 2.61   3.4441          \n 2.61   3.4414          \n 2.62   3.4421          \n 2.63   3.4572          \n 2.64   3.4419          \n 2.64   3.4407          \n 2.65   3.4446          \n 2.66   3.4396          \n 2.66   3.4529          \n 2.67   3.4380          \n 2.68   3.4654          \n 2.69   3.4386          \n 2.69   3.4652          \n 2.7    3.4506          \n 2.71   3.4427          \n 2.71   3.4465          \n 2.72   3.4470          \n 2.73   3.4480          \n 2.74   3.4456          \n 2.74   3.4431          \n 2.75   3.4374          \n 2.76   3.4447          \n 2.77   3.4476          \n 2.77   3.4380          \n 2.78   3.4526          \n 2.79   3.4428          \n 2.79   3.4424          \n 2.8    3.4479          \n 2.81   3.4580          \n 2.82   3.4585          \n 2.82   3.4567          \n 2.83   3.4377          \n 2.84   3.4413          \n 2.85   3.4548          \n 2.85   3.4411          \n 2.86   3.4552          \n 2.87   3.4471          \n 2.87   3.4378          \n 2.88   3.4605          \n 2.89   3.4489          \n 2.9    3.4644          \n 2.9    3.4514          \n 2.91   3.4543          \n 2.92   3.4450          \n 2.92   3.4459          \n 2.93   3.4481          \n 2.94   3.4554          \n 2.95   3.4486          \n 2.95   3.4415          \n 2.96   3.4521          \n 2.97   3.4395          \n 2.98   3.4622          \n 2.98   3.4407          \n 2.99   3.4422          \n 3.0    3.4383          \n 3.0    3.4482          \n 3.01   3.4366          \n 3.02   3.4376          \n 3.03   3.4511          \n 3.03   3.4397          \n 3.04   3.4486          \n 3.05   3.4626          \n 3.06   3.4462          \n 3.06   3.4435          \n 3.07   3.4450          \n 3.08   3.4394          \n 3.08   3.4539          \n 3.09   3.4443          \n 3.1    3.4384          \n 3.11   3.4625          \n 3.11   3.4419          \n 3.12   3.4653          \n 3.13   3.4380          \n 3.13   3.4534          \n 3.14   3.4448          \n 3.15   3.4665          \n 3.16   3.4473          \n 3.16   3.4509          \n 3.17   3.4493          \n 3.18   3.4384          \n 3.19   3.4388          \n 3.19   3.4491          \n 3.2    3.4404          \n 3.21   3.4426          \n 3.21   3.4690          \n 3.22   3.4378          \n 3.23   3.4395          \n 3.24   3.4431          \n 3.24   3.4557          \n 3.25   3.4489          \n 3.26   3.4393          \n 3.27   3.4463          \n 3.27   3.4505          \n 3.28   3.4475          \n 3.29   3.4430          \n 3.29   3.4417          \n 3.3    3.4503          \n 3.31   3.4414          \n 3.32   3.4485          \n 3.32   3.4477          \n 3.33   3.4508          \n 3.34   3.4440          \n 3.34   3.4382          \n 3.35   3.4524          \n 3.36   3.4451          \n 3.37   3.4526          \n 3.37   3.4462          \n 3.38   3.4432          \n 3.39   3.4419          \n 3.4    3.4405          \n 3.4    3.4434          \n 3.41   3.4444          \n 3.42   3.4501          \n 3.42   3.4498          \n 3.43   3.4505          \n 3.44   3.4448          \n 3.45   3.4450          \n 3.45   3.4493          \n 3.46   3.4466          \n 3.47   3.4385          \n 3.47   3.4427          \n 3.48   3.4667          \n 3.49   3.4440          \n 3.5    3.4427          \n 3.5    3.4398          \n 3.51   3.4421          \n 3.52   3.4517          \n 3.53   3.4429          \n 3.53   3.4422          \n 3.54   3.4439          \n 3.55   3.4544          \n 3.55   3.4415          \n 3.56   3.4474          \n 3.57   3.4498          \n 3.58   3.4554          \n 3.58   3.4393          \n 3.59   3.4594          \n 3.6    3.4514          \n 3.61   3.4514          \n 3.61   3.4462          \n 3.62   3.4466          \n 3.63   3.4575          \n 3.63   3.4392          \n 3.64   3.4556          \n 3.65   3.4498          \n 3.66   3.4569          \n 3.66   3.4463          \n 3.67   3.4445          \n 3.68   3.4475          \n 3.68   3.4370          \n 3.69   3.4476          \n 3.7    3.4382          \n 3.71   3.4526          \n 3.71   3.4414          \n 3.72   3.4386          \n 3.73   3.4458          \n 3.74   3.4450          \n 3.74   3.4493          \n 3.75   3.4369          \n 3.76   3.4472          \n 3.76   3.4580          \n 3.77   3.4469          \n 3.78   3.4423          \n 3.79   3.4411          \n 3.79   3.4456          \n 3.8    3.4389          \n 3.81   3.4403          \n 3.82   3.4458          \n 3.82   3.4447          \n 3.83   3.4448          \n 3.84   3.4456          \n 3.84   3.4427          \n 3.85   3.4379          \n 3.86   3.4482          \n 3.87   3.4393          \n 3.87   3.4474          \n 3.88   3.4573          \n 3.89   3.4475          \n 3.89   3.4659          \n 3.9    3.4411          \n 3.91   3.4446          \n 3.92   3.4406          \n 3.92   3.4422          \n 3.93   3.4383          \n 3.94   3.4423          \n 3.95   3.4377          \n 3.95   3.4433          \n 3.96   3.4457          \n 3.97   3.4420          \n 3.97   3.4501          \n 3.98   3.4501          \n 3.99   3.4511          \n 4.0    3.4409          \n 4.0    3.4527          \n 4.01   3.4436          \n 4.02   3.4458          \n 4.03   3.4409          \n 4.03   3.4378          \n 4.04   3.4558          \n 4.05   3.4523          \n 4.05   3.4446          \n 4.06   3.4488          \n 4.07   3.4497          \n 4.08   3.4443          \n 4.08   3.4547          \n 4.09   3.4476          \n 4.1    3.4459          \n 4.1    3.4461          \n 4.11   3.4407          \n 4.12   3.4498          \n 4.13   3.4535          \n 4.13   3.4470          \n 4.14   3.4461          \n 4.15   3.4405          \n 4.16   3.4582          \n 4.16   3.4462          \n 4.17   3.4507          \n 4.18   3.4476          \n 4.18   3.4412          \n 4.19   3.4605          \n 4.2    3.4458          \n 4.21   3.4368          \n 4.21   3.4464          \n 4.22   3.4601          \n 4.23   3.4449          \n 4.24   3.4544          \n 4.24   3.4412          \n 4.25   3.4408          \n 4.26   3.4463          \n 4.26   3.4402          \n 4.27   3.4537          \n 4.28   3.4435          \n 4.29   3.4491          \n 4.29   3.4480          \n 4.3    3.4403          \n 4.31   3.4433          \n 4.31   3.4419          \n 4.32   3.4420          \n 4.33   3.4477          \n 4.34   3.4557          \n 4.34   3.4421          \n 4.35   3.4470          \n 4.36   3.4457          \n 4.37   3.4416          \n 4.37   3.4415          \n 4.38   3.4398          \n 4.39   3.4532          \n 4.39   3.4465          \n 4.4    3.4498          \n 4.41   3.4542          \n 4.42   3.4436          \n 4.42   3.4405          \n 4.43   3.4467          \n 4.44   3.4436          \n 4.45   3.4473          \n 4.45   3.4400          \n 4.46   3.4536          \n 4.47   3.4489          \n 4.47   3.4443          \n 4.48   3.4472          \n 4.49   3.4421          \n 4.5    3.4480          \n 4.5    3.4435          \n 4.51   3.4399          \n 4.52   3.4457          \n 4.52   3.4453          \n 4.53   3.4564          \n 4.54   3.4392          \n 4.55   3.4491          \n 4.55   3.4400          \n 4.56   3.4454          \n 4.57   3.4411          \n 4.58   3.4464          \n 4.58   3.4454          \n 4.59   3.4437          \n 4.6    3.4382          \n 4.6    3.4478          \n 4.61   3.4439          \n 4.62   3.4400          \n 4.63   3.4542          \n 4.63   3.4548          \n 4.64   3.4442          \n 4.65   3.4396          \n 4.66   3.4444          \n 4.66   3.4642          \n 4.67   3.4397          \n 4.68   3.4529          \n 4.68   3.4423          \n 4.69   3.4376          \n 4.7    3.4396          \n 4.71   3.4419          \n 4.71   3.4419          \n 4.72   3.4392          \n 4.73   3.4369          \n 4.73   3.4461          \n 4.74   3.4442          \n 4.75   3.4433          \n 4.76   3.4529          \n 4.76   3.4446          \n 4.77   3.4507          \n 4.78   3.4468          \n 4.79   3.4400          \n 4.79   3.4573          \n 4.8    3.4400          \n 4.81   3.4374          \n 4.81   3.4379          \n 4.82   3.4364          \n 4.83   3.4444          \n 4.84   3.4380          \n 4.84   3.4595          \n 4.85   3.4433          \n 4.86   3.4550          \n 4.86   3.4374          \n 4.87   3.4423          \n 4.88   3.4398          \n 4.89   3.4425          \n 4.89   3.4492          \n 4.9    3.4512          \n 4.91   3.4512          \n 4.92   3.4493          \n 4.92   3.4413          \n 4.93   3.4474          \n 4.94   3.4426          \n 4.94   3.4457          \n 4.95   3.4437          \n 4.96   3.4438          \n 4.97   3.4477          \n 4.97   3.4508          \n 4.98   3.4453          \n 4.99   3.4560          \n 5.0    3.4457          \n 5.0    3.4575          \n 5.01   3.4436          \n 5.02   3.4494          \n 5.02   3.4432          \n 5.03   3.4371          \n 5.04   3.4426          \n 5.05   3.4383          \n 5.05   3.4472          \n 5.06   3.4437          \n 5.07   3.4544          \n 5.07   3.4459          \n 5.08   3.4552          \n 5.09   3.4513          \n 5.1    3.4446          \n 5.1    3.4424          \n 5.11   3.4482          \n 5.12   3.4514          \n 5.13   3.4491          \n 5.13   3.4573          \n 5.14   3.4420          \n 5.15   3.4469          \n 5.15   3.4467          \n 5.16   3.4479          \n 5.17   3.4426          \n 5.18   3.4419          \n 5.18   3.4434          \n 5.19   3.4472          \n 5.2    3.4443          \n 5.21   3.4413          \n 5.21   3.4491          \n 5.22   3.4423          \n 5.23   3.4420          \n 5.23   3.4415          \n 5.24   3.4453          \n 5.25   3.4534          \n 5.26   3.4396          \n 5.26   3.4472          \n 5.27   3.4448          \n 5.28   3.4571          \n 5.28   3.4440          \n 5.29   3.4507          \n 5.3    3.4430          \n 5.31   3.4443          \n 5.31   3.4525          \n 5.32   3.4520          \n 5.33   3.4402          \n 5.34   3.4383          \n 5.34   3.4492          \n 5.35   3.4428          \n 5.36   3.4534          \n 5.36   3.4603          \n 5.37   3.4512          \n 5.38   3.4474          \n 5.39   3.4402          \n 5.39   3.4576          \n 5.4    3.4518          \n 5.41   3.4526          \n 5.42   3.4380          \n 5.42   3.4525          \n 5.43   3.4483          \n 5.44   3.4389          \n 5.44   3.4511          \n 5.45   3.4498          \n 5.46   3.4496          \n 5.47   3.4502          \n 5.47   3.4400          \n 5.48   3.4461          \n 5.49   3.4419          \n 5.49   3.4470          \n 5.5    3.4430          \n 5.51   3.4479          \n 5.52   3.4469          \n 5.52   3.4420          \n 5.53   3.4475          \n 5.54   3.4431          \n 5.55   3.4392          \n 5.55   3.4434          \n 5.56   3.4485          \n 5.57   3.4453          \n 5.57   3.4456          \n 5.58   3.4471          \n 5.59   3.4434          \n 5.6    3.4579          \n 5.6    3.4459          \n 5.61   3.4450          \n 5.62   3.4446          \n 5.63   3.4458          \n 5.63   3.4417          \n 5.64   3.4582          \n 5.65   3.4382          \n 5.65   3.4399          \n 5.66   3.4389          \n 5.67   3.4528          \n 5.68   3.4411          \n 5.68   3.4416          \n 5.69   3.4501          \n 5.7    3.4469          \n 5.7    3.4499          \n 5.71   3.4489          \n 5.72   3.4456          \n 5.73   3.4457          \n 5.73   3.4480          \n 5.74   3.4398          \n 5.75   3.4454          \n 5.76   3.4511          \n 5.76   3.4480          \n 5.77   3.4425          \n 5.78   3.4522          \n 5.78   3.4398          \n 5.79   3.4395          \n 5.8    3.4401          \n 5.81   3.4414          \n 5.81   3.4576          \n 5.82   3.4524          \n 5.83   3.4445          \n 5.84   3.4437          \n 5.84   3.4539          \n 5.85   3.4466          \n 5.86   3.4471          \n 5.86   3.4367          \n 5.87   3.4395          \n 5.88   3.4395          \n 5.89   3.4488          \n 5.89   3.4626          \n 5.9    3.4393          \n 5.91   3.4430          \n 5.91   3.4421          \n 5.92   3.4458          \n 5.93   3.4411          \n 5.94   3.4402          \n 5.94   3.4524          \n 5.95   3.4465          \n 5.96   3.4429          \n 5.97   3.4535          \n 5.97   3.4445          \n 5.98   3.4459          \n 5.99   3.4475          \n 5.99   3.4455          \n 6.0    3.4454          \n 6.01   3.4569          \n 6.02   3.4562          \n 6.02   3.4452          \n 6.03   3.4684          \n 6.04   3.4555          \n 6.05   3.4429          \n 6.05   3.4436          \n 6.06   3.4383          \n 6.07   3.4421          \n 6.07   3.4444          \n 6.08   3.4461          \n 6.09   3.4441          \n 6.1    3.4423          \n 6.1    3.4381          \n 6.11   3.4458          \n 6.12   3.4491          \n 6.12   3.4476          \n 6.13   3.4443          \n 6.14   3.4391          \n 6.15   3.4447          \n 6.15   3.4410          \n 6.16   3.4471          \n 6.17   3.4516          \n 6.18   3.4458          \n 6.18   3.4441          \n 6.19   3.4384          \n 6.2    3.4457          \n 6.2    3.4542          \n 6.21   3.4572          \n 6.22   3.4442          \n 6.23   3.4430          \n 6.23   3.4410          \n 6.24   3.4521          \n 6.25   3.4441          \n 6.25   3.4386          \n 6.26   3.4507          \n 6.27   3.4488          \n 6.28   3.4477          \n 6.28   3.4421          \n 6.29   3.4458          \n 6.3    3.4536          \n 6.31   3.4452          \n 6.31   3.4491          \n 6.32   3.4550          \n 6.33   3.4414          \n 6.33   3.4407          \n 6.34   3.4409          \n 6.35   3.4402          \n 6.36   3.4422          \n 6.36   3.4480          \n 6.37   3.4380          \n 6.38   3.4449          \n 6.39   3.4443          \n 6.39   3.4471          \n 6.4    3.4370          \n 6.41   3.4455          \n 6.41   3.4416          \n 6.42   3.4424          \n 6.43   3.4485          \n 6.44   3.4421          \n 6.44   3.4442          \n 6.45   3.4478          \n 6.46   3.4493          \n 6.46   3.4422          \n 6.47   3.4484          \n 6.48   3.4493          \n 6.49   3.4404          \n 6.49   3.4427          \n 6.5    3.4409          \n 6.51   3.4435          \n 6.52   3.4442          \n 6.52   3.4389          \n 6.53   3.4461          \n 6.54   3.4469          \n 6.54   3.4456          \n 6.55   3.4600          \n 6.56   3.4541          \n 6.57   3.4411          \n 6.57   3.4448          \n 6.58   3.4437          \n 6.59   3.4401          \n 6.6    3.4412          \n 6.6    3.4383          \n 6.61   3.4468          \n 6.62   3.4410          \n 6.62   3.4429          \n 6.63   3.4501          \n 6.64   3.4466          \n 6.65   3.4507          \n 6.65   3.4440          \n 6.66   3.4431          \n 6.67   3.4421          \n 6.67   3.4477          \n 6.68   3.4384          \n 6.69   3.4379          \n 6.7    3.4444          \n 6.7    3.4525          \n 6.71   3.4529          \n 6.72   3.4386          \n 6.73   3.4421          \n 6.73   3.4421          \n 6.74   3.4476          \n 6.75   3.4479          \n 6.75   3.4433          \n 6.76   3.4455          \n 6.77   3.4466          \n 6.78   3.4444          \n 6.78   3.4389          \n 6.79   3.4411          \n 6.8    3.4447          \n 6.81   3.4432          \n 6.81   3.4473          \n 6.82   3.4468          \n 6.83   3.4379          \n 6.83   3.4394          \n 6.84   3.4433          \n 6.85   3.4431          \n 6.86   3.4385          \n 6.86   3.4435          \n 6.87   3.4457          \n 6.88   3.4401          \n 6.88   3.4526          \n 6.89   3.4456          \n 6.9    3.4419          \n 6.91   3.4406          \n 6.91   3.4582          \n 6.92   3.4446          \n 6.93   3.4538          \n 6.94   3.4380          \n 6.94   3.4385          \n 6.95   3.4476          \n 6.96   3.4376          \n 6.96   3.4442          \n 6.97   3.4419          \n 6.98   3.4444          \n 6.99   3.4496          \n 6.99   3.4499          \n 7.0    3.4408          \n 7.01   3.4395          \n 7.02   3.4373          \n 7.02   3.4431          \n 7.03   3.4419          \n 7.04   3.4411          \n 7.04   3.4413          \n 7.05   3.4389          \n 7.06   3.4447          \n 7.07   3.4418          \n 7.07   3.4448          \n 7.08   3.4479          \n 7.09   3.4522          \n 7.09   3.4465          \n 7.1    3.4444          \n 7.11   3.4380          \n 7.12   3.4413          \n 7.12   3.4519          \n 7.13   3.4447          \n 7.14   3.4424          \n 7.15   3.4403          \n 7.15   3.4469          \n 7.16   3.4477          \n 7.17   3.4437          \n 7.17   3.4376          \n 7.18   3.4397          \n 7.19   3.4414          \n 7.2    3.4438          \n 7.2    3.4463          \n 7.21   3.4558          \n 7.22   3.4420          \n 7.23   3.4485          \n 7.23   3.4416          \n 7.24   3.4375          \n 7.25   3.4437          \n 7.25   3.4438          \n 7.26   3.4448          \n 7.27   3.4414          \n 7.28   3.4464          \n 7.28   3.4456          \n 7.29   3.4494          \n 7.3    3.4554          \n 7.3    3.4499          \n 7.31   3.4394          \n 7.32   3.4396          \n 7.33   3.4454          \n 7.33   3.4484          \n 7.34   3.4416          \n 7.35   3.4433          \n 7.36   3.4482          \n 7.36   3.4420          \n 7.37   3.4428          \n 7.38   3.4455          \n 7.38   3.4481          \n 7.39   3.4414          \n 7.4    3.4510          \n 7.41   3.4375          \n 7.41   3.4522          \n 7.42   3.4400          \n 7.43   3.4413          \n 7.44   3.4472          \n 7.44   3.4452          \n 7.45   3.4418          \n 7.46   3.4496          \n 7.46   3.4498          \n 7.47   3.4465          \n 7.48   3.4488          \n 7.49   3.4486          \n 7.49   3.4440          \n 7.5    3.4534          \n 7.51   3.4502          \n 7.51   3.4429          \n 7.52   3.4442          \n 7.53   3.4516          \n 7.54   3.4469          \n 7.54   3.4545          \n 7.55   3.4449          \n 7.56   3.4479          \n 7.57   3.4424          \n 7.57   3.4507          \n 7.58   3.4437          \n 7.59   3.4403          \n 7.59   3.4432          \n 7.6    3.4423          \n 7.61   3.4418          \n 7.62   3.4398          \n 7.62   3.4497          \n 7.63   3.4429          \n 7.64   3.4410          \n 7.64   3.4437          \n 7.65   3.4457          \n 7.66   3.4404          \n 7.67   3.4458          \n 7.67   3.4535          \n 7.68   3.4436          \n 7.69   3.4470          \n 7.7    3.4410          \n 7.7    3.4525          \n 7.71   3.4513          \n 7.72   3.4381          \n 7.72   3.4395          \n 7.73   3.4426          \n 7.74   3.4508          \n 7.75   3.4464          \n 7.75   3.4365          \n 7.76   3.4496          \n 7.77   3.4543          \n 7.78   3.4490          \n 7.78   3.4404          \n 7.79   3.4492          \n 7.8    3.4448          \n 7.8    3.4465          \n 7.81   3.4424          \n 7.82   3.4446          \n 7.83   3.4432          \n 7.83   3.4460          \n 7.84   3.4499          \n 7.85   3.4429          \n 7.85   3.4398          \n 7.86   3.4376          \n 7.87   3.4428          \n 7.88   3.4492          \n 7.88   3.4414          \n 7.89   3.4506          \n 7.9    3.4421          \n 7.91   3.4389          \n 7.91   3.4444          \n 7.92   3.4464          \n 7.93   3.4379          \n 7.93   3.4401          \n 7.94   3.4437          \n 7.95   3.4455          \n 7.96   3.4447          \n 7.96   3.4404          \n 7.97   3.4520          \n 7.98   3.4409          \n 7.99   3.4430          \n 7.99   3.4430          \n 8.0    3.4412          \n 8.01   3.4440          \n 8.01   3.4456          \n 8.02   3.4518          \n 8.03   3.4411          \n 8.04   3.4430          \n 8.04   3.4394          \n 8.05   3.4425          \n 8.06   3.4422          \n 8.06   3.4458          \n 8.07   3.4429          \n 8.08   3.4438          \n 8.09   3.4480          \n 8.09   3.4434          \n 8.1    3.4417          \n 8.11   3.4404          \n 8.12   3.4442          \n 8.12   3.4433          \n 8.13   3.4403          \n 8.14   3.4464          \n 8.14   3.4454          \n 8.15   3.4421          \n 8.16   3.4403          \n 8.17   3.4445          \n 8.17   3.4457          \n 8.18   3.4479          \n 8.19   3.4413          \n 8.2    3.4415          \n 8.2    3.4414          \n 8.21   3.4502          \n 8.22   3.4507          \n 8.22   3.4455          \n 8.23   3.4540          \n 8.24   3.4473          \n 8.25   3.4431          \n 8.25   3.4511          \n 8.26   3.4434          \n 8.27   3.4435          \n 8.27   3.4415          \n 8.28   3.4472          \n 8.29   3.4469          \n 8.3    3.4427          \n 8.3    3.4433          \n 8.31   3.4474          \n 8.32   3.4475          \n 8.33   3.4394          \n 8.33   3.4524          \n 8.34   3.4438          \n 8.35   3.4447          \n 8.35   3.4493          \n 8.36   3.4507          \n 8.37   3.4384          \n 8.38   3.4538          \n 8.38   3.4408          \n 8.39   3.4446          \n 8.4    3.4445          \n 8.41   3.4530          \n 8.41   3.4446          \n 8.42   3.4447          \n 8.43   3.4410          \n 8.43   3.4546          \n 8.44   3.4400          \n 8.45   3.4458          \n 8.46   3.4443          \n 8.46   3.4390          \n 8.47   3.4405          \n 8.48   3.4459          \n 8.48   3.4403          \n 8.49   3.4480          \n 8.5    3.4447          \n 8.51   3.4525          \n 8.51   3.4516          \n 8.52   3.4396          \n 8.53   3.4510          \n 8.54   3.4439          \n 8.54   3.4370          \n 8.55   3.4472          \n 8.56   3.4437          \n 8.56   3.4448          \n 8.57   3.4500          \n 8.58   3.4410          \n 8.59   3.4486          \n 8.59   3.4485          \n 8.6    3.4408          \n 8.61   3.4379          \n 8.62   3.4388          \n 8.62   3.4522          \n 8.63   3.4436          \n 8.64   3.4475          \n 8.64   3.4411          \n 8.65   3.4451          \n 8.66   3.4527          \n 8.67   3.4495          \n 8.67   3.4523          \n 8.68   3.4612          \n 8.69   3.4491          \n 8.69   3.4427          \n 8.7    3.4457          \n 8.71   3.4545          \n 8.72   3.4570          \n 8.72   3.4432          \n 8.73   3.4391          \n 8.74   3.4530          \n 8.75   3.4425          \n 8.75   3.4420          \n 8.76   3.4463          \n 8.77   3.4509          \n 8.77   3.4418          \n 8.78   3.4495          \n 8.79   3.4432          \n 8.8    3.4455          \n 8.8    3.4483          \n 8.81   3.4476          \n 8.82   3.4525          \n 8.83   3.4409          \n 8.83   3.4415          \n 8.84   3.4385          \n 8.85   3.4428          \n 8.85   3.4417          \n 8.86   3.4472          \n 8.87   3.4406          \n 8.88   3.4492          \n 8.88   3.4411          \n 8.89   3.4412          \n 8.9    3.4464          \n 8.9    3.4382          \n 8.91   3.4442          \n 8.92   3.4423          \n 8.93   3.4492          \n 8.93   3.4390          \n 8.94   3.4411          \n 8.95   3.4467          \n 8.96   3.4409          \n 8.96   3.4456          \n 8.97   3.4413          \n 8.98   3.4464          \n 8.98   3.4478          \n 8.99   3.4474          \n 9.0    3.4427          \n 9.01   3.4455          \n 9.01   3.4485          \n 9.02   3.4389          \n 9.03   3.4433          \n 9.03   3.4402          \n 9.04   3.4452          \n 9.05   3.4458          \n 9.06   3.4431          \n 9.06   3.4370          \n 9.07   3.4359          \n 9.08   3.4439          \n 9.09   3.4538          \n 9.09   3.4410          \n 9.1    3.4397          \n 9.11   3.4457          \n 9.11   3.4448          \n 9.12   3.4409          \n 9.13   3.4439          \n 9.14   3.4388          \n 9.14   3.4427          \n 9.15   3.4396          \n 9.16   3.4536          \n 9.17   3.4533          \n 9.17   3.4491          \n 9.18   3.4402          \n 9.19   3.4398          \n 9.19   3.4424          \n 9.2    3.4409          \n 9.21   3.4497          \n 9.22   3.4435          \n 9.22   3.4478          \n 9.23   3.4479          \n 9.24   3.4519          \n 9.24   3.4408          \n 9.25   3.4430          \n 9.26   3.4427          \n 9.27   3.4430          \n 9.27   3.4458          \n 9.28   3.4435          \n 9.29   3.4468          \n 9.3    3.4401          \n 9.3    3.4481          \n 9.31   3.4450          \n 9.32   3.4467          \n 9.32   3.4385          \n 9.33   3.4435          \n 9.34   3.4467          \n 9.35   3.4415          \n 9.35   3.4458          \n 9.36   3.4420          \n 9.37   3.4433          \n 9.38   3.4450          \n 9.38   3.4426          \n 9.39   3.4473          \n 9.4    3.4413          \n 9.4    3.4462          \n 9.41   3.4408          \n 9.42   3.4495          \n 9.43   3.4450          \n 9.43   3.4457          \n 9.44   3.4409          \n 9.45   3.4412          \n 9.45   3.4495          \n 9.46   3.4428          \n 9.47   3.4398          \n 9.48   3.4408          \n 9.48   3.4443          \n 9.49   3.4458          \n 9.5    3.4437          \n 9.51   3.4388          \n 9.51   3.4498          \n 9.52   3.4415          \n 9.53   3.4556          \n 9.53   3.4421          \n 9.54   3.4394          \n 9.55   3.4420          \n 9.56   3.4425          \n 9.56   3.4370          \n 9.57   3.4479          \n 9.58   3.4433          \n 9.59   3.4431          \n 9.59   3.4442          \n 9.6    3.4433          \n 9.61   3.4447          \n 9.61   3.4394          \n 9.62   3.4397          \n 9.63   3.4440          \n 9.64   3.4396          \n 9.64   3.4448          \n 9.65   3.4447          \n 9.66   3.4387          \n 9.66   3.4460          \n 9.67   3.4513          \n 9.68   3.4506          \n 9.69   3.4415          \n 9.69   3.4439          \n 9.7    3.4390          \n 9.71   3.4490          \n 9.72   3.4463          \n 9.72   3.4440          \n 9.73   3.4498          \n 9.74   3.4514          \n 9.74   3.4463          \n 9.75   3.4402          \n 9.76   3.4425          \n 9.77   3.4429          \n 9.77   3.4415          \n 9.78   3.4406          \n 9.79   3.4405          \n 9.8    3.4416          \n 9.8    3.4427          \n 9.81   3.4477          \n 9.82   3.4388          \n 9.82   3.4448          \n 9.83   3.4462          \n 9.84   3.4413          \n 9.85   3.4459          \n 9.85   3.4432          \n 9.86   3.4467          \n 9.87   3.4442          \n 9.87   3.4444          \n 9.88   3.4498          \n 9.89   3.4380          \n 9.9    3.4501          \n 9.9    3.4403          \n 9.91   3.4461          \n 9.92   3.4460          \n 9.93   3.4429          \n 9.93   3.4442          \n 9.94   3.4400          \n 9.95   3.4464          \n 9.95   3.4496          \n 9.96   3.4446          \n 9.97   3.4487          \n 9.98   3.4441          \n 9.98   3.4456          \n 9.99   3.4417          \n 10.0   3.4401          \n 10.01  3.4392          \n 10.01  3.4411          \n 10.02  3.4446          \n 10.03  3.4465          \n 10.03  3.4434          \n 10.04  3.4430          \n 10.05  3.4449          \n 10.06  3.4455          \n 10.06  3.4442          \n 10.07  3.4375          \n 10.08  3.4447          \n 10.08  3.4429          \n 10.09  3.4415          \n 10.1   3.4415          \n 10.11  3.4405          \n 10.11  3.4435          \n 10.12  3.4473          \n 10.13  3.4428          \n 10.14  3.4513          \n 10.14  3.4484          \n 10.15  3.4481          \n 10.16  3.4416          \n 10.16  3.4483          \n 10.17  3.4390          \n 10.18  3.4389          \n 10.19  3.4482          \n 10.19  3.4416          \n 10.2   3.4435          \n 10.21  3.4458          \n 10.22  3.4454          \n 10.22  3.4425          \n 10.23  3.4465          \n 10.24  3.4438          \n 10.24  3.4425          \n 10.25  3.4389          \n 10.26  3.4468          \n 10.27  3.4430          \n 10.27  3.4456          \n 10.28  3.4517          \n 10.29  3.4484          \n 10.29  3.4420          \n 10.3   3.4472          \n 10.31  3.4439          \n 10.32  3.4408          \n 10.32  3.4448          \n 10.33  3.4423          \n 10.34  3.4463          \n 10.35  3.4437          \n 10.35  3.4436          \n 10.36  3.4393          \n 10.37  3.4406          \n 10.37  3.4403          \n 10.38  3.4397          \n 10.39  3.4559          \n 10.4   3.4460          \n 10.4   3.4441          \n 10.41  3.4397          \n 10.42  3.4488          \n 10.42  3.4462          \n 10.43  3.4446          \n 10.44  3.4447          \n 10.45  3.4392          \n 10.45  3.4513          \n 10.46  3.4458          \n 10.47  3.4442          \n 10.48  3.4403          \n 10.48  3.4405          \n 10.49  3.4447          \n 10.5   3.4455          \n 10.5   3.4476          \n 10.51  3.4436          \n 10.52  3.4488          \n 10.53  3.4495          \n 10.53  3.4461          \n 10.54  3.4512          \n 10.55  3.4473          \n 10.56  3.4435          \n 10.56  3.4428          \n 10.57  3.4369          \n 10.58  3.4431          \n 10.58  3.4426          \n 10.59  3.4436          \n 10.6   3.4417          \n 10.61  3.4419          \n 10.61  3.4396          \n 10.62  3.4471          \n 10.63  3.4453          \n 10.63  3.4447          \n 10.64  3.4458          \n 10.65  3.4441          \n 10.66  3.4439          \n 10.66  3.4425          \n 10.67  3.4427          \n 10.68  3.4436          \n 10.69  3.4438          \n 10.69  3.4548          \n 10.7   3.4414          \n 10.71  3.4408          \n 10.71  3.4407          \n 10.72  3.4404          \n 10.73  3.4423          \n 10.74  3.4455          \n 10.74  3.4426          \n 10.75  3.4427          \n 10.76  3.4429          \n 10.77  3.4479          \n 10.77  3.4428          \n 10.78  3.4400          \n 10.79  3.4412          \n 10.79  3.4491          \n 10.8   3.4464          \n 10.81  3.4509          \n 10.82  3.4420          \n 10.82  3.4441          \n 10.83  3.4377          \n 10.84  3.4484          \n 10.84  3.4460          \n 10.85  3.4428          \n 10.86  3.4469          \n 10.87  3.4500          \n 10.87  3.4454          \n 10.88  3.4433          \n 10.89  3.4434          \n 10.9   3.4401          \n 10.9   3.4459          \n 10.91  3.4400          \n 10.92  3.4389          \n 10.92  3.4451          \n 10.93  3.4417          \n 10.94  3.4520          \n 10.95  3.4448          \n 10.95  3.4453          \n 10.96  3.4445          \n 10.97  3.4457          \n 10.98  3.4409          \n 10.98  3.4426          \n 10.99  3.4471          \n 11.0   3.4465          \n 11.0   3.4466          \n 11.01  3.4379          \n 11.02  3.4421          \n 11.03  3.4429          \n 11.03  3.4473          \n 11.04  3.4428          \n 11.05  3.4387          \n 11.05  3.4410          \n 11.06  3.4392          \n 11.07  3.4536          \n 11.08  3.4479          \n 11.08  3.4425          \n 11.09  3.4394          \n 11.1   3.4389          \n 11.11  3.4484          \n 11.11  3.4468          \n 11.12  3.4445          \n 11.13  3.4456          \n 11.13  3.4422          \n 11.14  3.4434          \n 11.15  3.4444          \n 11.16  3.4411          \n 11.16  3.4457          \n 11.17  3.4451          \n 11.18  3.4391          \n 11.19  3.4515          \n 11.19  3.4498          \n 11.2   3.4521          \n 11.21  3.4482          \n 11.21  3.4467          \n 11.22  3.4417          \n 11.23  3.4438          \n 11.24  3.4382          \n 11.24  3.4381          \n 11.25  3.4403          \n 11.26  3.4469          \n 11.26  3.4492          \n 11.27  3.4407          \n 11.28  3.4461          \n 11.29  3.4505          \n 11.29  3.4440          \n 11.3   3.4475          \n 11.31  3.4517          \n 11.32  3.4429          \n 11.32  3.4398          \n 11.33  3.4382          \n 11.34  3.4472          \n 11.34  3.4413          \n 11.35  3.4496          \n 11.36  3.4465          \n 11.37  3.4419          \n 11.37  3.4416          \n 11.38  3.4406          \n 11.39  3.4420          \n 11.4   3.4409          \n 11.4   3.4430          \n 11.41  3.4442          \n 11.42  3.4413          \n 11.42  3.4491          \n 11.43  3.4413          \n 11.44  3.4436          \n 11.45  3.4465          \n 11.45  3.4463          \n 11.46  3.4444          \n 11.47  3.4386          \n 11.47  3.4449          \n 11.48  3.4399          \n 11.49  3.4376          \n 11.5   3.4462          \n 11.5   3.4398          \n 11.51  3.4457          \n 11.52  3.4481          \n 11.53  3.4496          \n 11.53  3.4435          \n 11.54  3.4441          \n 11.55  3.4466          \n 11.55  3.4462          \n 11.56  3.4444          \n 11.57  3.4481          \n 11.58  3.4456          \n 11.58  3.4445          \n 11.59  3.4431          \n 11.6   3.4489          \n 11.61  3.4409          \n 11.61  3.4427          \n 11.62  3.4454          \n 11.63  3.4419          \n 11.63  3.4460          \n 11.64  3.4439          \n 11.65  3.4432          \n 11.66  3.4412          \n 11.66  3.4430          \n 11.67  3.4426          \n 11.68  3.4479          \n 11.68  3.4417          \n 11.69  3.4455          \n 11.7   3.4458          \n 11.71  3.4398          \n 11.71  3.4445          \n 11.72  3.4422          \n 11.73  3.4466          \n 11.74  3.4460          \n 11.74  3.4520          \n 11.75  3.4447          \n 11.76  3.4373          \n 11.76  3.4432          \n 11.77  3.4435          \n 11.78  3.4452          \n 11.79  3.4393          \n 11.79  3.4418          \n 11.8   3.4434          \n 11.81  3.4463          \n 11.81  3.4421          \n 11.82  3.4417          \n 11.83  3.4454          \n 11.84  3.4490          \n 11.84  3.4468          \n 11.85  3.4450          \n 11.86  3.4439          \n 11.87  3.4455          \n 11.87  3.4400          \n 11.88  3.4391          \n 11.89  3.4435          \n 11.89  3.4440          \n 11.9   3.4452          \n 11.91  3.4445          \n 11.92  3.4424          \n 11.92  3.4398          \n 11.93  3.4414          \n 11.94  3.4388          \n 11.95  3.4410          \n 11.95  3.4453          \n 11.96  3.4435          \n 11.97  3.4439          \n 11.97  3.4416          \n 11.98  3.4465          \n 11.99  3.4434          \n 12.0   3.4420          \n 12.0   3.4429          \n 12.01  3.4418          \n 12.02  3.4432          \n 12.02  3.4493          \n 12.03  3.4429          \n 12.04  3.4460          \n 12.05  3.4413          \n 12.05  3.4425          \n 12.06  3.4429          \n 12.07  3.4427          \n 12.08  3.4436          \n 12.08  3.4473          \n 12.09  3.4431          \n 12.1   3.4425          \n 12.1   3.4438          \n 12.11  3.4420          \n 12.12  3.4402          \n 12.13  3.4427          \n 12.13  3.4468          \n 12.14  3.4431          \n 12.15  3.4405          \n 12.16  3.4420          \n 12.16  3.4487          \n 12.17  3.4453          \n 12.18  3.4469          \n 12.18  3.4452          \n 12.19  3.4466          \n 12.2   3.4416          \n 12.21  3.4479          \n 12.21  3.4476          \n 12.22  3.4431          \n 12.23  3.4424          \n 12.23  3.4423          \n 12.24  3.4449          \n 12.25  3.4432          \n 12.26  3.4424          \n 12.26  3.4472          \n 12.27  3.4456          \n 12.28  3.4443          \n 12.29  3.4423          \n 12.29  3.4408          \n 12.3   3.4468          \n 12.31  3.4415          \n 12.31  3.4426          \n 12.32  3.4424          \n 12.33  3.4434          \n 12.34  3.4454          \n 12.34  3.4451          \n 12.35  3.4456          \n 12.36  3.4472          \n 12.37  3.4409          \n 12.37  3.4412          \n 12.38  3.4403          \n 12.39  3.4400          \n 12.39  3.4417          \n 12.4   3.4440          \n 12.41  3.4470          \n 12.42  3.4450          \n 12.42  3.4456          \n 12.43  3.4465          \n 12.44  3.4500          \n 12.44  3.4459          \n 12.45  3.4422          \n 12.46  3.4405          \n 12.47  3.4439          \n 12.47  3.4465          \n 12.48  3.4495          \n 12.49  3.4451          \n 12.5   3.4404          \n 12.5   3.4407          \n 12.51  3.4417          \n 12.52  3.4452          \n 12.52  3.4437          \n 12.53  3.4452          \n 12.54  3.4428          \n 12.55  3.4445          \n 12.55  3.4450          \n 12.56  3.4419          \n 12.57  3.4420          \n 12.58  3.4465          \n 12.58  3.4437          \n 12.59  3.4441          \n 12.6   3.4444          \n 12.6   3.4443          \n 12.61  3.4422          \n 12.62  3.4426          \n 12.63  3.4420          \n 12.63  3.4463          \n 12.64  3.4464          \n 12.65  3.4435          \n 12.65  3.4409          \n 12.66  3.4467          \n 12.67  3.4423          \n 12.68  3.4444          \n 12.68  3.4420          \n 12.69  3.4455          \n 12.7   3.4502          \n 12.71  3.4428          \n 12.71  3.4452          \n 12.72  3.4418          \n 12.73  3.4446          \n 12.73  3.4433          \n 12.74  3.4484          \n 12.75  3.4464          \n 12.76  3.4468          \n 12.76  3.4441          \n 12.77  3.4419          \n 12.78  3.4407          \n 12.79  3.4430          \n 12.79  3.4437          \n 12.8   3.4457          \n 12.81  3.4437          \n 12.81  3.4422          \n 12.82  3.4447          \n 12.83  3.4457          \n 12.84  3.4413          \n 12.84  3.4425          \n 12.85  3.4435          \n 12.86  3.4518          \n 12.86  3.4481          \n 12.87  3.4463          \n 12.88  3.4454          \n 12.89  3.4452          \n 12.89  3.4430          \n 12.9   3.4441          \n 12.91  3.4422          \n 12.92  3.4459          \n 12.92  3.4451          \n 12.93  3.4438          \n 12.94  3.4411          \n 12.94  3.4415          \n 12.95  3.4433          \n 12.96  3.4422          \n 12.97  3.4426          \n 12.97  3.4458          \n 12.98  3.4429          \n 12.99  3.4429          \n 13.0   3.4463          \n 13.0   3.4496          \n 13.01  3.4436          \n 13.02  3.4484          \n 13.02  3.4418          \n 13.03  3.4425          \n 13.04  3.4412          \n 13.05  3.4464          \n 13.05  3.4484          \n 13.06  3.4446          \n 13.07  3.4453          \n 13.07  3.4425          \n 13.08  3.4465          \n 13.09  3.4437          \n 13.1   3.4481          \n 13.1   3.4465          \n 13.11  3.4460          \n 13.12  3.4423          \n 13.13  3.4402          \n 13.13  3.4432          \n 13.14  3.4408          \n 13.15  3.4455          \n 13.15  3.4476          \n 13.16  3.4415          \n 13.17  3.4409          \n 13.18  3.4405          \n 13.18  3.4431          \n 13.19  3.4393          \n 13.2   3.4410          \n 13.2   3.4391          \n 13.21  3.4387          \n 13.22  3.4409          \n 13.23  3.4399          \n 13.23  3.4458          \n 13.24  3.4494          \n 13.25  3.4430          \n 13.26  3.4442          \n 13.26  3.4425          \n 13.27  3.4410          \n 13.28  3.4400          \n 13.28  3.4436          \n 13.29  3.4445          \n 13.3   3.4477          \n 13.31  3.4463          \n 13.31  3.4451          \n 13.32  3.4483          \n 13.33  3.4471          \n 13.34  3.4465          \n 13.34  3.4459          \n 13.35  3.4451          \n 13.36  3.4425          \n 13.36  3.4432          \n 13.37  3.4437          \n 13.38  3.4420          \n 13.39  3.4445          \n 13.39  3.4459          \n 13.4   3.4424          \n 13.41  3.4406          \n 13.41  3.4424          \n 13.42  3.4440          \n 13.43  3.4464          \n 13.44  3.4444          \n 13.44  3.4443          \n 13.45  3.4475          \n 13.46  3.4447          \n 13.47  3.4458          \n 13.47  3.4420          \n 13.48  3.4410          \n 13.49  3.4481          \n 13.49  3.4440          \n 13.5   3.4447          \n 13.51  3.4447          \n 13.52  3.4443          \n 13.52  3.4447          \n 13.53  3.4424          \n 13.54  3.4392          \n 13.55  3.4387          \n 13.55  3.4393          \n 13.56  3.4404          \n 13.57  3.4400          \n 13.57  3.4422          \n 13.58  3.4391          \n 13.59  3.4420          \n 13.6   3.4445          \n 13.6   3.4437          \n 13.61  3.4463          \n 13.62  3.4438          \n 13.62  3.4452          \n 13.63  3.4446          \n 13.64  3.4509          \n 13.65  3.4509          \n 13.65  3.4479          \n 13.66  3.4428          \n 13.67  3.4410          \n 13.68  3.4422          \n 13.68  3.4400          \n 13.69  3.4411          \n 13.7   3.4417          \n 13.7   3.4431          \n 13.71  3.4446          \n 13.72  3.4449          \n 13.73  3.4456          \n 13.73  3.4417          \n 13.74  3.4439          \n 13.75  3.4442          \n 13.76  3.4458          \n 13.76  3.4427          \n 13.77  3.4465          \n 13.78  3.4422          \n 13.78  3.4441          \n 13.79  3.4446          \n 13.8   3.4447          \n 13.81  3.4442          \n 13.81  3.4418          \n 13.82  3.4437          \n 13.83  3.4418          \n 13.83  3.4429          \n 13.84  3.4417          \n 13.85  3.4420          \n 13.86  3.4400          \n 13.86  3.4430          \n 13.87  3.4409          \n 13.88  3.4439          \n 13.89  3.4451          \n 13.89  3.4424          \n 13.9   3.4425          \n 13.91  3.4450          \n 13.91  3.4440          \n 13.92  3.4449          \n 13.93  3.4430          \n 13.94  3.4440          \n 13.94  3.4441          \n 13.95  3.4436          \n 13.96  3.4435          \n 13.97  3.4427          \n 13.97  3.4414          \n 13.98  3.4409          \n 13.99  3.4413          \n 13.99  3.4431          \n 14.0   3.4439          \n 14.01  3.4429          \n 14.02  3.4454          \n 14.02  3.4439          \n 14.03  3.4429          \n 14.04  3.4429          \n 14.04  3.4420          \n 14.05  3.4433          \n 14.06  3.4419          \n 14.07  3.4434          \n 14.07  3.4432          \n 14.08  3.4433          \n 14.09  3.4443          \n 14.1   3.4438          \n 14.1   3.4441          \n 14.11  3.4433          \n 14.12  3.4437          \n 14.12  3.4442          \n 14.13  3.4467          \n 14.14  3.4450          \n 14.15  3.4421          \n 14.15  3.4421          \n 14.16  3.4416          \n 14.17  3.4412          \n 14.18  3.4418          \n 14.18  3.4440          \n 14.19  3.4444          \n 14.2   3.4433          \n 14.2   3.4432          \n 14.21  3.4426          \n 14.22  3.4418          \n 14.23  3.4422          \n 14.23  3.4431          \n 14.24  3.4421          \n 14.25  3.4420          \n 14.25  3.4412          \n 14.26  3.4412          \n 14.27  3.4425          \n 14.28  3.4424          \n 14.28  3.4424          \n 14.29  3.4435          \n 14.3   3.4432          \n 14.31  3.4432          \n 14.31  3.4442          \n 14.32  3.4426          \n 14.33  3.4446          \n 14.33  3.4448          \n 14.34  3.4448          \n 14.35  3.4432          \n 14.36  3.4431          \n 14.36  3.4426          \n 14.37  3.4436          \n 14.38  3.4436          \n 14.38  3.4442          \n 14.39  3.4446          \n 14.4   3.4441          \n 14.41  3.4430          \n 14.41  3.4431          \n 14.42  3.4431          \n 14.43  3.4436          \n 14.44  3.4436          \n 14.44  3.4432          \n 14.45  3.4432          \n 14.46  3.4435          \n 14.46  3.4432          \n 14.47  3.4418          \n 14.48  3.4427          \n 14.49  3.4435          \n 14.49  3.4430          \n 14.5   3.4434          \n 14.51  3.4444          \n 14.52  3.4429          \n 14.52  3.4430          \n 14.53  3.4431          \n 14.54  3.4432          \n 14.54  3.4436          \n 14.55  3.4430          \n 14.56  3.4416          \n 14.57  3.4404          \n 14.57  3.4408          \n 14.58  3.4420          \n 14.59  3.4420          \n 14.59  3.4430          \n 14.6   3.4439          \n 14.61  3.4435          \n 14.62  3.4426          \n 14.62  3.4430          \n 14.63  3.4434          \n 14.64  3.4444          \n 14.65  3.4445          \n 14.65  3.4441          \n 14.66  3.4445          \n 14.67  3.4445          \n 14.67  3.4440          \n 14.68  3.4431          \n 14.69  3.4431          \n 14.7   3.4435          \n 14.7   3.4440          \n 14.71  3.4439          \n 14.72  3.4435          \n 14.73  3.4430          \n 14.73  3.4435          \n 14.74  3.4431          \n 14.75  3.4431          \n 14.75  3.4435          \n 14.76  3.4435          \n 14.77  3.4440          \n 14.78  3.4439          \n 14.78  3.4445          \n 14.79  3.4444          \n 14.8   3.4440          \n 14.8   3.4439          \n 14.81  3.4439          \n 14.82  3.4439          \n 14.83  3.4443          \n 14.83  3.4443          \n 14.84  3.4443          \n 14.85  3.4443          \n 14.86  3.4438          \n 14.86  3.4443          \n 14.87  3.4433          \n 14.88  3.4433          \n 14.88  3.4438          \n 14.89  3.4438          \n 14.9   3.4438          \n 14.91  3.4438          \n 14.91  3.4439          \n 14.92  3.4439          \n 14.93  3.4444          \n 14.94  3.4443          \n 14.94  3.4438          \n 14.95  3.4439          \n 14.96  3.4439          \n 14.96  3.4439          \n 14.97  3.4439          \n 14.98  3.4439          \n 14.99  3.4439          \n 14.99  3.4439          \n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of phongdtd/wavlm-vindata-demo-dist?", "answers": [{"text": "wavlm", "answer_start": 136, "answer_end": 140}]}, {"id": "q2", "question": "What is the model task of phongdtd/wavlm-vindata-demo-dist?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 12, "answer_end": 39}]}]}]}, {"title": "phozon/harry-potter-medium", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# My Awesome Model", "qas": [{"id": "q2", "question": "What is the model task of phozon/harry-potter-medium?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "princeton-nlp/unsup-simcse-bert-large-uncased", "paragraphs": [{"context": "---\ntags:\n- feature-extraction\n- bert\n---\n\n# Model Card for unsup-simcse-bert-large-uncased \n \n# Model Details\n \n## Model Description\n \nMore information needed\n \n- **Developed by:** Princeton NLP group\n- **Shared by [Optional]:** Princeton NLP group\n\n- **Model type:** Feature Extraction\n- **Language(s) (NLP):** More information needed\n- **License:** More information needed\n- **Parent Model:** BERT\n- **Resources for more information:**\n \t- [GitHub Repo](\n \t - [Associated Paper](\n\n\n# Uses\n \n\n## Direct Use\nThis model can be used for the task of feature extraction. \n \n## Downstream Use [Optional]\n \nMore information needed.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)]( and [Bender et al. (2021)]( Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n\n\n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n \n## Training Data\n \n  The model craters note in the [associatedGithub Repository](\n> We train unsupervised SimCSE on 106 randomly sampled sentences from English Wikipedia, and train supervised SimCSE on the combination of MNLI and SNLI datasets (314k).\n \n \n \n## Training Procedure\n\n \n### Preprocessing\n \nMore information needed \n\n\n \n### Speeds, Sizes, Times\n \n \n**Hyperparameters**\nThe model craters note in the [associated GitHub Repo]( : \n \n Unsup. BERT \n:-----------:\n 64          \n 1e-5 \n \n\n\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \n The model craters note in the [associated paper](\n> Our evaluation code for sentence embeddings is based on a modified version of [SentEval]( It evaluates sentence embeddings on semantic textual similarity (STS) tasks and downstream transfer tasks. \n \n> For STS tasks, our evaluation takes the \"all\" setting, and report Spearman's correlation. See [associated paper]( (Appendix B) for evaluation details.\n\n\n \n### Factors\nMore information needed\n \n### Metrics\n \nMore information needed\n \n \n## Results \n \nMore information needed\n\n \n# Model Examination\n \n The model craters note in the [associated paper](\n \n>**Uniformity and alignment.**\n We also observe that (1) though pre-trained embeddings have good alignment, their uniformity is poor (i.e., the embeddings are highly anisotropic); (2) post-processing methods like BERT-flow and BERT-whitening greatly improve uniformity but also suffer a degeneration in alignment; (3) unsupervised SimCSE effectively improves uniformity of pre-trained embeddings whereas keeping a good alignment;(4) incorporating supervised data in SimCSE further amends alignment.\n \n\n\n \n# Environmental Impact\n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator]( presented in [Lacoste et al. (2019)](\n \n- **Hardware Type:** Nvidia 3090 GPUs with CUDA 11\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n \nMore information needed\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \n \nMore information needed\n \n### Software\n \nMore information needed.\n \n# Citation\n\n \n**BibTeX:**\n \n \n```bibtex\n@inproceedings{gao2021simcse,\n   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},\n   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},\n   booktitle={Empirical Methods in Natural Language Processing (EMNLP)},\n   year={2021}\n}\n```\n \n \n \n \n# Glossary [optional]\n \nMore information needed\n\n# More Information [optional]\nMore information needed \n\n \n# Model Card Authors [optional]\n \nPrinceton NLP group in collaboration with Ezi Ozoani and the Hugging Face team.\n\n# Model Card Contact\n \nIf you have any questions related to the code or the paper, feel free to email Tianyu (`tianyug@cs.princeton.edu`) and Xingcheng (`yxc18@mails.tsinghua.edu.cn`). If you encounter any problems when using the code, or want to report a bug, you can open an issue. Please try to specify the problem with details so we can help you better and quicker!\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/unsup-simcse-bert-large-uncased\")\n\nmodel = AutoModel.from_pretrained(\"princeton-nlp/unsup-simcse-bert-large-uncased\")\n ```\n</details>\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of princeton-nlp/unsup-simcse-bert-large-uncased?", "answers": [{"text": "bert", "answer_start": 33, "answer_end": 36}]}, {"id": "q2", "question": "What is the model task of princeton-nlp/unsup-simcse-bert-large-uncased?", "answers": [{"text": "feature-extraction", "answer_start": 12, "answer_end": 29}]}]}]}, {"title": "pritoms/distilroberta-base-YTTranscript23", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: distilroberta-base-YTTranscript23\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilroberta-base-YTTranscript23\n\nThis model is a fine-tuned version of [distilroberta-base]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.9258\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    2.9007          |\n 2.0    2.9651          |\n 3.0    2.9374          |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of pritoms/distilroberta-base-YTTranscript23?", "answers": [{"text": "roberta", "answer_start": 82, "answer_end": 88}]}]}]}, {"title": "pritoms/gpt-neo-125M-Byethon", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- null\nmodel-index:\n- name: gpt-neo-125M-Byethon\n  results:\n  - task:\n      name: Causal Language Modeling\n      type: text-generation\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# gpt-neo-125M-Byethon\n\nThis model is a fine-tuned version of [EleutherAI/gpt-neo-125M]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6609\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.8348          |\n 2.0    0.6931          |\n 3.0    0.6609          |\n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.11.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of pritoms/gpt-neo-125M-Byethon?", "answers": [{"text": "text-generation", "answer_start": 184, "answer_end": 198}]}]}]}, {"title": "pritoms/gpt-neo-125M-finetuned-pgt", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- null\nmodel-index:\n- name: gpt-neo-125M-finetuned-pgt\n  results:\n  - task:\n      name: Causal Language Modeling\n      type: text-generation\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# gpt-neo-125M-finetuned-pgt\n\nThis model is a fine-tuned version of [pritoms/gpt-neo-125M-finetuned-pgt]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6026\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    1.5947          |\n 2.0    1.5963          |\n 3.0    1.6026          |\n\n\n### Framework versions\n\n- Transformers 4.10.0\n- Pytorch 1.9.0+cu102\n- Datasets 1.11.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of pritoms/gpt-neo-125M-finetuned-pgt?", "answers": [{"text": "text-generation", "answer_start": 190, "answer_end": 204}]}]}]}, {"title": "pritoms/gpt-neo-125M-philosophical-investigation", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: gpt-neo-125M-philosophical-investigation\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# gpt-neo-125M-philosophical-investigation\n\nThis model is a fine-tuned version of [EleutherAI/gpt-neo-125M]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 3.4443\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    3.4901          |\n 2.0    3.4550          |\n 3.0    3.4443          |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": []}]}, {"title": "pritoms/gpt2-group2", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: gpt2-group2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# gpt2-group2\n\nThis model is a fine-tuned version of [gpt2]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 3.6769\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    3.7517          |\n 2.0    3.6951          |\n 3.0    3.6769          |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of pritoms/gpt2-group2?", "answers": [{"text": "gpt2", "answer_start": 69, "answer_end": 72}]}]}]}, {"title": "priyank/Generate_instructions_t5", "paragraphs": [{"context": "\n```\nimport torch\nfrom transformers import T5ForConditionalGeneration,T5Tokenizer\n\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    \n    set_seed(42)\n\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"priyank/Generate_instructions_t5\")\ntokenizer = T5Tokenizer.from_pretrained(\"priyank/Generate_instructions_t5\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n\nsentence = \"ask user to provide his date of birth\"\ntext =  \"paraphrase: \" + sentence + \" </s>\"\n\n\nmax_len = 256\n\nencoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n\n\n# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\nbeam_outputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    do_sample=True,\n    max_length=256,\n    top_k=120,\n    top_p=0.98,\n    early_stopping=True,\n    num_return_sequences=10\n)\n\n\nprint (\"\\\\\nApprentice Query ::\")\nprint (sentence)\nprint (\"\\\\\nAuto Generated Instruction ::\")\nfinal_outputs =[]\nfor beam_output in beam_outputs:\n    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    if sent.lower() != sentence.lower() and sent not in final_outputs:\n        final_outputs.append(sent)\n\nfor i, final_output in enumerate(final_outputs):\n    print(\"{}: {}\".format(i, final_output))\n\n\nApprentice Query ::\nif balance is greater than $100, then tell the user he needs more balance\n\nAuto Generated Instruction ::\n0: IF (assert(user.balance > $100)) THEN (say you need more balance)\n\n```\n\nReference: ", "qas": [{"id": "q1", "question": "What is the model architecture of priyank/Generate_instructions_t5?", "answers": [{"text": "t5", "answer_start": 313, "answer_end": 314}]}]}]}, {"title": "prk/roberta-base-squad2-finetuned-squad", "paragraphs": [{"context": "---\nlicense: cc-by-4.0\ntags:\n- generated_from_trainer\ndatasets:\n- squad_v2\nmodel-index:\n- name: roberta-base-squad2-finetuned-squad\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-base-squad2-finetuned-squad\n\nThis model is a fine-tuned version of [deepset/roberta-base-squad2]( on a custom dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.1894          |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of prk/roberta-base-squad2-finetuned-squad?", "answers": [{"text": "roberta", "answer_start": 96, "answer_end": 102}]}]}]}, {"title": "professional/DialoGPT-small-joshua", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Joshua DialoGPT model\n", "qas": [{"id": "q2", "question": "What is the model task of professional/DialoGPT-small-joshua?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "profoz/mlops-demo", "paragraphs": [{"context": "---\nlanguage:\n- en\ntags:\n- classification\n- sequence-classification\nlicense: apache-2.0\n\n---\n\nGithub repository [here](", "qas": []}]}, {"title": "project2you/wav2vec2-large-xlsr-53-demo-colab", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-large-xlsr-53-demo-colab\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xlsr-53-demo-colab\n\nThis model is a fine-tuned version of [facebook/wav2vec2-large-xlsr-53]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6901\n- Wer: 1.6299\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 3.42   3.5852          \n 6.83   0.7430          \n 10.26  0.6513          \n 13.67  0.6208          \n 17.09  0.6401          \n 20.51  0.6410          \n 23.93  0.6910          \n 27.35  0.6901          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.14.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of project2you/wav2vec2-large-xlsr-53-demo-colab?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "projectaligned/gpt2-xl-reddit-writingprompts-behavior-cloning", "paragraphs": [{"context": "_deprecated_\n\nThis model is fine-tuned on data from \n- The model is based on gpt2-xl\n- The prompt responses to the top 1000 prompts (by upvote) are used to fine-tune the model.", "qas": [{"id": "q1", "question": "What is the model architecture of projectaligned/gpt2-xl-reddit-writingprompts-behavior-cloning?", "answers": [{"text": "gpt2", "answer_start": 77, "answer_end": 80}]}]}]}, {"title": "projecte-aina/bart-base-ca-casum", "paragraphs": [{"context": "---\nlanguage: \"ca\"\nlicense: mit\ntags:\n- summarization\nwidget:\n- text: \"El projecte AINA generar\u00e0 els recursos digitals i ling\u00fc\u00edstics necessaris per facilitar el desenvolupament d\u2019aplicacions basades en la intel\u00b7lig\u00e8ncia artificial i les tecnologies de la llengua, com ara els assistents de veu, els traductors autom\u00e0tics o els agents conversacionals en catal\u00e0. L\u2019objectiu \u00faltim \u00e9s que la ciutadania pugui participar en catal\u00e0 en el m\u00f3n digital al mateix nivell que els parlants d\u2019una llengua global, com ara l\u2019angl\u00e8s, i evitar aix\u00ed l\u2019extinci\u00f3 digital de la llengua. El primer recurs generat \u00e9s el corpus del catal\u00e0 per entrenar els algoritmes d\u2019intel\u00b7lig\u00e8ncia artificial (IA), el m\u00e9s gran creat fins al moment, amb 1.770 milions de metadades associades a paraules. El proper pas ser\u00e0 generar els models de la llengua, models de la parla i models de traducci\u00f3 utilitzant xarxes neuronals multicapa, perqu\u00e8 les empreses que creen aplicacions basades en intel\u00b7lig\u00e8ncia artificial (IA), com ara assistents de veu, traductors autom\u00e0tics, agents conversacionals, etc., puguin fer-ho f\u00e0cilment en catal\u00e0.\"\n- text: \"El Govern vol que el catal\u00e0 tamb\u00e9 sigui una llengua \u00fatil per a la tecnologia i per comunciar-se amb les m\u00e0quines. Per aix\u00f2, el projecte AINA, impulsat pel Departament de la Vicepresid\u00e8ncia, Pol\u00edtiques Digitals i Territori en col\u00b7laboraci\u00f3 amb el Barcelona Supercomputing Center (BSC), llan\u00e7ar\u00e0 el 17 de febrer una campanya de captaci\u00f3 de veus per generar el primer corpus o \\\"diccionari\\\" de veu del catal\u00e0 amb l'objectiu de fer que la tecnologia parli i entengui el catal\u00e0 i la ciutadania s'hi pugui relacionar amb aquesta llengua. Per a l'executiu, aquest projecte \u00e9s d'una \\\"import\u00e0ncia cabdal\\\", com ha detallat el vicepresident, Jordi Puigner\u00f3, tamb\u00e9 per refor\u00e7ar la llengua catalana a Internet. El pressupost que s'hi destinar\u00e0 aquest any \u00e9s de tres milions d'euros. Per aix\u00f2, amb el lema \\\"La nostra llengua \u00e9s la teva veu\\\", convida la ciutadania de totes les variants dialectals del catal\u00e0 ha compartir la seva veu mitjan\u00e7ant la lectura d'uns textos. La fita que s'ha marcat AINA per aquest any \u00e9s la creaci\u00f3 de la primera versi\u00f3 d'aquest diccionari de veus en catal\u00e0, amb \\\"com m\u00e9s hores de veu i com m\u00e9s diverses millor\\\". El Govern confia en una bona resposta a la campanya, que arrencar\u00e0 a partir de dem\u00e0, i que es desplegar\u00e0 per tot el territori de parla catalana, per comptar amb diverses variants dialectals. No hi ha limitaci\u00f3 d'edat per a qui vulgui participar, i \u00e9s important que la gent que participi es registri per obtenir m\u00e9s informaci\u00f3 sobre genere, edat i distribuci\u00f3 geogr\u00e0fica. Ara com ara hi ha 1.000 hores de veu i el repte \u00e9s aconseguir arribar a les 2.000 (amb transcripci\u00f3) aquest any. El vicepresident i conseller de Pol\u00edtiques Digitals, Jordi Puigner\u00f3, ha recordat que fa un any es va donar el tret de sortida al projecte AINA, una aposta per a l'\u00fas del catal\u00e0 en l'\u00e0mbit tecnol\u00f2gic. El projecte implica un impuls del catal\u00e0 en les eines digitals i per \\\"conquerir nous territoris\\\", que passen per noves plataformes i nous dispositius. Tamb\u00e9 \u00e9s un projecte per \\\"garantir drets\\\". \\\"Els catalanes tenim dret a poder relacionar-nos en catal\u00e0 amb les maquines i evitar haver de canviar de llengua a l'hora de parlar amb les maquines\\\", ha remarcat Puigner\u00f3. Un altre objectiu d'aquest projecte passa per \\\"generar talent digital\\\" i un ecosistema en l'\u00e0mbit de la intel\u00b7lig\u00e8ncia artificial. \\\"Ens toca ser un pa\u00eds digital\\\", ha insistit Puigner\u00f3. I per qu\u00e8 AINA? \\\"La filla de la Norma, que porta el nom de la seva \u00e0via, Aina Moll, la primera directora de pol\u00edtica ling\u00fc\u00edstica de la Generalitat\\\", ha explicat el vicepresident. Per tot plegat, aquest dimecres arrenca la campanya de captaci\u00f3 de veus. \\\"Volem socialitzar AINA cap a la ciutadania i que molta gent vulgui ser la seva parella ling\u00fc\u00edstica i pugui aprendre el catal\u00e0\\\", ha dit Puigner\u00f3, que ha demanat que aquesta sigui una tasca de tots. El projecte, a dia d'avui, ja coneix la sintaxis del catal\u00e0. En aquesta nova fase, a partir de dem\u00e0, tamb\u00e9 ha de con\u00e8ixer el l\u00e8xic i la sem\u00e0ntica, i tota la part oral de la llengua catalana. \\\"Si ja tenim la columna vertebral i l'esquelet, ara hem de construir la seva musculatura\\\", ha apuntat el vicepresident. La campanya es far\u00e0 a trav\u00e9s d'una web que permetr\u00e0 que qualsevol persona pugui ensenyar a AINA a aprendre catal\u00e0. I com es pot fer? \u00c9s senzill. A partir que arrenqui la campanya, qui estigui interessat en col\u00b7laborar haur\u00e0 d'entrar a  i anar a l'espai corresponent. Un cop all\u00e0, haur\u00e0 de destinar una estona a llegir frases que li proposar\u00e0 la plataforma i podr\u00e0 validar tamb\u00e9 frases d'altres persones.\"\ndatasets:\n- projecte-aina/casum\n---\n## BART-Ca fine-tuned on the CaSum dataset for summarization\n\n## Table of Contents\n<details>\n<summary>Click to expand</summary>\n\n- [Model description](#model-description)\n- [Intended uses and limitations](#intended-use)\n- [How to use](#how-to-use)\n- [Limitations and bias](#limitations-and-bias)\n- [Training](#training)\n  - [Training data](#training-data)\n  - [Training procedure](#training-procedure)\n    - [Tokenization](#tokenization)\n    - [Hyperparameters](#hyperparameters)\n- [Evaluation](#evaluation)\n   - [Variable and metrics](#variable-and-metrics)\n   - [Evaluation results](#evaluation-results)\n- [Additional information](#additional-information)\n  - [Author](#author)\n  - [Contact information](#contact-information)\n  - [Copyright](#copyright)\n  - [Licensing information](#licensing-information)\n  - [Funding](#funding)\n  - [Citing information](#citing-information)\n  - [Disclaimer](#disclaimer)\n</details>\n\n## Model description\n\nThe [BART-ca]( model has been fine-tuned on summarization with the [CaSum]( dataset that has been created along with the model. We also evaluate on an out-of-distribution dataset, [VilaSum](\n\nThe model has been fine-tuned on news articles and is expected to work best with that type of text.\n\n## Intended uses and limitations\nYou can use this model for text summarization. \n\n## How to use\n\nHere is how to use this model with the [pipeline API](\n\n```python\nfrom transformers import pipeline\nsummarizer = pipeline(\"summarization\", model=\"projecte-aina/bart-base-ca-casum\")\nARTICLE = \"\"\"\"El projecte AINA generar\u00e0 els recursos digitals i ling\u00fc\u00edstics necessaris per facilitar el desenvolupament d\u2019aplicacions basades en la intel\u00b7lig\u00e8ncia artificial i les tecnologies de la llengua, com ara els assistents de veu, els traductors autom\u00e0tics o els agents conversacionals en catal\u00e0. L\u2019objectiu \u00faltim \u00e9s que la ciutadania pugui participar en catal\u00e0 en el m\u00f3n digital al mateix nivell que els parlants d\u2019una llengua global, com ara l\u2019angl\u00e8s, i evitar aix\u00ed l\u2019extinci\u00f3 digital de la llengua. El primer recurs generat \u00e9s el corpus del catal\u00e0 per entrenar els algoritmes d\u2019intel\u00b7lig\u00e8ncia artificial (IA), el m\u00e9s gran creat fins al moment, amb 1.770 milions de metadades associades a paraules. El proper pas ser\u00e0 generar els models de la llengua, models de la parla i models de traducci\u00f3 utilitzant xarxes neuronals multicapa, perqu\u00e8 les empreses que creen aplicacions basades en intel\u00b7lig\u00e8ncia artificial (IA), com ara assistents de veu, traductors autom\u00e0tics, agents conversacionals, etc., puguin fer-ho f\u00e0cilment en catal\u00e0.\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n>>> [{'summary_text': 'El projecte AINA generar\u00e0 els recursos digitals i ling\u00fc\u00edstics necessaris per al desenvolupament d\u2019aplicacions basades en la intel\u00b7lig\u00e8ncia artificial en catal\u00e0\u2019'}]\n```\n\n## Limitations and bias\nAt the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.\n\n## Training\n\n### Training data\n\nAs training data, we used the [CaSum]( dataset extracted from a newswire corpus crawled from the [Catalan News Agency](\n\n### Training procedure\n\n#### Tokenization\n\nThe training corpus has been tokenized using a byte version of [Byte-Pair Encoding (BPE)]( with a vocabulary size of 51,200 tokens. \n\n#### Hyperparameters\n\nThe fine-tuning hyperparameters were taken from the fairseq's [Fine-tuning BART on CNN-Dailymail summarization task]( example.\n\n## Evaluation\n\n### Variable and metrics\n\nWe use Rouge-1 and Rouge-L for evaluation on two different test sets: the [CaSum]( test set and an out of distribution test set, [VilaSum](\n\n### Evaluation results\n\nBelow the evaluation results on the summarization task compared with the multilingual mBERT and the Catalan [NASCA]( with two different testsets: [CaSum]( and [VilaSum](\n\n Model    Rouge-L  |\n:-------------::------|\n BART-Ca    36.14  |\n NASCA    19.89  |\n mBART    **38.11**  |\n BART-Ca    **29.70**  |\n NASCA    19.09 |\n mBART    27.52  |\n\n\n## Additional information\n\n### Author\nText Mining Unit (TeMU) at the Barcelona Supercomputing Center (bsc-temu@bsc.es)\n\n### Contact information\nFor further information, send an email to aina@bsc.es\n\n### Copyright\nCopyright (c) 2022 Text Mining Unit at Barcelona Supercomputing Center \n\n### Licensing information\n[Apache License, Version 2.0](\n\n### Funding\nThis work was funded by MT4All CEF project and the [Departament de la Vicepresid\u00e8ncia i de Pol\u00edtiques Digitals i Territori de la Generalitat de Catalunya]( within the framework of [Projecte AINA](\n\n### Citation information\n\nIf you use any of these resources (datasets or models) in your work, please cite our latest preprint:\n\n```bibtex\n@misc{degibert2022sequencetosequence,\n      title={Sequence-to-Sequence Resources for Catalan}, \n      author={Ona de Gibert and Ksenia Kharitonova and Blanca Calvo Figueras and Jordi Armengol-Estap\u00e9 and Maite Melero},\n      year={2022},\n      eprint={2202.06871},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n### Disclaimer\n\n<details>\n<summary>Click to expand</summary>\n\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\n\nWhen third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of Artificial Intelligence.\n\nIn no event shall the owner and creator of the models (BSC \u2013 Barcelona Supercomputing Center) be liable for any results arising from the use made by third parties of these models.\n", "qas": [{"id": "q1", "question": "What is the model architecture of projecte-aina/bart-base-ca-casum?", "answers": [{"text": "bart", "answer_start": 6188, "answer_end": 6191}]}, {"id": "q2", "question": "What is the model task of projecte-aina/bart-base-ca-casum?", "answers": [{"text": "summarization", "answer_start": 40, "answer_end": 52}]}]}]}, {"title": "projecte-aina/bart-base-ca", "paragraphs": [{"context": "---\nlanguage: ca\nlicense: apache-2.0\ninference: false\ndatasets:\n- projecte-aina/catalan_textual_corpus\n---\n\n# BART-Ca: The monolingual Catalan BART\n\n## Table of Contents\n<details>\n<summary>Click to expand</summary>\n\n- [Model description](#model-description)\n- [Intended uses and limitations](#intended-use)\n- [How to use](#how-to-use)\n- [Limitations and bias](#limitations-and-bias)\n- [Training](#training)\n  - [Training data](#training-data)\n  - [Training procedure](#training-procedure)\n    - [Tokenization](#tokenization)\n    - [Hyperparameters](#hyperparameters)\n- [Evaluation](#evaluation)\n   - [Variable and metrics](#variable-and-metrics)\n   - [Evaluation results](#evaluation-results)\n- [Additional information](#additional-information)\n  - [Author](#author)\n  - [Contact information](#contact-information)\n  - [Copyright](#copyright)\n  - [Licensing information](#licensing-information)\n  - [Funding](#funding)\n  - [Citing information](#citing-information)\n  - [Disclaimer](#disclaimer)\n</details>\n\n\n## Model description\n\nBART-ca is a transformer-based language model for the Catalan language and has been trained on a medium-size corpus collected from publicly available corpora and crawlers with the [Catalan Textual Corpus](\n\n## Intended uses and limitations\n\nYou can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\n\n## How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import BartTokenizer, BartModel\ntokenizer = BartTokenizer.from_pretrained('projecte-aina/bart-base-ca')\nmodel = BartModel.from_pretrained('projecte-aina/bart-base-ca')\ninputs = tokenizer(\"Hola, el meu gos \u00e9s molt bonic\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n```\n\n## Limitations and bias\nAt the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.\n\n## Training\n\n### Training data\n\nAs training data, we used the [Catalan Textual Corpus]( a 1760-million-token web corpus of Catalan built from several sources.\n\n### Training procedure\n\n#### Tokenization\n\nThe training corpus has been tokenized using a byte version of [Byte-Pair Encoding (BPE)]( with a vocabulary size of 51,200 tokens. \n\n#### Hyperparameters\n\nThe hyperparameters were adapted for [fairseq]( from the original BART's paper.\n\n Value  |\n--------|\n 5e-4 |\n Polynomial Decay |\n 10000   |\n 2048     |\n 0.01   |\n 125000     |\n\n## Evaluation\n\n### Variable and metrics\n\nThis model is intended to be fine-tuned for downstream tasks.\n\n### Evaluation results\n\nThis model is intended to be fine-tuned for downstream tasks.\n\n\n## Additional information\n\n### Author\nText Mining Unit (TeMU) at the Barcelona Supercomputing Center (bsc-temu@bsc.es)\n\n### Contact information\nFor further information, send an email to aina@bsc.es\n\n### Copyright\nCopyright (c) 2022 Text Mining Unit at Barcelona Supercomputing Center \n\n### Licensing information\n[Apache License, Version 2.0](\n\n### Funding\nThis work was funded by the [Departament de la Vicepresid\u00e8ncia i de Pol\u00edtiques Digitals i Territori de la Generalitat de Catalunya]( within the framework of [Projecte AINA](\n\n### Citation information\n\nIf you use any of these resources (datasets or models) in your work, please cite our latest preprint:\n\n```bibtex\n@misc{degibert2022sequencetosequence,\n      title={Sequence-to-Sequence Resources for Catalan}, \n      author={Ona de Gibert and Ksenia Kharitonova and Blanca Calvo Figueras and Jordi Armengol-Estap\u00e9 and Maite Melero},\n      year={2022},\n      eprint={2202.06871},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n### Disclaimer\n\n<details>\n<summary>Click to expand</summary>\n\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\n\nWhen third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of Artificial Intelligence.\n\nIn no event shall the owner and creator of the models (BSC \u2013 Barcelona Supercomputing Center) be liable for any results arising from the use made by third parties of these models.\n", "qas": [{"id": "q1", "question": "What is the model architecture of projecte-aina/bart-base-ca?", "answers": [{"text": "bart", "answer_start": 1763, "answer_end": 1766}]}]}]}, {"title": "projecte-aina/roberta-base-ca-cased-ner", "paragraphs": [{"context": "---\n\nlanguage:\n\n- ca\n\nlicense: apache-2.0\n\ntags:\n\n- \"catalan\"\n\n- \"named entity recognition\"\n\n- \"ner\"\n\n- \"CaText\"\n\n- \"Catalan Textual Corpus\"\n\ndatasets:\n\n- \"projecte-aina/ancora-ca-ner\"  \n\nmetrics:\n\n- f1\n\nmodel-index:\n- name: roberta-base-ca-cased-ner\n  results:\n  - task: \n      type: token-classification \n    dataset:\n      type:   projecte-aina/ancora-ca-ner\n      name: ancora-ca-ner\n    metrics:\n      - type: f1\n        value: 0.8813\n\nwidget:\n\n- text: \"Em dic Llu\u00efsa i visc a Santa Maria del Cam\u00ed.\" \n\n- text: \"L'Aina, la Berta i la Norma s\u00f3n molt amigues.\"\n\n- text: \"El Mart\u00ed llegeix el Cavall Fort.\"\n\n---\n\n# Catalan BERTa (RoBERTa-base) finetuned for Named Entity Recognition.\n\n## Table of Contents\n<details>\n<summary>Click to expand</summary>\n\n- [Model description](#model-description)\n- [Intended uses and limitations](#intended-uses-and-limitations)\n- [How to Use](#how-to-use)\n- [Training](#training)\n  - [Training data](#training-data)\n  - [Training procedure](#training-procedure)\n- [Evaluation](#evaluation)\n   - [Variable and metrics](#variable-and-metrics)\n   - [Evaluation results](#evaluation-results)\n- [Additional information](#addional-information)\n  - [Author](#author)\n  - [Contact information](#contact-information)\n  - [Copyright](#copyright)\n  - [Licensing information](#licensing-information)\n  - [Funding](#funding)\n  - [Citing information](#citing-information)\n  - [Disclaimer](#disclaimer))\n</details>\n\n\n## Model description\n\nThe **roberta-base-ca-cased-ner** is a Named Entity Recognition (NER) model for the Catalan language fine-tuned from the [BERTa]( model, a [RoBERTa]( base model pre-trained on a medium-size corpus collected from publicly available corpora and crawlers (check the BERTa model card for more details).\n\n## Intended uses and limitations\n\n\n## How to use\n\n\n## Limitations and bias\nAt the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.\n\n## Training\nWe used the NER dataset in Catalan called [Ancora-ca-ner]( for training and evaluation.\n\n## Evaluation \nWe evaluated the _roberta-base-ca-cased-ner_ on the Ancora-ca-ner test set against standard multilingual and monolingual baselines:\n\n Ancora-ca-ner (F1)| \n:-------------|\n **88.13** |\n 86.38 |\n 87.66 | \n 77.66 |\n\nFor more details, check the fine-tuning and evaluation scripts in the official [GitHub repository](\n\n## Additional information\n\n### Author\nText Mining Unit (TeMU) at the Barcelona Supercomputing Center (bsc-temu@bsc.es)\n\n### Contact information\nFor further information, send an email to aina@bsc.es\n\n### Copyright\nCopyright (c) 2021 Text Mining Unit at Barcelona Supercomputing Center \n\n### Licensing Information\n[Apache License, Version 2.0](\n\n### Funding\nThis work was funded by the [Departament de la Vicepresid\u00e8ncia i de Pol\u00edtiques Digitals i Territori de la Generalitat de Catalunya]( within the framework of [Projecte AINA](\n\n### Citation information\n\nIf you use any of these resources (datasets or models) in your work, please cite our latest paper:\n```bibtex\n@inproceedings{armengol-estape-etal-2021-multilingual,\n    title = \"Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? {A} Comprehensive Assessment for {C}atalan\",\n    author = \"Armengol-Estap{\\'e}, Jordi  and\n      Carrino, Casimiro Pio  and\n      Rodriguez-Penagos, Carlos  and\n      de Gibert Bonet, Ona  and\n      Armentano-Oller, Carme  and\n      Gonzalez-Agirre, Aitor  and\n      Melero, Maite  and\n      Villegas, Marta\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    doi = \"10.18653/v1/2021.findings-acl.437\",\n    pages = \"4933--4946\",\n}\n```\n\n### Disclaimer\n\n<details>\n<summary>Click to expand</summary>\n\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\n\nWhen third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of Artificial Intelligence.\n\nIn no event shall the owner and creator of the models (BSC \u2013 Barcelona Supercomputing Center) be liable for any results arising from the use made by third parties of these models.", "qas": [{"id": "q1", "question": "What is the model architecture of projecte-aina/roberta-base-ca-cased-ner?", "answers": [{"text": "roberta", "answer_start": 225, "answer_end": 231}]}, {"id": "q2", "question": "What is the model task of projecte-aina/roberta-base-ca-cased-ner?", "answers": [{"text": "token-classification", "answer_start": 285, "answer_end": 304}]}]}]}, {"title": "projecte-aina/roberta-base-ca-cased-pos", "paragraphs": [{"context": "---\n\nlanguage:\n\n- ca\n\nlicense: apache-2.0\n\ntags:\n\n- \"catalan\"\n\n- \"part of speech tagging\"\n\n- \"pos\"\n\n- \"CaText\"\n\n- \"Catalan Textual Corpus\"\n\ndatasets:\n\n- \"universal_dependencies\"\n\nmetrics:\n\n- f1\n\ninference:\n  parameters:\n    aggregation_strategy: \"first\"\n    \nmodel-index:\n- name: roberta-base-ca-cased-pos\n  results:\n  - task: \n      type: token-classification \n    dataset:\n      type:   universal_dependencies\n      name: Ancora-ca-POS\n    metrics:\n      - name: F1\n        type: f1\n        value: 0.9893832385244624\n\nwidget:\n\n- text: \"Em dic Llu\u00efsa i visc a Santa Maria del Cam\u00ed.\" \n\n- text: \"L'Aina, la Berta i la Norma s\u00f3n molt amigues.\"\n\n- text: \"El Mart\u00ed llegeix el Cavall Fort.\"\n\n---\n\n# Catalan BERTa (roberta-base-ca) finetuned for Part-of-speech-tagging (POS)\n\n## Table of Contents\n<details>\n<summary>Click to expand</summary>\n\n- [Model description](#model-description)\n- [Intended uses and limitations](#intended-use)\n- [How to use](#how-to-use)\n- [Limitations and bias](#limitations-and-bias)\n- [Training](#training)\n  - [Training data](#training-data)\n  - [Training procedure](#training-procedure)\n- [Evaluation](#evaluation)\n   - [Variable and metrics](#variable-and-metrics)\n   - [Evaluation results](#evaluation-results)\n- [Additional information](#additional-information)\n  - [Author](#author)\n  - [Contact information](#contact-information)\n  - [Copyright](#copyright)\n  - [Licensing information](#licensing-information)\n  - [Funding](#funding)\n  - [Citing information](#citing-information)\n  - [Disclaimer](#disclaimer)\n</details>\n\n## Model description\n\nThe **roberta-base-ca-cased-pos** is a Part-of-speech-tagging (POS) model for the Catalan language fine-tuned from the roberta-base-ca model, a [RoBERTa]( base model pre-trained on a medium-size corpus collected from publicly available corpora and crawlers.\n\n## Intended uses and limitations\n\n**roberta-base-ca-cased-pos** model can be used to Part-of-speech-tagging (POS) a text. The model is limited by its training dataset and may not generalize well for all use cases.\n\n## How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import pipeline\nfrom pprint import pprint\n\nnlp = pipeline(\"token-classification\", model=\"projecte-aina/roberta-base-ca-cased-pos\")\nexample = \"Em dic Llu\u00efsa i visc a Santa Maria del Cam\u00ed.\"\n\npos_results = nlp(example)\npprint(pos_results)\n```\n\n## Limitations and bias\nAt the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.\n\n## Training\n\n### Training data\nWe used the POS dataset in Catalan from the [Universal Dependencies Treebank]( we refer to _Ancora-ca-pos_ for training and evaluation.\n\n### Training procedure\nThe model was trained with a batch size of 16 and a learning rate of 5e-5 for 5 epochs. We then selected the best checkpoint using the downstream task metric in the corresponding development set and then evaluated it on the test set.\n\n## Evaluation\n\n### Variable and metrics\n\nThis model was finetuned maximizing F1 score.\n\n## Evaluation results\nWe evaluated the _roberta-base-ca-cased-pos_ on the Ancora-ca-ner test set against standard multilingual and monolingual baselines:\n\n\n AnCora-Ca-POS (F1)   | \n:-------------|\n**98.93** |\n 98.82 |\n 98.89 | \n 97.60 | \n\nFor more details, check the fine-tuning and evaluation scripts in the official [GitHub repository](\n\n## Additional information\n\n### Author\nText Mining Unit (TeMU) at the Barcelona Supercomputing Center (bsc-temu@bsc.es)\n\n### Contact information\nFor further information, send an email to aina@bsc.es\n\n### Copyright\nCopyright (c) 2022 Text Mining Unit at Barcelona Supercomputing Center \n\n### Licensing information\n[Apache License, Version 2.0](\n\n### Funding\nThis work was funded by the [Departament de la Vicepresid\u00e8ncia i de Pol\u00edtiques Digitals i Territori de la Generalitat de Catalunya]( within the framework of [Projecte AINA](\n\n### Citation Information  \nIf you use any of these resources (datasets or models) in your work, please cite our latest paper:\n```bibtex\n@inproceedings{armengol-estape-etal-2021-multilingual,\n    title = \"Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? {A} Comprehensive Assessment for {C}atalan\",\n    author = \"Armengol-Estap{\\'e}, Jordi  and\n      Carrino, Casimiro Pio  and\n      Rodriguez-Penagos, Carlos  and\n      de Gibert Bonet, Ona  and\n      Armentano-Oller, Carme  and\n      Gonzalez-Agirre, Aitor  and\n      Melero, Maite  and\n      Villegas, Marta\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    doi = \"10.18653/v1/2021.findings-acl.437\",\n    pages = \"4933--4946\",\n}\n```\n\n### Disclaimer\n\n<details>\n<summary>Click to expand</summary>\n\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\n\nWhen third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of Artificial Intelligence.\n\nIn no event shall the owner and creator of the models (BSC \u2013 Barcelona Supercomputing Center) be liable for any results arising from the use made by third parties of these models.\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of projecte-aina/roberta-base-ca-cased-pos?", "answers": [{"text": "roberta", "answer_start": 280, "answer_end": 286}]}, {"id": "q2", "question": "What is the model task of projecte-aina/roberta-base-ca-cased-pos?", "answers": [{"text": "token-classification", "answer_start": 340, "answer_end": 359}]}]}]}, {"title": "projecte-aina/roberta-base-ca-cased-qa", "paragraphs": [{"context": "---\nlanguage:\n- ca\nlicense: apache-2.0\ntags:\n- \"catalan\"\n- \"qa\"\ndatasets:\n- \"xquad-ca\"\n- \"viquiquad\"  \nmetrics:\n- \"f1\"\n- \"exact match\"\nwidget:\n- text: \"Quan va comen\u00e7ar el Super3?\"\n  context: \"El Super3 o Club Super3 \u00e9s un univers infantil catal\u00e0 creat a partir d'un programa em\u00e8s per Televisi\u00f3 de Catalunya des del 1991. Est\u00e0 format per un canal de televisi\u00f3, la revista S\u00fapers!, la Festa dels S\u00fapers i un club que t\u00e9 un mili\u00f3 i mig de socis.\"\n  \n- text: \"Quants eren els germans Marx?\"\n  context: \"Els germans Marx van ser un grup de c\u00f2mics dels Estats Units que origin\u00e0riament estava compost per cinc germans (entre par\u00e8ntesis els noms art\u00edstics): Leonard (Chico), Adolph (Harpo), Julius (Groucho), Milton (Gummo) i Herbert (Zeppo).\"\n  \n- text: \"On van ser els Jocs Ol\u00edmpics de 1992?\"\n  context: \"Els Jocs Ol\u00edmpics d'estiu de 1992, oficialment Jocs Ol\u00edmpics de la XXV Olimp\u00edada, es van celebrar a la ciutat de Barcelona entre els dies 25 de juliol i 9 d'agost de 1992. Hi participaren 9.356 atletes (6.652 homes i 2.704 dones) de 169 comit\u00e8s nacionals, que competiren en 32 esports i 286 especialitats.\"\n  \n- text: \"Qui va dissenyar la Sagrada Fam\u00edlia?\"\n  context: \"El Temple Expiatori de la Sagrada Fam\u00edlia, conegut habitualment com la Sagrada Fam\u00edlia, \u00e9s una bas\u00edlica cat\u00f2lica situada a la ciutat de Barcelona. \u00c9s un dels exemples m\u00e9s coneguts del modernisme catal\u00e0 i un edifici \u00fanic al m\u00f3n, que ha esdevingut tot un s\u00edmbol de la ciutat. Obra inacabada de l'arquitecte catal\u00e0 Antoni Gaud\u00ed, \u00e9s al barri de la Sagrada Fam\u00edlia, al districte de l'Eixample de la ciutat.\"\n  \n- text: \"Quin \u00e9s el tercer volc\u00e0 m\u00e9s gran de la Terra?\"\n  context: \"El Teide (o Pic del Teide) \u00e9s un estratovolc\u00e0 i muntanya de Tenerife, Illes Can\u00e0ries (28.27 N, 16.6 O). Amb una altitud de 3718 m sobre el nivell del mar i amb aproximadament uns 7000 m sobre el llit mar\u00ed adjacent, \u00e9s la muntanya m\u00e9s alta d'Espanya, la muntanya m\u00e9s alta de totes les illes atl\u00e0ntiques i el tercer volc\u00e0 m\u00e9s gran de la Terra.\"\n\n\n---\n\n# Catalan BERTa (roberta-base-ca) finetuned for Question Answering.\n\n## Table of Contents\n<details>\n<summary>Click to expand</summary>\n\n- [Model description](#model-description)\n- [Intended uses and limitations](#intended-use)\n- [How to use](#how-to-use)\n- [Limitations and bias](#limitations-and-bias)\n- [Training](#training)\n  - [Training data](#training-data)\n  - [Training procedure](#training-procedure)\n- [Evaluation](#evaluation)\n   - [Variable and metrics](#variable-and-metrics)\n   - [Evaluation results](#evaluation-results)\n- [Additional information](#additional-information)\n  - [Author](#author)\n  - [Contact information](#contact-information)\n  - [Copyright](#copyright)\n  - [Licensing information](#licensing-information)\n  - [Funding](#funding)\n  - [Citing information](#citing-information)\n  - [Disclaimer](#disclaimer)\n</details>\n\n## Model description\n\nThe **roberta-base-ca-cased-qa** is a Question Answering (QA) model for the Catalan language fine-tuned from the roberta-base-ca model, a [RoBERTa]( base model pre-trained on a medium-size corpus collected from publicly available corpora and crawlers.\n\n## Intended uses and limitations\n\n**roberta-base-ca-cased-qa** model can be used for extractive question answering. The model is limited by its training dataset and may not generalize well for all use cases.\n\n## How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import pipeline\nnlp = pipeline(\"question-answering\", model=\"projecte-aina/roberta-base-ca-cased-qa\")\ntext = \"Quan va comen\u00e7ar el Super3?\"\ncontext = \"El Super3 o Club Super3 \u00e9s un univers infantil catal\u00e0 creat a partir d'un programa em\u00e8s per Televisi\u00f3 de Catalunya des del 1991. Est\u00e0 format per un canal de televisi\u00f3, la revista S\u00fapers!, la Festa dels S\u00fapers i un club que t\u00e9 un mili\u00f3 i mig de socis.\"\n  \nqa_results = nlp(text, context)\nprint(qa_results)\n```\n\n## Limitations and bias\nAt the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.\n\n## Training\n\n### Training data\nWe used the QA dataset in Catalan called [CatalanQA]( for training and evaluation, and the [XQuAD-ca]( test set for evaluation.\n\n### Training procedure\nThe model was trained with a batch size of 16 and a learning rate of 5e-5 for 5 epochs. We then selected the best checkpoint using the downstream task metric in the corresponding development set and then evaluated it on the test set.\n\n## Evaluation\n\n### Variable and metrics\n\nThis model was finetuned maximizing F1 score.\n\n### Evaluation results\nWe evaluated the _roberta-base-ca-cased-qa_ on the CatalanQA and XQuAD-ca test sets against standard multilingual and monolingual baselines:\n\n\n ViquiQuAD (F1/EM)   \n:-------------:\n **86.99/73.25** \n 86.97/72.22 \n 85.50/70.47 \n 85.45/70.75 \n\nFor more details, check the fine-tuning and evaluation scripts in the official [GitHub repository](\n\n## Additional information\n\n### Author\nText Mining Unit (TeMU) at the Barcelona Supercomputing Center (bsc-temu@bsc.es)\n\n### Contact information\nFor further information, send an email to aina@bsc.es\n\n### Copyright\nCopyright (c) 2022 Text Mining Unit at Barcelona Supercomputing Center \n\n### Licensing information\n[Apache License, Version 2.0](\n\n### Funding\nThis work was funded by the [Departament de la Vicepresid\u00e8ncia i de Pol\u00edtiques Digitals i Territori de la Generalitat de Catalunya]( within the framework of [Projecte AINA](\n\n### Citation Information  \nIf you use any of these resources (datasets or models) in your work, please cite our latest paper:\n```bibtex\n@inproceedings{armengol-estape-etal-2021-multilingual,\n    title = \"Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? {A} Comprehensive Assessment for {C}atalan\",\n    author = \"Armengol-Estap{\\'e}, Jordi  and\n      Carrino, Casimiro Pio  and\n      Rodriguez-Penagos, Carlos  and\n      de Gibert Bonet, Ona  and\n      Armentano-Oller, Carme  and\n      Gonzalez-Agirre, Aitor  and\n      Melero, Maite  and\n      Villegas, Marta\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    doi = \"10.18653/v1/2021.findings-acl.437\",\n    pages = \"4933--4946\",\n}\n```\n\n### Disclaimer\n\n<details>\n<summary>Click to expand</summary>\n\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\n\nWhen third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of Artificial Intelligence.\n\nIn no event shall the owner and creator of the models (BSC \u2013 Barcelona Supercomputing Center) be liable for any results arising from the use made by third parties of these models.\n", "qas": [{"id": "q1", "question": "What is the model architecture of projecte-aina/roberta-base-ca-cased-qa?", "answers": [{"text": "roberta", "answer_start": 2010, "answer_end": 2016}]}, {"id": "q2", "question": "What is the model task of projecte-aina/roberta-base-ca-cased-qa?", "answers": [{"text": "question-answering", "answer_start": 3433, "answer_end": 3450}]}]}]}, {"title": "projecte-aina/roberta-base-ca-cased-tc", "paragraphs": [{"context": "---\n\nlanguage:\n\n- ca\n\ntags:\n\n- \"catalan\"\n\n- \"text classification\"\n\n- \"tecla\"\n\n- \"CaText\"\n\n- \"Catalan Textual Corpus\"\n\ndatasets:\n\n- \"projecte-aina/tecla\"\n\nmetrics:\n- accuracy\n\nmodel-index:\n- name: roberta-base-ca-cased-tc\n  results:\n  - task:\n      type: text-classification\n    dataset:\n      name: TeCla\n      type: projecte-aina/tecla\n    metrics:\n      - name: Accuracy\n        type: accuracy\n        value: 0.740388810634613\nwidget:\n\n- text: \"Els Pets presenten el seu nou treball al Palau Sant Jordi.\" \n\n- text: \"Els barcelonins incrementen un 23% l\u2019\u00fas del cotxe des de l\u2019inici de la pand\u00e8mia.\"\n\n- text: \"Retards a quatre l\u00ednies de Rodalies per una avaria entre Sants i pla\u00e7a de Catalunya.\"\n\n- text: \"Majors de 60 anys i sanitaris comen\u00e7aran a rebre la tercera dosi de la vacuna covid els propers dies.\"\n\n- text: \"Els cinemes Verdi estrenen Verdi Classics, un nou canal de televisi\u00f3.\"\n\n---\n\n# Catalan BERTa (roberta-base-ca) finetuned for Text Classification.\n\n## Table of Contents\n<details>\n<summary>Click to expand</summary>\n\n- [Model description](#model-description)\n- [Intended uses and limitations](#intended-use)\n- [How to use](#how-to-use)\n- [Limitations and bias](#limitations-and-bias)\n- [Training](#training)\n  - [Training data](#training-data)\n  - [Training procedure](#training-procedure)\n- [Evaluation](#evaluation)\n   - [Variable and metrics](#variable-and-metrics)\n   - [Evaluation results](#evaluation-results)\n- [Additional information](#additional-information)\n  - [Author](#author)\n  - [Contact information](#contact-information)\n  - [Copyright](#copyright)\n  - [Licensing information](#licensing-information)\n  - [Funding](#funding)\n  - [Citing information](#citing-information)\n  - [Disclaimer](#disclaimer)\n</details>\n\n## Model description\nThe **roberta-base-ca-cased-tc** is a Text Classification (TC) model for the Catalan language fine-tuned from the roberta-base-ca model, a [RoBERTa]( base model pre-trained on a medium-size corpus collected from publicly available corpora and crawlers.\n\n## Intended uses and limitations\n\n**roberta-base-ca-cased-tc** model can be used to classify texts. The model is limited by its training dataset and may not generalize well for all use cases.\n\n## How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import pipeline\nfrom pprint import pprint\n\nnlp = pipeline(\"text-classification\", model=\"projecte-aina/roberta-base-ca-cased-tc\")\nexample = \"Retards a quatre l\u00ednies de Rodalies per una avaria entre Sants i pla\u00e7a de Catalunya.\"\n\ntc_results = nlp(example)\npprint(tc_results)\n```\n\n## Limitations and bias\nAt the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.\n\n\n## Training\n\n### Training data\nWe used the TC dataset in Catalan called [TeCla]( for training and evaluation.\n\n### Training procedure\nThe model was trained with a batch size of 16 and a learning rate of 5e-5 for 5 epochs. We then selected the best checkpoint using the downstream task metric in the corresponding development set and then evaluated it on the test set.\n\n## Evaluation\n\n### Variable and metrics\n\nThis model was finetuned maximizing accuracy.\n\n## Evaluation results\nWe evaluated the _roberta-base-ca-cased-tc_ on the TeCla test set against standard multilingual and monolingual baselines:\n\n TeCla (accuracy) | \n:-------------|\n **74.04** |\n  70.56 | \n 71.68 | \n  73.22 | \n\nFor more details, check the fine-tuning and evaluation scripts in the official [GitHub repository](\n\n## Additional information\n\n### Author\nText Mining Unit (TeMU) at the Barcelona Supercomputing Center (bsc-temu@bsc.es)\n\n### Contact information\nFor further information, send an email to aina@bsc.es\n\n### Copyright\nCopyright (c) 2022 Text Mining Unit at Barcelona Supercomputing Center \n\n### Licensing information\n[Apache License, Version 2.0](\n\n### Funding\nThis work was funded by the [Departament de la Vicepresid\u00e8ncia i de Pol\u00edtiques Digitals i Territori de la Generalitat de Catalunya]( within the framework of [Projecte AINA](\n\n### Citation Information  \nIf you use any of these resources (datasets or models) in your work, please cite our latest paper:\n```bibtex\n@inproceedings{armengol-estape-etal-2021-multilingual,\n    title = \"Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? {A} Comprehensive Assessment for {C}atalan\",\n    author = \"Armengol-Estap{\\'e}, Jordi  and\n      Carrino, Casimiro Pio  and\n      Rodriguez-Penagos, Carlos  and\n      de Gibert Bonet, Ona  and\n      Armentano-Oller, Carme  and\n      Gonzalez-Agirre, Aitor  and\n      Melero, Maite  and\n      Villegas, Marta\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    doi = \"10.18653/v1/2021.findings-acl.437\",\n    pages = \"4933--4946\",\n}\n```\n\n### Disclaimer\n\n<details>\n<summary>Click to expand</summary>\n\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\n\nWhen third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of Artificial Intelligence.\n\nIn no event shall the owner and creator of the models (BSC \u2013 Barcelona Supercomputing Center) be liable for any results arising from the use made by third parties of these models.", "qas": [{"id": "q1", "question": "What is the model architecture of projecte-aina/roberta-base-ca-cased-tc?", "answers": [{"text": "roberta", "answer_start": 196, "answer_end": 202}]}, {"id": "q2", "question": "What is the model task of projecte-aina/roberta-base-ca-cased-tc?", "answers": [{"text": "text-classification", "answer_start": 254, "answer_end": 272}]}]}]}, {"title": "ps2102/DialoGPT-small-harrypotter", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of ps2102/DialoGPT-small-harrypotter?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "pstroe/roberta-base-latin-cased", "paragraphs": [{"context": "## RoBERTa Latin model\n\nThis is a Latin RoBERTa-based LM model.\n\nThe data it uses is the same as has been used to compute the text referenced HTR evaluation measures.\n\nThe intention of the Transformer-based LM is twofold: on the one hand, it will be used for the evaluation of HTR results, on the other, it should be used as a decoder for the TrOCR architecture.\n\nThe basis for the word unigram and character n-gram computations is the Latin part of the [cc100 corpus](\n\nThe overall corpus contains 2.5G of text data.\n\n### Preprocessing\n\nI undertook the following preprocessing steps:\n\n  - Removal of all \"pseudo-Latin\" text (\"Lorem ipsum ...\").\n  - Use of [CLTK]( for sentence splitting and normalisation.\n  - Retaining only lines containing letters of the Latin alphabet, numerals, and certain punctuation (--> `grep -P '^[A-z0-9\u00c4\u00d6\u00dc\u00e4\u00f6\u00fc\u00c6\u00e6\u0152\u0153\u1d6b\u0100\u0101\u016b\u014d\u014c.,;:?!\\- \u0118\u0119]+$' la.nolorem.tok.txt`\n  - deduplication of the corpus\n\nThe result is a corpus of ~390 million tokens.\n\nThe dataset used to train this model is available [HERE](\n\n### Contact\n\nFor contact, reach out to Phillip Str\u00f6bel [via mail](mailto:pstroebel@cl.uzh.ch) or [via Twitter](\n\n### How to cite\n\nIf you use this model, pleas cite it as:\n\n    @online{stroebel-roberta-base-latin-cased1,\n        author = {Str\u00f6bel, Phillip Benjamin},\n        title = {RoBERTa Base Latin Cased V1},\n        year = 2022,\n        url = {\n        urldate = {YYYY-MM-DD}\n    }", "qas": [{"id": "q1", "question": "What is the model architecture of pstroe/roberta-base-latin-cased?", "answers": [{"text": "roberta", "answer_start": 1216, "answer_end": 1222}]}]}]}, {"title": "pszemraj/Ballpark-Trivia-L", "paragraphs": [{"context": "---\n\nlanguage:\n- en\ntags:\n- text-generation\n- gpt2\n- gpt\nlicense: mit\ndatasets:\n- natural questions\n\nwidget:\n- text: \"how many ping-pong balls fit inside a standard 747 jet aeroplane?\\nperson beta:\\n\\n\"\n  example_title: \"ping-pong\"\n- text: \"What is the capital of Uganda?\\nperson beta:\\n\\n\"\n  example_title: \"geography\"\n- text: \"What is the most popular TV show of all time?\\nperson beta:\\n\\n\"\n  example_title: \"pseudo-culture\"\n- text: \"A man pushes his car to a hotel and tells the owner he\u2019s bankrupt. Why?\\nperson beta:\\n\\n\"\n  example_title: \"brain teaser\"\n\ninference:\n  parameters:\n    min_length: 2\n    max_length: 32\n    no_repeat_ngram_size: 2\n    do_sample: True\n    top_p: 0.90\n    top_k: 10\n    repetition_penalty: 2.1\n    \n\n---\n\n\n\n\n# Ballpark Trivia: Size L\n\nAre you frequently asked google-able Trivia questions and annoyed by it? Well, this is the model for you! Ballpark Trivia Bot answers any trivia question with something that sounds plausible but is probably not 100% correct. One might say.. the answers are in the right ballpark. Check out a demo of it [here](\n\n```\nhow many varieties of eggplant are there?\n\nperson beta: \nabout 4,000\n```\n\n## Training \n\nThis text gen model is a GPT-2 774M Parameter Size L Model, first trained on [Wizard of Wikipedia]( for 40k steps (34/36 layers frozen for the fine-tuning), and then subsequently trained for 40k steps on a parsed variant of [Natural Questions]( 34/36 layers frozen for the fine-tuning) to accidentally create this model. \n\nNote that because the model was originally trained for use in a [chatbot application]( it uses a named conversation dialogue structure, _, i.e. the questions are asked by person alpha, and responded to by person beta_. Even if you don't specify person alpha, it should hopefully respond to any question.\n\n## Example Prompt\n\n- the default examples are not great\n- you can type in any trivia question or delete the example and write `what`  or `when` in there, and it will generate the rest of the trivia question **and the answer**!\n\n```\nwhere is the tv show the arrow filmed\n\nperson beta: \nVancouver, British Columbia\n```", "qas": [{"id": "q1", "question": "What is the model architecture of pszemraj/Ballpark-Trivia-L?", "answers": [{"text": "gpt2", "answer_start": 46, "answer_end": 49}]}, {"id": "q2", "question": "What is the model task of pszemraj/Ballpark-Trivia-L?", "answers": [{"text": "text-generation", "answer_start": 28, "answer_end": 42}]}]}]}, {"title": "pszemraj/Ballpark-Trivia-XL", "paragraphs": [{"context": "---\n\nlanguage:\n- en\ntags:\n- text-generation\n- gpt2\n- gpt\n- trivia\n- chatbot\nlicense: mit\n\nwidget:\n- text: \"how many ping-pong balls fit inside a standard 747 jet aeroplane?\\nperson beta:\\n\\n\"\n  example_title: \"ping-pong\"\n- text: \"What is the capital of Uganda?\\nperson beta:\\n\\n\"\n  example_title: \"geography\"\n- text: \"What is the most popular TV show of all time?\\nperson beta:\\n\\n\"\n  example_title: \"pseudo-culture\"\n- text: \"A man pushes his car to a hotel and tells the owner he\u2019s bankrupt. Why?\\nperson beta:\\n\\n\"\n  example_title: \"brain teaser\"\n\ninference:\n  parameters:\n    min_length: 2\n    max_length: 32\n    no_repeat_ngram_size: 2\n    do_sample: False\n    num_beams: 4\n    early_stopping: True\n    repetition_penalty: 2.1\n    \n\n---\n\n# Ballpark Trivia: Size XL\n\n**Check out a demo on HF Spaces [here](\n\nAre you frequently asked google-able Trivia questions and annoyed by it? Well, this is the model for you! Ballpark Trivia Bot answers any trivia question with something that sounds plausible but is probably not 100% correct. One might say.. the answers are in the right ballpark. \n\nThis is by far the largest model trained and should be _more_ credible in its answers or at least able to handle more kinds of questions.\n\n``` \nwhat is the temperature of dry ice in kelvin\n\nperson beta: \n194.65 K\n```\n\n## Training \nThis text gen model is a GPT-2 ~1.5 B Parameter Size XL Model, first trained on [Wizard of Wikipedia]( for 40k steps (**33**/36 layers frozen for the fine-tuning), and then subsequently trained for 40k steps on a parsed variant of [Natural Questions]( **34**/36 layers frozen for the second fine-tuning) to accidentally create this model. \n\nNote that because the model was originally trained for use in a [chatbot application]( it uses a named conversation dialogue structure, _i.e. the questions are asked by person alpha, and responded to by person beta_. Even if you don't specify person alpha in the prompt, it hopefully responds to any question.\n\n\n## Example Prompt\n\n- the default examples are not great\n- you can type in any trivia question or delete the example and write `what`  or `when` in there, and it will generate the rest of the trivia question **and the answer**!\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of pszemraj/Ballpark-Trivia-XL?", "answers": [{"text": "gpt2", "answer_start": 46, "answer_end": 49}]}, {"id": "q2", "question": "What is the model task of pszemraj/Ballpark-Trivia-XL?", "answers": [{"text": "text-generation", "answer_start": 28, "answer_end": 42}]}]}]}, {"title": "pszemraj/GPT-Converse-1pt3B-Neo-WoW-DD-17", "paragraphs": [{"context": "---\n\nlanguage:\n- en\ntags:\n- text-generation\n- gpt2\n- gpt\nlicense: mit\ndatasets:\n- natural questions\n\nwidget:\n- text: \"hi, how are you doing bruh?\\nperson beta:\\n\\n\"\n  example_title: \"greeting\"\n- text: \"Can you actually take me for dinner somewhere nice this time?\\nperson beta:\\n\\n\"\n  example_title: \"dinner\"\n- text: \"Honey, I have clogged the toilet for the third time this month.. sorry..\\nperson beta:\\n\\n\"\n  example_title: \"overflow\"\n- text: \"A man pushes his car to a hotel and tells the owner he\u2019s bankrupt. Why?\\nperson beta:\\n\\n\"\n  example_title: \"brain teaser\"\n\ninference:\n  parameters:\n    min_length: 2\n    max_length: 64\n    length_penalty: 0.7\n    no_repeat_ngram_size: 3\n    do_sample: True\n    top_p: 0.85\n    top_k: 10\n    repetition_penalty: 2.1\n    \n\n---\n\n# GPT-Neo 1.3 B Conversational - 17 total epochs\n\n- trained on the Wizard of Wikipedia parl.ai dataset + Daily Dialogues dataset\n  - 13 on WoW 4 on Daily Dialogues\n- the aim is to use the model as a customizable chatbot with the personID labels as pseudo-SOT/EOT tokens, i.e. ending the prompt with `person beta:` means that it is extremely likely that _person beta:_ responds, as opposed to the entered prompt being added on to.\n- a link to the project repo that details how to effectively use such a trained model is [here](", "qas": [{"id": "q2", "question": "What is the model task of pszemraj/GPT-Converse-1pt3B-Neo-WoW-DD-17?", "answers": [{"text": "text-generation", "answer_start": 28, "answer_end": 42}]}]}]}, {"title": "ramsrigouthamg/t5-large-paraphraser-diverse-high-quality", "paragraphs": [{"context": "Blog post with more details as well as easy to use Google Colab link: \n\n!pip install transformers==4.10.2\n\n!pip install sentencepiece==0.1.96\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\ntokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint (\"device \",device)\nmodel = model.to(device)\n\n# Beam Search\n\ncontext = \"Once, a group of frogs were roaming around the forest in search of water.\"\ntext = \"paraphrase: \"+context + \" </s>\"\n\nencoding = tokenizer.encode_plus(text,max_length =128, padding=True, return_tensors=\"pt\")\ninput_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n\nmodel.eval()\nbeam_outputs = model.generate(\n    input_ids=input_ids,attention_mask=attention_mask,\n    max_length=128,\n    early_stopping=True,\n    num_beams=15,\n    num_return_sequences=3\n\n)\n\nprint (\"\\n\\n\")\nprint (\"Original: \",context)\nfor beam_output in beam_outputs:\n    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    print (sent)\n```\n    \n**Output from the above code**\n    \n```\nOriginal:  Once, a group of frogs were roaming around the forest in search of water.\nparaphrasedoutput: A herd of frogs were wandering around the woods in search of water.\nparaphrasedoutput: A herd of frogs was wandering around the woods in search of water.\nparaphrasedoutput: A herd of frogs were wandering around the forest in search of water at one time.\n```", "qas": [{"id": "q1", "question": "What is the model architecture of ramsrigouthamg/t5-large-paraphraser-diverse-high-quality?", "answers": [{"text": "t5", "answer_start": 272, "answer_end": 273}]}]}]}, {"title": "ravirajoshi/wav2vec2-large-xls-r-300m-hindi-lm-boosted", "paragraphs": [{"context": "---\nlanguage:\n- hi\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- robust-speech-event\n- hf-asr-leaderboard\nmodel-index:\n- name: wav2vec2-large-xls-r-300m-hindi\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xls-r-300m-hindi\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7049\n- Wer: 0.3200\n", "qas": [{"id": "q1", "question": "What is the model architecture of ravirajoshi/wav2vec2-large-xls-r-300m-hindi-lm-boosted?", "answers": [{"text": "wav2vec2", "answer_start": 134, "answer_end": 141}]}]}]}, {"title": "ravirajoshi/wav2vec2-large-xls-r-300m-marathi-lm-boosted", "paragraphs": [{"context": "---\nlanguage:\n- mr\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- robust-speech-event\n- hf-asr-leaderboard\nmodel-index:\n- name: wav2vec2-large-xls-r-300m-marathi\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xls-r-300m-marathi\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5656\n- Wer: 0.2156\n", "qas": [{"id": "q1", "question": "What is the model architecture of ravirajoshi/wav2vec2-large-xls-r-300m-marathi-lm-boosted?", "answers": [{"text": "wav2vec2", "answer_start": 134, "answer_end": 141}]}]}]}, {"title": "raynardj/classical-chinese-punctuation-guwen-biaodian", "paragraphs": [{"context": "---\nlanguage:\n- zh\ntags:\n- ner\n- punctuation\n- \u53e4\u6587\n- \u6587\u8a00\u6587\n- ancient\n- classical\nwidget:\n- text: \"\u90e1\u9091\u7f6e\u592b\u5b50\u5e99\u4e8e\u5b66\u4ee5\u5d57\u65f6\u91ca\u5960\u76d6\u81ea\u5510\u8d1e\u89c2\u4ee5\u6765\u672a\u4e4b\u6216\u6539\u6211\u5b8b\u6709\u5929\u4e0b\u56e0\u5176\u5236\u800c\u635f\u76ca\u4e4b\u59d1\u82cf\u5f53\u6d59\u53f3\u8981\u533a\u89c4\u6a21\u5c24\u5927\u66f4\u5efa\u708e\u620e\u9a6c\u8361\u7136\u65e0\u9057\u867d\u4fee\u5b66\u5bab\u4e8e\u8346\u699b\u74e6\u783e\u4e4b\u4f59\u72ec\u6bbf\u5b87\u672a\u9051\u8bae\u4e5f\u6bcf\u6625\u79cb\u5c55\u793c\u4e8e\u658b\u5e90\u5df2\u5219\u7f6e\u4e0d\u95ee\u6b86\u4e3a\u9619\u5178\u4eca\u5bf3\u6587\u9601\u76f4\u5b66\u58eb\u62ec\u82cd\u6881\u516c\u6765\u7267\u4e4b\u660e\u5e74\u5b9e\u7ecd\u5174\u5341\u6709\u4e00\u79a9\u4e5f\u4e8c\u6708\u4e0a\u4e01\u4fee\u7940\u65e2\u6bd5\u4e43\u6113\u7136\u81ea\u548e\u63d6\u8bf8\u751f\u800c\u544a\u4e4b\u66f0\u5929\u5b50\u4e0d\u4ee5\u6c5d\u5609\u4e3a\u4e0d\u8096\u4ffe\u518d\u5b88\u5179\u571f\u987e\u6cbb\u6c11\u4e8b\u795e\u7686\u5b88\u4e4b\u804c\u60df\u662f\u592b\u5b50\u4e4b\u7940\u6559\u5316\u6240\u57fa\u5c24\u5b9c\u4e25\u4e14\u8c28\u800c\u62dc\u8dea\u8350\u796d\u4e4b\u5730\u5351\u964b\u4e43\u5c14\u5176\u4f55\u4ee5\u63b2\u9632\u59a5\u7075\u6c5d\u5609\u4e0d\u6562\u907f\u5176\u8d23\u66e9\u5e38\u53bb\u6b64\u5f25\u5e74\u82e5\u6709\u6240\u8d1f\u5c1a\u5b89\u5f97\u4ee5\u7f62\u8f2d\u81ea\u6055\u590d\u7d2f\u540e\u4eba\u4e4e\u4ed6\u65e5\u6216\u514b\u5c31\u7eea\u613f\u4e0e\u8bf8\u541b\u843d\u4e4b\u4e8e\u662f\u8c0b\u4e4b\u50da\u540f\u641c\u6545\u5e9c\u5f97\u9057\u6750\u5343\u679a\u53d6\u8d62\u8d44\u4ee5\u7ed9\u5176\u8d39\u9e20\u5de5\u5e80\u5f79\u5404\u4e3e\u5176\u4efb\u5d57\u6708\u8bab\u5de5\u6c11\u4e0d\u4e0e\u77e5\u50cf\u8bbe\u793c\u5668\u767e\u7528\u5177\u4fee\u81f3\u4e8e\u5802\u5ba4\u5eca\u5e8f\u95e8\u7256\u57a3\u5899\u7686\u4e00\u65b0\u4e4b\"\n\n---\n\n# Classical Chinese Punctuation\n\n> \u6b22\u8fce\u524d\u5f80[\u6211\u7684github\u6587\u8a00\u8bd7\u8bcd\u9879\u76ee\u9875\u9762\u63a2\u8ba8\u3001\u52a0\u2b50\ufe0f ]( Please check the github repository for more about the [model, hit \ud83c\udf1f if you like](\n \n* This model punctuates Classical(ancient) Chinese, you might feel strange about this task, but **many of my ancestors think writing articles without punctuation is brilliant idea** \ud83e\uddd0. What we have here are articles from books, letters or carved on stones where you can see no punctuation, just a long string of characters. As you can guess, NLP tech is usually a good tool to tackle this problem, and the entire pipeline can be borrowed from usual **NER task**.\n\n* Since there are also many articles are punctuated, hence with some regex operations, labeled data is more than abundant \ud83d\udcda. That's why this problem is pretty much a low hanging fruit.\n\n* so I guess who's interested in the problem set can speak at least modern Chinese, hence... let me continue the documentation in Chinese. \n\n# \u6587\u8a00\u6587(\u53e4\u6587) \u65ad\u53e5\u6a21\u578b\n> \u8f93\u5165\u4e00\u4e32\u672a\u65ad\u53e5\u6587\u8a00\u6587\uff0c \u53ef\u4ee5\u65ad\u53e5\uff0c \u76ee\u524d\u652f\u6301\u4e8c\u5341\u591a\u79cd\u6807\u70b9\u7b26\u53f7\n\n## \u5176\u4ed6\u6587\u8a00\u8bd7\u8bcd\u7684\u8d44\u6e90\n* [\u9879\u76ee\u6e90\u4ee3\u7801 \ud83c\udf1f, \u6b22\u8fce+star\u63d0pr](\n* [\u8de8\u8bed\u79cd\u641c\u7d22 \ud83d\udd0e](\n* [\u73b0\u4ee3\u6587\u7ffb\u8bd1\u53e4\u6c49\u8bed\u7684\u6a21\u578b \u26f0](\n* [\u53e4\u6c49\u8bed\u5230\u73b0\u4ee3\u6587\u7684\u7ffb\u8bd1\u6a21\u578b, \u8f93\u5165\u53ef\u4ee5\u662f\u672a\u65ad\u53e5\u7684\u53e5\u5b50 \ud83d\ude80](\n* [\u65ad\u53e5\u6a21\u578b \ud83d\udde1](\n* [\u610f\u5883\u5173\u952e\u8bcd \u548c \u85cf\u5934\u5199\u8bd7\ud83e\udd16](", "qas": []}]}, {"title": "raynardj/ner-chemical-bionlp-bc5cdr-pubmed", "paragraphs": [{"context": "---\nlanguage:\n- en\ntags:\n- ner\n- chemical\n- bionlp\n- bc4cdr\n- bioinfomatics\nlicense: apache-2.0\ndatasets:\n- bionlp\n- bc4cdr\nwidget:\n- text: \"Serotonin receptor 2A (HTR2A) gene polymorphism predicts treatment response to venlafaxine XR in generalized anxiety disorder.\"\n\n---\n\n# NER to find Gene & Gene products\n> The model was trained on bionlp and bc4cdr dataset, pretrained on this [pubmed-pretrained roberta model](/raynardj/roberta-pubmed)\nAll the labels, the possible token classes.\n```json\n{\"label2id\":\n  {\n    \"O\": 0,\n    \"Chemical\": 1,\n  }\n }\n```\n \nNotice, we removed the 'B-','I-' etc from data label.\ud83d\udde1\n \n## This is the template we suggest for using the model\nOf course I'm well aware of the ```aggregation_strategy``` arguments offered by hf, but by the way of training, I discard any entropy loss for appending subwords, like only the label for the 1st subword token is not -100, after many search effort, I can't find a way to achieve that with default pipeline, hence I fancy an inference class myself.\n```python\n!pip install forgebox\nfrom forgebox.hf.train import NERInference\nner = NERInference.from_pretrained(\"raynardj/ner-chemical-bionlp-bc5cdr-pubmed\")\na_df = ner.predict([\"text1\", \"text2\"])\n```\n\n> check our NER model on\n* [gene and gene products](/raynardj/ner-gene-dna-rna-jnlpba-pubmed)\n* [chemical substance](/raynardj/ner-chemical-bionlp-bc5cdr-pubmed).\n* [disease](/raynardj/ner-disease-ncbi-bionlp-bc5cdr-pubmed)", "qas": [{"id": "q1", "question": "What is the model architecture of raynardj/ner-chemical-bionlp-bc5cdr-pubmed?", "answers": [{"text": "roberta", "answer_start": 402, "answer_end": 408}]}]}]}, {"title": "raynardj/wenyanwen-ancient-translate-to-modern", "paragraphs": [{"context": "---\nlanguage:\n- zh\n- zh\ntags:\n- translation\n- \u53e4\u6587\n- \u6587\u8a00\u6587\n- ancient\n- classical\nwidget:\n- text: \"\u6b64\u8bda\u5371\u6025\u5b58\u4ea1\u4e4b\u79cb\u4e5f\"\n\n---\n\n# From Classical(ancient) Chinese to Modern Chinese\n> This model translate Classical(ancient) Chinese to Modern Chinese, so I guess who's interested in the problemset can speak at least modern Chinese, hence... let me continue the documentation in Chinese\n\n# \u6587\u8a00\u6587\uff08\u53e4\u6587\uff09\u5230\u73b0\u4ee3\u6587\u7684\u7ffb\u8bd1\u5668\n> \u8fd9\u4e2a\u6a21\u578b\u5df2\u6709\u505a\u6210\u5e94\u7528\uff0c [\u3010\u968f\u65e0\u6daf\u3011]( spaces + streamlit \u7684\u53e4\u6587\u9605\u8bfb\u5e94\u7528\uff08\u542b\u6d77\u91cf\u4e66\u7c4d\uff09\uff0c \u53ef\u4ee5\u5728\u9605\u8bfb\u65f6\u7ffb\u8bd1\n> \u8f93\u5165\u6587\u8a00\u6587\uff0c \u53ef\u4ee5\u662f\u65ad\u53e5 \u6216\u8005 \u672a\u65ad\u53e5\u7684\u6587\u8a00\u6587\uff0c \u6a21\u578b\u4f1a\u9884\u6d4b\u73b0\u4ee3\u6587\u7684\u8868\u8ff0\u3002 \u5176\u4ed6\u6a21\u578b\uff1a\n* \u4ece[\u73b0\u4ee3\u6587\u7ffb\u8bd1\u5230\u6587\u8a00\u6587](\n\n> \u4ece\u6587\u8a00\u6587\u5230\u73b0\u4ee3\u6587\u7684\u7ffb\u8bd1\u5668, \u6b22\u8fce\u524d\u5f80[\u6211\u7684github\u6587\u8a00\u8bd7\u8bcd\u9879\u76ee\u9875\u9762\u63a2\u8ba8\u3001\u52a0\u2b50\ufe0f ](\n\n> \u8bad\u7ec3\u8bed\u6599\u662f\u5c31\u662f\u4e5d\u5341\u591a\u4e07\u53e5\u53e5\u5bf9\uff0c [\u6570\u636e\u96c6\u94fe\u63a5\ud83d\udcda]( \u8bad\u7ec3\u65f6source\u5e8f\u5217\uff08\u53e4\u6587\u5e8f\u5217\uff09\uff0c \u6309\u716750%\u7684\u6982\u7387\u6574\u53e5\u53bb\u9664\u6240\u6709\u6807\u70b9\u7b26\u53f7\u3002\n\n## \u63a8\u8350\u7684inference \u901a\u9053\n**\u6ce8\u610f**\n* \u4f60\u5fc5\u987b\u5c06```generate```\u51fd\u6570\u7684```eos_token_id```\u8bbe\u7f6e\u4e3a102\u5c31\u53ef\u4ee5\u7ffb\u8bd1\u51fa\u5b8c\u6574\u7684\u8bed\u53e5\uff0c \u4e0d\u7136\u7ffb\u8bd1\u5b8c\u4e86\u4f1a\u6709\u6b8b\u7559\u7684\u8bed\u53e5(\u56e0\u4e3a\u505a\u71b5\u7684\u65f6\u5019\u7528pad\u6807\u7b7e=-100\u5bfc\u81f4)\u3002\n\u76ee\u524dhuggingface \u9875\u9762\u4e0acompute\u6309\u94ae\u4f1a\u6709\u8fd9\u4e2a\u95ee\u9898\uff0c \u63a8\u8350\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u6765\u5f97\u5230\u7ffb\u8bd1\u7ed3\u679c\n* \u8bf7\u8bbe\u7f6e```generate```\u7684\u53c2\u6570```num_beams>=3```, \u4ee5\u8fbe\u5230\u8f83\u597d\u7684\u7ffb\u8bd1\u6548\u679c\n* \u8bf7\u8bbe\u7f6e```generate```\u7684\u53c2\u6570```max_length```256\uff0c \u4e0d\u7136\u7ed3\u679c\u4f1a\u5403\u6389\u53e5\u5b50\n```python\nfrom transformers import (\n  EncoderDecoderModel,\n  AutoTokenizer\n)\nPRETRAINED = \"raynardj/wenyanwen-ancient-translate-to-modern\"\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\nmodel = EncoderDecoderModel.from_pretrained(PRETRAINED)\ndef inference(text):\n    tk_kwargs = dict(\n      truncation=True,\n      max_length=128,\n      padding=\"max_length\",\n      return_tensors='pt')\n   \n    inputs = tokenizer([text,],**tk_kwargs)\n    with torch.no_grad():\n        return tokenizer.batch_decode(\n            model.generate(\n            inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            num_beams=3,\n            max_length=256,\n            bos_token_id=101,\n            eos_token_id=tokenizer.sep_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n        ), skip_special_tokens=True)\n```\n\n## \u76ee\u524d\u7248\u672c\u7684\u6848\u4f8b\n> \u5f53\u7136\uff0c \u62ff\u6bd4\u8f83\u719f\u77e5\u7684\u8bed\u53e5\u8fc7\u6765\uff0c \u901a\u5e38\u4f1a\u6709\u4e9b\u8d3b\u7b11\u5927\u65b9\u7684\u5931\u8bef\uff0c \u5927\u5bb6\u5982\u679c\u6709\u597d\u73a9\u7684\u8c03\u620f\u6848\u4f8b\uff0c \u4e5f\u6b22\u8fce\u53cd\u9988\n```python\n>>> inference('\u975e\u6211\u65cf\u7c7b\u5176\u5fc3\u5fc5\u5f02')\n['\u4e0d \u662f \u6211 \u4eec \u7684 \u65cf \u7c7b \uff0c \u4ed6 \u4eec \u7684 \u5fc3 \u601d \u5fc5 \u7136 \u4e0d \u540c \u3002']\n>>> inference('\u8089\u98df\u8005\u9119\u672a\u80fd\u8fdc\u8c0b')\n['\u5403 \u8089 \u7684 \u4eba \u9119 \u964b \uff0c \u4e0d \u80fd \u957f \u8fdc \u8c0b \u5212 \u3002']\n# \u8fd9\u91cc\u6211\u597d\u51e0\u6279\u6a21\u578b\u90fd\u7ffb\u4e0d\u51fa\u8fd9\u4e2a**\u8f93**\u5b57\uff08\u751a\u81f3\u6709\u4e00\u4e2a\u7248\u672c\u7ffb\u6210\u4e86\u79e6\u59cb\u7687\u548c\u6c49\u6b66\u5e1d\uff09\uff0c \u53ef\u80fd\u5e76\u4e0d\u662f\u5f88\u53e4\u6734\u7684\u7528\u6cd5\uff0c \n>>> inference('\u6c5f\u5c71\u5982\u6b64\u591a\u5a07\u5f15\u65e0\u6570\u82f1\u96c4\u7ade\u6298\u8170\u60dc\u79e6\u7687\u6c49\u6b66\u7565\u8f93\u6587\u91c7\u5510\u5b97\u5b8b\u7956\u7a0d\u900a\u98ce\u9a9a')\n['\u6c5f \u5c71 \u5982 \u6b64 \u591a \uff0c \u62db \u5f15 \u65e0 \u6570 \u7684 \u82f1 \u96c4 \uff0c \u7ade \u76f8 \u6298 \u8170 \uff0c \u53ef \u60dc \u79e6 \u7687 \u3001 \u6c49 \u6b66 \uff0c \u7565 \u5fae \u6709 \u6587 \u91c7 \uff0c \u5510 \u5b97 \u3001 \u5b8b \u7956 \u7a0d \u7a0d \u900a \u51fa \u98ce \u96c5 \u3002']\n>>> inference(\"\u6e05\u98ce\u5f90\u6765\u6c34\u6ce2\u4e0d\u5174\")\n['\u6e05 \u98ce \u6162 \u6162 \u5439 \u6765 \uff0c \u6c34 \u6ce2 \u4e0d \u5174 \u3002']\n>>> inference(\"\u65e0\u4ed6\u552f\u624b\u719f\u5c14\")\n['\u6ca1 \u6709 \u522b \u7684 \u4e8b \uff0c \u53ea \u662f \u624b \u719f \u7f62 \u4e86 \u3002']\n>>> inference(\"\u6b64\u8bda\u5371\u6025\u5b58\u4ea1\u4e4b\u79cb\u4e5f\")\n['\u8fd9 \u5b9e \u5728 \u662f \u5371 \u6025 \u5b58 \u4ea1 \u7684 \u65f6 \u5019 \u3002']\n```\n\n## \u5176\u4ed6\u6587\u8a00\u8bd7\u8bcd\u7684\u8d44\u6e90\n* [\u9879\u76ee\u6e90\u4ee3\u7801 \ud83c\udf1f, \u6b22\u8fce+star\u63d0pr](\n* [\u8de8\u8bed\u79cd\u641c\u7d22 \ud83d\udd0e](\n* [\u73b0\u4ee3\u6587\u7ffb\u8bd1\u53e4\u6c49\u8bed\u7684\u6a21\u578b \u26f0](\n* [\u53e4\u6c49\u8bed\u5230\u73b0\u4ee3\u6587\u7684\u7ffb\u8bd1\u6a21\u578b, \u8f93\u5165\u53ef\u4ee5\u662f\u672a\u65ad\u53e5\u7684\u53e5\u5b50 \ud83d\ude80](\n* [\u65ad\u53e5\u6a21\u578b \ud83d\udde1](\n* [\u610f\u5883\u5173\u952e\u8bcd \u548c \u85cf\u5934\u5199\u8bd7\ud83e\udd16](", "qas": [{"id": "q2", "question": "What is the model task of raynardj/wenyanwen-ancient-translate-to-modern?", "answers": [{"text": "translation", "answer_start": 32, "answer_end": 42}]}]}]}, {"title": "raynardj/xlsearch-cross-lang-search-zh-vs-classicical-cn", "paragraphs": [{"context": "---\n\nlanguage:\n- zh\ntags:\n- search\n\n---\n\n# Cross Language Search\n## Search cliassical CN with modern ZH\n* In some cases, Classical Chinese feels like another language, I even trained 2 translation models ([1]( and [2]( to prove this point.\n* That's why, when people wants to be savvy about their words, we choose to quote our ancestors. It's exactly like westerners like to quote Latin or Shakespeare, the difference is we have a much bigger pool to choose.\n* This model helps you **find** text within **ancient Chinese** literature, but you can **search with modern Chinese**\n\n# \u8de8\u8bed\u79cd\u641c\u7d22\n## \u535a\u53e4\u641c\u4eca\n* \u6211\u4e0d\u8bb0\u5f97\u662f\u8c01\uff0c \u54ea\u4e2a\u671d\u4ee3\uff0c\u6211\u53ea\u8bb0\u5f97\u5927\u6982\u8fd9\u4e48\u4e00\u4e2a\u4e8b\u513f\uff0c\u6211\u5c31\u80fd\u6a21\u7cca\u627e\u5230\u539f\u6587\n* \u6211\u4e0d\u8bb0\u5f97\u539f\u6587\uff0c \u4f46\u662f\u6211\u53ea\u8bb0\u5f97\u539f\u6587\u60f3\u8868\u8fbe\u7684\u73b0\u4ee3\u6c49\u8bed\u610f\u601d\uff0c \u5e0c\u671b\u80fd\u627e\u51fa\u6765\u5f15\u7528\u4e00\u4e0b\u3002\n* \u6211\u5728\u5199\u6587\u7ae0\uff0c \u6709\u4e2a\u89c2\u70b9\uff0c \u6211\u60f3\u78b0\u8fd0\u6c14\u770b\u770b\u53e4\u4eba\u6709\u6ca1\u6709\u63d0\u8fc7\u540c\u6837\u7c7b\u4f3c\u7684\u8bf4\u6cd5\u3002\n* \u6211\u53ea\u662f\u60f3\u66f4\u6709\u6548\u7387\u5730\u9605\u8bfb\u53e4\u6587\n\n\u63a8\u8350\u7684\u4f7f\u7528\u901a\u9053\u5982\u4e0b\uff0c\u5f53\u7136\uff0c cosine\u8ddd\u79bb\u641c\u7d22\u76f8\u5173\u7684\u6846\u67b6\u548c\u5f15\u64ce\u5f88\u591a\uff0c \u5927\u5bb6\u81ea\u5df1\u770b\u7740\u9002\u7528\u7684\u9009\n\n\u88c5\u5305\n```shell\npip install -Uqq unpackai\npip install -Uqq SentenceTransformer\n```\n\n\u641c\u7d22\u8bed\u53e5\u7684\u51fd\u6570\n```python\nfrom unpackai.interp import CosineSearch\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\n\nTAG = \"raynardj/xlsearch-cross-lang-search-zh-vs-classicical-cn\"\nencoder = SentenceTransformer(TAG)\n\n# all_lines is a list of all your sentences\n# all_lines \u662f\u4e00\u4e2a\u4f60\u6240\u6709\u53e5\u5b50\u7684\u5217\u8868\uff0c \u53ef\u4ee5\u662f\u4e00\u672c\u4e66\uff0c \u6309\u7167\u53e5\u5b50\u5206\u5272\uff0c \u4e5f\u53ef\u4ee5\u662f\u5f88\u591a\u5f88\u591a\u4e66\nall_lines = [\"\u53e5\u5b501\",\"\u53e5\u5b502\",...]\nvec = encoder.encode(all_lines, batch_size=32, show_progress_bar=True)\n\n# consine\u8ddd\u79bb\u641c\u7d22\u5668\ncosine = CosineSearch(vec)\n\ndef search(text):\n    enc = encoder.encode(text) # encode the search key\n    order = cosine(enc) # distance array\n    sentence_df = pd.DataFrame({\"sentence\":np.array(all_lines)[order[:5]]})\n    return sentence_df\n```\n\n\u5c06\u53f2\u8bb0\u6253\u6210\u53e5\u5b50\u4ee5\u540e\uff0c \u641c\u7d22\u6548\u679c\u662f\u8fd9\u6837\u7684\uff1a\n\n```python\n>>> search(\"\u4ed6\u662f\u4e00\u4e2a\u5f88\u6177\u6168\u7684\u4eba\")\n```\n```\nsentence\n0\t\u5b63\u5e03\u8005\uff0c\u695a\u4eba\u4e5f\u3002\u4e3a\u6c14\u4efb\u4fa0\uff0c\u6709\u540d\u65bc\u695a\u3002\n1\t\u8463\u4ef2\u8212\u4e3a\u4eba\u5ec9\u76f4\u3002\n2\t\u5927\u5c06\u519b\u4e3a\u4eba\u4ec1\u5584\u9000\u8ba9\uff0c\u4ee5\u548c\u67d4\u81ea\u5a9a\u65bc\u4e0a\uff0c\u7136\u5929\u4e0b\u672a\u6709\u79f0\u4e5f\u3002\n3\t\u52c3\u4e3a\u4eba\u6728\u5f4a\u6566\u539a\uff0c\u9ad8\u5e1d\u4ee5\u4e3a\u53ef\u5c5e\u5927\u4e8b\u3002\n4\t\u77f3\u5962\u8005\uff0c\u695a\u662d\u738b\u76f8\u4e5f\u3002\u575a\u76f4\u5ec9\u6b63\uff0c\u65e0\u6240\u963f\u907f\u3002\n```\n\n```python\n>>> search(\"\u8fdb\u5165\u519b\u8425\uff0c\u5fc5\u987b\u7f13\u7f13\u7275\u7740\u9a6c\u9a91\")\n```\n```\nsentence\n0\t\u58c1\u95e8\u58eb\u540f\u8c13\u4ece\u5c5e\u8f66\u9a91\u66f0\uff1a\u5c06\u519b\u7ea6\uff0c\u519b\u4e2d\u4e0d\u5f97\u9a71\u9a70\u3002\n1\t\u8d77\u4e4b\u4e3a\u5c06\uff0c\u4e0e\u58eb\u5352\u6700\u4e0b\u8005\u540c\u8863\u98df\u3002\u5367\u4e0d\u8bbe\u5e2d\uff0c\u884c\u4e0d\u9a91\u4e58\uff0c\u4eb2\u88f9\u8d62\u7cae\uff0c\u4e0e\u58eb\u5352\u5206\u52b3\u82e6\u3002\n2\t\u65e2\u51fa\uff0c\u6c9b\u516c\u7559\u8f66\u9a91\uff0c\u72ec\u9a91\u4e00\u9a6c\uff0c\u4e0e\u6a0a\u54d9\u7b49\u56db\u4eba\u6b65\u4ece\uff0c\u4ece\u95f4\u9053\u5c71\u4e0b\u5f52\u8d70\u9738\u4e0a\u519b\uff0c\u800c\u4f7f\u5f20\u826f\u8c22\u9879\u7fbd\u3002\n3\t\u9877\u4e4b\uff0c\u4e0a\u884c\u51fa\u4e2d\u6e2d\u6865\uff0c\u6709\u4e00\u4eba\u4ece\u7a5a\u4e0b\u8d70\u51fa\uff0c\u4e58\u8206\u9a6c\u60ca\u3002\n4\t\u5143\u72e9\u56db\u5e74\u6625\uff0c\u4e0a\u4ee4\u5927\u5c06\u519b\u9752\u3001\u9aa0\u9a91\u5c06\u519b\u53bb\u75c5\u5c06\u5404\u4e94\u4e07\u9a91\uff0c\u6b65\u5175\u8f6c\u8005\u8e35\u519b\u6570\u5341\u4e07\uff0c\u800c\u6562\u529b\u6218\u6df1\u5165\u4e4b\u58eb\u7686\u5c5e\u9aa0\u9a91\u3002\n```\n\n## \u5176\u4ed6\u8d44\u6e90\u6e05\u5355\n* [\u9879\u76ee\u6e90\u4ee3\u7801 \ud83c\udf1f, \u6b22\u8fce+star\u63d0pr](\n* [\u8de8\u8bed\u79cd\u641c\u7d22 \ud83d\udd0e](\n* [\u73b0\u4ee3\u6587\u7ffb\u8bd1\u53e4\u6c49\u8bed\u7684\u6a21\u578b \u26f0](\n* [\u53e4\u6c49\u8bed\u5230\u73b0\u4ee3\u6587\u7684\u7ffb\u8bd1\u6a21\u578b, \u8f93\u5165\u53ef\u4ee5\u662f\u672a\u65ad\u53e5\u7684\u53e5\u5b50 \ud83d\ude80](\n* [\u65ad\u53e5\u6a21\u578b \ud83d\udde1](\n* [\u610f\u5883\u5173\u952e\u8bcd \u548c \u85cf\u5934\u5199\u8bd7\ud83e\udd16](", "qas": []}]}, {"title": "rayschwartz/new-text-classifications", "paragraphs": [{"context": "", "qas": []}]}, {"title": "razent/SciFive-base-Pubmed_PMC", "paragraphs": [{"context": "---\nlanguage: \n  - en\n\ntags:\n- token-classification\n- text-classification\n- question-answering\n- text2text-generation\n- text-generation\n\ndatasets:\n- pubmed\n- pmc/open_access\n---\n\n# SciFive Pubmed+PMC Base\n\n## Introduction\nPaper: [SciFive: a text-to-text transformer model for biomedical literature](\n\nAuthors: _Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, Gr\u00e9goire Altan-Bonnet_\n\n## How to use\nFor more details, do check out [our Github repo]( \n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"razent/SciFive-base-Pubmed_PMC\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"razent/SciFive-base-Pubmed_PMC\")\n\u200b\nsentence = \"Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\"\ntext =  sentence + \"</s>\"\n\nencoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    max_length=256,\n    early_stopping=True\n)\n\nfor output in outputs:\n    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    print(line)\n```", "qas": [{"id": "q2", "question": "What is the model task of razent/SciFive-base-Pubmed_PMC?", "answers": [{"text": "text-classification", "answer_start": 54, "answer_end": 72}]}]}]}, {"title": "razent/SciFive-large-Pubmed", "paragraphs": [{"context": "---\nlanguage: \n  - en\n\ntags:\n- token-classification\n- text-classification\n- question-answering\n- text2text-generation\n- text-generation\n\ndatasets:\n- pubmed\n\n---\n\n\n# SciFive Pubmed Large\n\n## Introduction\nPaper: [SciFive: a text-to-text transformer model for biomedical literature](\n\nAuthors: _Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, Gr\u00e9goire Altan-Bonnet_\n\n## How to use\nFor more details, do check out [our Github repo]( \n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"razent/SciFive-large-Pubmed\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"razent/SciFive-large-Pubmed\")\n\u200b\nsentence = \"Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\"\ntext =  sentence + \" </s>\"\n\nencoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    max_length=256,\n    early_stopping=True\n)\n\nfor output in outputs:\n    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    print(line)\n```", "qas": [{"id": "q2", "question": "What is the model task of razent/SciFive-large-Pubmed?", "answers": [{"text": "text-classification", "answer_start": 54, "answer_end": 72}]}]}]}, {"title": "recobo/chemical-bert-uncased-squad2", "paragraphs": [{"context": "```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"recobo/chemical-bert-uncased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between pytorch and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```", "qas": [{"id": "q1", "question": "What is the model architecture of recobo/chemical-bert-uncased-squad2?", "answers": [{"text": "bert", "answer_start": 121, "answer_end": 124}]}, {"id": "q2", "question": "What is the model task of recobo/chemical-bert-uncased-squad2?", "answers": [{"text": "question-answering", "answer_start": 180, "answer_end": 197}]}]}]}, {"title": "researchaccount/sa_sub1", "paragraphs": [{"context": "---\nlanguage: en\n\nwidget:\n- text: \"USER USER USER USER \u0644\u0627\u062d\u0648\u0644 \u0648\u0644\u0627\u0642\u0648\u0647 \u0627\u0644\u0627 \u0628\u0627\u0644\u0644\u0647 \ud83d\udc94 \ud83d\udc94 \ud83d\udc94 \ud83d\udc94 HASH TAG \u0645\u062a\u064a \u064a\u0635\u062f\u0631 \u0642\u0631\u0627\u0631 \u0627\u0644\u0639\u0634\u0631\u064a\u0646 ! ! ! ! ! !\"\n \n---\n\nSub 1", "qas": []}]}, {"title": "researchaccount/sa_sub2", "paragraphs": [{"context": "---\nlanguage: en\n\nwidget:\n- text: \"USER USER USER USER \u0644\u0627\u062d\u0648\u0644 \u0648\u0644\u0627\u0642\u0648\u0647 \u0627\u0644\u0627 \u0628\u0627\u0644\u0644\u0647 \ud83d\udc94 \ud83d\udc94 \ud83d\udc94 \ud83d\udc94 HASH TAG \u0645\u062a\u064a \u064a\u0635\u062f\u0631 \u0642\u0631\u0627\u0631 \u0627\u0644\u0639\u0634\u0631\u064a\u0646 ! ! ! ! ! !\"\n \n---\n\nSub 2", "qas": []}]}, {"title": "researchaccount/sa_sub3", "paragraphs": [{"context": "---\nlanguage: en\n\nwidget:\n- text: \"USER USER USER USER \u0644\u0627\u062d\u0648\u0644 \u0648\u0644\u0627\u0642\u0648\u0647 \u0627\u0644\u0627 \u0628\u0627\u0644\u0644\u0647 \ud83d\udc94 \ud83d\udc94 \ud83d\udc94 \ud83d\udc94 HASH TAG \u0645\u062a\u064a \u064a\u0635\u062f\u0631 \u0642\u0631\u0627\u0631 \u0627\u0644\u0639\u0634\u0631\u064a\u0646 ! ! ! ! ! !\"\n \n---\n\nSub 3", "qas": []}]}, {"title": "researchaccount/sa_sub4", "paragraphs": [{"context": "---\nlanguage: en\n\nwidget:\n- text: \"USER USER USER USER \u0644\u0627\u062d\u0648\u0644 \u0648\u0644\u0627\u0642\u0648\u0647 \u0627\u0644\u0627 \u0628\u0627\u0644\u0644\u0647 \ud83d\udc94 \ud83d\udc94 \ud83d\udc94 \ud83d\udc94 HASH TAG \u0645\u062a\u064a \u064a\u0635\u062f\u0631 \u0642\u0631\u0627\u0631 \u0627\u0644\u0639\u0634\u0631\u064a\u0646 ! ! ! ! ! !\"\n \n---\n\nSub 4", "qas": []}]}, {"title": "reshinthadith/FlashFill-T5", "paragraphs": [{"context": "", "qas": []}]}, {"title": "ricardo-filho/sbertimbau-large-nli-sts", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\n# {MODEL_NAME}\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n<!--- Describe your model here -->\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('{MODEL_NAME}')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\nmodel = AutoModel.from_pretrained('{MODEL_NAME}')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Evaluation Results\n\n<!--- Describe how your model was evaluated -->\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [\n\n\n## Training\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 356 with parameters:\n```\n{'batch_size': 16, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"callback\": null,\n    \"epochs\": 4,\n    \"evaluation_steps\": 1000,\n    \"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 2e-05\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": null,\n    \"warmup_steps\": 143,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 64, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->", "qas": [{"id": "q2", "question": "What is the model task of ricardo-filho/sbertimbau-large-nli-sts?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "richiellei/Childe3", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Childe3 DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of richiellei/Childe3?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "richiellei/DialoGPT-small-rick", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Rick DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of richiellei/DialoGPT-small-rick?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "ridwanpratama/DialoGPT-small-misaki", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Misaki Ayuzawa Model", "qas": [{"id": "q2", "question": "What is the model task of ridwanpratama/DialoGPT-small-misaki?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "rifkat/pubchem_1M", "paragraphs": [{"context": "Ushbu model, HuggingFace-da RoBERTa transformatorini amalga oshirishga asoslangan. Bizning RoBERTa dasturimiz 12 ta diqqat boshi va 6 ta qatlamdan foydalanadi, natijada 72 ta aniq e'tibor mexanizmlari paydo bo'ladi. Biz har bir kirish satridagi tokenlarning 15 foizini niqoblaydigan RoBERTa-dan dastlabki tekshirish protsedurasini qabul qildik. Biz maksimal 52K tokenli lug'atdan va maksimal 512 ta ketma-ketlik uzunligidan foydalanganmiz. Biz 1M PubChem to'plamlarida 10 ta davr uchun o'qitdik. Loss funksiya 2.9 dan 0.33 gacha tushdi. Ushbu modelni sizga taqdim qilamiz.\n<pre><code class=\"language-python\">\n\n@misc {rifkat_davronov_2022,\n\tauthor       = { {Adilova Fatima,Rifkat Davronov, Samariddin Kushmuratov, Ruzmat Safarov} },\n\ttitle        = { pubchem_1M (Revision 89e2ba6) },\n\tyear         = 2022,\n\turl          = {  },\n\tdoi          = { 10.57967/hf/0177 },\n\tpublisher    = { Hugging Face }\n}\n\n</code></pre>", "qas": []}]}, {"title": "rifkat/uztext-3Gb-BPE-Roberta", "paragraphs": [{"context": "\n---\nlanguage:\n- uz\ntags:\n- transformers\n- mit\n- robert\n- uzrobert\n- uzbek\n- cyrillic\n- latin\nlicense: apache-2.0\nwidget:\n- text: \"Kuchli yomg\u2018irlar tufayli bir qator <mask> kuchli sel oqishi kuzatildi.\"\n  example_title: \"Latin script\"\n- text: \"\u0410\u043b\u0438\u0448\u0435\u0440 \u041d\u0430\u0432\u043e\u0438\u0439 \u2013 \u0443\u043b\u0443\u0493 \u045e\u0437\u0431\u0435\u043a \u0432\u0430 \u0431\u043e\u0448\u049b\u0430 \u0442\u0443\u0440\u043a\u0438\u0439 \u0445\u0430\u043b\u049b\u043b\u0430\u0440\u043d\u0438\u043d\u0433 <mask>, \u043c\u0443\u0442\u0430\u0444\u0430\u043a\u043a\u0438\u0440\u0438 \u0432\u0430 \u0434\u0430\u0432\u043b\u0430\u0442 \u0430\u0440\u0431\u043e\u0431\u0438 \u0431\u045e\u043b\u0433\u0430\u043d.\"\n  example_title: \"Cyrillic script\"\n---\n\n\n<p><b>UzRoBerta model.</b>\n\nPre-prepared model in Uzbek (Cyrillic and latin script) to model the masked language and predict the next sentences.\n\n<p><b>How to use.</b>\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n<pre><code class=\"language-python\">\nfrom transformers import pipeline\n\nunmasker = pipeline('fill-mask', model='rifkat/uztext-3Gb-BPE-Roberta')\n\nunmasker(\"\u0410\u043b\u0438\u0448\u0435\u0440 \u041d\u0430\u0432\u043e\u0438\u0439 \u2013 \u0443\u043b\u0443\u0493 \u045e\u0437\u0431\u0435\u043a \u0432\u0430 \u0431\u043e\u0448\u049b\u0430 \u0442\u0443\u0440\u043a\u0438\u0439 \u0445\u0430\u043b\u049b\u043b\u0430\u0440\u043d\u0438\u043d\u0433 [mask], \u043c\u0443\u0442\u0430\u0444\u0430\u043a\u043a\u0438\u0440\u0438 \u0432\u0430 \u0434\u0430\u0432\u043b\u0430\u0442 \u0430\u0440\u0431\u043e\u0431\u0438 \u0431\u045e\u043b\u0433\u0430\u043d.\")\n\n[{'score': 0.5902208685874939,\n  'sequence': '\u0410\u043b\u0438\u0448\u0435\u0440 \u041d\u0430\u0432\u043e\u0438\u0439 \u2013 \u0443\u043b\u0443\u0493 \u045e\u0437\u0431\u0435\u043a \u0432\u0430 \u0431\u043e\u0448\u049b\u0430 \u0442\u0443\u0440\u043a\u0438\u0439 \u0445\u0430\u043b\u049b\u043b\u0430\u0440\u043d\u0438\u043d\u0433 \u0448\u043e\u0438\u0440\u0438, \u043c\u0443\u0442\u0430\u0444\u0430\u043a\u043a\u0438\u0440\u0438 \u0432\u0430 \u0434\u0430\u0432\u043b\u0430\u0442 \u0430\u0440\u0431\u043e\u0431\u0438 \u0431\u045e\u043b\u0433\u0430\u043d.',\n  'token': 28809,\n  'token_str': ' \u0448\u043e\u0438\u0440\u0438'},\n {'score': 0.08303504437208176,\n  'sequence': '\u0410\u043b\u0438\u0448\u0435\u0440 \u041d\u0430\u0432\u043e\u0438\u0439 \u2013 \u0443\u043b\u0443\u0493 \u045e\u0437\u0431\u0435\u043a \u0432\u0430 \u0431\u043e\u0448\u049b\u0430 \u0442\u0443\u0440\u043a\u0438\u0439 \u0445\u0430\u043b\u049b\u043b\u0430\u0440\u043d\u0438\u043d\u0433 \u0443\u0441\u0442\u043e\u0437\u0438, \u043c\u0443\u0442\u0430\u0444\u0430\u043a\u043a\u0438\u0440\u0438 \u0432\u0430 \u0434\u0430\u0432\u043b\u0430\u0442 \u0430\u0440\u0431\u043e\u0431\u0438 \u0431\u045e\u043b\u0433\u0430\u043d.',\n  'token': 17484,\n  'token_str': ' \u0443\u0441\u0442\u043e\u0437\u0438'},\n {'score': 0.035882771015167236,\n  'sequence': '\u0410\u043b\u0438\u0448\u0435\u0440 \u041d\u0430\u0432\u043e\u0438\u0439 \u2013 \u0443\u043b\u0443\u0493 \u045e\u0437\u0431\u0435\u043a \u0432\u0430 \u0431\u043e\u0448\u049b\u0430 \u0442\u0443\u0440\u043a\u0438\u0439 \u0445\u0430\u043b\u049b\u043b\u0430\u0440\u043d\u0438\u043d\u0433 \u0430\u0440\u0431\u043e\u0431\u0438, \u043c\u0443\u0442\u0430\u0444\u0430\u043a\u043a\u0438\u0440\u0438 \u0432\u0430 \u0434\u0430\u0432\u043b\u0430\u0442 \u0430\u0440\u0431\u043e\u0431\u0438 \u0431\u045e\u043b\u0433\u0430\u043d.',\n  'token': 34552,\n  'token_str': ' \u0430\u0440\u0431\u043e\u0431\u0438'},\n {'score': 0.03447483479976654,\n  'sequence': '\u0410\u043b\u0438\u0448\u0435\u0440 \u041d\u0430\u0432\u043e\u0438\u0439 \u2013 \u0443\u043b\u0443\u0493 \u045e\u0437\u0431\u0435\u043a \u0432\u0430 \u0431\u043e\u0448\u049b\u0430 \u0442\u0443\u0440\u043a\u0438\u0439 \u0445\u0430\u043b\u049b\u043b\u0430\u0440\u043d\u0438\u043d\u0433 \u0430\u0441\u043e\u0441\u0447\u0438\u0441\u0438, \u043c\u0443\u0442\u0430\u0444\u0430\u043a\u043a\u0438\u0440\u0438 \u0432\u0430 \u0434\u0430\u0432\u043b\u0430\u0442 \u0430\u0440\u0431\u043e\u0431\u0438 \u0431\u045e\u043b\u0433\u0430\u043d.',\n  'token': 14034,\n  'token_str': ' \u0430\u0441\u043e\u0441\u0447\u0438\u0441\u0438'},\n {'score': 0.03044942207634449,\n  'sequence': '\u0410\u043b\u0438\u0448\u0435\u0440 \u041d\u0430\u0432\u043e\u0438\u0439 \u2013 \u0443\u043b\u0443\u0493 \u045e\u0437\u0431\u0435\u043a \u0432\u0430 \u0431\u043e\u0448\u049b\u0430 \u0442\u0443\u0440\u043a\u0438\u0439 \u0445\u0430\u043b\u049b\u043b\u0430\u0440\u043d\u0438\u043d\u0433 \u0434\u045e\u0441\u0442\u0438, \u043c\u0443\u0442\u0430\u0444\u0430\u043a\u043a\u0438\u0440\u0438 \u0432\u0430 \u0434\u0430\u0432\u043b\u0430\u0442 \u0430\u0440\u0431\u043e\u0431\u0438 \u0431\u045e\u043b\u0433\u0430\u043d.',\n  'token': 28100,\n  'token_str': ' \u0434\u045e\u0441\u0442\u0438'}]\n  \n  \n  unmasker(\"Kuchli yomg\u2018irlar tufayli bir qator [mask] kuchli sel oqishi kuzatildi.\")\n  \n  [{'score': 0.410250186920166,\n  'sequence': 'Kuchli yomg\u2018irlar tufayli bir qator hududlarda kuchli sel oqishi kuzatildi.',\n  'token': 11009,\n  'token_str': ' hududlarda'},\n {'score': 0.2023029774427414,\n  'sequence': 'Kuchli yomg\u2018irlar tufayli bir qator tumanlarda kuchli sel oqishi kuzatildi.',\n  'token': 35370,\n  'token_str': ' tumanlarda'},\n {'score': 0.129830002784729,\n  'sequence': 'Kuchli yomg\u2018irlar tufayli bir qator viloyatlarda kuchli sel oqishi kuzatildi.',\n  'token': 33584,\n  'token_str': ' viloyatlarda'},\n {'score': 0.04539087787270546,\n  'sequence': 'Kuchli yomg\u2018irlar tufayli bir qator mamlakatlarda kuchli sel oqishi kuzatildi.',\n  'token': 19315,\n  'token_str': ' mamlakatlarda'},\n {'score': 0.0369882769882679,\n  'sequence': 'Kuchli yomg\u2018irlar tufayli bir qator joylarda kuchli sel oqishi kuzatildi.',\n  'token': 5853,\n  'token_str': ' joylarda'}]\n</code></pre>\n\n<p><b>Training data.</b>\n\nUzBERT model was pretrained on &asymp;2M news articles (&asymp;3Gb).\n\n<pre><code class=\"language-python\">\n@misc {rifkat_davronov_2022,\n\tauthor       = { {Adilova Fatima,Rifkat Davronov, Samariddin Kushmuratov, Ruzmat Safarov} },\n\ttitle        = { uztext-3Gb-BPE-Roberta (Revision 0c87494) },\n\tyear         = 2022,\n\turl          = {  },\n\tdoi          = { 10.57967/hf/0140 },\n\tpublisher    = { Hugging Face }\n}\n</code></pre>\n", "qas": [{"id": "q2", "question": "What is the model task of rifkat/uztext-3Gb-BPE-Roberta?", "answers": [{"text": "fill-mask", "answer_start": 727, "answer_end": 735}]}]}]}, {"title": "rifkat/uztext_568Mb_Roberta_BPE", "paragraphs": [{"context": "<p><b>UzRoBerta model.</b>\n\nPre-prepared model in Uzbek (Cyrillic script) to model the masked language and predict the next sentences.\n\n<p><b>Training data.</b>\n\nUzBERT model was pretrained on &asymp;167K news articles (&asymp;568Mb).\n", "qas": []}]}, {"title": "ringabelle/bert-base-cased-finetuned-COVID-tweets", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bert-base-cased-finetuned-COVID-tweets\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-cased-finetuned-COVID-tweets\n\nThis model is a fine-tuned version of [bert-base-cased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.2694\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    2.4419          |\n 2.0    2.4230          |\n 3.0    2.3678          |\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.9.0+cu111\n- Datasets 1.13.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of ringabelle/bert-base-cased-finetuned-COVID-tweets?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "rinna/japanese-gpt-1b", "paragraphs": [{"context": "---\nlanguage: ja\nthumbnail: \ntags:\n- ja\n- japanese\n- gpt\n- text-generation\n- lm\n- nlp\nlicense: mit\ndatasets:\n- cc100\n- wikipedia\n- c4\nwidget:\n- text: \"\u897f\u7530\u5e7e\u591a\u90ce\u306f\u3001\"\n---\n\n# japanese-gpt-1b\n\n![rinna-icon](./rinna.png)\n\nThis repository provides a 1.3B-parameter Japanese GPT model. The model was trained by [rinna Co., Ltd.](\n\n# How to use the model\n\n~~~~\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-1b\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-1b\")\n\nif torch.cuda.is_available():\n    model = model.to(\"cuda\")\n\ntext = \"\u897f\u7530\u5e7e\u591a\u90ce\u306f\u3001\"\ntoken_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    output_ids = model.generate(\n        token_ids.to(model.device),\n        max_length=100,\n        min_length=100,\n        do_sample=True,\n        top_k=500,\n        top_p=0.95,\n        pad_token_id=tokenizer.pad_token_id,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        bad_words_ids=[[tokenizer.unk_token_id]]\n    )\n\noutput = tokenizer.decode(output_ids.tolist()[0])\nprint(output)  \n# sample output: \u897f\u7530\u5e7e\u591a\u90ce\u306f\u3001\u305d\u306e\u4e3b\u8457\u306e\u300c\u5584\u306e\u7814\u7a76\u300d\u306a\u3069\u3067\u3001\u4eba\u9593\u306e\u5185\u9762\u306b\u81ea\u7136\u3068\u305d\u306e\u6839\u6e90\u304c\u3042\u308b\u3068\u6307\u6458\u3057\u3001\u305d\u306e\u6839\u6e90\u7684\u306a\u6027\u683c\u306f\u3001\u3053\u306e\u897f\u7530\u54f2\u5b66\u3092\u8c61\u5fb4\u3057\u3066\u3044\u308b\u3068\u3057\u3066\u3001\u30ab\u30f3\u30c8\u306e\u300c\u7d14\u7c8b\u7406\u6027\u6279\u5224\u300d\u3068\u300c\u5224\u65ad\u529b\u6279\u5224\u300d\u3092\u5bfe\u6bd4\u3057\u3066\u6349\u3048\u307e\u3059\u3002\u305d\u308c\u306f\u3001\u300c\u4eba\u304c\u7406\u6027\u7684\u5b58\u5728\u3067\u3042\u308b\u304b\u304e\u308a\u306b\u304a\u3044\u3066\u3001\u4eba\u306f\u305d\u306e\u5f53\u4eba\u306b\u56fa\u6709\u306a\u9053\u5fb3\u7684\u306b\u81ea\u899a\u3055\u308c\u305f\u5584\u60aa\u306e\u57fa\u6e96\u3092\u6301\u3063\u3066\u3044\u308b\u300d\u3068\u3059\u308b\u3082\u306e\u3067\u3001\u3053\u306e\u7406\u6027\u7684\u306a\u5584\u60aa\u306e\u89b3\u5ff5\u3092\u5426\u5b9a\u3059\u308b\u306e\u304c\u30ab\u30f3\u30c8\u306e\n~~~~\n\n# Model architecture\nA 24-layer, 2048-hidden-size transformer-based language model.\n\n# Training\nThe model was trained on [Japanese C4]( [Japanese CC-100]( and [Japanese Wikipedia]( to optimize a traditional language modelling objective. It reaches around 14 perplexity on a chosen validation set from the same data.\n# Tokenization\nThe model uses a [sentencepiece]( tokenizer. The vocabulary was first trained on a selected subset from the training data using the official sentencepiece training script, and then augmented with emojis and symbols.\n# Licenese\n[The MIT license](\n", "qas": [{"id": "q2", "question": "What is the model task of rinna/japanese-gpt-1b?", "answers": [{"text": "text-generation", "answer_start": 59, "answer_end": 73}]}]}]}, {"title": "rinna/japanese-gpt2-medium", "paragraphs": [{"context": "---\nlanguage: ja\nthumbnail: \ntags:\n- ja\n- japanese\n- gpt2\n- text-generation\n- lm\n- nlp\nlicense: mit\ndatasets:\n- cc100\n- wikipedia\nwidget:\n- text: \"\u751f\u547d\u3001\u5b87\u5b99\u3001\u305d\u3057\u3066\u4e07\u7269\u306b\u3064\u3044\u3066\u306e\u7a76\u6975\u306e\u7591\u554f\u306e\u7b54\u3048\u306f\"\n---\n\n# japanese-gpt2-medium\n\n![rinna-icon](./rinna.png)\n\nThis repository provides a medium-sized Japanese GPT-2 model. The model was trained using code from Github repository [rinnakk/japanese-pretrained-models]( by [rinna Co., Ltd.](\n\n# How to use the model\n\n~~~~\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\", use_fast=False)\ntokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n\nmodel = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n~~~~\n\n# Model architecture\nA 24-layer, 1024-hidden-size transformer-based language model.\n\n# Training\nThe model was trained on [Japanese CC-100]( and [Japanese Wikipedia]( to optimize a traditional language modelling objective on 8\\\\*V100 GPUs for around 30 days. It reaches around 18 perplexity on a chosen validation set from the same data.\n\n# Tokenization\nThe model uses a [sentencepiece]( tokenizer, the vocabulary was trained on the Japanese Wikipedia using the official sentencepiece training script.\n\n# Licenese\n[The MIT license](\n", "qas": [{"id": "q1", "question": "What is the model architecture of rinna/japanese-gpt2-medium?", "answers": [{"text": "gpt2", "answer_start": 53, "answer_end": 56}]}, {"id": "q2", "question": "What is the model task of rinna/japanese-gpt2-medium?", "answers": [{"text": "text-generation", "answer_start": 60, "answer_end": 74}]}]}]}, {"title": "rinna/japanese-gpt2-small", "paragraphs": [{"context": "---\nlanguage: ja\nthumbnail: \ntags:\n- ja\n- japanese\n- gpt2\n- text-generation\n- lm\n- nlp\nlicense: mit\ndatasets:\n- cc100\n- wikipedia\nwidget:\n- text: \"\u751f\u547d\u3001\u5b87\u5b99\u3001\u305d\u3057\u3066\u4e07\u7269\u306b\u3064\u3044\u3066\u306e\u7a76\u6975\u306e\u7591\u554f\u306e\u7b54\u3048\u306f\"\n---\n\n# japanese-gpt2-small\n\n![rinna-icon](./rinna.png)\n\nThis repository provides a small-sized Japanese GPT-2 model. The model was trained using code from Github repository [rinnakk/japanese-pretrained-models]( by [rinna Co., Ltd.](\n\n# How to use the model\n\n~~~~\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-small\", use_fast=False)\ntokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n\nmodel = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-small\")\n~~~~\n\n# Model architecture\nA 12-layer, 768-hidden-size transformer-based language model.\n\n# Training\nThe model was trained on [Japanese CC-100]( and [Japanese Wikipedia]( to optimize a traditional language modelling objective on 8\\\\*V100 GPUs for around 15 days. It reaches around 21 perplexity on a chosen validation set from CC-100.\n\n# Tokenization\nThe model uses a [sentencepiece]( tokenizer, the vocabulary was trained on the Japanese Wikipedia using the official sentencepiece training script.\n\n# Licenese\n[The MIT license](\n", "qas": [{"id": "q1", "question": "What is the model architecture of rinna/japanese-gpt2-small?", "answers": [{"text": "gpt2", "answer_start": 53, "answer_end": 56}]}, {"id": "q2", "question": "What is the model task of rinna/japanese-gpt2-small?", "answers": [{"text": "text-generation", "answer_start": 60, "answer_end": 74}]}]}]}, {"title": "rinna/japanese-gpt2-xsmall", "paragraphs": [{"context": "---\nlanguage: ja\nthumbnail: \ntags:\n- ja\n- japanese\n- gpt2\n- text-generation\n- lm\n- nlp\nlicense: mit\ndatasets:\n- cc100\n- wikipedia\nwidget:\n- text: \"\u751f\u547d\u3001\u5b87\u5b99\u3001\u305d\u3057\u3066\u4e07\u7269\u306b\u3064\u3044\u3066\u306e\u7a76\u6975\u306e\u7591\u554f\u306e\u7b54\u3048\u306f\"\n---\n\n# japanese-gpt2-xsmall\n\n![rinna-icon](./rinna.png)\n\nThis repository provides an extra-small-sized Japanese GPT-2 model. The model was trained using code from Github repository [rinnakk/japanese-pretrained-models]( by [rinna Co., Ltd.](\n\n# How to use the model\n\n~~~~\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-xsmall\", use_fast=False)\ntokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n\nmodel = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-xsmall\")\n~~~~\n\n# Model architecture\nA 6-layer, 512-hidden-size transformer-based language model.\n\n# Training\nThe model was trained on [Japanese CC-100]( and [Japanese Wikipedia]( to optimize a traditional language modelling objective on 8\\\\*V100 GPUs for around 4 days. It reaches around 28 perplexity on a chosen validation set from CC-100.\n\n# Tokenization\nThe model uses a [sentencepiece]( tokenizer, the vocabulary was trained on the Japanese Wikipedia using the official sentencepiece training script.\n\n# Licenese\n[The MIT license](\n", "qas": [{"id": "q1", "question": "What is the model architecture of rinna/japanese-gpt2-xsmall?", "answers": [{"text": "gpt2", "answer_start": 53, "answer_end": 56}]}, {"id": "q2", "question": "What is the model task of rinna/japanese-gpt2-xsmall?", "answers": [{"text": "text-generation", "answer_start": 60, "answer_end": 74}]}]}]}, {"title": "rinna/japanese-roberta-base", "paragraphs": [{"context": "---\nlanguage: ja\nthumbnail: \ntags:\n- ja\n- japanese\n- roberta\n- masked-lm\n- nlp\nlicense: mit\ndatasets:\n- cc100\n- wikipedia\nmask_token: \"[MASK]\"\nwidget:\n- text: \"[CLS]4\u5e74\u306b1\u5ea6[MASK]\u306f\u958b\u304b\u308c\u308b\u3002\"\n---\n\n# japanese-roberta-base\n\n![rinna-icon](./rinna.png)\n\nThis repository provides a base-sized Japanese RoBERTa model. The model was trained using code from Github repository [rinnakk/japanese-pretrained-models]( by [rinna Co., Ltd.](\n\n# How to load the model\n\n~~~~\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-roberta-base\", use_fast=False)\ntokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n\nmodel = AutoModelForMaskedLM.from_pretrained(\"rinna/japanese-roberta-base\")\n~~~~\n\n# How to use the model for masked token prediction\n\n## Note 1: Use `[CLS]`\n\nTo predict a masked token, be sure to add a `[CLS]` token before the sentence for the model to correctly encode it, as it is used during the model training.\n\n## Note 2: Use `[MASK]` after tokenization\n\nA) Directly typing `[MASK]` in an input string and B) replacing a token with `[MASK]` after tokenization will yield different token sequences, and thus different prediction results. It is more appropriate to use `[MASK]` after tokenization (as it is consistent with how the model was pretrained). However, the Huggingface Inference API only supports typing `[MASK]` in the input string and produces less robust predictions.\n\n## Note 3: Provide `position_ids` as an argument explicitly\n\nWhen `position_ids` are not provided for a `Roberta*` model, Huggingface's `transformers` will automatically construct it but start from `padding_idx` instead of `0` (see [issue]( and function `create_position_ids_from_input_ids()` in Huggingface's [implementation]( which unfortunately does not work as expected with `rinna/japanese-roberta-base` since the `padding_idx` of the corresponding tokenizer is not `0`. So please be sure to constrcut the `position_ids` by yourself and make it start from position id `0`.\n\n## Example\n\nHere is an example by to illustrate how our model works as a masked language model. Notice the difference between running the following code example and running the Huggingface Inference API. \n\n~~~~\n# original text\ntext = \"4\u5e74\u306b1\u5ea6\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u306f\u958b\u304b\u308c\u308b\u3002\"\n\n# prepend [CLS]\ntext = \"[CLS]\" + text\n\n# tokenize\ntokens = tokenizer.tokenize(text)\nprint(tokens)  # output: ['[CLS]', '\u25814', '\u5e74\u306b', '1', '\u5ea6', '\u30aa\u30ea\u30f3\u30d4\u30c3\u30af', '\u306f', '\u958b\u304b\u308c\u308b', '\u3002']\n\n# mask a token\nmasked_idx = 5\ntokens[masked_idx] = tokenizer.mask_token\nprint(tokens)  # output: ['[CLS]', '\u25814', '\u5e74\u306b', '1', '\u5ea6', '[MASK]', '\u306f', '\u958b\u304b\u308c\u308b', '\u3002']\n\n# convert to ids\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(token_ids)  # output: [4, 1602, 44, 24, 368, 6, 11, 21583, 8]\n\n# convert to tensor\nimport torch\ntoken_tensor = torch.LongTensor([token_ids])\n\n# provide position ids explicitly\nposition_ids = list(range(0, token_tensor.size(1)))\nprint(position_ids)  # output: [0, 1, 2, 3, 4, 5, 6, 7, 8]\nposition_id_tensor = torch.LongTensor([position_ids])\n\n# get the top 10 predictions of the masked token\nwith torch.no_grad():\n    outputs = model(input_ids=token_tensor, position_ids=position_id_tensor)\n    predictions = outputs[0][0, masked_idx].topk(10)\n\nfor i, index_t in enumerate(predictions.indices):\n    index = index_t.item()\n    token = tokenizer.convert_ids_to_tokens([index])[0]\n    print(i, token)\n\n\"\"\"\n0 \u7dcf\u4f1a\n1 \u30b5\u30df\u30c3\u30c8\n2 \u30ef\u30fc\u30eb\u30c9\u30ab\u30c3\u30d7\n3 \u30d5\u30a7\u30b9\u30c6\u30a3\u30d0\u30eb\n4 \u5927\u4f1a\n5 \u30aa\u30ea\u30f3\u30d4\u30c3\u30af\n6 \u5168\u56fd\u5927\u4f1a\n7 \u515a\u5927\u4f1a\n8 \u30a4\u30d9\u30f3\u30c8\n9 \u4e16\u754c\u9078\u624b\u6a29\n\"\"\"\n~~~~\n\n# Model architecture\nA 12-layer, 768-hidden-size transformer-based masked language model.\n\n# Training\nThe model was trained on [Japanese CC-100]( and [Japanese Wikipedia]( to optimize a masked language modelling objective on 8*V100 GPUs for around 15 days. It reaches ~3.9 perplexity on a dev set sampled from CC-100.\n\n# Tokenization\nThe model uses a [sentencepiece]( tokenizer, the vocabulary was trained on the Japanese Wikipedia using the official sentencepiece training script.\n\n# Licenese\n[The MIT license](\n", "qas": [{"id": "q1", "question": "What is the model architecture of rinna/japanese-roberta-base?", "answers": [{"text": "roberta", "answer_start": 53, "answer_end": 59}]}]}]}, {"title": "rinz/DialoGPT-small-Harry-Potterrr", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Harry Potter model", "qas": [{"id": "q2", "question": "What is the model task of rinz/DialoGPT-small-Harry-Potterrr?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "risingodegua/hate-speech-detector", "paragraphs": [{"context": "---\nlanguage: en\ntag: text-classification\ndatasets:\n- twitter\n- movies subtitles\n---\n\n# Hate Speech Detector \nThis model is a fork of the [bert-based-uncased-hatespeech-movies]( model. It is used to classify text as **normal**, **offensive**, **hatespeech**. The model is initially a pre-trained transformer model(bert-based-uncased) which is further trained on Twitter comments which can be normal, offensive and hate to learn the context from social media data. It is then fine-tuned using the movie subtitles dataset.\n\n## Test it out\nYou can test this model live on [Spaces](\n", "qas": [{"id": "q1", "question": "What is the model architecture of risingodegua/hate-speech-detector?", "answers": [{"text": "bert", "answer_start": 139, "answer_end": 142}]}, {"id": "q2", "question": "What is the model task of risingodegua/hate-speech-detector?", "answers": [{"text": "text-classification", "answer_start": 22, "answer_end": 40}]}]}]}, {"title": "riyadhctg/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel_index:\n- name: distilbert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metric:\n      name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5526838482765232\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7691\n- Matthews Correlation: 0.5527\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5390          \n 2.0    0.5273          \n 3.0    0.6391          \n 4.0    0.7691          \n 5.0    0.8483          \n\n\n### Framework versions\n\n- Transformers 4.9.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.11.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of riyadhctg/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 125, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of riyadhctg/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 229, "answer_end": 247}]}]}]}, {"title": "rjbownes/Magic-The-Generating", "paragraphs": [{"context": "---\nwidget:\n- text: \"Even the Dwarves\"\n- text: \"The secrets of\"\n---\n\n# Model name\nMagic The Generating\n\n## Model description\n\nThis is a fine tuned GPT-2 model trained on a corpus of all available English language Magic the Gathering card flavour texts.\n\n## Intended uses & limitations\n\nThis is intended only for use in generating new, novel, and sometimes surprising, MtG like flavour texts.\n\n#### How to use\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"rjbownes/Magic-The-Generating\")\n\nmodel = GPT2LMHeadModel.from_pretrained(\"rjbownes/Magic-The-Generating\")\n\n```\n\n#### Limitations and bias\n\nThe training corpus was surprisingly small, only ~29000 cards, I had suspected there were more. This might mean there is a real limit to the number of entirely original strings this will generate.\nThis is also only based on the 117M parameter GPT2, it's a pretty obvious upgrade to retrain with medium, large or XL models. However, despite this, the outputs I tested were very convincing!\n\n## Training data\n\nThe data was 29222 MtG card flavour texts. The model was based on the \"gpt2\" pretrained transformer: \n\n## Training procedure\n\nOnly English language MtG flavour texts were scraped from the [Scryfall]( API. Empty strings and any non-UTF-8 encoded tokens were removed leaving 29222 entries.\nThis was trained using google Colab with a T4 instance. 4 epochs, adamW optimizer with default parameters and a batch size of 32. Token embedding lengths were capped at 98 tokens as this was the longest string and an attention mask was added to the training model to ignore all padding tokens.\n\n## Eval results\n\nAverage Training Loss: 0.44866578806635815.\nValidation loss: 0.5606984243444775.\n\nSample model outputs:\n\n1. \"Every branch a crossroads, every vine a swift steed.\"\n\t\u2014Gwendlyn Di Corci\n\n2. \"The secrets of this world will tell their masters where to strike if need be.\"\n\t\u2014Noyan Dar, Tazeem roilmage\n\n3. \"The secrets of nature are expensive. You'd be better off just to have more freedom.\"\n\n4. \"Even the Dwarves knew to leave some stones unturned.\"\n\n5. \"The wise always keep an ear open to the whispers of power.\"\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{BownesLM,\n  title={Fine Tuning GPT-2 for Magic the Gathering flavour text generation.},\n  author={Richard J. Bownes},\n  journal={Medium},\n  year={2020}\n}\n\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of rjbownes/Magic-The-Generating?", "answers": [{"text": "gpt2", "answer_start": 1138, "answer_end": 1141}]}]}]}, {"title": "rkmt/wav2vec2-base-timit-demo-colab", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: wav2vec2-base-timit-demo-colab\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-timit-demo-colab\n\nThis model is a fine-tuned version of [facebook/hubert-large-ls960-ft]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0280\n- Wer: 0.0082\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.42   0.0416          \n 2.83   0.0372          \n 4.25   0.0345          \n 5.67   0.0338          \n 7.08   0.0307          \n 8.5    0.0343          \n 9.92   0.0300          \n 11.33  0.0314          \n 12.75  0.0283          \n 14.16  0.0302          \n 15.58  0.0298          \n 17.0   0.0320          \n 18.41  0.0286          \n 19.83  0.0284          \n 21.25  0.0290          \n 22.66  0.0284          \n 24.08  0.0280          \n 25.5   0.0281          \n 26.91  0.0280          \n 28.33  0.0280          \n 29.75  0.0280          \n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.9.0+cu111\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of rkmt/wav2vec2-base-timit-demo-colab?", "answers": [{"text": "hubert", "answer_start": 396, "answer_end": 401}]}]}]}, {"title": "rlagusrlagus123/XTC20000", "paragraphs": [{"context": "---\ntags:\n- conversational \n---\n---\n\n#12 epochs, each batch size 2, gradient accumulation steps 2, tail 20000", "qas": [{"id": "q2", "question": "What is the model task of rlagusrlagus123/XTC20000?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "rlagusrlagus123/XTC4096", "paragraphs": [{"context": "---\ntags:\n- conversational \n---\n---\n\n\n#12 epochs, each batch size 4, gradient accumulation steps 1, tail 4096.\n\n#THIS SEEMS TO BE THE OPTIMAL SETUP.", "qas": [{"id": "q2", "question": "What is the model task of rlagusrlagus123/XTC4096?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "rmicheal48/DialoGPT-small-steven_universe", "paragraphs": [{"context": "---\ntags:\n-  conversational\n---\n\n\n# Steven Universe DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of rmicheal48/DialoGPT-small-steven_universe?", "answers": [{"text": "conversational", "answer_start": 13, "answer_end": 26}]}]}]}, {"title": "stefan-it/electra-base-gc4-64k-100000-cased-generator", "paragraphs": [{"context": "---\nlanguage: de\nlicense: mit\ndatasets:\n- german-nlp-group/german_common_crawl\nwidget:\n- text: \"Heute ist ein [MASK] Tag\"\n---\n\n# GC4LM: A Colossal (Biased) language model for German\nThis repository presents a colossal (and biased) language model for German trained on the recently released\n[\"German colossal, clean Common Crawl corpus\"]( (GC4),\nwith a total dataset size of ~844GB.\n\n---\n\n**Disclaimer**: the presented and trained language models in this repository are for **research only** purposes.\nThe GC4 corpus - that was used for training - contains crawled texts from the internet. Thus, the language models can\nbe considered as highly biased, resulting in a model that encodes stereotypical associations along gender, race,\nethnicity and disability status. Before using and working with the released checkpoints, it is highly recommended\nto read:\n\n[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](\n\nfrom Emily M. Bender, Timnit Gebru, Angelina McMillan-Major and Shmargaret Shmitchell.\n\nThe aim of the released checkpoints is to boost research on large pre-trained language models for German, especially\nfor identifying biases and how to prevent them, as most research is currently done only for English.\n\n---\n\nPlease use the new GitHub Discussions feature in order to discuss or present further research questions.\nFeel free to use `#gc4lm` on Twitter \ud83d\udc26.\n\n", "qas": []}]}, {"title": "stefan-it/electra-base-gc4-64k-200000-cased-generator", "paragraphs": [{"context": "---\nlanguage: de\nlicense: mit\ndatasets:\n- german-nlp-group/german_common_crawl\nwidget:\n- text: \"Heute ist ein [MASK] Tag\"\n---\n\n# GC4LM: A Colossal (Biased) language model for German\nThis repository presents a colossal (and biased) language model for German trained on the recently released\n[\"German colossal, clean Common Crawl corpus\"]( (GC4),\nwith a total dataset size of ~844GB.\n\n---\n\n**Disclaimer**: the presented and trained language models in this repository are for **research only** purposes.\nThe GC4 corpus - that was used for training - contains crawled texts from the internet. Thus, the language models can\nbe considered as highly biased, resulting in a model that encodes stereotypical associations along gender, race,\nethnicity and disability status. Before using and working with the released checkpoints, it is highly recommended\nto read:\n\n[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](\n\nfrom Emily M. Bender, Timnit Gebru, Angelina McMillan-Major and Shmargaret Shmitchell.\n\nThe aim of the released checkpoints is to boost research on large pre-trained language models for German, especially\nfor identifying biases and how to prevent them, as most research is currently done only for English.\n\n---\n\nPlease use the new GitHub Discussions feature in order to discuss or present further research questions.\nFeel free to use `#gc4lm` on Twitter \ud83d\udc26.\n\n", "qas": []}]}, {"title": "stefan-it/electra-base-gc4-64k-300000-cased-generator", "paragraphs": [{"context": "---\nlanguage: de\nlicense: mit\ndatasets:\n- german-nlp-group/german_common_crawl\nwidget:\n- text: \"Heute ist ein [MASK] Tag\"\n---\n\n# GC4LM: A Colossal (Biased) language model for German\nThis repository presents a colossal (and biased) language model for German trained on the recently released\n[\"German colossal, clean Common Crawl corpus\"]( (GC4),\nwith a total dataset size of ~844GB.\n\n---\n\n**Disclaimer**: the presented and trained language models in this repository are for **research only** purposes.\nThe GC4 corpus - that was used for training - contains crawled texts from the internet. Thus, the language models can\nbe considered as highly biased, resulting in a model that encodes stereotypical associations along gender, race,\nethnicity and disability status. Before using and working with the released checkpoints, it is highly recommended\nto read:\n\n[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](\n\nfrom Emily M. Bender, Timnit Gebru, Angelina McMillan-Major and Shmargaret Shmitchell.\n\nThe aim of the released checkpoints is to boost research on large pre-trained language models for German, especially\nfor identifying biases and how to prevent them, as most research is currently done only for English.\n\n---\n\nPlease use the new GitHub Discussions feature in order to discuss or present further research questions.\nFeel free to use `#gc4lm` on Twitter \ud83d\udc26.\n\n", "qas": []}]}, {"title": "stefan-it/electra-base-gc4-64k-400000-cased-generator", "paragraphs": [{"context": "---\nlanguage: de\nlicense: mit\ndatasets:\n- german-nlp-group/german_common_crawl\nwidget:\n- text: \"Heute ist ein [MASK] Tag\"\n---\n\n# GC4LM: A Colossal (Biased) language model for German\nThis repository presents a colossal (and biased) language model for German trained on the recently released\n[\"German colossal, clean Common Crawl corpus\"]( (GC4),\nwith a total dataset size of ~844GB.\n\n---\n\n**Disclaimer**: the presented and trained language models in this repository are for **research only** purposes.\nThe GC4 corpus - that was used for training - contains crawled texts from the internet. Thus, the language models can\nbe considered as highly biased, resulting in a model that encodes stereotypical associations along gender, race,\nethnicity and disability status. Before using and working with the released checkpoints, it is highly recommended\nto read:\n\n[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](\n\nfrom Emily M. Bender, Timnit Gebru, Angelina McMillan-Major and Shmargaret Shmitchell.\n\nThe aim of the released checkpoints is to boost research on large pre-trained language models for German, especially\nfor identifying biases and how to prevent them, as most research is currently done only for English.\n\n---\n\nPlease use the new GitHub Discussions feature in order to discuss or present further research questions.\nFeel free to use `#gc4lm` on Twitter \ud83d\udc26.\n\n", "qas": []}]}, {"title": "stefan-it/electra-base-gc4-64k-500000-cased-generator", "paragraphs": [{"context": "---\nlanguage: de\nlicense: mit\ndatasets:\n- german-nlp-group/german_common_crawl\nwidget:\n- text: \"Heute ist ein [MASK] Tag\"\n---\n\n# GC4LM: A Colossal (Biased) language model for German\nThis repository presents a colossal (and biased) language model for German trained on the recently released\n[\"German colossal, clean Common Crawl corpus\"]( (GC4),\nwith a total dataset size of ~844GB.\n\n---\n\n**Disclaimer**: the presented and trained language models in this repository are for **research only** purposes.\nThe GC4 corpus - that was used for training - contains crawled texts from the internet. Thus, the language models can\nbe considered as highly biased, resulting in a model that encodes stereotypical associations along gender, race,\nethnicity and disability status. Before using and working with the released checkpoints, it is highly recommended\nto read:\n\n[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](\n\nfrom Emily M. Bender, Timnit Gebru, Angelina McMillan-Major and Shmargaret Shmitchell.\n\nThe aim of the released checkpoints is to boost research on large pre-trained language models for German, especially\nfor identifying biases and how to prevent them, as most research is currently done only for English.\n\n---\n\nPlease use the new GitHub Discussions feature in order to discuss or present further research questions.\nFeel free to use `#gc4lm` on Twitter \ud83d\udc26.\n\n", "qas": []}]}, {"title": "ufal/byt5-small-multilexnorm2021-da", "paragraphs": [{"context": "---\nlanguage: da\ndatasets:\n- mc4\n- wikipedia\n- multilexnorm\ntags:\n- lexical normalization\nlicense: apache-2.0\n\n---\n\n# Fine-tuned ByT5-small for MultiLexNorm (Danish version)\n\n![model image](\n\nThis is the official release of the fine-tuned models for **the winning entry** to the [*W-NUT 2021: Multilingual Lexical Normalization (MultiLexNorm)* shared task]( which evaluates lexical-normalization systems on 12 social media datasets in 11 languages.\n\nOur system is based on [ByT5]( which we first pre-train on synthetic data and then fine-tune on authentic normalization data. It achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. In addition to these fine-tuned models, we also release the source files on [GitHub]( and an interactive demo on [Google Colab](\n\n\n## How to use\n\nThe model was *not* fine-tuned in a standard sentence-to-sentence setting \u2013 instead, it was tailored to the token-to-token definition of MultiLexNorm data. Please refer to [**the interactive demo on Colab notebook**]( to learn how to use these models.\n\n\n## How to cite\n\n```bibtex\n@inproceedings{wnut-ufal,\n  title= \"{\u00daFAL} at {MultiLexNorm} 2021: Improving Multilingual Lexical Normalization by Fine-tuning {ByT5}\",\n  author = \"Samuel, David and Straka, Milan\",\n  booktitle = \"Proceedings of the 7th Workshop on Noisy User-generated Text (W-NUT 2021)\",\n  year = \"2021\",\n  publisher = \"Association for Computational Linguistics\",\n  address = \"Punta Cana, Dominican Republic\"\n}\n```\n\n\n## ByT5 - Small\n\nByT5 is a tokenizer-free version of [Google's T5]( and generally follows the architecture of [MT5](\n\nByT5 was only pre-trained on [mC4]( excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\n\nByT5 works especially well on noisy text data,*e.g.*, `google/byt5-small` significantly outperforms [mt5-small]( on [TweetQA](\n\nPaper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](\n\nAuthors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel* \n", "qas": [{"id": "q1", "question": "What is the model architecture of ufal/byt5-small-multilexnorm2021-da?", "answers": [{"text": "t5", "answer_start": 1950, "answer_end": 1951}]}]}]}, {"title": "unicamp-dl/ptt5-base-pt-msmarco-10k-v1", "paragraphs": [{"context": "---\nlanguage: pt\nlicense: mit\ntags:\n- msmarco\n- t5\n- pytorch\n- tensorflow\n- pt\n- pt-br\ndatasets:\n- msmarco\nwidget:\n- text: \"Texto de exemplo em portugu\u00eas\"\ninference: false\n---\n# PTT5-base Reranker finetuned on Portuguese MS MARCO\n## Introduction\nptt5-base-msmarco-pt-10k-v1 is a T5-based model pretrained in the BrWac corpus, finetuned on Portuguese translated version of MS MARCO passage dataset. In the version v1, the Portuguese dataset was translated using [Helsinki]( NMT model. This model was finetuned for 10k steps. \nFurther information about the dataset or the translation method can be found on our [**mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset**]( and [mMARCO]( repository.\n\n## Usage\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\nmodel_name = 'unicamp-dl/ptt5-base-msmarco-pt-10k-v1'\ntokenizer  = T5Tokenizer.from_pretrained(model_name)\nmodel      = T5ForConditionalGeneration.from_pretrained(model_name)\n\n```\n# Citation\nIf you use ptt5-base-msmarco-pt-10k-v1, please cite:\n\n    @misc{bonifacio2021mmarco,\n      title={mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset}, \n      author={Luiz Henrique Bonifacio and Vitor Jeronymo and Hugo Queiroz Abonizio and Israel Campiotti and Marzieh Fadaee and  and Roberto Lotufo and Rodrigo Nogueira},\n      year={2021},\n      eprint={2108.13897},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n", "qas": [{"id": "q1", "question": "What is the model architecture of unicamp-dl/ptt5-base-pt-msmarco-10k-v1?", "answers": [{"text": "t5", "answer_start": 48, "answer_end": 49}]}]}]}, {"title": "unicamp-dl/ptt5-base-pt-msmarco-10k-v2", "paragraphs": [{"context": "---\nlanguage: pt\nlicense: mit\ntags:\n- msmarco\n- t5\n- pytorch\n- tensorflow\n- pt\n- pt-br\ndatasets:\n- msmarco\nwidget:\n- text: \"Texto de exemplo em portugu\u00eas\"\ninference: false\n---\n# PTT5-base Reranker finetuned on Portuguese MS MARCO\n## Introduction\nptt5-base-msmarco-pt-10k-v2 is a T5-based model pretrained in the BrWac corpus, finetuned on Portuguese translated version of MS MARCO passage dataset. In the v2 version, the Portuguese dataset was translated using Google Translate. This model was finetuned for 10k steps. \nFurther information about the dataset or the translation method can be found on our [**mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset**]( and [mMARCO]( repository.\n\n## Usage\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\nmodel_name = 'unicamp-dl/ptt5-base-msmarco-pt-10k-v2'\ntokenizer  = T5Tokenizer.from_pretrained(model_name)\nmodel      = T5ForConditionalGeneration.from_pretrained(model_name)\n\n```\n# Citation\nIf you use ptt5-base-msmarco-pt-10k-v2, please cite:\n\n    @misc{bonifacio2021mmarco,\n      title={mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset}, \n      author={Luiz Henrique Bonifacio and Vitor Jeronymo and Hugo Queiroz Abonizio and Israel Campiotti and Marzieh Fadaee and  and Roberto Lotufo and Rodrigo Nogueira},\n      year={2021},\n      eprint={2108.13897},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of unicamp-dl/ptt5-base-pt-msmarco-10k-v2?", "answers": [{"text": "t5", "answer_start": 48, "answer_end": 49}]}]}]}, {"title": "unicamp-dl/ptt5-large-portuguese-vocab", "paragraphs": [{"context": "---\nlanguage: pt\nlicense: mit\ntags:\n- t5\n- pytorch\n- tensorflow\n- pt\n- pt-br\ndatasets:\n- brWaC\nwidget:\n- text: \"Texto de exemplo em portugu\u00eas\"\ninference: false\n---\n\n# Portuguese T5 (aka \"PTT5\")\n\n## Introduction\nPTT5 is a T5 model pretrained in the BrWac corpus, a large  collection  of  web  pages in Portuguese, improving T5's performance on Portuguese sentence similarity and entailment tasks. It's available in three sizes (small, base and large) and two vocabularies (Google's T5 original and ours, trained on Portuguese Wikipedia).\n\nFor further information or requests, please go to [PTT5 repository](\n\n## Available models\n Size                                                    Vocabulary         |\n :-:                                                             :-:                |\n small  Google's T5 |\n base   Google's T5 |\n large  Google's T5 |\n small  Portuguese  |\n **base**   **Portuguese**  |\n large  Portuguese  |\n\n## Usage\n```python\n# Tokenizer \nfrom transformers import T5Tokenizer\n\n# PyTorch (bare model, baremodel + language modeling head)\nfrom transformers import T5Model, T5ForConditionalGeneration\n\n# Tensorflow (bare model, baremodel + language modeling head)\nfrom transformers import TFT5Model, TFT5ForConditionalGeneration\n\nmodel_name = 'unicamp-dl/ptt5-base-portuguese-vocab'\n\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# PyTorch\nmodel_pt = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# TensorFlow\nmodel_tf = TFT5ForConditionalGeneration.from_pretrained(model_name)\n```\n\n\n# Citation\nIf you use PTT5, please cite:\n\n    @article{ptt5_2020,\n      title={PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data},\n      author={Carmo, Diedre and Piau, Marcos and Campiotti, Israel and Nogueira, Rodrigo and Lotufo, Roberto},\n      journal={arXiv preprint arXiv:2008.09144},\n      year={2020}\n    }\n", "qas": [{"id": "q1", "question": "What is the model architecture of unicamp-dl/ptt5-large-portuguese-vocab?", "answers": [{"text": "t5", "answer_start": 38, "answer_end": 39}]}]}]}, {"title": "unicamp-dl/ptt5-small-t5-vocab", "paragraphs": [{"context": "---\nlanguage: pt\nlicense: mit\ntags:\n- t5\n- pytorch\n- tensorflow\n- pt\n- pt-br\ndatasets:\n- brWaC\nwidget:\n- text: \"Texto de exemplo em portugu\u00eas\"\ninference: false\n---\n\n# Portuguese T5 (aka \"PTT5\")\n\n## Introduction\nPTT5 is a T5 model pretrained in the BrWac corpus, a large  collection  of  web  pages in Portuguese, improving T5's performance on Portuguese sentence similarity and entailment tasks. It's available in three sizes (small, base and large) and two vocabularies (Google's T5 original and ours, trained on Portuguese Wikipedia).\n\nFor further information or requests, please go to [PTT5 repository](\n\n## Available models\n Size                                                    Vocabulary         |\n :-:                                                             :-:                |\n small  Google's T5 |\n base   Google's T5 |\n large  Google's T5 |\n small  Portuguese  |\n **base**   **Portuguese**  |\n large  Portuguese  |\n\n## Usage\n```python\n# Tokenizer \nfrom transformers import T5Tokenizer\n\n# PyTorch (bare model, baremodel + language modeling head)\nfrom transformers import T5Model, T5ForConditionalGeneration\n\n# Tensorflow (bare model, baremodel + language modeling head)\nfrom transformers import TFT5Model, TFT5ForConditionalGeneration\n\nmodel_name = 'unicamp-dl/ptt5-base-portuguese-vocab'\n\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# PyTorch\nmodel_pt = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# TensorFlow\nmodel_tf = TFT5ForConditionalGeneration.from_pretrained(model_name)\n```\n\n\n# Citation\nIf you use PTT5, please cite:\n\n    @article{ptt5_2020,\n      title={PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data},\n      author={Carmo, Diedre and Piau, Marcos and Campiotti, Israel and Nogueira, Rodrigo and Lotufo, Roberto},\n      journal={arXiv preprint arXiv:2008.09144},\n      year={2020}\n    }\n", "qas": [{"id": "q1", "question": "What is the model architecture of unicamp-dl/ptt5-small-t5-vocab?", "answers": [{"text": "t5", "answer_start": 38, "answer_end": 39}]}]}]}, {"title": "unicamp-dl/translation-en-pt-t5", "paragraphs": [{"context": "---\n\nlanguage:\n\n- en\n\n- pt\n\ndatasets:\n\n- EMEA\n\n- ParaCrawl 99k\n\n- CAPES\n\n- Scielo\n\n- JRC-Acquis\n\n- Biomedical Domain Corpora\n\ntags:\n\n- translation\n\nmetrics:\n\n- bleu\n\n---\n\n\n# Introduction\n\nThis repository brings an implementation of T5 for translation in EN-PT tasks using a modest hardware setup. We propose some changes in tokenizator and post-processing that improves the result and used a Portuguese pretrained model for the translation. You can collect more informations in [our repository]( Also, check [our paper](\n\n# Usage\n\nJust follow \"Use in Transformers\" instructions. It is necessary to add a few words before to define the task to T5. \n\nYou can also create a pipeline for it. An example with the phrase \"I like to eat rice\" is:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n  \ntokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n\nenpt_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n\nenpt_pipeline(\"translate English to Portuguese: I like to eat rice.\")\n\n```\n\n# Citation\n\n```bibtex\n@inproceedings{lopes-etal-2020-lite,\n    title = \"Lite Training Strategies for {P}ortuguese-{E}nglish and {E}nglish-{P}ortuguese Translation\",\n    author = \"Lopes, Alexandre  and\n      Nogueira, Rodrigo  and\n      Lotufo, Roberto  and\n      Pedrini, Helio\",\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"833--840\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of unicamp-dl/translation-en-pt-t5?", "answers": [{"text": "t5", "answer_start": 898, "answer_end": 899}]}, {"id": "q2", "question": "What is the model task of unicamp-dl/translation-en-pt-t5?", "answers": [{"text": "translation", "answer_start": 135, "answer_end": 145}]}]}]}, {"title": "unicamp-dl/translation-pt-en-t5", "paragraphs": [{"context": "---\n\nlanguage:\n\n- en\n\n- pt\n\ndatasets:\n\n- EMEA\n\n- ParaCrawl 99k\n\n- CAPES\n\n- Scielo\n\n- JRC-Acquis\n\n- Biomedical Domain Corpora\n\ntags:\n\n- translation\n\nmetrics:\n\n- bleu\n\n---\n\n# Introduction\n\nThis repository brings an implementation of T5 for translation in PT-EN tasks using a modest hardware setup. We propose some changes in tokenizator and post-processing that improves the result and used a Portuguese pretrained model for the translation. You can collect more informations in [our repository]( Also, check [our paper](\n\n# Usage\n\nJust follow \"Use in Transformers\" instructions. It is necessary to add a few words before to define the task to T5. \n\nYou can also create a pipeline for it. An example with the phrase \" Eu gosto de comer arroz\" is:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n  \ntokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n\npten_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n\npten_pipeline(\"translate Portuguese to English: Eu gosto de comer arroz.\")\n\n```\n\n# Citation\n\n```bibtex\n@inproceedings{lopes-etal-2020-lite,\n    title = \"Lite Training Strategies for {P}ortuguese-{E}nglish and {E}nglish-{P}ortuguese Translation\",\n    author = \"Lopes, Alexandre  and\n      Nogueira, Rodrigo  and\n      Lotufo, Roberto  and\n      Pedrini, Helio\",\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"833--840\",\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of unicamp-dl/translation-pt-en-t5?", "answers": [{"text": "t5", "answer_start": 903, "answer_end": 904}]}, {"id": "q2", "question": "What is the model task of unicamp-dl/translation-pt-en-t5?", "answers": [{"text": "translation", "answer_start": 135, "answer_end": 145}]}]}]}, {"title": "unideeplearning/polibert_sa", "paragraphs": [{"context": "---\nlanguage: it\ntags:\n- sentiment\n- Italian\nlicense: mit\nwidget:\n- text: Giuseppe Rossi \u00e8 un ottimo politico\n---\n\n# \ud83e\udd17 + polibert_SA - POLItic BERT based Sentiment Analysis\n  \n## Model description  \n  \nThis model performs sentiment analysis on Italian political twitter sentences. It was trained starting from an instance of \"bert-base-italian-uncased-xxl\" and fine-tuned on an Italian dataset of tweets. You can try it out at  (in italian!)\n  \n#### Hands-on  \n  \n```python\nimport torch\nfrom torch import nn \nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"unideeplearning/polibert_sa\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"unideeplearning/polibert_sa\")\n\t\t\t\n\n\n\ntext = \"Giuseppe Rossi \u00e8 un pessimo politico\"\ninput_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors= 'pt')\n\nlogits, = model(input_ids)\nlogits = logits.squeeze(0)\n\nprob = nn.functional.softmax(logits, dim=0)\n\n# 0 Negative, 1 Neutral, 2 Positive \nprint(prob.argmax().tolist())\n```  \n  \n#### Hyperparameters\n\n- Optimizer: **AdamW** with learning rate of **2e-5**, epsilon of **1e-8**\n- Max epochs: **2**\n- Batch size: **16**\n\n## Acknowledgments\n\nThanks to the support from: \nthe [Hugging Face]( \n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of unideeplearning/polibert_sa?", "answers": [{"text": "bert", "answer_start": 125, "answer_end": 128}]}]}]}, {"title": "unitary/multilingual-toxic-xlm-roberta", "paragraphs": [{"context": "---\npipeline_tag: \"text-classification\"\n---\n<div align=\"center\">    \n\n**\u26a0\ufe0f Disclaimer:**\nThe huggingface models currently give different results to the detoxify library (see issue [here]( For the most up to date models we recommend using the models from \n\n# \ud83d\ude4a Detoxify\n##  Toxic Comment Classification with \u26a1 Pytorch Lightning and \ud83e\udd17 Transformers   \n\n![CI testing](\n![Lint](\n\n</div>\n \n![Examples image](examples.png)\n\n## Description   \n\nTrained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended\u00a0Bias in Toxic comments, Multilingual toxic comment classification.\n\nBuilt by [Laura Hanu]( at [Unitary]( where we are working to stop harmful content online by interpreting visual content in context. \n\nDependencies:\n- For inference:\n  - \ud83e\udd17 Transformers\n  - \u26a1 Pytorch lightning \n- For training will also need:\n  - Kaggle API (to download data)\n\n\n Year  Original Data Source  Top Kaggle Leaderboard Score | Detoxify Score\n---\n 2018  Wikipedia Comments  0.98856 | 0.98636\n 2019  Civil Comments  0.94734 | 0.93639\n 2020  Wikipedia Comments + Civil Comments  0.9536 | 0.91655*\n\n*Score not directly comparable since it is obtained on the validation set provided and not on the test set. To update when the test labels are made available. \n\nIt is also noteworthy to mention that the top leadearboard scores have been achieved using model ensembles. The purpose of this library was to build something user-friendly and straightforward to use.\n\n## Limitations and ethical considerations\n\nIf words that are associated with swearing, insults or profanity are present in a comment, it is likely that it will be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating. This could present some biases towards already vulnerable minority groups.\n\nThe intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics  and/or to aid content moderators in flagging out harmful content quicker.\n\nSome useful resources about the risk of different biases in toxicity or hate speech detection are:\n- [The Risk of Racial Bias in Hate Speech Detection](\n- [Automated Hate Speech Detection and the Problem of Offensive Language](\n- [Racial Bias in Hate Speech and Abusive Language Detection Datasets](\n\n## Quick prediction\n\n\nThe `multilingual` model has been trained on 7 different languages so it should only be tested on: `english`, `french`, `spanish`, `italian`, `portuguese`, `turkish` or `russian`.\n\n```bash\n# install detoxify  \n\npip install detoxify\n\n```\n```python\n\nfrom detoxify import Detoxify\n\n# each model takes in either a string or a list of strings\n\nresults = Detoxify('original').predict('example text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# optional to display results nicely (will need to pip install pandas)\n\nimport pandas as pd\n\nprint(pd.DataFrame(results, index=input_text).round(5))\n\n```\nFor more details check the Prediction section.\n\n\n## Labels\nAll challenges have a toxicity label. The toxicity labels represent the aggregate ratings of up to 10 annotators according the following schema:\n- **Very Toxic** (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n- **Toxic** (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n- **Hard to Say**\n- **Not Toxic**\n\nMore information about the labelling schema can be found [here](\n\n### Toxic Comment Classification Challenge\nThis challenge includes the following labels:\n\n- `toxic`\n- `severe_toxic`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_hate`\n\n### Jigsaw Unintended Bias in Toxicity Classification\nThis challenge has 2 types of labels: the main toxicity labels and some additional identity labels that represent the identities mentioned in the comments. \n\nOnly identities with more than 500 examples in the test set (combined public and private) are included during training as additional labels and in the evaluation calculation.\n\n- `toxicity`\n- `severe_toxicity`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_attack`\n- `sexual_explicit`\n\nIdentity labels used:\n- `male`\n- `female`\n- `homosexual_gay_or_lesbian`\n- `christian`\n- `jewish`\n- `muslim`\n- `black`\n- `white`\n- `psychiatric_or_mental_illness`\n\nA complete list of all the identity labels available can be found [here](\n\n\n### Jigsaw Multilingual Toxic Comment Classification\n\nSince this challenge combines the data from the previous 2 challenges, it includes all labels from above, however the final evaluation is only on:\n\n- `toxicity`\n\n## How to run   \n\nFirst, install dependencies   \n```bash\n# clone project   \n\ngit clone \n\n# create virtual env\n\npython3 -m venv toxic-env\nsource toxic-env/bin/activate\n\n# install project   \n\npip install -e detoxify\ncd detoxify\n\n# for training\npip install -r requirements.txt\n\n ```   \n\n## Prediction\n\nTrained models summary:\n\n Transformer type| Data from\n:--:\n `bert-base-uncased` | Toxic Comment Classification Challenge\n `roberta-base`| Unintended Bias in Toxicity Classification\n `xlm-roberta-base`| Multilingual Toxic Comment Classification\n\nFor a quick prediction can run the example script on a comment directly or from a txt containing a list of comments. \n```bash\n\n# load model via torch.hub\n\npython run_prediction.py --input 'example' --model_name original\n\n# load model from from checkpoint path\n\npython run_prediction.py --input 'example' --from_ckpt_path model_path\n\n# save results to a .csv file\n\npython run_prediction.py --input test_set.txt --model_name original --save_to results.csv\n\n# to see usage\n\npython run_prediction.py --help\n\n```\n\nCheckpoints can be downloaded from the latest release or via the Pytorch hub API with the following names:\n- `toxic_bert`\n- `unbiased_toxic_roberta`\n- `multilingual_toxic_xlm_r`\n```bash\nmodel = torch.hub.load('unitaryai/detoxify','toxic_bert')\n```\n\nImporting detoxify in python:\n\n```python\n\nfrom detoxify import Detoxify\n\nresults = Detoxify('original').predict('some text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# to display results nicely\n\nimport pandas as pd\n\nprint(pd.DataFrame(results,index=input_text).round(5))\n\n```\n\n\n## Training\n\n If you do not already have a Kaggle account: \n - you need to create one to be able to download the data\n \n - go to My Account and click on Create New API Token - this will download a kaggle.json file\n\n - make sure this file is located in ~/.kaggle\n\n ```bash\n\n# create data directory\n\nmkdir jigsaw_data\ncd jigsaw_data\n\n# download data\n\nkaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n\nkaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n\nkaggle competitions download -c jigsaw-multilingual-toxic-comment-classification\n\n```\n## Start Training\n ### Toxic Comment Classification Challenge\n\n ```bash\n\npython create_val_set.py\n\npython train.py --config configs/Toxic_comment_classification_BERT.json\n``` \n ### Unintended Bias in Toxicicity Challenge\n\n```bash\n\npython train.py --config configs/Unintended_bias_toxic_comment_classification_RoBERTa.json\n\n```\n ### Multilingual Toxic Comment Classification\n\n This is trained in 2 stages. First, train on all available data, and second, train only on the translated versions of the first challenge. \n \n The [translated data]( can be downloaded from Kaggle in french, spanish, italian, portuguese, turkish, and russian (the languages available in the test set).\n\n ```bash\n\n# stage 1\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR.json\n\n# stage 2\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR_stage2.json\n\n```\n### Monitor progress with tensorboard\n\n ```bash\n\ntensorboard --logdir=./saved\n\n```\n## Model Evaluation\n\n### Toxic Comment Classification Challenge\n\nThis challenge is evaluated on the mean AUC score of all the labels.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n### Unintended Bias in Toxicicity Challenge\n\nThis challenge is evaluated on a novel bias metric that combines different AUC scores to balance overall performance. More information on this metric [here](\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n# to get the final bias metric\npython model_eval/compute_bias_metric.py\n\n```\n### Multilingual Toxic Comment Classification\n\nThis challenge is evaluated on the AUC score of the main toxic label.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n\n### Citation   \n```\n@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. \n  year={2020}\n}\n```   \n", "qas": [{"id": "q1", "question": "What is the model architecture of unitary/multilingual-toxic-xlm-roberta?", "answers": [{"text": "xlm-roberta", "answer_start": 5387, "answer_end": 5397}]}, {"id": "q2", "question": "What is the model task of unitary/multilingual-toxic-xlm-roberta?", "answers": [{"text": "text-classification", "answer_start": 19, "answer_end": 37}]}]}]}, {"title": "unitary/toxic-bert", "paragraphs": [{"context": "\n      \n<div align=\"center\">    \n\n**\u26a0\ufe0f Disclaimer:**\nThe huggingface models currently give different results to the detoxify library (see issue [here]( For the most up to date models we recommend using the models from \n\n# \ud83d\ude4a Detoxify\n##  Toxic Comment Classification with \u26a1 Pytorch Lightning and \ud83e\udd17 Transformers   \n\n![CI testing](\n![Lint](\n\n</div>\n \n![Examples image](examples.png)\n\n## Description   \n\nTrained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended\u00a0Bias in Toxic comments, Multilingual toxic comment classification.\n\nBuilt by [Laura Hanu]( at [Unitary]( where we are working to stop harmful content online by interpreting visual content in context. \n\nDependencies:\n- For inference:\n  - \ud83e\udd17 Transformers\n  - \u26a1 Pytorch lightning \n- For training will also need:\n  - Kaggle API (to download data)\n\n\n Year  Original Data Source  Top Kaggle Leaderboard Score | Detoxify Score\n---\n 2018  Wikipedia Comments  0.98856 | 0.98636\n 2019  Civil Comments  0.94734 | 0.93639\n 2020  Wikipedia Comments + Civil Comments  0.9536 | 0.91655*\n\n*Score not directly comparable since it is obtained on the validation set provided and not on the test set. To update when the test labels are made available. \n\nIt is also noteworthy to mention that the top leadearboard scores have been achieved using model ensembles. The purpose of this library was to build something user-friendly and straightforward to use.\n\n## Limitations and ethical considerations\n\nIf words that are associated with swearing, insults or profanity are present in a comment, it is likely that it will be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating. This could present some biases towards already vulnerable minority groups.\n\nThe intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics  and/or to aid content moderators in flagging out harmful content quicker.\n\nSome useful resources about the risk of different biases in toxicity or hate speech detection are:\n- [The Risk of Racial Bias in Hate Speech Detection](\n- [Automated Hate Speech Detection and the Problem of Offensive Language](\n- [Racial Bias in Hate Speech and Abusive Language Detection Datasets](\n\n## Quick prediction\n\n\nThe `multilingual` model has been trained on 7 different languages so it should only be tested on: `english`, `french`, `spanish`, `italian`, `portuguese`, `turkish` or `russian`.\n\n```bash\n# install detoxify  \n\npip install detoxify\n\n```\n```python\n\nfrom detoxify import Detoxify\n\n# each model takes in either a string or a list of strings\n\nresults = Detoxify('original').predict('example text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# optional to display results nicely (will need to pip install pandas)\n\nimport pandas as pd\n\nprint(pd.DataFrame(results, index=input_text).round(5))\n\n```\nFor more details check the Prediction section.\n\n\n## Labels\nAll challenges have a toxicity label. The toxicity labels represent the aggregate ratings of up to 10 annotators according the following schema:\n- **Very Toxic** (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n- **Toxic** (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n- **Hard to Say**\n- **Not Toxic**\n\nMore information about the labelling schema can be found [here](\n\n### Toxic Comment Classification Challenge\nThis challenge includes the following labels:\n\n- `toxic`\n- `severe_toxic`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_hate`\n\n### Jigsaw Unintended Bias in Toxicity Classification\nThis challenge has 2 types of labels: the main toxicity labels and some additional identity labels that represent the identities mentioned in the comments. \n\nOnly identities with more than 500 examples in the test set (combined public and private) are included during training as additional labels and in the evaluation calculation.\n\n- `toxicity`\n- `severe_toxicity`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_attack`\n- `sexual_explicit`\n\nIdentity labels used:\n- `male`\n- `female`\n- `homosexual_gay_or_lesbian`\n- `christian`\n- `jewish`\n- `muslim`\n- `black`\n- `white`\n- `psychiatric_or_mental_illness`\n\nA complete list of all the identity labels available can be found [here](\n\n\n### Jigsaw Multilingual Toxic Comment Classification\n\nSince this challenge combines the data from the previous 2 challenges, it includes all labels from above, however the final evaluation is only on:\n\n- `toxicity`\n\n## How to run   \n\nFirst, install dependencies   \n```bash\n# clone project   \n\ngit clone \n\n# create virtual env\n\npython3 -m venv toxic-env\nsource toxic-env/bin/activate\n\n# install project   \n\npip install -e detoxify\ncd detoxify\n\n# for training\npip install -r requirements.txt\n\n ```   \n\n## Prediction\n\nTrained models summary:\n\n Transformer type| Data from\n:--:\n `bert-base-uncased` | Toxic Comment Classification Challenge\n `roberta-base`| Unintended Bias in Toxicity Classification\n `xlm-roberta-base`| Multilingual Toxic Comment Classification\n\nFor a quick prediction can run the example script on a comment directly or from a txt containing a list of comments. \n```bash\n\n# load model via torch.hub\n\npython run_prediction.py --input 'example' --model_name original\n\n# load model from from checkpoint path\n\npython run_prediction.py --input 'example' --from_ckpt_path model_path\n\n# save results to a .csv file\n\npython run_prediction.py --input test_set.txt --model_name original --save_to results.csv\n\n# to see usage\n\npython run_prediction.py --help\n\n```\n\nCheckpoints can be downloaded from the latest release or via the Pytorch hub API with the following names:\n- `toxic_bert`\n- `unbiased_toxic_roberta`\n- `multilingual_toxic_xlm_r`\n```bash\nmodel = torch.hub.load('unitaryai/detoxify','toxic_bert')\n```\n\nImporting detoxify in python:\n\n```python\n\nfrom detoxify import Detoxify\n\nresults = Detoxify('original').predict('some text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# to display results nicely\n\nimport pandas as pd\n\nprint(pd.DataFrame(results,index=input_text).round(5))\n\n```\n\n\n## Training\n\n If you do not already have a Kaggle account: \n - you need to create one to be able to download the data\n \n - go to My Account and click on Create New API Token - this will download a kaggle.json file\n\n - make sure this file is located in ~/.kaggle\n\n ```bash\n\n# create data directory\n\nmkdir jigsaw_data\ncd jigsaw_data\n\n# download data\n\nkaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n\nkaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n\nkaggle competitions download -c jigsaw-multilingual-toxic-comment-classification\n\n```\n## Start Training\n ### Toxic Comment Classification Challenge\n\n ```bash\n\npython create_val_set.py\n\npython train.py --config configs/Toxic_comment_classification_BERT.json\n``` \n ### Unintended Bias in Toxicicity Challenge\n\n```bash\n\npython train.py --config configs/Unintended_bias_toxic_comment_classification_RoBERTa.json\n\n```\n ### Multilingual Toxic Comment Classification\n\n This is trained in 2 stages. First, train on all available data, and second, train only on the translated versions of the first challenge. \n \n The [translated data]( can be downloaded from Kaggle in french, spanish, italian, portuguese, turkish, and russian (the languages available in the test set).\n\n ```bash\n\n# stage 1\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR.json\n\n# stage 2\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR_stage2.json\n\n```\n### Monitor progress with tensorboard\n\n ```bash\n\ntensorboard --logdir=./saved\n\n```\n## Model Evaluation\n\n### Toxic Comment Classification Challenge\n\nThis challenge is evaluated on the mean AUC score of all the labels.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n### Unintended Bias in Toxicicity Challenge\n\nThis challenge is evaluated on a novel bias metric that combines different AUC scores to balance overall performance. More information on this metric [here](\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n# to get the final bias metric\npython model_eval/compute_bias_metric.py\n\n```\n### Multilingual Toxic Comment Classification\n\nThis challenge is evaluated on the AUC score of the main toxic label.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n\n### Citation   \n```\n@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. \n  year={2020}\n}\n```   \n", "qas": [{"id": "q1", "question": "What is the model architecture of unitary/toxic-bert?", "answers": [{"text": "bert", "answer_start": 5229, "answer_end": 5232}]}]}]}, {"title": "wangfan/jdt-fin-roberta-wwm-large", "paragraphs": [{"context": "---\nlanguage: zh\ntags:\n- roberta-wwm\nlicense: apache-2.0\ndatasets:\n- finance\n---\n\n\u5728\u4f17\u591a\u4e1a\u52a1\u4e2d\uff0c\u8d8a\u6765\u8d8a\u9891\u7e41\u7684\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08Pre-trained Language Models\uff09\uff0c\u4e3a\u4e86\u5728\u91d1\u878d\u573a\u666f\u4e0b\u5404\u4efb\u52a1\u4e2d\u53d6\u5f97\u66f4\u597d\u6548\u679c\uff0c\u6211\u4eec\u53d1\u5e03\u4e86jdt-fin-roberta-wwm\u6a21\u578b\n\n## \u6a21\u578b\n* `base\u6a21\u578b`\uff1a12-layer, 768-hidden, 12-heads, 110M parameters  \n\n \u8bed\u6599 \n - \n \u91d1\u878d\u8bed\u6599 \n\n## \u5feb\u901f\u52a0\u8f7d\n### \u4f7f\u7528Huggingface-Transformers\n\u4f9d\u6258\u4e8e[Huggingface-Transformers](\n```\ntokenizer = BertTokenizer.from_pretrained(\"MODEL_NAME\")\nmodel = BertModel.from_pretrained(\"MODEL_NAME\")\n```\n**\u6ce8\u610f\uff1a\u672c\u76ee\u5f55\u4e2d\u7684\u6240\u6709\u6a21\u578b\u5747\u4f7f\u7528BertTokenizer\u4ee5\u53caBertModel\u52a0\u8f7d\uff0c\u8bf7\u52ff\u4f7f\u7528RobertaTokenizer/RobertaModel\uff01**\n\u5176\u4e2d`MODEL_NAME`\u5bf9\u5e94\u5217\u8868\u5982\u4e0b\uff1a\n\n MODEL_NAME |\n - |\n wangfan/jdt-fin-roberta-wwm |\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of wangfan/jdt-fin-roberta-wwm-large?", "answers": [{"text": "bert", "answer_start": 27, "answer_end": 30}]}]}]}, {"title": "yerevann/x-r-hy", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-large-xls-r-2b-armenian-colab\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-large-xls-r-2b-armenian-colab\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-2b]( on the common_voice dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.5166\n- Wer: 0.7397\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 1\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 4\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 120\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch   Validation Loss \n:------::---------------:\n 2.38    0.7731          \n 4.76    0.8279          \n 7.14    1.0343          \n 9.52    1.0551          \n 11.9    1.0686          \n 14.29   1.1329          \n 16.67   1.3234          \n 19.05   1.2432          \n 21.43   1.2780          \n 23.81   1.2228          \n 26.19   1.3484          \n 28.57   1.2881          \n 30.95   1.1972          \n 33.33   1.3702          \n 35.71   1.3963          \n 38.1    1.4690          \n 40.48   1.5045          \n 42.86   1.4749          \n 45.24   1.5047          \n 47.62   1.4216          \n 50.0    1.4676          \n 52.38   1.4273          \n 54.76   1.3999          \n 57.14   1.6130          \n 59.52   1.5586          \n 61.9    1.3959          \n 64.29   1.5318          \n 66.67   1.5300          \n 69.05   1.5051          \n 71.43   1.5647          \n 73.81   1.4919          \n 76.19   1.5259          \n 78.57   1.3985          \n 80.95   1.5515          \n 83.33   1.5737          \n 85.71   1.4876          \n 88.1    1.6331          \n 90.48   1.5108          \n 92.86   1.7125          \n 95.24   1.6042          \n 97.62   1.4608          \n 100.0   1.4784          \n 102.38  1.4471          \n 104.76  1.4852          \n 107.14  1.5679          \n 109.52  1.5090          \n 111.9   1.4994          \n 114.29  1.5008          \n 116.67  1.5166          \n 119.05  1.5166          \n\n\n### Framework versions\n\n- Transformers 4.14.1\n- Pytorch 1.10.0\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of yerevann/x-r-hy?", "answers": [{"text": "wav2vec2", "answer_start": 101, "answer_end": 108}]}]}]}, {"title": "nielsr/segformer-b0-finetuned-segments-sidewalk", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-segmentation\n- generated_from_trainer\nmodel-index:\n- name: segformer-b0-finetuned-segments-sidewalk\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# segformer-b0-finetuned-segments-sidewalk\n\nThis model is a fine-tuned version of [nvidia/mit-b0]( on the segments/sidewalk-semantic dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5679\n- Miou: 0.2769\n- Macc: 0.3331\n- Overall Accuracy: 0.8424\n- Per Category Iou: [nan, 0.7174911859423314, 0.8790751054409742, 0.6065232798410057, 0.6975274018055722, 0.3486407385349508, nan, 0.40093167116703843, 0.28779837903852556, 0.0, 0.7870339041746186, 0.0, 0.0, 0.0, 0.0, 0.1464360606454247, 0.0, 0.0, 0.6770283275082656, 0.0, 0.338555175257431, 0.14697310016578427, 0.0, nan, 0.0, 0.27163002251763635, 0.0, 0.0, 0.8257437911843676, 0.7169333376341568, 0.9108105550493353, 0.0, 0.0, 0.1016801552778885, 0.0]\n- Per Category Accuracy: [nan, 0.9199960254104915, 0.9327745517652714, 0.7304629327758765, 0.7378309547498484, 0.45295941407150275, nan, 0.5188608021128075, 0.5327441812670195, 0.0, 0.9353764765979435, 0.0, 0.0, 0.0, 0.0, 0.1588525415198792, 0.0, 0.0, 0.9238854794385364, 0.0, 0.4400394213522207, 0.15130051149615126, 0.0, nan, 0.0, 0.3570096986572905, 0.0, 0.0, 0.9359897980968498, 0.8570458108260572, 0.9549583230619891, 0.0, 0.0, 0.11786971668879294, 0.0]\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 6e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss  Macc    Per Category Iou                                                                                                                                                                                                                                                                                                                                                                                                                                    \n:-----::---------------::------::---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:\n 1.0    1.0006           0.2069  [nan, 0.5642795884663824, 0.7491853309192827, 0.0, 0.40589649630192104, 0.02723606910696284, nan, 0.0002207740938439576, 0.0, 0.0, 0.6632462867093903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5671699281129761, 0.0, 0.0009207911027492868, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.7507253434892517, 0.6157793573905029, 0.8774768871968204, 0.0, 0.0, 0.0, 0.0]                                                                                        \n 2.0    0.7856           0.2334  [nan, 0.6276046255936906, 0.8379492348238635, 0.0, 0.5220035981992285, 0.19441920935217594, nan, 0.16135703555333, 0.0, 0.0, 0.7357165628674137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.567598980063164, 0.0, 0.07867871139133086, 0.0, 0.0, nan, 0.0, 0.02123705398363847, 0.0, 0.0, 0.7917172051343153, 0.6589515948064048, 0.8916684207946344, 0.0, 0.0, 0.00013685918191589503, 0.0]                                                              \n 3.0    0.6798           0.2687  [nan, 0.6728474586764454, 0.8404607924530816, 0.21147709475332813, 0.5407350347311378, 0.23535489130104167, nan, 0.3087159264982809, 0.0060319580742948155, 0.0, 0.7331305064022374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6378031991744924, 0.0, 0.35289337122777764, 6.24997656258789e-05, 0.0, nan, 0.0, 0.14698390926256938, 0.0, 0.0, 0.8019042204623998, 0.669283249725758, 0.8928145424856038, 0.0, 0.0, 0.03847722460691187, 0.0]            \n 4.0    0.5999           0.2998  [nan, 0.7078353465279917, 0.8661728761172196, 0.3857324719136883, 0.6338278880825696, 0.3440050078187208, nan, 0.35980405625532347, 0.23875867241702606, 0.0, 0.773703347865372, 0.0, 0.0, 0.0, 0.0, 0.0004931363471679884, 0.0, 0.0, 0.6554146448850521, 0.0, 0.367673493717809, 0.03089804641909161, 0.0, nan, 0.0, 0.21529017459808872, 0.0, 0.0, 0.818951849158376, 0.7007504838794707, 0.9053929635423027, 0.0, 0.0, 0.06626212301200333, 0.0] \n 5.0    0.5679           0.3331  [nan, 0.7174911859423314, 0.8790751054409742, 0.6065232798410057, 0.6975274018055722, 0.3486407385349508, nan, 0.40093167116703843, 0.28779837903852556, 0.0, 0.7870339041746186, 0.0, 0.0, 0.0, 0.0, 0.1464360606454247, 0.0, 0.0, 0.6770283275082656, 0.0, 0.338555175257431, 0.14697310016578427, 0.0, nan, 0.0, 0.27163002251763635, 0.0, 0.0, 0.8257437911843676, 0.7169333376341568, 0.9108105550493353, 0.0, 0.0, 0.1016801552778885, 0.0]   \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of nielsr/segformer-b0-finetuned-segments-sidewalk?", "answers": [{"text": "segformer", "answer_start": 106, "answer_end": 114}]}, {"id": "q2", "question": "What is the model task of nielsr/segformer-b0-finetuned-segments-sidewalk?", "answers": [{"text": "image-segmentation", "answer_start": 41, "answer_end": 58}]}]}]}, {"title": "katanaml/layoutlmv2-finetuned-cord", "paragraphs": [{"context": "---\nlicense: cc-by-nc-sa-4.0\ndatasets:\n- katanaml/cord\ntags:\n- generated_from_trainer\nmodel-index:\n- name: layoutlmv2-finetuned-cord\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# layoutlmv2-finetuned-cord\n\nThis model is a fine-tuned version of [microsoft/layoutlmv2-base-uncased]( on CORD dataset.\n\n## Model description\n\nModel implementation code [Sparrow](\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- training_steps: 3000\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of katanaml/layoutlmv2-finetuned-cord?", "answers": [{"text": "layoutlmv2", "answer_start": 107, "answer_end": 116}]}]}]}, {"title": "microsoft/dit-base-finetuned-rvlcdip", "paragraphs": [{"context": "---\ntags:\n- dit\n- vision\n- image-classification\ndatasets:\n- rvl_cdip\nwidget:\n- src: \n  example_title: Advertisement\n- src: \n  example_title: Scientific publication\n---\n\n# Document Image Transformer (base-sized model) \n\nDocument Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images and fine-tuned on [RVL-CDIP]( a dataset consisting of 400,000 grayscale images in 16 classes, with 25,000 images per class. It was introduced in the paper [DiT: Self-supervised Pre-training for Document Image Transformer]( by Li et al. and first released in [this repository]( Note that DiT is identical to the architecture of [BEiT]( \n\nDisclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.\n\n## Intended uses & limitations\n\nYou can use the raw model for encoding document images into a vector space, but it's mostly meant to be fine-tuned on tasks like document image classification, table detection or document layout analysis. See the [model hub]( to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nimport torch\nfrom PIL import Image\n\nimage = Image.open('path_to_your_document_image').convert('RGB')\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/dit-base-finetuned-rvlcdip\")\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/dit-base-finetuned-rvlcdip\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 16 RVL-CDIP classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Lewis2006BuildingAT,\n  title={Building a test collection for complex document information processing},\n  author={David D. Lewis and Gady Agam and Shlomo Engelson Argamon and Ophir Frieder and David A. Grossman and Jefferson Heard},\n  journal={Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},\n  year={2006}\n}\n```", "qas": [{"id": "q2", "question": "What is the model task of microsoft/dit-base-finetuned-rvlcdip?", "answers": [{"text": "image-classification", "answer_start": 27, "answer_end": 46}]}]}]}, {"title": "microsoft/dit-large-finetuned-rvlcdip", "paragraphs": [{"context": "---\ntags:\n- dit\ndatasets:\n- rvl_cdip\ninference: false\n---\n\n# Document Image Transformer (large-sized model) \n\nDocument Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images and fine-tuned on [RVL-CDIP]( a dataset consisting of 400,000 grayscale images in 16 classes, with 25,000 images per class. It was introduced in the paper [DiT: Self-supervised Pre-training for Document Image Transformer]( by Li et al. and first released in [this repository]( Note that DiT is identical to the architecture of [BEiT]( \n\nDisclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.\n\n## Intended uses & limitations\n\nYou can use the raw model for encoding document images into a vector space, but it's mostly meant to be fine-tuned on tasks like document image classification, table detection or document layout analysis. See the [model hub]( to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nimport torch\nfrom PIL import Image\n\nimage = Image.open('path_to_your_document_image').convert('RGB')\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/dit-large-finetuned-rvlcdip\")\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/dit-large-finetuned-rvlcdip\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 16 RVL-CDIP classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Lewis2006BuildingAT,\n  title={Building a test collection for complex document information processing},\n  author={David D. Lewis and Gady Agam and Shlomo Engelson Argamon and Ophir Frieder and David A. Grossman and Jefferson Heard},\n  journal={Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},\n  year={2006}\n}\n```", "qas": []}]}, {"title": "StivenLancheros/biobert-base-cased-v1.2-finetuned-ner-Concat_CRAFT_es", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: biobert-base-cased-v1.2-finetuned-ner-Concat_CRAFT_es\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# biobert-base-cased-v1.2-finetuned-ner-Concat_CRAFT_es\n\nThis model is a fine-tuned version of [dmis-lab/biobert-base-cased-v1.2]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2079\n- Precision: 0.8487\n- Recall: 0.8443\n- F1: 0.8465\n- Accuracy: 0.9693\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.1463           0.8233  0.9643   |\n 2.0    0.1612           0.8463  0.9681   |\n 3.0    0.1832           0.8404  0.9683   |\n 4.0    0.2079           0.8443  0.9693   |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of StivenLancheros/biobert-base-cased-v1.2-finetuned-ner-Concat_CRAFT_es?", "answers": [{"text": "bert", "answer_start": 105, "answer_end": 108}]}]}]}, {"title": "anton-l/xtreme_s_xlsr_minds14_fr", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- google/xtreme_s\n- generated_from_trainer\ndatasets:\n- xtreme_s\nmetrics:\n- accuracy\nmodel-index:\n- name: xtreme_s_xlsr_minds14_fr\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xtreme_s_xlsr_minds14_fr\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the GOOGLE/XTREME_S - MINDS14.FR-FR dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3922\n- Accuracy: 0.9135\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 100\n- num_epochs: 50.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 10.0   2.0203          \n 20.0   0.7434          \n 30.0   0.7686          \n 40.0   0.3922          \n 50.0   0.4859          \n\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.4.dev0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of anton-l/xtreme_s_xlsr_minds14_fr?", "answers": [{"text": "wav2vec2", "answer_start": 474, "answer_end": 481}]}, {"id": "q2", "question": "What is the model task of anton-l/xtreme_s_xlsr_minds14_fr?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 32, "answer_end": 59}]}]}]}, {"title": "BigSalmon/InformalToFormalLincoln26", "paragraphs": [{"context": "```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"BigSalmon/InformalToFormalLincoln26\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"BigSalmon/InformalToFormalLincoln26\")\n\n```\n\n```\nHow To Make Prompt:\ninformal english: i am very ready to do that just that.\nTranslated into the Style of Abraham Lincoln: you can assure yourself of my readiness to work toward this end.\nTranslated into the Style of Abraham Lincoln: please be assured that i am most ready to undertake this laborious task.\n\n***\n\ninformal english: space is huge and needs to be explored.\nTranslated into the Style of Abraham Lincoln: space awaits traversal, a new world whose boundaries are endless.\nTranslated into the Style of Abraham Lincoln: space is a ( limitless / boundless ) expanse, a vast virgin domain awaiting exploration.\n\n***\n\ninformal english: corn fields are all across illinois, visible once you leave chicago.\nTranslated into the Style of Abraham Lincoln: corn fields ( permeate illinois / span the state of illinois / ( occupy / persist in ) all corners of illinois / line the horizon of illinois / envelop the landscape of illinois ), manifesting themselves visibly as one ventures beyond chicago.\n\ninformal english:\n```\n```\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\nText: failing to draw in the masses, the NBA has fallen into disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap solutions could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n\n***\n-\n\n```\n\n```\ninfill: chrome extensions [MASK] accomplish everyday tasks.\nTranslated into the Style of Abraham Lincoln: chrome extensions ( expedite the ability to / unlock the means to more readily ) accomplish everyday tasks.\n\ninfill: at a time when nintendo has become inflexible, [MASK] consoles that are tethered to a fixed iteration, sega diligently curates its legacy of classic video games on handheld devices.\nTranslated into the Style of Abraham Lincoln: at a time when nintendo has become inflexible, ( stubbornly [MASK] on / firmly set on / unyielding in its insistence on ) consoles that are tethered to a fixed iteration, sega diligently curates its legacy of classic video games on handheld devices.\n\ninfill:\n\n```\n```\nEssay Intro (California High-Speed Rail): built with an eye on the future, california's high-speed rail service resolves to change the face of travel.\n\nEssay Intro (YIMBY's Need To Win): home to the most expensive housing market in the united states, san francisco is the city in which the yimby and anti-yimby hordes wage an eternal battle.\n\nEssay Intro (\n```\n\n```\nSearch: What is the definition of Checks and Balances?\n\n\nChecks and Balances is the idea of having a system where each and every action in government should be subject to one or more checks that would not allow one branch or the other to overly dominate.\n\n\nChecks and Balances is a system that allows each branch of government to limit the powers of the other branches in order to prevent abuse of power\n\n\nChecks and Balances is a system of separation through which branches of government can control the other, thus preventing excess power.\n\n***\n\nSearch: What is the definition of Separation of Powers?\n\n\nThe separation of powers is a principle in government, whereby governmental powers are separated into different branches, each with their own set of powers, that are prevent one branch from aggregating too much power.\n\n\nSeparation of Powers is the division of governmental functions between the executive, legislative and judicial branches, clearly demarcating each branch's authority, in the interest of ensuring that individual liberty or security is not undermined.\n\n***\n\nSearch: What is the definition of Connection of Powers?\n\n\nConnection of Powers is a feature of some parliamentary forms of government where different branches of government are intermingled, typically the executive and legislative branches.\n\n\nThe term Connection of Powers describes a system of government in which there is overlap between different parts of the government.\n\n***\nSearch: What is the definition of\n```\n\n```\nSearch: What are phrase synonyms for \"second-guess\"?\n\nShortest to Longest:\n- feel dubious about\n- raise an eyebrow at\n- wrinkle their noses at\n- cast a jaundiced eye at\n- teeter on the fence about\n\n***\n\nSearch: What are phrase synonyms for \"mean to newbies\"?\n\nShortest to Longest:\n- readiness to balk at rookies\n- absence of tolerance for novices\n- hostile attitude toward newcomers\n\n***\n\nSearch: What are phrase synonyms for \"make use of\"?\n\nShortest to Longest:\n- call upon\n- glean value from\n- reap benefits from\n- derive utility from\n- seize on the merits of\n- draw on the strength of\n- tap into the potential of\n\n***\n\nSearch: What are phrase synonyms for \"hurting itself\"?\n\nShortest to Longest:\n- erring\n- slighting itself\n- forfeiting its integrity\n- doing itself a disservice\n- evincing a lack of backbone\n\n***\n\nSearch: What are phrase synonyms for \"\n```\n```\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\ntext: failing to draw in the masses, the nba has ( fallen into / succumb to / bowed to ) disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap ( solutions / interventions / enhancements ) could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n\n***\n\n-\n```\n```\noriginal: sports teams are profitable for owners. [MASK], their valuations experience a dramatic uptick. \ninfill: sports teams are profitable for owners. ( accumulating vast sums / stockpiling treasure / realizing benefits / cashing in / registering robust financials / scoring on balance sheets ), their valuations experience a dramatic uptick. \n\n***\n\noriginal:\n```", "qas": []}]}, {"title": "orzhan/ruroberta-ruatd-binary", "paragraphs": [{"context": "sberbank-ai/ruRoberta-large fine-tuned for Russian Artificial Text Detection shared task\n", "qas": []}]}, {"title": "responsibility-framing/predict-perception-bert-blame-assassin", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-bert-blame-assassin\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-bert-blame-assassin\n\nThis model is a fine-tuned version of [dbmdz/bert-base-italian-xxl-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5128\n- Rmse: 1.0287\n- Rmse Blame::a L'assassino: 1.0287\n- Mae: 0.8883\n- Mae Blame::a L'assassino: 0.8883\n- R2: 0.5883\n- R2 Blame::a L'assassino: 0.5883\n- Cos: 0.6522\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.5795\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Blame::a L'assassino  Mae Blame::a L'assassino  R2 Blame::a L'assassino  Pair  Neighbors \n:-----::---------------::-------------------------::------------------------::-----------------------::----::---------:\n 1.0    1.2219           1.5879                     1.4308                    0.0191                   0.0   0.3781    \n 2.0    1.0927           1.5017                     1.3634                    0.1227                   0.0   0.4512    \n 3.0    0.8206           1.3013                     1.1808                    0.3412                   0.0   0.3819    \n 4.0    0.5894           1.1029                     1.0145                    0.5268                   0.0   0.6408    \n 5.0    0.4759           0.9910                     0.8868                    0.6180                   0.0   0.4884    \n 6.0    0.4220           0.9332                     0.8083                    0.6612                   0.0   0.4249    \n 7.0    0.4477           0.9612                     0.8046                    0.6406                   0.0   0.6101    \n 8.0    0.4389           0.9518                     0.8050                    0.6476                   0.0   0.5795    \n 9.0    0.4832           0.9985                     0.8356                    0.6121                   0.0   0.6616    \n 10.0   0.4368           0.9494                     0.8060                    0.6493                   0.0   0.5795    \n 11.0   0.4538           0.9677                     0.8174                    0.6357                   0.0   0.4884    \n 12.0   0.4672           0.9819                     0.8384                    0.6249                   0.0   0.4884    \n 13.0   0.4401           0.9530                     0.8107                    0.6467                   0.0   0.4884    \n 14.0   0.4464           0.9598                     0.8251                    0.6416                   0.0   0.4884    \n 15.0   0.4834           0.9988                     0.8604                    0.6119                   0.0   0.4884    \n 16.0   0.4846           1.0001                     0.8651                    0.6109                   0.0   0.4884    \n 17.0   0.4970           1.0128                     0.8743                    0.6010                   0.0   0.4884    \n 18.0   0.4803           0.9956                     0.8503                    0.6144                   0.0   0.5795    \n 19.0   0.4936           1.0093                     0.8740                    0.6037                   0.0   0.5795    \n 20.0   0.5138           1.0297                     0.8943                    0.5875                   0.0   0.5795    \n 21.0   0.5240           1.0399                     0.9050                    0.5793                   0.0   0.4884    \n 22.0   0.5275           1.0434                     0.9048                    0.5765                   0.0   0.4884    \n 23.0   0.5350           1.0508                     0.8872                    0.5705                   0.0   0.5795    \n 24.0   0.4963           1.0120                     0.8754                    0.6016                   0.0   0.4884    \n 25.0   0.5009           1.0167                     0.8809                    0.5979                   0.0   0.5795    \n 26.0   0.5060           1.0219                     0.8781                    0.5938                   0.0   0.5795    \n 27.0   0.5027           1.0185                     0.8838                    0.5964                   0.0   0.4884    \n 28.0   0.5071           1.0230                     0.8867                    0.5929                   0.0   0.4884    \n 29.0   0.5124           1.0283                     0.8883                    0.5887                   0.0   0.5795    \n 30.0   0.5128           1.0287                     0.8883                    0.5883                   0.0   0.5795    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-bert-blame-assassin?", "answers": [{"text": "bert", "answer_start": 88, "answer_end": 91}]}]}]}, {"title": "responsibility-framing/predict-perception-bert-blame-object", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-bert-blame-object\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-bert-blame-object\n\nThis model is a fine-tuned version of [dbmdz/bert-base-italian-xxl-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5837\n- Rmse: 0.5589\n- Rmse Blame::a Un oggetto: 0.5589\n- Mae: 0.3862\n- Mae Blame::a Un oggetto: 0.3862\n- R2: 0.2884\n- R2 Blame::a Un oggetto: 0.2884\n- Cos: 0.3913\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.5024\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Blame::a Un oggetto  Mae Blame::a Un oggetto  R2 Blame::a Un oggetto  Pair  Neighbors \n:-----::---------------::------------------------::-----------------------::----------------------::----::---------:\n 1.0    0.8503           0.6745                    0.4386                   -0.0365                 0.0   0.5197    \n 2.0    0.8510           0.6748                    0.4548                   -0.0374                 0.0   0.4840    \n 3.0    0.7622           0.6386                    0.4541                   0.0709                  0.0   0.4635    \n 4.0    0.8301           0.6665                    0.4305                   -0.0119                 0.0   0.3499    \n 5.0    0.7306           0.6252                    0.3814                   0.1094                  0.0   0.5098    \n 6.0    0.7434           0.6307                    0.4005                   0.0937                  0.0   0.4335    \n 7.0    0.7218           0.6214                    0.4090                   0.1202                  0.0   0.4470    \n 8.0    0.7434           0.6307                    0.4042                   0.0938                  0.0   0.4470    \n 9.0    0.7719           0.6426                    0.3975                   0.0591                  0.0   0.4470    \n 10.0   0.7117           0.6171                    0.4126                   0.1324                  0.0   0.3489    \n 11.0   0.6683           0.5980                    0.3952                   0.1853                  0.0   0.3989    \n 12.0   0.6772           0.6019                    0.4201                   0.1745                  0.0   0.3989    \n 13.0   0.6576           0.5932                    0.4237                   0.1984                  0.0   0.3491    \n 14.0   0.6281           0.5797                    0.4208                   0.2344                  0.0   0.3491    \n 15.0   0.6254           0.5785                    0.3752                   0.2376                  0.0   0.5756    \n 16.0   0.6074           0.5701                    0.3985                   0.2596                  0.0   0.4142    \n 17.0   0.6045           0.5687                    0.4036                   0.2631                  0.0   0.5024    \n 18.0   0.6038           0.5684                    0.3914                   0.2640                  0.0   0.5024    \n 19.0   0.6199           0.5759                    0.4078                   0.2443                  0.0   0.5024    \n 20.0   0.6119           0.5722                    0.3954                   0.2540                  0.0   0.5024    \n 21.0   0.6193           0.5756                    0.3987                   0.2451                  0.0   0.5024    \n 22.0   0.6218           0.5768                    0.3995                   0.2420                  0.0   0.5024    \n 23.0   0.6207           0.5763                    0.4100                   0.2433                  0.0   0.4142    \n 24.0   0.5646           0.5496                    0.3687                   0.3117                  0.0   0.5024    \n 25.0   0.5582           0.5465                    0.3714                   0.3196                  0.0   0.5024    \n 26.0   0.5650           0.5498                    0.3704                   0.3112                  0.0   0.5024    \n 27.0   0.5713           0.5529                    0.3735                   0.3036                  0.0   0.5024    \n 28.0   0.5773           0.5558                    0.3759                   0.2962                  0.0   0.5024    \n 29.0   0.5818           0.5579                    0.3832                   0.2908                  0.0   0.5024    \n 30.0   0.5837           0.5589                    0.3862                   0.2884                  0.0   0.5024    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-bert-blame-object?", "answers": [{"text": "bert", "answer_start": 88, "answer_end": 91}]}]}]}, {"title": "responsibility-framing/predict-perception-bert-blame-concept", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-bert-blame-concept\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-bert-blame-concept\n\nThis model is a fine-tuned version of [dbmdz/bert-base-italian-xxl-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7359\n- Rmse: 0.6962\n- Rmse Blame::a Un concetto astratto o un'emozione: 0.6962\n- Mae: 0.5010\n- Mae Blame::a Un concetto astratto o un'emozione: 0.5010\n- R2: 0.3974\n- R2 Blame::a Un concetto astratto o un'emozione: 0.3974\n- Cos: 0.3913\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.5507\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Blame::a Un concetto astratto o un'emozione  Mae Blame::a Un concetto astratto o un'emozione  R2 Blame::a Un concetto astratto o un'emozione  Pair  Neighbors \n:-----::---------------::------------------------------------------------::-----------------------------------------------::----------------------------------------------::----::---------:\n 1.0    1.2387           0.9033                                            0.6603                                           -0.0144                                         0.0   0.3432    \n 2.0    1.1498           0.8703                                            0.5964                                           0.0584                                          0.0   0.2935    \n 3.0    1.2139           0.8942                                            0.6197                                           0.0060                                          0.0   0.4582    \n 4.0    1.1152           0.8571                                            0.5982                                           0.0867                                          0.0   0.3921    \n 5.0    1.0607           0.8358                                            0.5959                                           0.1314                                          0.0   0.4165    \n 6.0    1.0031           0.8128                                            0.5827                                           0.1786                                          0.0   0.3862    \n 7.0    0.9715           0.7999                                            0.5796                                           0.2044                                          0.0   0.3665    \n 8.0    0.8984           0.7692                                            0.5699                                           0.2643                                          0.0   0.3390    \n 9.0    0.8532           0.7497                                            0.5849                                           0.3013                                          0.0   0.3100    \n 10.0   0.8737           0.7586                                            0.5822                                           0.2846                                          0.0   0.3830    \n 11.0   0.8159           0.7331                                            0.5752                                           0.3318                                          0.0   0.4439    \n 12.0   0.8367           0.7424                                            0.6071                                           0.3148                                          0.0   0.3561    \n 13.0   0.8353           0.7417                                            0.5567                                           0.3160                                          0.0   0.5850    \n 14.0   0.8050           0.7282                                            0.5824                                           0.3408                                          0.0   0.3975    \n 15.0   0.7833           0.7183                                            0.5570                                           0.3585                                          0.0   0.4604    \n 16.0   0.8148           0.7326                                            0.5475                                           0.3328                                          0.0   0.4891    \n 17.0   0.8715           0.7576                                            0.5537                                           0.2863                                          0.0   0.5017    \n 18.0   0.7944           0.7234                                            0.5276                                           0.3495                                          0.0   0.5797    \n 19.0   0.7885           0.7207                                            0.5208                                           0.3543                                          0.0   0.5507    \n 20.0   0.7682           0.7113                                            0.5132                                           0.3709                                          0.0   0.5797    \n 21.0   0.7653           0.7100                                            0.5215                                           0.3733                                          0.0   0.5415    \n 22.0   0.7688           0.7116                                            0.5124                                           0.3704                                          0.0   0.5507    \n 23.0   0.7756           0.7148                                            0.5144                                           0.3648                                          0.0   0.5507    \n 24.0   0.7423           0.6993                                            0.5015                                           0.3921                                          0.0   0.5507    \n 25.0   0.7255           0.6913                                            0.5063                                           0.4059                                          0.0   0.4604    \n 26.0   0.7635           0.7091                                            0.5083                                           0.3748                                          0.0   0.5797    \n 27.0   0.7128           0.6852                                            0.5020                                           0.4163                                          0.0   0.5415    \n 28.0   0.7430           0.6996                                            0.5023                                           0.3915                                          0.0   0.5507    \n 29.0   0.7367           0.6966                                            0.5007                                           0.3967                                          0.0   0.5507    \n 30.0   0.7359           0.6962                                            0.5010                                           0.3974                                          0.0   0.5507    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-bert-blame-concept?", "answers": [{"text": "bert", "answer_start": 88, "answer_end": 91}]}]}]}, {"title": "responsibility-framing/predict-perception-bert-blame-none", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-bert-blame-none\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-bert-blame-none\n\nThis model is a fine-tuned version of [dbmdz/bert-base-italian-xxl-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8646\n- Rmse: 1.1072\n- Rmse Blame::a Nessuno: 1.1072\n- Mae: 0.8721\n- Mae Blame::a Nessuno: 0.8721\n- R2: 0.3083\n- R2 Blame::a Nessuno: 0.3083\n- Cos: 0.5652\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.5070\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Blame::a Nessuno  Mae Blame::a Nessuno  R2 Blame::a Nessuno  Pair  Neighbors \n:-----::---------------::---------------------::--------------------::-------------------::----::---------:\n 1.0    1.2585           1.3358                 1.1752                -0.0068              0.0   0.2970    \n 2.0    1.1310           1.2663                 1.0633                0.0952               0.0   0.4012    \n 3.0    1.0603           1.2261                 1.0574                0.1518               0.0   0.2970    \n 4.0    0.8347           1.0879                 0.8854                0.3323               0.0   0.5209    \n 5.0    0.7426           1.0261                 0.8340                0.4059               0.0   0.5209    \n 6.0    0.6671           0.9725                 0.7932                0.4663               0.0   0.5209    \n 7.0    0.6447           0.9561                 0.7424                0.4842               0.0   0.4307    \n 8.0    0.7198           1.0102                 0.8113                0.4241               0.0   0.4307    \n 9.0    0.7221           1.0118                 0.8319                0.4223               0.0   0.4150    \n 10.0   0.6999           0.9962                 0.7945                0.4401               0.0   0.4056    \n 11.0   0.7335           1.0198                 0.7969                0.4132               0.0   0.4150    \n 12.0   0.8277           1.0833                 0.8839                0.3378               0.0   0.4440    \n 13.0   0.8644           1.1070                 0.8726                0.3085               0.0   0.5070    \n 14.0   0.8874           1.1217                 0.9024                0.2900               0.0   0.4440    \n 15.0   0.8663           1.1083                 0.8914                0.3070               0.0   0.5070    \n 16.0   0.8678           1.1093                 0.8762                0.3057               0.0   0.5931    \n 17.0   0.8497           1.0976                 0.8868                0.3202               0.0   0.4440    \n 18.0   0.8533           1.1000                 0.8796                0.3173               0.0   0.5070    \n 19.0   0.8563           1.1018                 0.8768                0.3150               0.0   0.5070    \n 20.0   0.8433           1.0935                 0.8684                0.3254               0.0   0.5070    \n 21.0   0.8449           1.0945                 0.8758                0.3240               0.0   0.4440    \n 22.0   0.8305           1.0851                 0.8469                0.3356               0.0   0.5070    \n 23.0   0.8369           1.0893                 0.8555                0.3305               0.0   0.5070    \n 24.0   0.8441           1.0940                 0.8648                0.3247               0.0   0.5070    \n 25.0   0.8470           1.0959                 0.8633                0.3224               0.0   0.5070    \n 26.0   0.8562           1.1018                 0.8708                0.3151               0.0   0.4440    \n 27.0   0.8600           1.1042                 0.8714                0.3120               0.0   0.5070    \n 28.0   0.8657           1.1079                 0.8763                0.3074               0.0   0.4440    \n 29.0   0.8654           1.1077                 0.8734                0.3077               0.0   0.5070    \n 30.0   0.8646           1.1072                 0.8721                0.3083               0.0   0.5070    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-bert-blame-none?", "answers": [{"text": "bert", "answer_start": 88, "answer_end": 91}]}]}]}, {"title": "responsibility-framing/predict-perception-bert-cause-concept", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-bert-cause-concept\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-bert-cause-concept\n\nThis model is a fine-tuned version of [dbmdz/bert-base-italian-xxl-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4044\n- Rmse: 0.6076\n- Rmse Cause::a Causata da un concetto astratto (es. gelosia): 0.6076\n- Mae: 0.4548\n- Mae Cause::a Causata da un concetto astratto (es. gelosia): 0.4548\n- R2: 0.5463\n- R2 Cause::a Causata da un concetto astratto (es. gelosia): 0.5463\n- Cos: 0.2174\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.3931\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Cause::a Causata da un concetto astratto (es. gelosia)  Mae Cause::a Causata da un concetto astratto (es. gelosia)  R2 Cause::a Causata da un concetto astratto (es. gelosia)  Pair  Neighbors \n:-----::---------------::-----------------------------------------------------------::----------------------------------------------------------::---------------------------------------------------------::----::---------:\n 1.0    0.9520           0.9323                                                       0.6560                                                      -0.0680                                                    0.0   0.3188    \n 2.0    0.8621           0.8872                                                       0.5962                                                      0.0328                                                     0.0   0.4066    \n 3.0    0.9223           0.9176                                                       0.6608                                                      -0.0347                                                    0.0   0.3632    \n 4.0    0.8273           0.8691                                                       0.5874                                                      0.0719                                                     0.0   0.3754    \n 5.0    0.8741           0.8933                                                       0.6136                                                      0.0193                                                     0.0   0.3529    \n 6.0    0.7781           0.8428                                                       0.5732                                                      0.1271                                                     0.0   0.4152    \n 7.0    0.7257           0.8139                                                       0.5519                                                      0.1859                                                     0.0   0.4152    \n 8.0    0.7122           0.8064                                                       0.5792                                                      0.2010                                                     0.0   0.3955    \n 9.0    0.6771           0.7862                                                       0.5701                                                      0.2403                                                     0.0   0.3955    \n 10.0   0.6704           0.7823                                                       0.5735                                                      0.2479                                                     0.0   0.4847    \n 11.0   0.6852           0.7909                                                       0.5987                                                      0.2313                                                     0.0   0.4847    \n 12.0   0.6106           0.7466                                                       0.5696                                                      0.3150                                                     0.0   0.2935    \n 13.0   0.5867           0.7318                                                       0.5209                                                      0.3418                                                     0.0   0.3119    \n 14.0   0.5120           0.6837                                                       0.5007                                                      0.4256                                                     0.0   0.3849    \n 15.0   0.4789           0.6612                                                       0.4883                                                      0.4627                                                     0.0   0.3849    \n 16.0   0.4526           0.6428                                                       0.4775                                                      0.4922                                                     0.0   0.3849    \n 17.0   0.4383           0.6325                                                       0.4616                                                      0.5083                                                     0.0   0.3931    \n 18.0   0.4141           0.6148                                                       0.4478                                                      0.5355                                                     0.0   0.3849    \n 19.0   0.3952           0.6007                                                       0.4407                                                      0.5566                                                     0.0   0.3849    \n 20.0   0.4217           0.6205                                                       0.4505                                                      0.5269                                                     0.0   0.3931    \n 21.0   0.4065           0.6091                                                       0.4508                                                      0.5440                                                     0.0   0.3931    \n 22.0   0.3937           0.5995                                                       0.4470                                                      0.5584                                                     0.0   0.3849    \n 23.0   0.4132           0.6142                                                       0.4617                                                      0.5364                                                     0.0   0.3931    \n 24.0   0.4214           0.6203                                                       0.4659                                                      0.5272                                                     0.0   0.4066    \n 25.0   0.3863           0.5939                                                       0.4470                                                      0.5666                                                     0.0   0.3849    \n 26.0   0.4353           0.6304                                                       0.4760                                                      0.5117                                                     0.0   0.3931    \n 27.0   0.4078           0.6101                                                       0.4612                                                      0.5426                                                     0.0   0.3931    \n 28.0   0.4118           0.6132                                                       0.4616                                                      0.5380                                                     0.0   0.3931    \n 29.0   0.4041           0.6074                                                       0.4551                                                      0.5466                                                     0.0   0.3931    \n 30.0   0.4044           0.6076                                                       0.4548                                                      0.5463                                                     0.0   0.3931    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-bert-cause-concept?", "answers": [{"text": "bert", "answer_start": 88, "answer_end": 91}]}]}]}, {"title": "responsibility-framing/predict-perception-bert-focus-assassin", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-bert-focus-assassin\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-bert-focus-assassin\n\nThis model is a fine-tuned version of [dbmdz/bert-base-italian-xxl-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2964\n- Rmse: 0.8992\n- Rmse Focus::a Sull'assassino: 0.8992\n- Mae: 0.7331\n- Mae Focus::a Sull'assassino: 0.7331\n- R2: 0.6500\n- R2 Focus::a Sull'assassino: 0.6500\n- Cos: 0.7391\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.6131\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Focus::a Sull'assassino  Mae Focus::a Sull'assassino  R2 Focus::a Sull'assassino  Pair  Neighbors \n:-----::---------------::----------------------------::---------------------------::--------------------------::----::---------:\n 1.0    0.9851           1.6393                        1.5316                       -0.1633                     0.0   0.2457    \n 2.0    0.8921           1.5601                        1.4317                       -0.0535                     0.0   0.4734    \n 3.0    0.7345           1.4155                        1.3113                       0.1327                      0.0   0.3596    \n 4.0    0.7282           1.4094                        1.2678                       0.1401                      0.0   0.5367    \n 5.0    0.5966           1.2758                        1.1144                       0.2955                      0.0   0.3911    \n 6.0    0.4578           1.1175                        0.9105                       0.4594                      0.0   0.3911    \n 7.0    0.3539           0.9826                        0.7770                       0.5821                      0.0   0.5522    \n 8.0    0.2938           0.8953                        0.7110                       0.6530                      0.0   0.6021    \n 9.0    0.3455           0.9708                        0.7607                       0.5921                      0.0   0.3911    \n 10.0   0.2719           0.8612                        0.6768                       0.6790                      0.0   0.6131    \n 11.0   0.2855           0.8826                        0.7053                       0.6628                      0.0   0.6131    \n 12.0   0.3000           0.9046                        0.7255                       0.6458                      0.0   0.5261    \n 13.0   0.2817           0.8766                        0.7236                       0.6674                      0.0   0.6131    \n 14.0   0.3504           0.9777                        0.7631                       0.5863                      0.0   0.6131    \n 15.0   0.3031           0.9094                        0.7565                       0.6420                      0.0   0.6131    \n 16.0   0.3041           0.9109                        0.7409                       0.6408                      0.0   0.6131    \n 17.0   0.3496           0.9767                        0.7812                       0.5871                      0.0   0.6131    \n 18.0   0.3260           0.9430                        0.7757                       0.6151                      0.0   0.6131    \n 19.0   0.3118           0.9222                        0.7442                       0.6318                      0.0   0.6131    \n 20.0   0.3062           0.9140                        0.7459                       0.6384                      0.0   0.6131    \n 21.0   0.3200           0.9344                        0.7592                       0.6221                      0.0   0.6131    \n 22.0   0.3132           0.9244                        0.7532                       0.6301                      0.0   0.6131    \n 23.0   0.3006           0.9056                        0.7321                       0.6450                      0.0   0.5261    \n 24.0   0.2985           0.9024                        0.7463                       0.6475                      0.0   0.6131    \n 25.0   0.3039           0.9105                        0.7359                       0.6412                      0.0   0.6131    \n 26.0   0.2989           0.9030                        0.7210                       0.6471                      0.0   0.6131    \n 27.0   0.2997           0.9042                        0.7418                       0.6461                      0.0   0.6131    \n 28.0   0.2970           0.9001                        0.7346                       0.6493                      0.0   0.6131    \n 29.0   0.2970           0.9001                        0.7281                       0.6493                      0.0   0.6131    \n 30.0   0.2964           0.8992                        0.7331                       0.6500                      0.0   0.6131    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-bert-focus-assassin?", "answers": [{"text": "bert", "answer_start": 88, "answer_end": 91}]}]}]}, {"title": "Ayham/albert_ernie_50beam_summarization_cnn_dailymail", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- cnn_dailymail\nmodel-index:\n- name: albert_ernie_summarization_cnn_dailymail\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# albert_ernie_summarization_cnn_dailymail\n\nThis model is a fine-tuned version of []( on the cnn_dailymail dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 3.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.12.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.4\n- Tokenizers 0.10.3\n", "qas": []}]}, {"title": "GanjinZero/biobart-large", "paragraphs": [{"context": "---\nlanguage: \n  - en\nlicense: apache-2.0\n\ntags:\n- bart\n- biobart\n- biomedical\n\ninference: true\n\nwidget:\n- text: \"Influenza is a <mask> disease.\"\n- type: \"text-generation\"\n\n---\n\nPaper: [BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model](\n\n```\n@misc{BioBART,\n  title={BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model},\n  author={Hongyi Yuan and Zheng Yuan and Ruyi Gan and Jiaxing Zhang and Yutao Xie and Sheng Yu},\n  year={2022},\n  eprint={2204.03905},\n  archivePrefix={arXiv}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of GanjinZero/biobart-large?", "answers": [{"text": "bart", "answer_start": 51, "answer_end": 54}]}]}]}, {"title": "RobertoMCA97/distilbert-base-uncased-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotion\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: default\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9255\n    - name: F1\n      type: f1\n      value: 0.9257511693451751\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2157\n- Accuracy: 0.9255\n- F1: 0.9258\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.3093           0.9081 |\n 2.0    0.2157           0.9258 |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of RobertoMCA97/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 121, "answer_end": 130}]}, {"id": "q2", "question": "What is the model task of RobertoMCA97/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "text-classification", "answer_start": 228, "answer_end": 246}]}]}]}, {"title": "richielo/small-e-czech-finetuned-ner-wikiann", "paragraphs": [{"context": "---\nlicense: cc-by-4.0\ntags:\n- generated_from_trainer\ndatasets:\n- wikiann\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: small-e-czech-finetuned-ner-wikiann\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: wikiann\n      type: wikiann\n      args: cs\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.8713322894683097\n    - name: Recall\n      type: recall\n      value: 0.8970423324922905\n    - name: F1\n      type: f1\n      value: 0.8840004144075699\n    - name: Accuracy\n      type: accuracy\n      value: 0.9557089381093997\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# small-e-czech-finetuned-ner-wikiann\n\nThis model is a fine-tuned version of [Seznam/small-e-czech]( on the wikiann dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2547\n- Precision: 0.8713\n- Recall: 0.8970\n- F1: 0.8840\n- Accuracy: 0.9557\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 20\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.2449           0.8088  0.9320   |\n 2.0    0.2137           0.8398  0.9400   |\n 3.0    0.1912           0.8593  0.9466   |\n 4.0    0.1931           0.8671  0.9488   |\n 5.0    0.1892           0.8776  0.9519   |\n 6.0    0.2058           0.8811  0.9508   |\n 7.0    0.2020           0.8849  0.9531   |\n 8.0    0.2118           0.8837  0.9528   |\n 9.0    0.2171           0.8906  0.9550   |\n 10.0   0.2228           0.8912  0.9545   |\n 11.0   0.2293           0.8898  0.9544   |\n 12.0   0.2276           0.8958  0.9554   |\n 13.0   0.2384           0.8940  0.9552   |\n 14.0   0.2443           0.8931  0.9554   |\n 15.0   0.2464           0.8958  0.9557   |\n 16.0   0.2477           0.8948  0.9552   |\n 17.0   0.2525           0.8973  0.9559   |\n 18.0   0.2529           0.8962  0.9561   |\n 19.0   0.2533           0.8966  0.9557   |\n 20.0   0.2547           0.8970  0.9557   |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q2", "question": "What is the model task of richielo/small-e-czech-finetuned-ner-wikiann?", "answers": [{"text": "token-classification", "answer_start": 243, "answer_end": 262}]}]}]}, {"title": "anton-l/xtreme_s_xlsr_minds14", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- f1\nmodel-index:\n- name: xtreme_s_xlsr_minds14\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xtreme_s_xlsr_minds14\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2566\n- F1: {'f1': 0.9460569664921582, 'accuracy': 0.9468540012217471}\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1500\n- num_epochs: 50.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 2.7    2.5921          \n 5.41   1.5353          \n 8.11   0.7337          \n 10.81  0.5076          \n 13.51  0.4917          \n 16.22  0.4751          \n 18.92  0.5228          \n 21.62  0.5910          \n 24.32  0.4464          \n 27.03  0.3760          \n 29.73  0.4178          \n 32.43  0.4152          \n 35.14  0.2903          \n 37.84  0.3046          \n 40.54  0.3111          \n 43.24  0.2748          \n 45.95  0.2800          \n 48.65  0.2566          \n\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.4.dev0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of anton-l/xtreme_s_xlsr_minds14?", "answers": [{"text": "wav2vec2", "answer_start": 392, "answer_end": 399}]}]}]}, {"title": "huggingtweets/mikepompeo", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Mike Pompeo</div>\n    <div style=\"text-align: center; font-size: 14px;\">@mikepompeo</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Mike Pompeo.\n\n Mike Pompeo |\n --- |\n 1899 |\n 68 |\n 60 |\n 1771 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @mikepompeo's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/mikepompeo')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/mikepompeo?", "answers": [{"text": "text-generation", "answer_start": 1954, "answer_end": 1968}]}]}]}, {"title": "Sivakumar/distilbert-base-uncased-finetuned-squad", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- squad_v2\nmodel-index:\n- name: distilbert-base-uncased-finetuned-squad\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-squad\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the squad_v2 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.4101\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    1.2303          |\n 2.0    1.2412          |\n 3.0    1.4101          |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Sivakumar/distilbert-base-uncased-finetuned-squad?", "answers": [{"text": "distilbert", "answer_start": 97, "answer_end": 106}]}]}]}, {"title": "wanyu/IteraTeR-ROBERTA-Intention-Classifier", "paragraphs": [{"context": "---\ndatasets:\n- IteraTeR_full_sent\n---\n\n# IteraTeR RoBERTa model\nThis model was obtained by fine-tuning [roberta-large]( on [IteraTeR-human-sent]( dataset.\n\nPaper: [Understanding Iterative Revision from Human-Written Text]( <br>\nAuthors: Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa Lopez, Dongyeop Kang\n\n## Edit Intention Prediction Task\nGiven a pair of original sentence and revised sentence, our model can predict the edit intention for this revision pair.<br>\nMore specifically, the model will predict the probability of the following edit intentions:\n<table>\n  <tr>\n    <th>Edit Intention</th>\n    <th>Definition</th>\n    <th>Example</th>\n  </tr>\n  <tr>\n    <td>clarity</td>\n    <td>Make the text more formal, concise, readable and understandable.</td>\n    <td>\n    Original: It's like a house which anyone can enter in it. <br>\n    Revised: It's like a house which anyone can enter.\n    </td>\n  </tr>\n  <tr>\n    <td>fluency</td>\n    <td>Fix grammatical errors in the text.</td>\n    <td>\n    Original: In the same year he became the Fellow of the Royal Society. <br>\n    Revised: In the same year, he became the Fellow of the Royal Society.\n    </td>\n  </tr>\n  <tr>\n    <td>coherence</td>\n    <td>Make the text more cohesive, logically linked and consistent as a whole.</td>\n    <td>\n    Original: Achievements and awards Among his other activities, he founded the Karachi Film Guild and Pakistan Film and TV Academy. <br>\n    Revised: Among his other activities, he founded the Karachi Film Guild and Pakistan Film and TV Academy.\n    </td>\n  </tr>\n  <tr>\n    <td>style</td>\n    <td>Convey the writer\u2019s writing preferences, including emotions, tone, voice, etc..</td>\n    <td>\n    Original: She was last seen on 2005-10-22. <br>\n    Revised: She was last seen on October 22, 2005.\n    </td>\n  </tr>\n  <tr>\n    <td>meaning-changed</td>\n    <td>Update or add new information to the text.</td>\n    <td>\n    Original: This method improves the model accuracy from 64% to 78%. <br>\n    Revised: This method improves the model accuracy from 64% to 83%.\n    </td>\n  </tr>\n</table>\n\n\n\n## Usage\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"wanyu/IteraTeR-ROBERTA-Intention-Classifier\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"wanyu/IteraTeR-ROBERTA-Intention-Classifier\")\n\nid2label = {0: \"clarity\", 1: \"fluency\", 2: \"coherence\", 3: \"style\", 4: \"meaning-changed\"}\n\nbefore_text = 'I likes coffee.'\nafter_text = 'I like coffee.'\nmodel_input = tokenizer(before_text, after_text, return_tensors='pt')\nmodel_output = model(**model_input)\nsoftmax_scores = torch.softmax(model_output.logits, dim=-1)\npred_id = torch.argmax(softmax_scores)\npred_label = id2label[pred_id.int()]\n```", "qas": [{"id": "q1", "question": "What is the model architecture of wanyu/IteraTeR-ROBERTA-Intention-Classifier?", "answers": [{"text": "roberta", "answer_start": 105, "answer_end": 111}]}]}]}, {"title": "kapilchauhan/efl-finetuned-cola", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: efl-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.6097804486545971\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# efl-finetuned-cola\n\nThis model is a fine-tuned version of [nghuyong/ernie-2.0-en]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4688\n- Matthews Correlation: 0.6098\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.4795          \n 2.0    0.4061          \n 3.0    0.4688          \n 4.0    0.5332          \n 5.0    0.6316          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q2", "question": "What is the model task of kapilchauhan/efl-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 189, "answer_end": 207}]}]}]}, {"title": "GPL/signal1m-distilbert-tas-b-gpl-self_miner", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\n# {MODEL_NAME}\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n<!--- Describe your model here -->\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('{MODEL_NAME}')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers]( you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\ndef cls_pooling(model_output, attention_mask):\n    return model_output[0][:,0]\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\nmodel = AutoModel.from_pretrained('{MODEL_NAME}')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, cls pooling.\nsentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Evaluation Results\n\n<!--- Describe how your model was evaluated -->\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [\n\n\n## Training\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 140000 with parameters:\n```\n{'batch_size': 32, 'sampler': 'torch.utils.data.sampler.SequentialSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`gpl.toolkit.loss.MarginDistillationLoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"epochs\": 1,\n    \"evaluation_steps\": 0,\n    \"evaluator\": \"NoneType\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 2e-05\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": 140000,\n    \"warmup_steps\": 1000,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 350, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->", "qas": [{"id": "q2", "question": "What is the model task of GPL/signal1m-distilbert-tas-b-gpl-self_miner?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "Simply-divine/finetune_indian_asr", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: finetune_indian_asr\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# finetune_indian_asr\n\nThis model is a fine-tuned version of [Harveenchadha/vakyansh-wav2vec2-indian-english-enm-700]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4215\n- Wer: 0.3403\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 3.45   2.9944          \n 6.9    1.4455          \n 10.34  0.4299          \n 13.79  0.3628          \n 17.24  0.3835          \n 20.69  0.3802          \n 24.14  0.3842          \n 27.59  0.4215          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Simply-divine/finetune_indian_asr?", "answers": [{"text": "wav2vec2", "answer_start": 368, "answer_end": 375}]}]}]}, {"title": "StivenLancheros/biobert-base-cased-v1.2-finetuned-ner-CRAFT_English", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: biobert-base-cased-v1.2-finetuned-ner-CRAFT_English\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# biobert-base-cased-v1.2-finetuned-ner-CRAFT_English\n\nThis model is a fine-tuned version of [dmis-lab/biobert-base-cased-v1.2]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1614\n- Precision: 0.8585\n- Recall: 0.8623\n- F1: 0.8604\n- Accuracy: 0.9724\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.1242           0.8698  0.9681   |\n 2.0    0.1541           0.8549  0.9705   |\n 3.0    0.1510           0.8681  0.9711   |\n 4.0    0.1614           0.8623  0.9724   |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of StivenLancheros/biobert-base-cased-v1.2-finetuned-ner-CRAFT_English?", "answers": [{"text": "bert", "answer_start": 105, "answer_end": 108}]}]}]}, {"title": "RobertoMCA97/xlm-roberta-base-finetuned-panx-de", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- xtreme\nmetrics:\n- f1\nmodel-index:\n- name: xlm-roberta-base-finetuned-panx-de\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: xtreme\n      type: xtreme\n      args: PAN-X.de\n    metrics:\n    - name: F1\n      type: f1\n      value: 0.8590909090909091\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-finetuned-panx-de\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the xtreme dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1380\n- F1: 0.8591\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.1624          \n 2.0    0.1445          \n 3.0    0.1380          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of RobertoMCA97/xlm-roberta-base-finetuned-panx-de?", "answers": [{"text": "xlm-roberta", "answer_start": 102, "answer_end": 112}]}, {"id": "q2", "question": "What is the model task of RobertoMCA97/xlm-roberta-base-finetuned-panx-de?", "answers": [{"text": "token-classification", "answer_start": 203, "answer_end": 222}]}]}]}, {"title": "abinternet143/t5-small-finetuned-xsum", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- xsum\nmodel-index:\n- name: t5-small-finetuned-xsum\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-finetuned-xsum\n\nThis model is a fine-tuned version of [t5-small]( on the xsum dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.11.0a0+bfe5ad2\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of abinternet143/t5-small-finetuned-xsum?", "answers": [{"text": "t5", "answer_start": 93, "answer_end": 94}]}]}]}, {"title": "responsibility-framing/predict-perception-xlmr-blame-concept", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-xlmr-blame-concept\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-xlmr-blame-concept\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9414\n- Rmse: 0.7875\n- Rmse Blame::a Un concetto astratto o un'emozione: 0.7875\n- Mae: 0.6165\n- Mae Blame::a Un concetto astratto o un'emozione: 0.6165\n- R2: 0.2291\n- R2 Blame::a Un concetto astratto o un'emozione: 0.2291\n- Cos: 0.1304\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.3509\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Blame::a Un concetto astratto o un'emozione  Mae Blame::a Un concetto astratto o un'emozione  R2 Blame::a Un concetto astratto o un'emozione  Pair  Neighbors \n:-----::---------------::------------------------------------------------::-----------------------------------------------::----------------------------------------------::----::---------:\n 1.0    1.2093           0.8925                                            0.6659                                           0.0097                                          0.0   0.4013    \n 2.0    1.2199           0.8964                                            0.6494                                           0.0010                                          0.0   0.4515    \n 3.0    1.1798           0.8815                                            0.6412                                           0.0339                                          0.0   0.2402    \n 4.0    1.1726           0.8788                                            0.6370                                           0.0397                                          0.0   0.2911    \n 5.0    1.1194           0.8587                                            0.5925                                           0.0833                                          0.0   0.3303    \n 6.0    1.0776           0.8425                                            0.6265                                           0.1175                                          0.0   0.4190    \n 7.0    1.0513           0.8321                                            0.6087                                           0.1391                                          0.0   0.3303    \n 8.0    1.0537           0.8331                                            0.6265                                           0.1372                                          0.0   0.3303    \n 9.0    0.9104           0.7744                                            0.5887                                           0.2544                                          0.0   0.3680    \n 10.0   0.9055           0.7723                                            0.6222                                           0.2585                                          0.0   0.3987    \n 11.0   1.0173           0.8186                                            0.6168                                           0.1669                                          0.0   0.3303    \n 12.0   0.9155           0.7765                                            0.6284                                           0.2503                                          0.0   0.3987    \n 13.0   0.9255           0.7808                                            0.6140                                           0.2421                                          0.0   0.3987    \n 14.0   0.8506           0.7485                                            0.6076                                           0.3035                                          0.0   0.2862    \n 15.0   1.0272           0.8226                                            0.6699                                           0.1588                                          0.0   0.2862    \n 16.0   0.9969           0.8103                                            0.6461                                           0.1836                                          0.0   0.2862    \n 17.0   0.9066           0.7727                                            0.5849                                           0.2576                                          0.0   0.3303    \n 18.0   0.8741           0.7588                                            0.5953                                           0.2842                                          0.0   0.3303    \n 19.0   1.0022           0.8125                                            0.6549                                           0.1793                                          0.0   0.2862    \n 20.0   0.9238           0.7801                                            0.6180                                           0.2435                                          0.0   0.2862    \n 21.0   0.9868           0.8062                                            0.6457                                           0.1919                                          0.0   0.2862    \n 22.0   0.9514           0.7916                                            0.6204                                           0.2209                                          0.0   0.2862    \n 23.0   0.9227           0.7796                                            0.6053                                           0.2444                                          0.0   0.3509    \n 24.0   0.9147           0.7762                                            0.5979                                           0.2510                                          0.0   0.3509    \n 25.0   0.9645           0.7970                                            0.6296                                           0.2102                                          0.0   0.2862    \n 26.0   0.9587           0.7946                                            0.6279                                           0.2149                                          0.0   0.2862    \n 27.0   0.9519           0.7918                                            0.6273                                           0.2205                                          0.0   0.2862    \n 28.0   0.9398           0.7868                                            0.6181                                           0.2304                                          0.0   0.2862    \n 29.0   0.9492           0.7907                                            0.6228                                           0.2227                                          0.0   0.2862    \n 30.0   0.9414           0.7875                                            0.6165                                           0.2291                                          0.0   0.3509    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-xlmr-blame-concept?", "answers": [{"text": "xlm-roberta", "answer_start": 394, "answer_end": 404}]}]}]}, {"title": "responsibility-framing/predict-perception-xlmr-blame-none", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-xlmr-blame-none\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-xlmr-blame-none\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8941\n- Rmse: 1.1259\n- Rmse Blame::a Nessuno: 1.1259\n- Mae: 0.8559\n- Mae Blame::a Nessuno: 0.8559\n- R2: 0.2847\n- R2 Blame::a Nessuno: 0.2847\n- Cos: 0.3043\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.3537\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Blame::a Nessuno  Mae Blame::a Nessuno  R2 Blame::a Nessuno  Pair  Neighbors \n:-----::---------------::---------------------::--------------------::-------------------::----::---------:\n 1.0    1.2746           1.3443                 1.1788                -0.0197              0.0   0.2970    \n 2.0    1.3264           1.3714                 1.1967                -0.0612              0.0   0.2961    \n 3.0    1.2511           1.3319                 1.0932                -0.0009              0.0   0.2681    \n 4.0    1.0204           1.2028                 0.9818                0.1836               0.0   0.3686    \n 5.0    0.8607           1.1047                 0.8145                0.3115               0.0   0.4044    \n 6.0    0.8574           1.1026                 0.8095                0.3140               0.0   0.4044    \n 7.0    0.8548           1.1009                 0.8560                0.3161               0.0   0.3686    \n 8.0    0.6974           0.9944                 0.7503                0.4421               0.0   0.3686    \n 9.0    0.7955           1.0620                 0.7907                0.3636               0.0   0.4044    \n 10.0   0.8954           1.1267                 0.8036                0.2837               0.0   0.4058    \n 11.0   0.8449           1.0945                 0.8748                0.3241               0.0   0.3931    \n 12.0   0.7960           1.0624                 0.8000                0.3632               0.0   0.4044    \n 13.0   0.9027           1.1313                 0.8441                0.2778               0.0   0.3537    \n 14.0   0.8914           1.1242                 0.8998                0.2869               0.0   0.3324    \n 15.0   0.9184           1.1411                 0.8633                0.2652               0.0   0.3537    \n 16.0   0.9284           1.1473                 0.8919                0.2573               0.0   0.3537    \n 17.0   0.9495           1.1602                 0.8768                0.2404               0.0   0.3537    \n 18.0   0.9850           1.1818                 0.9303                0.2120               0.0   0.3324    \n 19.0   0.9603           1.1669                 0.8679                0.2317               0.0   0.3537    \n 20.0   0.9269           1.1464                 0.8391                0.2585               0.0   0.3537    \n 21.0   0.8936           1.1256                 0.8357                0.2851               0.0   0.3537    \n 22.0   0.8894           1.1230                 0.8593                0.2884               0.0   0.3537    \n 23.0   0.8997           1.1294                 0.8568                0.2802               0.0   0.3537    \n 24.0   0.8748           1.1137                 0.8541                0.3002               0.0   0.3324    \n 25.0   0.9264           1.1461                 0.8682                0.2588               0.0   0.3901    \n 26.0   0.8829           1.1188                 0.8608                0.2937               0.0   0.3324    \n 27.0   0.9137           1.1382                 0.8656                0.2691               0.0   0.3537    \n 28.0   0.8774           1.1154                 0.8488                0.2980               0.0   0.3324    \n 29.0   0.8985           1.1287                 0.8562                0.2812               0.0   0.3537    \n 30.0   0.8941           1.1259                 0.8559                0.2847               0.0   0.3537    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-xlmr-blame-none?", "answers": [{"text": "xlm-roberta", "answer_start": 388, "answer_end": 398}]}]}]}, {"title": "responsibility-framing/predict-perception-xlmr-cause-human", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: predict-perception-xlmr-cause-human\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# predict-perception-xlmr-cause-human\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7632\n- Rmse: 1.2675\n- Rmse Cause::a Causata da un essere umano: 1.2675\n- Mae: 0.9299\n- Mae Cause::a Causata da un essere umano: 0.9299\n- R2: 0.4188\n- R2 Cause::a Causata da un essere umano: 0.4188\n- Cos: 0.3913\n- Pair: 0.0\n- Rank: 0.5\n- Neighbors: 0.4082\n- Rsa: nan\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 20\n- eval_batch_size: 8\n- seed: 1996\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss  Rmse Cause::a Causata da un essere umano  Mae Cause::a Causata da un essere umano  R2 Cause::a Causata da un essere umano  Pair  Neighbors \n:-----::---------------::----------------------------------------::---------------------------------------::--------------------------------------::----::---------:\n 1.0    1.3796           1.7041                                    1.3614                                   -0.0506                                 0.0   0.2971    \n 2.0    1.1173           1.5336                                    1.2624                                   0.1491                                  0.0   0.4446    \n 3.0    1.0580           1.4923                                    1.2451                                   0.1943                                  0.0   0.4957    \n 4.0    1.0200           1.4653                                    1.2087                                   0.2232                                  0.0   0.5123    \n 5.0    1.1496           1.5556                                    1.2573                                   0.1245                                  0.0   0.3007    \n 6.0    0.9641           1.4246                                    1.1763                                   0.2658                                  0.0   0.3619    \n 7.0    0.8328           1.3240                                    1.0948                                   0.3658                                  0.0   0.3628    \n 8.0    0.6890           1.2043                                    1.0112                                   0.4753                                  0.0   0.4082    \n 9.0    1.0380           1.4782                                    1.1215                                   0.2095                                  0.0   0.3781    \n 10.0   1.1780           1.5747                                    1.2852                                   0.1029                                  0.0   0.4082    \n 11.0   0.8714           1.3544                                    1.1388                                   0.3364                                  0.0   0.4082    \n 12.0   0.7260           1.2362                                    0.9563                                   0.4471                                  0.0   0.4957    \n 13.0   0.7241           1.2346                                    0.8998                                   0.4485                                  0.0   0.4727    \n 14.0   0.9070           1.3818                                    1.1145                                   0.3093                                  0.0   0.4082    \n 15.0   0.7280           1.2380                                    0.9210                                   0.4456                                  0.0   0.4446    \n 16.0   0.7921           1.2912                                    0.9738                                   0.3968                                  0.0   0.4082    \n 17.0   0.8368           1.3272                                    0.9717                                   0.3627                                  0.0   0.4082    \n 18.0   0.7782           1.2799                                    0.9615                                   0.4073                                  0.0   0.3768    \n 19.0   0.7594           1.2644                                    0.9441                                   0.4216                                  0.0   0.4446    \n 20.0   0.7230           1.2336                                    0.8953                                   0.4494                                  0.0   0.3787    \n 21.0   0.7836           1.2843                                    0.9577                                   0.4033                                  0.0   0.3768    \n 22.0   0.7248           1.2352                                    0.9133                                   0.4480                                  0.0   0.4446    \n 23.0   0.7608           1.2655                                    0.9435                                   0.4206                                  0.0   0.4446    \n 24.0   0.7447           1.2520                                    0.9277                                   0.4329                                  0.0   0.4446    \n 25.0   0.7437           1.2512                                    0.9236                                   0.4336                                  0.0   0.4082    \n 26.0   0.7301           1.2397                                    0.9182                                   0.4440                                  0.0   0.4446    \n 27.0   0.7748           1.2770                                    0.9619                                   0.4100                                  0.0   0.4082    \n 28.0   0.7415           1.2493                                    0.9097                                   0.4353                                  0.0   0.4082    \n 29.0   0.7525           1.2586                                    0.9189                                   0.4269                                  0.0   0.4082    \n 30.0   0.7632           1.2675                                    0.9299                                   0.4188                                  0.0   0.4082    \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of responsibility-framing/predict-perception-xlmr-cause-human?", "answers": [{"text": "xlm-roberta", "answer_start": 390, "answer_end": 400}]}]}]}, {"title": "ixa-ehu/roberta-eus-cc100-base-cased", "paragraphs": [{"context": "---\nlanguage: eu\nlicense: cc-by-nc-4.0\ntags:\n- basque\n- roberta\n---\n\n# Roberta-eus cc100 base cased\n\nThis is a RoBERTa model for Basque model presented in [Does corpus quality really matter for low-resource languages?]( There are several models for Basque using the RoBERTa architecture, using different corpora:\n\n- roberta-eus-euscrawl-base-cased: Basque RoBERTa model trained on Euscrawl, a corpus created using tailored crawling from Basque sites. EusCrawl contains 12,528k documents and 423M tokens.\n- roberta-eus-euscrawl-large-cased: RoBERTa large trained on EusCrawl.\n- roberta-eus-mC4-base-cased: Basque RoBERTa model trained on the Basque portion of mc4 dataset.\n- roberta-eus-CC100-base-cased: Basque RoBERTa model trained on  Basque portion of cc100 dataset.\n\nThe models have been tested on five different downstream tasks for Basque: Topic classification, Sentiment analysis, Stance detection, Named Entity Recognition (NER), and Question Answering (refer to the [paper]( for more details). See summary of results below:\n\n\n Topic class.  Stance det.      QA   \n-------------------------------------\n         76.2         57.4     34.6  \n     **77.6**         62.9  **38.3** \n         75.3         59.1     35.2  \n         76.2     **63.4**     35.8  \n\n\nIf you use any of these models, please cite the following paper:\n\n```\n@misc{artetxe2022euscrawl,\n title={Does corpus quality really matter for low-resource languages?},\n author={Mikel Artetxe, Itziar Aldabe, Rodrigo Agerri,\n         Olatz Perez-de-Vi\u00f1aspre, Aitor Soroa},\n year={2022},\n eprint={2203.08111},\n archivePrefix={arXiv},\n primaryClass={cs.CL}\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of ixa-ehu/roberta-eus-cc100-base-cased?", "answers": [{"text": "roberta", "answer_start": 56, "answer_end": 62}]}]}]}, {"title": "ixa-ehu/roberta-eus-euscrawl-base-cased", "paragraphs": [{"context": "---\nlanguage: eu\nlicense: cc-by-nc-4.0\ntags:\n- basque\n- roberta\n---\n\n# Roberta-eus Euscrawl base cased\n\nThis is a RoBERTa model for Basque model presented in [Does corpus quality really matter for low-resource languages?]( There are several models for Basque using the RoBERTa architecture, which are pre-trained using different corpora:\n\n- roberta-eus-euscrawl-base-cased: Basque RoBERTa trained on Euscrawl, a corpus created using tailored crawling from Basque sites. EusCrawl contains 12,528k documents and 423M tokens.\n- roberta-eus-euscrawl-large-cased: Basque RoBERTa large trained on EusCrawl.\n- roberta-eus-mC4-base-cased: Basque RoBERTa trained on the Basque portion of mc4 dataset.\n- roberta-eus-CC100-base-cased: Basque RoBERTa trained on  Basque portion of cc100 dataset.\n\nThe models have been tested on five different downstream tasks for Basque: Topic classification, Sentiment analysis, Stance detection, Named Entity Recognition (NER), and Question Answering (refer to the [paper]( for more details). See summary of results below:\n\n\n Topic class.  Stance det.      QA   \n-------------------------------------\n         76.2         57.4     34.6  \n     **77.6**         62.9  **38.3** \n         75.3         59.1     35.2  \n         76.2     **63.4**     35.8  \n\n\nIf you use any of these models, please cite the following paper:\n\n```\n@misc{artetxe2022euscrawl,\n title={Does corpus quality really matter for low-resource languages?},\n author={Mikel Artetxe, Itziar Aldabe, Rodrigo Agerri,\n         Olatz Perez-de-Vi\u00f1aspre, Aitor Soroa},\n year={2022},\n eprint={2203.08111},\n archivePrefix={arXiv},\n primaryClass={cs.CL}\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of ixa-ehu/roberta-eus-euscrawl-base-cased?", "answers": [{"text": "roberta", "answer_start": 56, "answer_end": 62}]}]}]}, {"title": "ixa-ehu/roberta-eus-euscrawl-large-cased", "paragraphs": [{"context": "---\nlanguage: eu\nlicense: cc-by-nc-4.0\ntags:\n- basque\n- roberta\n---\n\n# Roberta-eus Euscrawl large cased\n\nThis is a RoBERTa model for Basque model presented in [Does corpus quality really matter for low-resource languages?]( There are several models for Basque using the RoBERTa architecture, using different corpora:\n\n- roberta-eus-euscrawl-base-cased: Basque RoBERTa model trained on Euscrawl, a corpus created using tailored crawling from Basque sites. EusCrawl contains 12,528k documents and 423M tokens.\n- roberta-eus-euscrawl-large-cased: RoBERTa large trained on EusCrawl.\n- roberta-eus-mC4-base-cased: Basque RoBERTa model trained on the Basque portion of mc4 dataset.\n- roberta-eus-CC100-base-cased: Basque RoBERTa model trained on  Basque portion of cc100 dataset.\n\nThe models have been tested on five different downstream tasks for Basque: Topic classification, Sentiment analysis, Stance detection, Named Entity Recognition (NER), and Question Answering (refer to the [paper]( for more details). See summary of results below:\n\n\n Topic class.  Stance det.      QA   \n-------------------------------------\n         76.2         57.4     34.6  \n     **77.6**         62.9  **38.3** \n         75.3         59.1     35.2  \n         76.2     **63.4**     35.8  \n\n\nIf you use any of these models, please cite the following paper:\n\n```\n@misc{artetxe2022euscrawl,\n title={Does corpus quality really matter for low-resource languages?},\n author={Mikel Artetxe, Itziar Aldabe, Rodrigo Agerri,\n         Olatz Perez-de-Vi\u00f1aspre, Aitor Soroa},\n year={2022},\n eprint={2203.08111},\n archivePrefix={arXiv},\n primaryClass={cs.CL}\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of ixa-ehu/roberta-eus-euscrawl-large-cased?", "answers": [{"text": "roberta", "answer_start": 56, "answer_end": 62}]}]}]}, {"title": "ixa-ehu/roberta-eus-mc4-base-cased", "paragraphs": [{"context": "---\nlanguage: eu\nlicense: cc-by-nc-4.0\ntags:\n- basque\n- roberta\n---\n\n# Roberta-eus mc4 base cased\n\nThis is a RoBERTa model for Basque model presented in [Does corpus quality really matter for low-resource languages?]( There are several models for Basque using the RoBERTa architecture, using different corpora:\n\n- roberta-eus-euscrawl-base-cased: Basque RoBERTa model trained on Euscrawl, a corpus created using tailored crawling from Basque sites. EusCrawl contains 12,528k documents and 423M tokens.\n- roberta-eus-euscrawl-large-cased: RoBERTa large trained on EusCrawl.\n- roberta-eus-mC4-base-cased: Basque RoBERTa model trained on the Basque portion of mc4 dataset.\n- roberta-eus-CC100-base-cased: Basque RoBERTa model trained on  Basque portion of cc100 dataset.\n\nThe models have been tested on five different downstream tasks for Basque: Topic classification, Sentiment analysis, Stance detection, Named Entity Recognition (NER), and Question Answering (refer to the [paper]( for more details). See summary of results below:\n\n\n Topic class.  Stance det.      QA   \n-------------------------------------\n         76.2         57.4     34.6  \n     **77.6**         62.9  **38.3** \n         75.3         59.1     35.2  \n         76.2     **63.4**     35.8  \n\n\nIf you use any of these models, please cite the following paper:\n\n```\n@misc{artetxe2022euscrawl,\n title={Does corpus quality really matter for low-resource languages?},\n author={Mikel Artetxe, Itziar Aldabe, Rodrigo Agerri,\n         Olatz Perez-de-Vi\u00f1aspre, Aitor Soroa},\n year={2022},\n eprint={2203.08111},\n archivePrefix={arXiv},\n primaryClass={cs.CL}\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of ixa-ehu/roberta-eus-mc4-base-cased?", "answers": [{"text": "roberta", "answer_start": 56, "answer_end": 62}]}]}]}, {"title": "RobertoMCA97/xlm-roberta-base-finetuned-panx-de-fr", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- f1\nmodel-index:\n- name: xlm-roberta-base-finetuned-panx-de-fr\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-finetuned-panx-de-fr\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1667\n- F1: 0.8582\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.1817          \n 2.0    0.1618          \n 3.0    0.1667          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of RobertoMCA97/xlm-roberta-base-finetuned-panx-de-fr?", "answers": [{"text": "xlm-roberta", "answer_start": 83, "answer_end": 93}]}]}]}, {"title": "RobertoMCA97/xlm-roberta-base-finetuned-panx-fr", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- xtreme\nmetrics:\n- f1\nmodel-index:\n- name: xlm-roberta-base-finetuned-panx-fr\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: xtreme\n      type: xtreme\n      args: PAN-X.fr\n    metrics:\n    - name: F1\n      type: f1\n      value: 0.8354854938789199\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-finetuned-panx-fr\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the xtreme dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2651\n- F1: 0.8355\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.3346          \n 2.0    0.2900          \n 3.0    0.2651          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of RobertoMCA97/xlm-roberta-base-finetuned-panx-fr?", "answers": [{"text": "xlm-roberta", "answer_start": 102, "answer_end": 112}]}, {"id": "q2", "question": "What is the model task of RobertoMCA97/xlm-roberta-base-finetuned-panx-fr?", "answers": [{"text": "token-classification", "answer_start": 203, "answer_end": 222}]}]}]}, {"title": "RobertoMCA97/xlm-roberta-base-finetuned-panx-it", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- xtreme\nmetrics:\n- f1\nmodel-index:\n- name: xlm-roberta-base-finetuned-panx-it\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: xtreme\n      type: xtreme\n      args: PAN-X.it\n    metrics:\n    - name: F1\n      type: f1\n      value: 0.822805578342904\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-finetuned-panx-it\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the xtreme dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2323\n- F1: 0.8228\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.3361          \n 2.0    0.2526          \n 3.0    0.2323          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of RobertoMCA97/xlm-roberta-base-finetuned-panx-it?", "answers": [{"text": "xlm-roberta", "answer_start": 102, "answer_end": 112}]}, {"id": "q2", "question": "What is the model task of RobertoMCA97/xlm-roberta-base-finetuned-panx-it?", "answers": [{"text": "token-classification", "answer_start": 203, "answer_end": 222}]}]}]}, {"title": "RobertoMCA97/xlm-roberta-base-finetuned-panx-en", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- xtreme\nmetrics:\n- f1\nmodel-index:\n- name: xlm-roberta-base-finetuned-panx-en\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: xtreme\n      type: xtreme\n      args: PAN-X.en\n    metrics:\n    - name: F1\n      type: f1\n      value: 0.7075365579302588\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-finetuned-panx-en\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the xtreme dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3925\n- F1: 0.7075\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5884          \n 2.0    0.4088          \n 3.0    0.3925          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of RobertoMCA97/xlm-roberta-base-finetuned-panx-en?", "answers": [{"text": "xlm-roberta", "answer_start": 102, "answer_end": 112}]}, {"id": "q2", "question": "What is the model task of RobertoMCA97/xlm-roberta-base-finetuned-panx-en?", "answers": [{"text": "token-classification", "answer_start": 203, "answer_end": 222}]}]}]}, {"title": "osanseviero/distilbert-base-uncased-finetuned-squad-d5716d28", "paragraphs": [{"context": "---\nlanguage:\n- en\nthumbnail: \ntags:\n- question-answering\nlicense: apache-2.0\ndatasets:\n- squad\nmetrics:\n- squad\nmodel-index:\n- name: osanseviero/distilbert-base-uncased-finetuned-squad-d5716d28\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: train\n    metrics:\n    - name: Loss\n      type: loss\n      value: 4.052208423614502\n      verified: true\n---\n\n# DistilBERT with a second step of distillation\n\n## Model description\n\nThis model replicates the \"DistilBERT (D)\" model from Table 2 of the [DistilBERT paper]( In this approach, a DistilBERT student is fine-tuned on SQuAD v1.1, but with a BERT model (also fine-tuned on SQuAD v1.1) acting as a teacher for a second step of task-specific distillation.\n\nIn this version, the following pre-trained models were used:\n\n* Student: `distilbert-base-uncased`\n* Teacher: `lewtun/bert-base-uncased-finetuned-squad-v1`\n\n## Training data\n\nThis model was trained on the SQuAD v1.1 dataset which can be obtained from the `datasets` library as follows:\n\n```python\nfrom datasets import load_dataset\nsquad = load_dataset('squad')\n```\n\n## Training procedure\n\n## Eval results\n\n Exact Match \n-------------\n 79.1        \n 78.4        \n\nThe scores were calculated using the `squad` metric from `datasets`.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{sanh2020distilbert,\n      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, \n      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n      year={2020},\n      eprint={1910.01108},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "qas": [{"id": "q1", "question": "What is the model architecture of osanseviero/distilbert-base-uncased-finetuned-squad-d5716d28?", "answers": [{"text": "distilbert", "answer_start": 146, "answer_end": 155}]}, {"id": "q2", "question": "What is the model task of osanseviero/distilbert-base-uncased-finetuned-squad-d5716d28?", "answers": [{"text": "question-answering", "answer_start": 39, "answer_end": 56}]}]}]}, {"title": "ai4bharat/MultiIndicParaphraseGeneration", "paragraphs": [{"context": "---\ntags:\n- paraphrase-generation\n- multilingual\n- nlp\n- indicnlp\ndatasets:\n- ai4bharat/IndicParaphrase\nlanguage:\n- as\n- bn\n- gu\n- hi\n- kn\n- ml\n- mr\n- or\n- pa\n- ta\n- te\nlicense:\n- mit\n\n\n---\n\n# MultiIndicParaphraseGeneration\n\nThis repository contains the [IndicBART]( checkpoint finetuned on the 11 languages of [IndicParaphrase]( dataset. For finetuning details,\nsee the [paper]( \n<ul>\n<li >Supported languages: Assamese, Bengali, Gujarati, Hindi, Marathi, Odiya, Punjabi, Kannada, Malayalam, Tamil, and Telugu. Not all of these languages are supported by mBART50 and mT5. </li>\n<li >The model is much smaller than the mBART and mT5(-base) models, so less computationally expensive for decoding. </li>\n<li> Trained on large Indic language corpora (5.53 million sentences). </li>\n<li> All languages, have been represented in Devanagari script to encourage transfer learning among the related languages. </li>\n</ul>\n\n\n\n## Using this model in `transformers`\n\n```\nfrom transformers import MBartForConditionalGeneration, AutoModelForSeq2SeqLM\nfrom transformers import AlbertTokenizer, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/MultiIndicParaphraseGeneration\", do_lower_case=False, use_fast=False, keep_accents=True)\n# Or use tokenizer = AlbertTokenizer.from_pretrained(\"ai4bharat/MultiIndicParaphraseGeneration\", do_lower_case=False, use_fast=False, keep_accents=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/MultiIndicParaphraseGeneration\")\n# Or use model = MBartForConditionalGeneration.from_pretrained(\"ai4bharat/MultiIndicParaphraseGeneration\")\n\n# Some initial mapping\nbos_id = tokenizer._convert_token_to_id_with_added_voc(\"<s>\")\neos_id = tokenizer._convert_token_to_id_with_added_voc(\"</s>\")\npad_id = tokenizer._convert_token_to_id_with_added_voc(\"<pad>\")\n\n# To get lang_id use any of ['<2as>', '<2bn>', '<2en>', '<2gu>', '<2hi>', '<2kn>', '<2ml>', '<2mr>', '<2or>', '<2pa>', '<2ta>', '<2te>']\n# First tokenize the input. The format below is how IndicBART was trained so the input should be \"Sentence </s> <2xx>\" where xx is the language code. Similarly, the output should be \"<2yy> Sentence </s>\".\ninp = tokenizer(\"\u0926\u093f\u0932\u094d\u0932\u0940 \u092f\u0942\u0928\u093f\u0935\u0930\u094d\u0938\u093f\u091f\u0940 \u0926\u0947\u0936 \u0915\u0940 \u092a\u094d\u0930\u0938\u093f\u0926\u094d\u0927 \u092f\u0942\u0928\u093f\u0935\u0930\u094d\u0938\u093f\u091f\u0940 \u092e\u0947\u0902 \u0938\u0947 \u090f\u0915 \u0939\u0948. </s> <2hi>\", add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids \n\n# For generation. Pardon the messiness. Note the decoder_start_token_id.\n\nmodel_output=model.generate(inp, use_cache=True,no_repeat_ngram_size=3,encoder_no_repeat_ngram_size=3, num_beams=4, max_length=20, min_length=1, early_stopping=True, pad_token_id=pad_id, bos_token_id=bos_id, eos_token_id=eos_id, decoder_start_token_id=tokenizer._convert_token_to_id_with_added_voc(\"<2hi>\"))\n\n# Decode to get output strings\ndecoded_output=tokenizer.decode(model_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(decoded_output) # \u0926\u093f\u0932\u094d\u0932\u0940 \u0935\u093f\u0936\u094d\u0935\u0935\u093f\u0926\u094d\u092f\u093e\u0932\u092f \u0926\u0947\u0936 \u0915\u0940 \u092a\u094d\u0930\u092e\u0941\u0916 \u0935\u093f\u0936\u094d\u0935\u0935\u093f\u0926\u094d\u092f\u093e\u0932\u092f\u094b\u0902 \u092e\u0947\u0902 \u0936\u093e\u092e\u093f\u0932 \u0939\u0948\u0964\n\n# Note that if your output language is not Hindi or Marathi, you should convert its script from Devanagari to the desired language using the Indic NLP Library.\n\n```\n# Note:\nIf you wish to use any language written in a non-Devanagari script, then you should first convert it to Devanagari using the <a href=\" NLP Library</a>. After you get the output, you should convert it back into the original script.\n\n## Benchmarks\n\nScores on the `IndicParaphrase` test sets are as follows:\n\nLanguage | BLEU / Self-BLEU / iBLEU\n---------|----------------------------\nas | 1.66 / 2.06 / 0.54\nbn | 11.57 / 1.69 / 7.59\ngu | 22.10 / 2.76 / 14.64\nhi | 27.29 / 2.87 / 18.24\nkn | 15.40 / 2.98 / 9.89\nml | 10.57 / 1.70 / 6.89\nmr | 20.38 / 2.20 / 13.61\nor | 19.26 / 2.10 / 12.85\npa | 14.87 / 1.35 / 10.00\nta | 18.52 / 2.88 / 12.10\nte | 16.70 / 3.34 / 10.69\n\n\n\n## Citation\n\nIf you use this model, please cite the following paper:\n```\n@inproceedings{Kumar2022IndicNLGSM,\n  title={IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages},\n  author={Aman Kumar and Himani Shrotriya and Prachi Sahu and Raj Dabre and Ratish Puduppully and Anoop Kunchukuttan and Amogh Mishra and Mitesh M. Khapra and Pratyush Kumar},\n  year={2022},\n  url = \"\n  }\n```\n", "qas": []}]}, {"title": "richardc7/electricidad-small-finetuned-amazon-review-classification", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- amazon_reviews_multi\nmetrics:\n- accuracy\nmodel-index:\n- name: electricidad-small-finetuned-amazon-review-classification\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: amazon_reviews_multi\n      type: amazon_reviews_multi\n      args: es\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.581\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# electricidad-small-finetuned-amazon-review-classification\n\nThis model is a fine-tuned version of [mrm8488/electricidad-small-discriminator]( on the amazon_reviews_multi dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9601\n- Accuracy: 0.581\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    1.0153          \n 2.0    0.9942          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q2", "question": "What is the model task of richardc7/electricidad-small-finetuned-amazon-review-classification?", "answers": [{"text": "text-classification", "answer_start": 232, "answer_end": 250}]}]}]}, {"title": "BigSalmon/InformalToFormalLincoln27", "paragraphs": [{"context": "```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"BigSalmon/InformalToFormalLincoln27\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"BigSalmon/InformalToFormalLincoln27\")\n\n```\n\n```\nHow To Make Prompt:\ninformal english: i am very ready to do that just that.\nTranslated into the Style of Abraham Lincoln: you can assure yourself of my readiness to work toward this end.\nTranslated into the Style of Abraham Lincoln: please be assured that i am most ready to undertake this laborious task.\n\n***\n\ninformal english: space is huge and needs to be explored.\nTranslated into the Style of Abraham Lincoln: space awaits traversal, a new world whose boundaries are endless.\nTranslated into the Style of Abraham Lincoln: space is a ( limitless / boundless ) expanse, a vast virgin domain awaiting exploration.\n\n***\n\ninformal english: corn fields are all across illinois, visible once you leave chicago.\nTranslated into the Style of Abraham Lincoln: corn fields ( permeate illinois / span the state of illinois / ( occupy / persist in ) all corners of illinois / line the horizon of illinois / envelop the landscape of illinois ), manifesting themselves visibly as one ventures beyond chicago.\n\ninformal english:\n```\n```\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\nText: failing to draw in the masses, the NBA has fallen into disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap solutions could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n\n***\n-\n\n```\n\n```\ninfill: chrome extensions [MASK] accomplish everyday tasks.\nTranslated into the Style of Abraham Lincoln: chrome extensions ( expedite the ability to / unlock the means to more readily ) accomplish everyday tasks.\n\ninfill: at a time when nintendo has become inflexible, [MASK] consoles that are tethered to a fixed iteration, sega diligently curates its legacy of classic video games on handheld devices.\nTranslated into the Style of Abraham Lincoln: at a time when nintendo has become inflexible, ( stubbornly [MASK] on / firmly set on / unyielding in its insistence on ) consoles that are tethered to a fixed iteration, sega diligently curates its legacy of classic video games on handheld devices.\n\ninfill:\n\n```\n```\nEssay Intro (Warriors vs. Rockets in Game 7):\ntext: eagerly anticipated by fans, game 7's are the highlight of the post-season.\ntext: ever-building in suspense, game 7's have the crowd captivated.\n\n***\n\nEssay Intro (South Korean TV Is Becoming Popular):\ntext: maturing into a bona fide paragon of programming, south korean television ( has much to offer / entertains without fail / never disappoints ).\ntext: increasingly held in critical esteem, south korean television continues to impress.\ntext: at the forefront of quality content, south korea is quickly achieving celebrity status.\n\n***\n\nEssay Intro (\n```\n\n```\nSearch: What is the definition of Checks and Balances?\n\n\nChecks and Balances is the idea of having a system where each and every action in government should be subject to one or more checks that would not allow one branch or the other to overly dominate.\n\n\nChecks and Balances is a system that allows each branch of government to limit the powers of the other branches in order to prevent abuse of power\n\n\nChecks and Balances is a system of separation through which branches of government can control the other, thus preventing excess power.\n\n***\n\nSearch: What is the definition of Separation of Powers?\n\n\nThe separation of powers is a principle in government, whereby governmental powers are separated into different branches, each with their own set of powers, that are prevent one branch from aggregating too much power.\n\n\nSeparation of Powers is the division of governmental functions between the executive, legislative and judicial branches, clearly demarcating each branch's authority, in the interest of ensuring that individual liberty or security is not undermined.\n\n***\n\nSearch: What is the definition of Connection of Powers?\n\n\nConnection of Powers is a feature of some parliamentary forms of government where different branches of government are intermingled, typically the executive and legislative branches.\n\n\nThe term Connection of Powers describes a system of government in which there is overlap between different parts of the government.\n\n***\nSearch: What is the definition of\n```\n\n```\nSearch: What are phrase synonyms for \"second-guess\"?\n\nShortest to Longest:\n- feel dubious about\n- raise an eyebrow at\n- wrinkle their noses at\n- cast a jaundiced eye at\n- teeter on the fence about\n\n***\n\nSearch: What are phrase synonyms for \"mean to newbies\"?\n\nShortest to Longest:\n- readiness to balk at rookies\n- absence of tolerance for novices\n- hostile attitude toward newcomers\n\n***\n\nSearch: What are phrase synonyms for \"make use of\"?\n\nShortest to Longest:\n- call upon\n- glean value from\n- reap benefits from\n- derive utility from\n- seize on the merits of\n- draw on the strength of\n- tap into the potential of\n\n***\n\nSearch: What are phrase synonyms for \"hurting itself\"?\n\nShortest to Longest:\n- erring\n- slighting itself\n- forfeiting its integrity\n- doing itself a disservice\n- evincing a lack of backbone\n\n***\n\nSearch: What are phrase synonyms for \"\n```\n```\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\ntext: failing to draw in the masses, the nba has ( fallen into / succumb to / bowed to ) disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap ( solutions / interventions / enhancements ) could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n\n***\n\n-\n```\n```\noriginal: sports teams are profitable for owners. [MASK], their valuations experience a dramatic uptick. \ninfill: sports teams are profitable for owners. ( accumulating vast sums / stockpiling treasure / realizing benefits / cashing in / registering robust financials / scoring on balance sheets ), their valuations experience a dramatic uptick. \n\n***\n\noriginal:\n```", "qas": []}]}, {"title": "ShahafAricha/nqg-gpt2", "paragraphs": [{"context": "---\nlicense: other\n---\n---\ndatasets:\n- squad\ntags:\n- question-generation\nwidget:\n- text: \"The Technikum was conceived in the early 1900s by the German-Jewish fund Ezrah as a school of [HL]engineering and sciences[HL].[SEP]\"\n---\n\n# Transformer QG on SQuAD\nHLQG is Proposed by [Ying-Hong Chan & Yao-Chung Fan. (2019). A Re-current BERT-based Model for Question Generation.](\n\n**This is a Reproduce Version from distilled squad dataset**\n\nMore detail: [p208p2002/Transformer-QG-on-SQuAD](\n\n## Usage\n### Input Format\n```\nC' = [c1, c2, ..., [HL], a1, ..., a, [HL], ..., c]\n```", "qas": []}]}, {"title": "Slavka/distil-bert-finetuned-log-parser-1", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: distil-bert-finetuned-log-parser-1\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# distil-bert-finetuned-log-parser-1\n\nThis model is a fine-tuned version of [distilbert-base-uncased-distilled-squad]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 33, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: mixed_float16\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- TensorFlow 2.8.0\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Slavka/distil-bert-finetuned-log-parser-1?", "answers": [{"text": "distilbert", "answer_start": 396, "answer_end": 405}]}]}]}, {"title": "Makinitas/DialoGPT-small-RickAndMortyScripts", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Rick And Morty DialoGPT Model\n", "qas": [{"id": "q2", "question": "What is the model task of Makinitas/DialoGPT-small-RickAndMortyScripts?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "darthrussel/DialoGPT-small-rickandmorty", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Rick and Morty DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of darthrussel/DialoGPT-small-rickandmorty?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "chocoduck/Joey_bot", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# My Awesome Model\n", "qas": [{"id": "q2", "question": "What is the model task of chocoduck/Joey_bot?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "razent/SciFive-large-Pubmed_PMC-MedNLI", "paragraphs": [{"context": "---\nlanguage: \n  - en\n\ntags:\n- text2text-generation\n- mednli\n\ndatasets:\n- pubmed\n- pmc/open_access\nwidget:\n- text: \"mednli: sentence1: In the ED, initial VS revealed T 98.9, HR 73, BP 121/90, RR 15, O2 sat 98% on RA. sentence2: The patient is hemodynamically stable\"\n---\n\n# SciFive Pubmed+PMC Large on MedNLI\n\n## Introduction\n\nFinetuned SciFive Pubmed+PMC Large model achieved state-of-the-art results on [MedNLI (Medical Natural Language Inference)](\n\nPaper: [SciFive: a text-to-text transformer model for biomedical literature](\n\nAuthors: _Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, Gr\u00e9goire Altan-Bonnet_\n\n## How to use\nFor more details, do check out [our Github repo]( \n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"razent/SciFive-large-Pubmed_PMC-MedNLI\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"razent/SciFive-large-Pubmed_PMC-MedNLI\")\nmodel.cuda()\n\u200b\nsent_1 = \"In the ED, initial VS revealed T 98.9, HR 73, BP 121/90, RR 15, O2 sat 98% on RA.\"\nsent_2 = \"The patient is hemodynamically stable\"\ntext =  f\"mednli: sentence1: {sent_1} sentence2: {sent_2}\"\n\nencoding = tokenizer.encode_plus(text, padding='max_length', max_length=256, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    max_length=8,\n    early_stopping=True\n)\n\nfor output in outputs:\n    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    print(line)\n```", "qas": [{"id": "q2", "question": "What is the model task of razent/SciFive-large-Pubmed_PMC-MedNLI?", "answers": [{"text": "text2text-generation", "answer_start": 31, "answer_end": 50}]}]}]}, {"title": "BigSalmon/InformalToFormalLincoln28", "paragraphs": [{"context": "```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"BigSalmon/InformalToFormalLincoln28\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"BigSalmon/InformalToFormalLincoln28\")\n\n```\n\n```\nHow To Make Prompt:\ninformal english: i am very ready to do that just that.\nTranslated into the Style of Abraham Lincoln: you can assure yourself of my readiness to work toward this end.\nTranslated into the Style of Abraham Lincoln: please be assured that i am most ready to undertake this laborious task.\n\n***\n\ninformal english: space is huge and needs to be explored.\nTranslated into the Style of Abraham Lincoln: space awaits traversal, a new world whose boundaries are endless.\nTranslated into the Style of Abraham Lincoln: space is a ( limitless / boundless ) expanse, a vast virgin domain awaiting exploration.\n\n***\n\ninformal english: corn fields are all across illinois, visible once you leave chicago.\nTranslated into the Style of Abraham Lincoln: corn fields ( permeate illinois / span the state of illinois / ( occupy / persist in ) all corners of illinois / line the horizon of illinois / envelop the landscape of illinois ), manifesting themselves visibly as one ventures beyond chicago.\n\ninformal english:\n```\n```\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\nText: failing to draw in the masses, the NBA has fallen into disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap solutions could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n\n***\n-\n\n```\n\n```\ninfill: chrome extensions [MASK] accomplish everyday tasks.\nTranslated into the Style of Abraham Lincoln: chrome extensions ( expedite the ability to / unlock the means to more readily ) accomplish everyday tasks.\n\ninfill: at a time when nintendo has become inflexible, [MASK] consoles that are tethered to a fixed iteration, sega diligently curates its legacy of classic video games on handheld devices.\nTranslated into the Style of Abraham Lincoln: at a time when nintendo has become inflexible, ( stubbornly [MASK] on / firmly set on / unyielding in its insistence on ) consoles that are tethered to a fixed iteration, sega diligently curates its legacy of classic video games on handheld devices.\n\ninfill:\n\n```\n```\nEssay Intro (Warriors vs. Rockets in Game 7):\ntext: eagerly anticipated by fans, game 7's are the highlight of the post-season.\ntext: ever-building in suspense, game 7's have the crowd captivated.\n\n***\n\nEssay Intro (South Korean TV Is Becoming Popular):\ntext: maturing into a bona fide paragon of programming, south korean television ( has much to offer / entertains without fail / never disappoints ).\ntext: increasingly held in critical esteem, south korean television continues to impress.\ntext: at the forefront of quality content, south korea is quickly achieving celebrity status.\n\n***\n\nEssay Intro (\n```\n\n```\nSearch: What is the definition of Checks and Balances?\n\n\nChecks and Balances is the idea of having a system where each and every action in government should be subject to one or more checks that would not allow one branch or the other to overly dominate.\n\n\nChecks and Balances is a system that allows each branch of government to limit the powers of the other branches in order to prevent abuse of power\n\n\nChecks and Balances is a system of separation through which branches of government can control the other, thus preventing excess power.\n\n***\n\nSearch: What is the definition of Separation of Powers?\n\n\nThe separation of powers is a principle in government, whereby governmental powers are separated into different branches, each with their own set of powers, that are prevent one branch from aggregating too much power.\n\n\nSeparation of Powers is the division of governmental functions between the executive, legislative and judicial branches, clearly demarcating each branch's authority, in the interest of ensuring that individual liberty or security is not undermined.\n\n***\n\nSearch: What is the definition of Connection of Powers?\n\n\nConnection of Powers is a feature of some parliamentary forms of government where different branches of government are intermingled, typically the executive and legislative branches.\n\n\nThe term Connection of Powers describes a system of government in which there is overlap between different parts of the government.\n\n***\nSearch: What is the definition of\n```\n\n```\nSearch: What are phrase synonyms for \"second-guess\"?\n\nShortest to Longest:\n- feel dubious about\n- raise an eyebrow at\n- wrinkle their noses at\n- cast a jaundiced eye at\n- teeter on the fence about\n\n***\n\nSearch: What are phrase synonyms for \"mean to newbies\"?\n\nShortest to Longest:\n- readiness to balk at rookies\n- absence of tolerance for novices\n- hostile attitude toward newcomers\n\n***\n\nSearch: What are phrase synonyms for \"make use of\"?\n\nShortest to Longest:\n- call upon\n- glean value from\n- reap benefits from\n- derive utility from\n- seize on the merits of\n- draw on the strength of\n- tap into the potential of\n\n***\n\nSearch: What are phrase synonyms for \"hurting itself\"?\n\nShortest to Longest:\n- erring\n- slighting itself\n- forfeiting its integrity\n- doing itself a disservice\n- evincing a lack of backbone\n\n***\n\nSearch: What are phrase synonyms for \"\n```\n```\n- declining viewership facing the nba.\n- does not have to be this way.\n- in fact, many solutions exist.\n- the four point line would surely draw in eyes.\ntext: failing to draw in the masses, the nba has ( fallen into / succumb to / bowed to ) disrepair. such does not have to be the case, however. in fact, a myriad of simple, relatively cheap ( solutions / interventions / enhancements ) could revive the league. the addition of the much-hyped four-point line would surely juice viewership.\n\n***\n\n-\n```\n```\noriginal: sports teams are profitable for owners. [MASK], their valuations experience a dramatic uptick. \ninfill: sports teams are profitable for owners. ( accumulating vast sums / stockpiling treasure / realizing benefits / cashing in / registering robust financials / scoring on balance sheets ), their valuations experience a dramatic uptick. \n\n***\n\noriginal:\n```", "qas": []}]}, {"title": "KBLab/megatron-bert-large-swedish-cased-165k", "paragraphs": [{"context": "---\nlanguage:\n- sv\n\n---\n\n# Megatron-BERT-large Swedish 165k\n\nThis BERT model was trained using the Megatron-LM library.\nThe size of the model is a regular BERT-large with 340M parameters.\nThe model was trained on about 70GB of data, consisting mostly of OSCAR and Swedish newspaper text curated by the National Library of Sweden.\n\nTraining was done for 165k training steps using a batch size of 8k; the number of training steps is set to 500k, meaning that this version is a checkpoint.\nThe hyperparameters for training followed the setting for RoBERTa.\n\n\nThe model has three sister models trained on the same dataset:\n- [\ud83e\udd17 BERT Swedish](\n- [Megatron-BERT-base-600k](\n- [Megatron-BERT-base-125k](\n\nand an earlier checkpoint\n- [Megatron-BERT-large-110k](\n\n## Acknowledgements\n\nWe gratefully acknowledge the HPC RIVR consortium ( and EuroHPC JU ( for funding this research by providing computing resources of the HPC system Vega at the Institute of Information Science (", "qas": []}]}, {"title": "Dahn/wav2vec2-base-timit-demo-colab", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: wav2vec2-base-timit-demo-colab\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-timit-demo-colab\n\nThis model is a fine-tuned version of [facebook/wav2vec2-base]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4796\n- Wer: 0.3434\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 4.0    1.3259          \n 8.0    0.4682          \n 12.0   0.4490          \n 16.0   0.4595          \n 20.0   0.4819          \n 24.0   0.4524          \n 28.0   0.4796          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Dahn/wav2vec2-base-timit-demo-colab?", "answers": [{"text": "wav2vec2", "answer_start": 76, "answer_end": 83}]}]}]}, {"title": "Yaxin/xlm-roberta-base-conll2003-ner", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- conll2003\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: test-conll2003-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      args: conll2003\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.9459188783174762\n    - name: Recall\n      type: recall\n      value: 0.9537192864355436\n    - name: F1\n      type: f1\n      value: 0.94980306712478\n    - name: Accuracy\n      type: accuracy\n      value: 0.9911218410498034\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# test-conll2003-ner\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the conll2003 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0470\n- Precision: 0.9459\n- Recall: 0.9537\n- F1: 0.9498\n- Accuracy: 0.9911\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- Pytorch 1.10.0\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Yaxin/xlm-roberta-base-conll2003-ner?", "answers": [{"text": "xlm-roberta", "answer_start": 869, "answer_end": 879}]}, {"id": "q2", "question": "What is the model task of Yaxin/xlm-roberta-base-conll2003-ner?", "answers": [{"text": "token-classification", "answer_start": 222, "answer_end": 241}]}]}]}, {"title": "Gare/opus-mt-en-ro-finetuned-en-to-ro", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- wmt16\nmetrics:\n- bleu\nmodel-index:\n- name: opus-mt-en-ro-finetuned-en-to-ro\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: wmt16\n      type: wmt16\n      args: ro-en\n    metrics:\n    - name: Bleu\n      type: bleu\n      value: 28.0527\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# opus-mt-en-ro-finetuned-en-to-ro\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-en-ro]( on the wmt16 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.2878\n- Bleu: 28.0527\n- Gen Len: 34.079\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Gen Len |\n:-----::---------------::-------:|\n 1.0    1.2878           34.079  |\n\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- Pytorch 1.11.0\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q2", "question": "What is the model task of Gare/opus-mt-en-ro-finetuned-en-to-ro?", "answers": [{"text": "text2text-generation", "answer_start": 227, "answer_end": 246}]}]}]}, {"title": "Rocketknight1/mt5-small-finetuned-amazon-en-es", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/mt5-small-finetuned-amazon-en-es\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/mt5-small-finetuned-amazon-en-es\n\nThis model is a fine-tuned version of [google/mt5-small]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 10.2613\n- Validation Loss: 4.5342\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 5.6e-05, 'decay_steps': 9672, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: mixed_float16\n\n### Training results\n\n Validation Loss \n:---------------:\n 4.5342          \n\n\n### Framework versions\n\n- Transformers 4.24.0.dev0\n- TensorFlow 2.10.0\n- Datasets 2.6.1\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/mt5-small-finetuned-amazon-en-es?", "answers": [{"text": "mt5", "answer_start": 97, "answer_end": 99}]}]}]}, {"title": "Rocketknight1/temp-colab-upload-test", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/temp-colab-upload-test\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/temp-colab-upload-test\n\nThis model is a fine-tuned version of [distilbert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.5386\n- Validation Loss: 0.0000\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 0.0000          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- TensorFlow 2.8.0\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/temp-colab-upload-test?", "answers": [{"text": "distilbert", "answer_start": 400, "answer_end": 409}]}]}]}, {"title": "Rocketknight1/temp-colab-upload-test2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/temp-colab-upload-test2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/temp-colab-upload-test2\n\nThis model is a fine-tuned version of [distilbert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.6931\n- Validation Loss: 0.6931\n- Epoch: 1\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 0.6931          \n 0.6931          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- TensorFlow 2.8.0\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/temp-colab-upload-test2?", "answers": [{"text": "distilbert", "answer_start": 402, "answer_end": 411}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-base-uk-ces_slk", "paragraphs": [{"context": "---\nlanguage:\n- cs\n- sk\n- uk\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-base-uk-ces_slk\n  results:\n  - task:\n      name: Translation ukr-ces\n      type: translation\n      args: ukr-ces\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: ukr ces devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 23.0\n  - task:\n      name: Translation ukr-slk\n      type: translation\n      args: ukr-slk\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: ukr slk devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 22.1\n  - task:\n      name: Translation ukr-ces\n      type: translation\n      args: ukr-ces\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: ukr-ces\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 54.2\n---\n# opus-mt-tc-base-uk-ces_slk\n\nNeural machine translation model for translating from Ukrainian (uk) to Czech and Slovak (cs+sk).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-17\n* source language(s): ukr\n* target language(s): ces\n* valid target language labels: >>ces<< >>slk<<\n* model: transformer-align\n* data: opusTCv20210807+pft ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807+pft_transformer-align_2022-03-17.zip](\n* more information released models: [OPUS-MT ukr-ces+slk README](\n* more information about the model: [MarianMT](\n\nThis is a multilingual translation model with multiple target languages. A sentence initial language token is required in the form of `>>id<<` (id = valid target language ID), e.g. `>>ces<<`\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \">>ces<< \u0410 \u0447\u043e\u0433\u043e \u0442\u0430\u043a?\",\n    \">>ces<< \u042f \u0437\u0430\u0433\u0443\u0431\u0438\u0432 \u0441\u0432\u043e\u0457 \u043e\u043a\u0443\u043b\u044f\u0440\u0438.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-base-uk-ces_slk\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     Pro\u010d to tak je?\n#     Ztratil jsem br\u00fdle.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-base-uk-ces_slk\")\nprint(pipe(\">>ces<< \u0410 \u0447\u043e\u0433\u043e \u0442\u0430\u043a?\"))\n\n# expected output: Pro\u010d to tak je?\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807+pft_transformer-align_2022-03-17.test.txt](\n* test set scores: [opusTCv20210807+pft_transformer-align_2022-03-17.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n tatoeba-test-v2021-08-07  54.2  8550 |\n flores101-devtest  23.0  22101 |\n flores101-devtest  22.1  22543 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: f084bad\n* port time: Wed Mar 23 21:54:02 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-base-uk-ces_slk?", "answers": [{"text": "translation", "answer_start": 37, "answer_end": 47}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-base-uk-hu", "paragraphs": [{"context": "---\nlanguage:\n- hu\n- uk\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-base-uk-hu\n  results:\n  - task:\n      name: Translation ukr-hun\n      type: translation\n      args: ukr-hun\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: ukr hun devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 20.2\n  - task:\n      name: Translation ukr-hun\n      type: translation\n      args: ukr-hun\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: ukr-hun\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 44.0\n---\n# opus-mt-tc-base-uk-hu\n\nNeural machine translation model for translating from Ukrainian (uk) to Hungarian (hu).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-08\n* source language(s): ukr\n* target language(s): hun\n* model: transformer-align\n* data: opusTCv20210807+pft ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807+pft_transformer-align_2022-03-08.zip](\n* more information released models: [OPUS-MT ukr-hun README](\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \"\u042f \u0442\u043e\u0431\u0456 \u0432\u0438\u043d\u043d\u0438\u0439 1000 \u0434\u043e\u043b\u0430\u0440\u0456\u0432.\",\n    \"\u042f \u043f'\u044e \u0432\u043e\u0434\u0443.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-base-uk-hu\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     1000 doll\u00e1r a te hib\u00e1d.\n#     Vizet iszom.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-base-uk-hu\")\nprint(pipe(\"\u042f \u0442\u043e\u0431\u0456 \u0432\u0438\u043d\u043d\u0438\u0439 1000 \u0434\u043e\u043b\u0430\u0440\u0456\u0432.\"))\n\n# expected output: 1000 doll\u00e1r a te hib\u00e1d.\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807+pft_transformer-align_2022-03-08.test.txt](\n* test set scores: [opusTCv20210807+pft_transformer-align_2022-03-08.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n tatoeba-test-v2021-08-07  44.0  2472 |\n flores101-devtest  20.2  22183 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: f084bad\n* port time: Wed Mar 23 21:54:12 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-base-uk-hu?", "answers": [{"text": "translation", "answer_start": 32, "answer_end": 42}]}]}]}, {"title": "Yaxin/xlm-roberta-base-yelp-mlm", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- yelp_review_full\nmetrics:\n- accuracy\nmodel-index:\n- name: xlm-roberta-base-yelp-mlm\n  results:\n  - task:\n      name: Masked Language Modeling\n      type: fill-mask\n    dataset:\n      name: yelp_review_full yelp_review_full\n      type: yelp_review_full\n      args: yelp_review_full\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.7356223359340127\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-yelp-mlm\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the yelp_review_full yelp_review_full dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1743\n- Accuracy: 0.7356\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- Pytorch 1.10.0\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of Yaxin/xlm-roberta-base-yelp-mlm?", "answers": [{"text": "xlm-roberta", "answer_start": 118, "answer_end": 128}]}, {"id": "q2", "question": "What is the model task of Yaxin/xlm-roberta-base-yelp-mlm?", "answers": [{"text": "fill-mask", "answer_start": 214, "answer_end": 222}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-base-uk-fi", "paragraphs": [{"context": "---\nlanguage:\n- fi\n- uk\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-base-uk-fi\n  results:\n  - task:\n      name: Translation ukr-fin\n      type: translation\n      args: ukr-fin\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: ukr fin devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 19.6\n---\n# opus-mt-tc-base-uk-fi\n\nNeural machine translation model for translating from Ukrainian (uk) to Finnish (fi).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-17\n* source language(s): ukr\n* target language(s): fin\n* model: transformer-align\n* data: opusTCv20210807+pft+pbt ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807+pft+pbt_transformer-align_2022-03-17.zip](\n* more information released models: [OPUS-MT ukr-fin README](\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \"\u0410\u0444\u0440\u0438\u043a\u0430 \u0454 \u043a\u043e\u043b\u0438\u0441\u043a\u043e\u044e \u043b\u044e\u0434\u0441\u0442\u0432\u0430.\",\n    \"\u041e\u0434\u0438\u043d, \u0434\u0432\u0430, \u0442\u0440\u0438, \u0447\u043e\u0442\u0438\u0440\u0438, \u043f'\u044f\u0442\u044c, \u0448\u0456\u0441\u0442\u044c, \u0441\u0456\u043c, \u0432\u0456\u0441\u0456\u043c, \u0434\u0435\u0432'\u044f\u0442\u044c, \u0434\u0435\u0441\u044f\u0442\u044c.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-base-uk-fi\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     Afrikka on ihmiskunnan kehto.\n#     Yksi, kaksi, kolme, nelj\u00e4, viisi, kuusi, seitsem\u00e4n, kahdeksan, yhdeks\u00e4n, kymmenen.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-base-uk-fi\")\nprint(pipe(\"\u0410\u0444\u0440\u0438\u043a\u0430 \u0454 \u043a\u043e\u043b\u0438\u0441\u043a\u043e\u044e \u043b\u044e\u0434\u0441\u0442\u0432\u0430.\"))\n\n# expected output: Afrikka on ihmiskunnan kehto.\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807+pft+pbt_transformer-align_2022-03-17.test.txt](\n* test set scores: [opusTCv20210807+pft+pbt_transformer-align_2022-03-17.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n flores101-devtest  19.6  18781 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: 1bdabf7\n* port time: Thu Mar 24 09:10:42 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-base-uk-fi?", "answers": [{"text": "translation", "answer_start": 32, "answer_end": 42}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-big-zle-fi", "paragraphs": [{"context": "---\nlanguage:\n- fi\n- ru\n- uk\n- zle\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-big-zle-fi\n  results:\n  - task:\n      name: Translation rus-fin\n      type: translation\n      args: rus-fin\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: rus fin devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 17.4\n  - task:\n      name: Translation ukr-fin\n      type: translation\n      args: ukr-fin\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: ukr fin devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 18.0\n  - task:\n      name: Translation rus-fin\n      type: translation\n      args: rus-fin\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: rus-fin\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 42.2\n---\n# opus-mt-tc-big-zle-fi\n\nNeural machine translation model for translating from East Slavic languages (zle) to Finnish (fi).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-07\n* source language(s): rus ukr\n* target language(s): fin\n* model: transformer-big\n* data: opusTCv20210807+bt ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807+bt_transformer-big_2022-03-07.zip](\n* more information released models: [OPUS-MT zle-fin README](\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \"\u041c\u044b \u0443\u0436\u0435 \u043f\u0440\u043e\u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043b\u0438.\",\n    \"\u041e\u0434\u0438\u043d, \u0434\u0432\u0430, \u0442\u0440\u0438, \u0447\u043e\u0442\u0438\u0440\u0438, \u043f'\u044f\u0442\u044c, \u0448\u0456\u0441\u0442\u044c, \u0441\u0456\u043c, \u0432\u0456\u0441\u0456\u043c, \u0434\u0435\u0432'\u044f\u0442\u044c, \u0434\u0435\u0441\u044f\u0442\u044c.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-big-zle-fi\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     Olemme jo \u00e4\u00e4nest\u00e4neet.\n#     Yksi, kaksi, kolme, nelj\u00e4, viisi, kuusi, seitsem\u00e4n, kahdeksan, yhdeks\u00e4n, kymmenen.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-zle-fi\")\nprint(pipe(\"\u041c\u044b \u0443\u0436\u0435 \u043f\u0440\u043e\u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043b\u0438.\"))\n\n# expected output: Olemme jo \u00e4\u00e4nest\u00e4neet.\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807+bt_transformer-big_2022-03-07.test.txt](\n* test set scores: [opusTCv20210807+bt_transformer-big_2022-03-07.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n tatoeba-test-v2021-08-07  42.2  19319 |\n flores101-devtest  17.4  18781 |\n flores101-devtest  18.0  18781 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: 42126b6\n* port time: Thu Mar 24 09:28:52 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-big-zle-fi?", "answers": [{"text": "translation", "answer_start": 43, "answer_end": 53}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-big-zle-es", "paragraphs": [{"context": "---\nlanguage:\n- be\n- es\n- ru\n- rue\n- uk\n- zle\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-big-zle-es\n  results:\n  - task:\n      name: Translation rus-spa\n      type: translation\n      args: rus-spa\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: rus spa devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 22.5\n  - task:\n      name: Translation ukr-spa\n      type: translation\n      args: ukr-spa\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: ukr spa devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 22.7\n  - task:\n      name: Translation bel-spa\n      type: translation\n      args: bel-spa\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: bel-spa\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 46.3\n  - task:\n      name: Translation rus-spa\n      type: translation\n      args: rus-spa\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: rus-spa\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 52.3\n  - task:\n      name: Translation ukr-spa\n      type: translation\n      args: ukr-spa\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: ukr-spa\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 51.6\n  - task:\n      name: Translation rus-spa\n      type: translation\n      args: rus-spa\n    dataset:\n      name: newstest2012\n      type: wmt-2012-news\n      args: rus-spa\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 29.0\n  - task:\n      name: Translation rus-spa\n      type: translation\n      args: rus-spa\n    dataset:\n      name: newstest2013\n      type: wmt-2013-news\n      args: rus-spa\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 31.7\n---\n# opus-mt-tc-big-zle-es\n\nNeural machine translation model for translating from East Slavic languages (zle) to Spanish (es).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-23\n* source language(s): bel rue rus ukr\n* target language(s): spa\n* model: transformer-big\n* data: opusTCv20210807 ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807_transformer-big_2022-03-23.zip](\n* more information released models: [OPUS-MT zle-spa README](\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \"\u0422\u043e\u043c \u0431\u0443\u0432 \u043f'\u044f\u043d\u0438\u0447\u043a\u043e\u044e.\",\n    \"\u041e\u043d \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0432\u0437\u0440\u043e\u0441\u043b\u044b\u0439, \u0447\u0442\u043e\u0431\u044b \u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u043e\u0434\u043d\u043e\u043c\u0443.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-big-zle-es\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     Tom era un borracho.\n#     Es lo suficientemente mayor como para viajar solo.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-zle-es\")\nprint(pipe(\"\u0422\u043e\u043c \u0431\u0443\u0432 \u043f'\u044f\u043d\u0438\u0447\u043a\u043e\u044e.\"))\n\n# expected output: Tom era un borracho.\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807_transformer-big_2022-03-23.test.txt](\n* test set scores: [opusTCv20210807_transformer-big_2022-03-23.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n tatoeba-test-v2021-08-07  46.3  1412 |\n tatoeba-test-v2021-08-07  52.3  75246 |\n tatoeba-test-v2021-08-07  51.6  59284 |\n flores101-devtest  14.1  29199 |\n flores101-devtest  22.5  29199 |\n flores101-devtest  22.7  29199 |\n newstest2012  29.0  79006 |\n newstest2013  31.7  70528 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: 1bdabf7\n* port time: Thu Mar 24 00:12:49 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-big-zle-es?", "answers": [{"text": "translation", "answer_start": 54, "answer_end": 64}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-base-hu-uk", "paragraphs": [{"context": "---\nlanguage:\n- hu\n- uk\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-base-hu-uk\n  results:\n  - task:\n      name: Translation hun-ukr\n      type: translation\n      args: hun-ukr\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: hun-ukr\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 38.1\n---\n# opus-mt-tc-base-hu-uk\n\nNeural machine translation model for translating from Hungarian (hu) to Ukrainian (uk).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-08\n* source language(s): hun\n* target language(s): ukr\n* model: transformer-align\n* data: opusTCv20210807+pbt ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807+pbt_transformer-align_2022-03-08.zip](\n* more information released models: [OPUS-MT hun-ukr README](\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \"1000 doll\u00e1rral tartozom neked.\",\n    \"Vizet iszom.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-base-hu-uk\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     \u042f \u0437\u043e\u0431\u043e\u0432'\u044f\u0437\u0430\u043d\u0438\u0439 \u0432\u0430\u043c 1000 \u0434\u043e\u043b\u0430\u0440\u0456\u0432.\n#     \u042f \u043f'\u044e \u0432\u043e\u0434\u0443.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-base-hu-uk\")\nprint(pipe(\"1000 doll\u00e1rral tartozom neked.\"))\n\n# expected output: \u042f \u0437\u043e\u0431\u043e\u0432'\u044f\u0437\u0430\u043d\u0438\u0439 \u0432\u0430\u043c 1000 \u0434\u043e\u043b\u0430\u0440\u0456\u0432.\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807+pbt_transformer-align_2022-03-08.test.txt](\n* test set scores: [opusTCv20210807+pbt_transformer-align_2022-03-08.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n tatoeba-test-v2021-08-07  38.1  2606 |\n flores101-devtest  19.8  22810 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: 1bdabf7\n* port time: Thu Mar 24 02:19:16 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-base-hu-uk?", "answers": [{"text": "translation", "answer_start": 32, "answer_end": 42}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-big-pt-zle", "paragraphs": [{"context": "---\nlanguage:\n- pt\n- ru\n- uk\n- zle\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-big-pt-zle\n  results:\n  - task:\n      name: Translation por-rus\n      type: translation\n      args: por-rus\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: por rus devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 26.8\n  - task:\n      name: Translation por-ukr\n      type: translation\n      args: por-ukr\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: por ukr devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 25.1\n  - task:\n      name: Translation por-rus\n      type: translation\n      args: por-rus\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: por-rus\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 47.6\n  - task:\n      name: Translation por-ukr\n      type: translation\n      args: por-ukr\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: por-ukr\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 44.7\n---\n# opus-mt-tc-big-pt-zle\n\nNeural machine translation model for translating from Portuguese (pt) to East Slavic languages (zle).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-23\n* source language(s): por\n* target language(s): rus ukr\n* valid target language labels: >>rus<< >>ukr<<\n* model: transformer-big\n* data: opusTCv20210807 ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807_transformer-big_2022-03-23.zip](\n* more information released models: [OPUS-MT por-zle README](\n* more information about the model: [MarianMT](\n\nThis is a multilingual translation model with multiple target languages. A sentence initial language token is required in the form of `>>id<<` (id = valid target language ID), e.g. `>>rus<<`\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \">>ukr<< Esse \u00e9 o meu lugar.\",\n    \">>rus<< Tom tem problemas de sa\u00fade.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-big-pt-zle\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     \u0426\u0435 \u043c\u043e\u0454 \u043c\u0456\u0441\u0446\u0435.\n#     \u0423 \u0422\u043e\u043c\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u044c\u0435\u043c.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-pt-zle\")\nprint(pipe(\">>ukr<< Esse \u00e9 o meu lugar.\"))\n\n# expected output: \u0426\u0435 \u043c\u043e\u0454 \u043c\u0456\u0441\u0446\u0435.\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807_transformer-big_2022-03-23.test.txt](\n* test set scores: [opusTCv20210807_transformer-big_2022-03-23.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n tatoeba-test-v2021-08-07  47.6  65326 |\n tatoeba-test-v2021-08-07  44.7  18933 |\n flores101-devtest  26.8  23295 |\n flores101-devtest  25.1  22810 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: 1bdabf7\n* port time: Thu Mar 24 03:20:20 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-big-pt-zle?", "answers": [{"text": "translation", "answer_start": 43, "answer_end": 53}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-base-ro-uk", "paragraphs": [{"context": "---\nlanguage:\n- ro\n- uk\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-base-ro-uk\n  results:\n  - task:\n      name: Translation ron-ukr\n      type: translation\n      args: ron-ukr\n    dataset:\n      name: flores101-devtest\n      type: flores_101\n      args: ron ukr devtest\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 22.3\n---\n# opus-mt-tc-base-ro-uk\n\nNeural machine translation model for translating from Romanian (ro) to Ukrainian (uk).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-08\n* source language(s): \n* target language(s): \n* valid target language labels: \n* model: transformer-align\n* data: opusTCv20210807+pbt ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807+pbt_transformer-align_2022-03-08.zip](\n* more information released models: [OPUS-MT ron-ukr README](\n* more information about the model: [MarianMT](\n\nThis is a multilingual translation model with multiple target languages. A sentence initial language token is required in the form of `>>id<<` (id = valid target language ID), e.g. `>><<`\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \"Articolul exprim\u0103 opinia personal\u0103 a autorului.\",\n    \"Ornitorincii tr\u0103iesc \u00een estul Austriei.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-base-ro-uk\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     \u0421\u0442\u0430\u0442\u0442\u044f \u0432\u0438\u0441\u043b\u043e\u0432\u043b\u044e\u0454 \u043e\u0441\u043e\u0431\u0438\u0441\u0442\u0443 \u0434\u0443\u043c\u043a\u0443 \u0430\u0432\u0442\u043e\u0440\u0430.\n#     \u041e\u0440\u043d\u0456\u0442\u043e\u0440\u0456\u043d\u0446\u0456 \u0436\u0438\u0432\u0443\u0442\u044c \u043d\u0430 \u0441\u0445\u043e\u0434\u0456 \u0410\u0432\u0441\u0442\u0440\u0456\u0457.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-base-ro-uk\")\nprint(pipe(\"Articolul exprim\u0103 opinia personal\u0103 a autorului.\"))\n\n# expected output: \u0421\u0442\u0430\u0442\u0442\u044f \u0432\u0438\u0441\u043b\u043e\u0432\u043b\u044e\u0454 \u043e\u0441\u043e\u0431\u0438\u0441\u0442\u0443 \u0434\u0443\u043c\u043a\u0443 \u0430\u0432\u0442\u043e\u0440\u0430.\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807+pbt_transformer-align_2022-03-08.test.txt](\n* test set scores: [opusTCv20210807+pbt_transformer-align_2022-03-08.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n flores101-devtest  22.3  22810 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: 1bdabf7\n* port time: Thu Mar 24 03:30:40 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-base-ro-uk?", "answers": [{"text": "translation", "answer_start": 32, "answer_end": 42}]}]}]}, {"title": "Helsinki-NLP/opus-mt-tc-base-tr-uk", "paragraphs": [{"context": "---\nlanguage:\n- tr\n- uk\ntags:\n- translation\n- opus-mt-tc\nlicense: cc-by-4.0\nmodel-index:\n- name: opus-mt-tc-base-tr-uk\n  results:\n  - task:\n      name: Translation tur-ukr\n      type: translation\n      args: tur-ukr\n    dataset:\n      name: tatoeba-test-v2021-08-07\n      type: tatoeba_mt\n      args: tur-ukr\n    metrics:\n    - name: BLEU\n      type: bleu\n      value: 40.5\n---\n# opus-mt-tc-base-tr-uk\n\nNeural machine translation model for translating from Turkish (tr) to Ukrainian (uk).\n\nThis model is part of the [OPUS-MT project]( an effort to make neural machine translation models widely available and accessible for many languages in the world. All models are originally trained using the amazing framework of [Marian NMT]( an efficient NMT implementation written in pure C++. The models have been converted to pyTorch using the transformers library by huggingface. Training data is taken from [OPUS]( and training pipelines use the procedures of [OPUS-MT-train](\n\n* Publications: [OPUS-MT \u2013 Building open translation services for the World]( and [The Tatoeba Translation Challenge \u2013 Realistic Data Sets for Low Resource and Multilingual MT]( (Please, cite if you use this model.)\n\n```\n@inproceedings{tiedemann-thottingal-2020-opus,\n    title = \"{OPUS}-{MT} {--} Building open translation services for the World\",\n    author = {Tiedemann, J{\\\"o}rg  and Thottingal, Santhosh},\n    booktitle = \"Proceedings of the 22nd Annual Conference of the European Association for Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Lisboa, Portugal\",\n    publisher = \"European Association for Machine Translation\",\n    url = \"\n    pages = \"479--480\",\n}\n\n@inproceedings{tiedemann-2020-tatoeba,\n    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n    author = {Tiedemann, J{\\\"o}rg},\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"\n    pages = \"1174--1182\",\n}\n```\n\n## Model info\n\n* Release: 2022-03-07\n* source language(s): \n* target language(s): ukr\n* model: transformer-align\n* data: opusTCv20210807+pbt ([source](\n* tokenization: SentencePiece (spm32k,spm32k)\n* original model: [opusTCv20210807+pbt_transformer-align_2022-03-07.zip](\n* more information released models: [OPUS-MT tur-ukr README](\n\n## Usage\n\nA short example code:\n\n```python\nfrom transformers import MarianMTModel, MarianTokenizer\n\nsrc_text = [\n    \"1000 yen yeterli mi?\",\n    \"Z\u00fcrih, \u0130svi\u00e7re'de bir \u015fehirdir.\"\n]\n\nmodel_name = \"pytorch-models/opus-mt-tc-base-tr-uk\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n\nfor t in translated:\n    print( tokenizer.decode(t, skip_special_tokens=True) )\n\n# expected output:\n#     \u0427\u0438 \u0434\u043e\u0441\u0442\u0430\u0442\u043d\u044c\u043e 1000 \u0456\u0454\u043d?\n#     \u0426\u044e\u0440\u0438\u0445 - \u043c\u0456\u0441\u0442\u043e \u0432 \u0428\u0432\u0435\u0439\u0446\u0430\u0440\u0456\u0457.\n```\n\nYou can also use OPUS-MT models with the transformers pipelines, for example:\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-base-tr-uk\")\nprint(pipe(\"1000 yen yeterli mi?\"))\n\n# expected output: \u0427\u0438 \u0434\u043e\u0441\u0442\u0430\u0442\u043d\u044c\u043e 1000 \u0456\u0454\u043d?\n```\n\n## Benchmarks\n\n* test set translations: [opusTCv20210807+pbt_transformer-align_2022-03-07.test.txt](\n* test set scores: [opusTCv20210807+pbt_transformer-align_2022-03-07.eval.txt](\n* benchmark results: [benchmark_results.txt](benchmark_results.txt)\n* benchmark output: [benchmark_translations.zip](benchmark_translations.zip)\n\n testset  BLEU   #words |\n------------------------|\n tatoeba-test-v2021-08-07  40.5  13079 |\n flores101-devtest  19.9  22810 |\n\n## Acknowledgements\n\nThe work is supported by the [European Language Grid]( as [pilot project 2866]( by the [FoTran project]( funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 771113), and the [MeMAD project]( funded by the European Union\u2019s Horizon 2020 Research and Innovation Programme under grant agreement No 780069. We are also grateful for the generous computational resources and IT infrastructure provided by [CSC -- IT Center for Science]( Finland.\n\n## Model conversion info\n\n* transformers version: 4.16.2\n* OPUS-MT git hash: 1bdabf7\n* port time: Thu Mar 24 03:37:19 EET 2022\n* port machine: LM0-400-22516.local\n", "qas": [{"id": "q2", "question": "What is the model task of Helsinki-NLP/opus-mt-tc-base-tr-uk?", "answers": [{"text": "translation", "answer_start": 32, "answer_end": 42}]}]}]}, {"title": "Roshan777/finetuning-sentiment-model-300-samples", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- imdb\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: finetuning-sentiment-model-300-samples\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: imdb\n      type: imdb\n      args: plain_text\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.6833333333333333\n    - name: F1\n      type: f1\n      value: 0.6153846153846154\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# finetuning-sentiment-model-300-samples\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the imdb dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6567\n- Accuracy: 0.6833\n- F1: 0.6154\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Roshan777/finetuning-sentiment-model-300-samples?", "answers": [{"text": "distilbert", "answer_start": 735, "answer_end": 744}]}, {"id": "q2", "question": "What is the model task of Roshan777/finetuning-sentiment-model-300-samples?", "answers": [{"text": "text-classification", "answer_start": 222, "answer_end": 240}]}]}]}, {"title": "huggingtweets/untiltrees", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Dancing Box</div>\n    <div style=\"text-align: center; font-size: 14px;\">@untiltrees</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from Dancing Box.\n\n Dancing Box |\n --- |\n 994 |\n 41 |\n 91 |\n 862 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @untiltrees's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/untiltrees')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/untiltrees?", "answers": [{"text": "text-generation", "answer_start": 1952, "answer_end": 1966}]}]}]}, {"title": "RomanEnikeev/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: distilbert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5670814703238499\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8265\n- Matthews Correlation: 0.5671\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5536          \n 2.0    0.5242          \n 3.0    0.6162          \n 4.0    0.7704          \n 5.0    0.8265          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of RomanEnikeev/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 125, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of RomanEnikeev/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 229, "answer_end": 247}]}]}]}, {"title": "Rocketknight1/temp-colab-upload-test4", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: Rocketknight1/temp-colab-upload-test4\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# Rocketknight1/temp-colab-upload-test4\n\nThis model is a fine-tuned version of [distilbert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.0000\n- Validation Loss: 0.0000\n- Epoch: 1\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 0.0000          \n 0.0000          \n\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- TensorFlow 2.8.0\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rocketknight1/temp-colab-upload-test4?", "answers": [{"text": "distilbert", "answer_start": 402, "answer_end": 411}]}]}]}, {"title": "nikhedward/t5-small-finetuned-multi-news", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- multi_news\nmetrics:\n- rouge\nmodel-index:\n- name: t5-small-finetuned-multi-news\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: multi_news\n      type: multi_news\n      args: default\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 14.5549\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-finetuned-multi-news\n\nThis model is a fine-tuned version of [t5-small]( on the multi_news dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.7775\n- Rouge1: 14.5549\n- Rouge2: 4.5934\n- Rougel: 11.1178\n- Rougelsum: 12.8964\n- Gen Len: 19.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n\n### Training results\n\n Epoch  Validation Loss  Rouge2  Rougelsum \n:-----::---------------::------::---------:\n 1.0    2.7775           4.5934  12.8964   \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of nikhedward/t5-small-finetuned-multi-news?", "answers": [{"text": "t5", "answer_start": 116, "answer_end": 117}]}, {"id": "q2", "question": "What is the model task of nikhedward/t5-small-finetuned-multi-news?", "answers": [{"text": "text2text-generation", "answer_start": 230, "answer_end": 249}]}]}]}, {"title": "Danik51002/Example", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Example\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Example\n\nThis model is a fine-tuned version of [sberbank-ai/rugpt3small_based_on_gpt2]( on an unknown dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 42\n- eval_batch_size: 42\n- seed: 42\n- gradient_accumulation_steps: 20\n- total_train_batch_size: 840\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 15\n- num_epochs: 300\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Danik51002/Example?", "answers": [{"text": "gpt2", "answer_start": 354, "answer_end": 357}]}]}]}, {"title": "Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm-v2", "paragraphs": [{"context": "---\nlicense: apache-2.0\nlanguage: fi\nmetrics:\n- wer\n- cer\ntags:\n- automatic-speech-recognition\n- fi\n- finnish\n- generated_from_trainer\n- hf-asr-leaderboard\n- robust-speech-event\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmodel-index:\n- name: wav2vec2-xlsr-1b-finnish-lm-v2\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 7\n      type: mozilla-foundation/common_voice_7_0\n      args: fi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 4.09\n    - name: Test CER\n      type: cer\n      value: 0.88\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: FLEURS ASR\n      type: google/fleurs\n      args: fi_fi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 12.11\n    - name: Test CER\n      type: cer\n      value: 5.65\n---\n\n# Wav2vec2-xls-r-1b for Finnish ASR\n\nThis acoustic model is a fine-tuned version of [facebook/wav2vec2-xls-r-1b]( for Finnish ASR. The model has been fine-tuned with 275.6 hours of Finnish transcribed speech data. Wav2Vec2 XLS-R was introduced in\n[this paper]( and first released at [this page](\n\nThis repository also includes Finnish KenLM language model used in the decoding phase with the acoustic model.\n\n**Note**: this model is exactly the same as the [aapot/wav2vec2-xlsr-1b-finnish-lm-v2]( model so that model has just been copied/moved to this `Finnish-NLP` Hugging Face organization.\n\n## Model description\n\nWav2Vec2 XLS-R is Facebook AI's large-scale multilingual pretrained model for speech. It is pretrained on 436k hours of unlabeled speech, including VoxPopuli, MLS, CommonVoice, BABEL, and VoxLingua107. It uses the wav2vec 2.0 objective, in 128 languages.\n\nYou can read more about the pretrained model from [this blog]( and [this paper](\n\nThis model is fine-tuned version of the pretrained model (1 billion parameter variant) for Finnish ASR.\n\n## Intended uses & limitations\n\nYou can use this model for Finnish ASR (speech-to-text) task. \n\n### How to use\n\nCheck the [run-finnish-asr-models.ipynb]( notebook in this repository for an detailed example on how to use this model.\n\n### Limitations and bias\n\nThis model was fine-tuned with audio samples which maximum length was 20 seconds so this model most likely works the best for quite short audios of similar length. However, you can try this model with a lot longer audios too and see how it works. If you encounter out of memory errors with very long audio files you can use the audio chunking method introduced in [this blog post](\n\nA vast majority of the data used for fine-tuning was from the Finnish Parliament dataset so this model may not generalize so well to very different domains like common daily spoken Finnish with dialects etc. In addition, audios of the datasets tend to be adult male dominated so this model may not work as well for speeches of children and women, for example.\n\nThe Finnish KenLM language model used in the decoding phase has been trained with text data from the audio transcriptions and from a subset of Finnish Wikipedia. Thus, the decoder's language model may not generalize to very different language, for example to spoken daily language with dialects (because especially the Wikipedia contains mostly formal Finnish language). It may be beneficial to train your own KenLM language model for your domain language and use that in the decoding.\n\n## Training data\n\nThis model was fine-tuned with 275.6 hours of Finnish transcribed speech data from following datasets:\n\n Hours    \n:--------:\n 9.70 h   \n 0.24 h   \n 21.97 h  \n 10.32 h  \n 228.00 h \n 5.37 h   \n\nDatasets were filtered to include maximum length of 20 seconds long audio samples.\n\n## Training procedure\n\nThis model was trained during [Robust Speech Challenge Event]( organized by Hugging Face. Training was done on a Tesla V100 GPU, sponsored by OVHcloud.\n\nTraining script was provided by Hugging Face and it is available [here]( We only modified its data loading for our custom datasets.\n\nFor the KenLM language model training, we followed the [blog post tutorial]( provided by Hugging Face. Training data for the 5-gram KenLM were text transcriptions of the audio training data and 100k random samples of cleaned [Finnish Wikipedia]( (August 2021) dataset.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: [8-bit Adam]( with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\nThe pretrained `facebook/wav2vec2-xls-r-1b` model was initialized with following hyperparameters:\n- attention_dropout: 0.094\n- hidden_dropout: 0.047\n- feat_proj_dropout: 0.04\n- mask_time_prob: 0.082\n- layerdrop: 0.041\n- activation_dropout: 0.055\n- ctc_loss_reduction: \"mean\"\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.17   0.2851          \n 0.34   0.1595          \n 0.5    0.1458          \n 0.67   0.1374          \n 0.84   0.1390          \n 1.01   0.1266          \n 1.17   0.1441          \n 1.34   0.1232          \n 1.51   0.1209          \n 1.68   0.1149          \n 1.84   0.1121          \n 2.01   0.1091          \n 2.18   0.1115          \n 2.35   0.1144          \n 2.51   0.1028          \n 2.68   0.1129          \n 2.85   0.1025          \n 3.02   0.1068          \n 3.18   0.1072          \n 3.35   0.0928          \n 3.52   0.1042          \n 3.69   0.0979          \n 3.85   0.0947          \n 4.02   0.0991          \n 4.19   0.0919          \n 4.36   0.0888          \n 4.52   0.0888          \n 4.69   0.0894          \n 4.86   0.0917          \n 5.03   0.0942          \n 5.19   0.0902          \n 5.36   0.0871          \n 5.53   0.0861          \n 5.7    0.0876          \n 5.86   0.0848          \n 6.03   0.0914          \n 6.2    0.0841          \n 6.37   0.0858          \n 6.53   0.0874          \n 6.7    0.0835          \n 6.87   0.0835          \n 7.04   0.0852          \n 7.21   0.0832          \n 7.37   0.0829          \n 7.54   0.0822          \n 7.71   0.0819          \n 7.88   0.0787          \n 8.04   0.0807          \n 8.21   0.0784          \n 8.38   0.0772          \n 8.55   0.0772          \n 8.71   0.0763          \n 8.88   0.0756          \n 9.05   0.0771          \n 9.22   0.0756          \n 9.38   0.0741          \n 9.55   0.0739          \n 9.72   0.0756          \n 9.89   0.0737          \n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n\n## Evaluation results\n\nEvaluation was done with the [Common Voice 7.0 Finnish test split]( [Common Voice 9.0 Finnish test split]( and with the [FLEURS ASR Finnish test split]( \n\nThis model's training data includes the training splits of Common Voice 7.0 but our newer `Finnish-NLP/wav2vec2-base-fi-voxpopuli-v2-finetuned` and `Finnish-NLP/wav2vec2-large-uralic-voxpopuli-v2-finnish` models include the Common Voice 9.0 so we ran tests for both Common Voice versions. Note: Common Voice doesn't seem to fully preserve the test split as fixed between the dataset versions so it is possible that some of the training examples of Common Voice 9.0 are in the test split of the Common Voice 7.0 and vice versa. Thus, Common Voice test result comparisons are not fully accurate between the models trained with different Common Voice versions but the comparison should still be meaningful enough.\n\n### Common Voice 7.0 testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm-v2  --dataset mozilla-foundation/common_voice_7_0 --config fi --split test\n```\n\nThis model (the fift row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       13.52             2.44              |\n 300 million      **9.66**          1.66              |\n 300 million      17.92             3.36              |\n 1000 million     13.11             2.23              |\n 1000 million     9.73              **1.65**          |\n\n### Common Voice 9.0 testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm-v2  --dataset mozilla-foundation/common_voice_9_0 --config fi --split test\n```\n\nThis model (the fift row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       14.08             2.59              |\n 300 million      9.83              1.71              |\n 300 million      16.45             3.07              |\n 1000 million     13.00             2.20              |\n 1000 million     **8.96**          **1.52**          |\n\n### FLEURS ASR testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm-v2 --dataset google/fleurs --config fi_fi --split test\n```\n\nThis model (the fift row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       17.16             6.61              |\n 300 million      **14.63**         6.22              |\n 300 million      23.30             7.67              |\n 1000 million     16.67             6.35              |\n 1000 million     14.89             **6.06**          |\n\n## Team Members\n\n- Aapo Tanskanen, [Hugging Face profile]( [LinkedIn profile](\n- Rasmus Toivanen, [Hugging Face profile]( [LinkedIn profile](\n\nFeel free to contact us for more details \ud83e\udd17", "qas": [{"id": "q1", "question": "What is the model architecture of Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm-v2?", "answers": [{"text": "wav2vec2", "answer_start": 247, "answer_end": 254}]}, {"id": "q2", "question": "What is the model task of Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm-v2?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 66, "answer_end": 93}]}, {"id": "q3", "question": "What is the model category of Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm-v2?", "answers": [{"text": "audio", "answer_start": 2257, "answer_end": 2261}]}]}]}, {"title": "Intel/bert-large-uncased-squadv1.1-sparse-80-1x4-block-pruneofa", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\n---\n# 80% 1x4 Block Sparse BERT-Large (uncased) Fine Tuned on SQuADv1.1\nThis model is a result of fine-tuning a Prune OFA 80% 1x4 block sparse pre-trained BERT-Large combined with knowledge distillation.\nThis model yields the following results on SQuADv1.1 development set:<br>\n`{\"exact_match\": 84.673, \"f1\": 91.174}`\n\nFor further details see our paper, [Prune Once for All: Sparse Pre-Trained Language Models]( and our open source implementation available [here](\n", "qas": []}]}, {"title": "21iridescent/distilbert-base-uncased-finetuned-squad", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- squad_v2\nmodel-index:\n- name: distilbert-base-uncased-finetuned-squad\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-squad\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the squad_v2 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.3466\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    1.2801          |\n 2.0    1.2823          |\n 3.0    1.3466          |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of 21iridescent/distilbert-base-uncased-finetuned-squad?", "answers": [{"text": "distilbert", "answer_start": 97, "answer_end": 106}]}]}]}, {"title": "bhadresh-savani/electra-base-emotion", "paragraphs": [{"context": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-classification\n- emotion\n- pytorch\ndatasets:\n- emotion\nmetrics:\n- Accuracy, F1 Score\nthumbnail: \nmodel-index:\n- name: bhadresh-savani/electra-base-emotion\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: emotion\n      type: emotion\n      config: default\n      split: test\n    metrics:\n    - type: accuracy\n      value: 0.9265\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjYwNGQxMzRmMjViNzVhODJjM2UxOGNkYmNjOTE3OTczNzUxN2IyNGY1ZmFiY2VlNzNkOWY3M2I5YmZlNDlmMyIsInZlcnNpb24iOjF9.4e7MLUVHIBXYIwOgAcSDRJ7ziMXMSwk2-Ip8DH1RjxBDthc4MiBglMxbOUUjSzTPtSSEZKqfTZonUq7yR_rwBQ\n    - type: precision\n      value: 0.911532655431019\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzFkYzRjZGUwYmJmNmUxYjM3NzY3NWY0NzBhZjU5MDQxZWY4ZjA3OWMwMjQxMWJlODg5ZjIxZWFhYTg0ZGY2NCIsInZlcnNpb24iOjF9.I0j92y0SToxjoKkKX7AD8h5p3pDePSdQwOCBeZj-OGF0MRBeqo1Ejg-1snFFplU0mtoFF6rCvRq9WosqvRhfCA\n    - type: precision\n      value: 0.9265\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNGE2YzUyN2ZhYTdjZjQ4OWVkN2M4MzhjZWM0YzAyYWU2YjllZDYzOTYxYTZlZDAxNjA4ODY5NTk1MmE3ODQwZiIsInZlcnNpb24iOjF9.VQSaLzlreAIfy0iDJsCo-Mg1xF4gMv-KQkeIzoTKLhyp3V7rn5d5oaD8EEsay3gDamSC-xj8LndOqFL1AokZCg\n    - type: precision\n      value: 0.9305456360257519\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjcyZDdjMWE5YzhlNWUyZDg5YWUwOGRkYWFiMDNmMTY4N2QxZDg1YTU0MGQ2ZWI1ZDI5Mjk2MTVmN2JmZTA1YiIsInZlcnNpb24iOjF9.EvcL-mfmJ3rGQCaVRejoWplButUT_dQjgwPw-rWlqSC7Ex3reLa3hQ9PtYuXtYM3ymVl77rFgW2Yxf3lIn6RBg\n    - type: recall\n      value: 0.8536923122511134\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjIwNjU1ZmIwYzgyYzNmNmY2NDkyNjA2MDg0NDcxOWQwMmJmZTFlYzg0NjI0YWMxNzhmYTQwNzU0Yzg5ZTk4MCIsInZlcnNpb24iOjF9.8he8WOjzHqJp5h2TUig7oDrn4jwSbSB1J69fmh-2UUrpH46VpwxD5bO0MG3Nm4HHYK2ZIzPb-sTX7hhMJHM7Bw\n    - type: recall\n      value: 0.9265\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjRiODljZTc0ZDU3YWNlZTRlYjQwY2M5YWFiY2VkOWM5Yjg5NjYzZTNkYTA1ZTc3ZjU3YjY3ZGMzNWFiNTNhNSIsInZlcnNpb24iOjF9.W74pDxOq18_Wr3Mmd0f1whXMJuVT3DhmYCWh3Z_VKB6QMSgNUf4l1iBYukIT8Lrwr50z4pscBGY3YktlUgg5Bg\n    - type: recall\n      value: 0.9265\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjgxNDhjMDcwZTA2ZWQ2NTFlMTFjOGU4NDE4ZDY0MDJjNGMwOGYzMDViYTM5Y2M5ZTc2NDM3OTdmYTc1NzhhMiIsInZlcnNpb24iOjF9.x4sUtEJWliLYqyKkKMEvb10lSxqN8vhrmSAnwtyCp0tEag6DUNEUA6_nojaC3ABIDb4ZwVd7JIcQ5yD2PKU-Dg\n    - type: f1\n      value: 0.8657529340483895\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZmUzZjNiOTZhNjE0ZWE5NDI2NzBmOGViYTc0NWYwYWQ3ZjA1ZTE1NmM5ZWRiZjA0NGYyZDM2OWE5YzA4NDY1MyIsInZlcnNpb24iOjF9.OLYrJI7nW4-nvCbEsJDIwyGL9lI1UNM-TBpMmosbkUCLu8MhhCdMo0tdKRaCRoDUtfLlwcUG9mOayAsDdfrqCw\n    - type: f1\n      value: 0.9265\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWE1ZDI1MzA3YTcwODU0NTgxYTNmYjc5ZDZkYzI3OWZmYjNlNjI5OWI4MDE4NDRhOWMyNWZiMjZlMTIwNWU3YSIsInZlcnNpb24iOjF9.ZpLdxeqJjKiLxUxRIVbBZa9u5w0UMPKVwvOha4tHMTiyq3RaW8TNOkFdO7TIsgxoPdQb6wzWNDojrqJOY4vsDg\n    - type: f1\n      value: 0.924844632421077\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2VhNWZmM2E4NDI5NmRiMWJkNDk2MDMyMDZmYmE2ODBlNTA2NTdhYTc4NzRkOGU1ODczZDU4MTdhYTZlOTRiZCIsInZlcnNpb24iOjF9.93XiZO_2N0nLa2PU3TICEOT8HjURPzpaAVD_5e5MFMHrtMIB1Barg0cvzc3TCisKxV_vlt1i20d2YwtfWKgrBQ\n    - type: loss\n      value: 0.3268870413303375\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2IyMjdlMWZkNjQwNWVkYzU1MWYyODJkMzAwOWJmZWJiYTI0OGRlZjhkMmZkN2JhMjJmMDdkMzQ1Y2U3NDY3MyIsInZlcnNpb24iOjF9.aEnyBFvFKixU1zh5GYkIUDcf4uD6PV7pESdbdvG_oJ1lIisOg6CEb6nekcYtDebcoL3q1cbrBdhgK6dgdShJBQ\n---\n# Electra-base-emotion\n\n## Model description:\n\n## Model Performance Comparision on Emotion Dataset from Twitter:\n\n Accuracy   Test Sample per Second |\n ---  --- |\n 93.8  398.69 |\n 94.05  190.152 |\n 93.95  195.639 |\n 93.6  182.794 |\n 91.95  472.72 |\n\n## How to Use the model:\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='bhadresh-savani/electra-base-emotion', return_all_scores=True)\nprediction = classifier(\"I love using transformers. The best part is wide range of support and its easy to use\", )\nprint(prediction)\n\n\"\"\"\nOutput:\n[[\n{'label': 'sadness', 'score': 0.0006792712374590337}, \n{'label': 'joy', 'score': 0.9959300756454468}, \n{'label': 'love', 'score': 0.0009452480007894337}, \n{'label': 'anger', 'score': 0.0018055217806249857}, \n{'label': 'fear', 'score': 0.00041110432357527316}, \n{'label': 'surprise', 'score': 0.0002288572577526793}\n]]\n\"\"\"\n```\n\n## Dataset:\n[Twitter-Sentiment-Analysis](\n\n## Training procedure\n[Colab Notebook](\n\n## Eval results\n```json\n{\n 'epoch': 8.0,\n 'eval_accuracy': 0.9195,\n 'eval_f1': 0.918975455617076,\n 'eval_loss': 0.3486028015613556,\n 'eval_runtime': 4.2308,\n 'eval_samples_per_second': 472.726,\n 'eval_steps_per_second': 7.564\n }\n ```\n\n## Reference:\n* [Natural Language Processing with Transformer By Lewis Tunstall, Leandro von Werra, Thomas Wolf](", "qas": [{"id": "q1", "question": "What is the model architecture of bhadresh-savani/electra-base-emotion?", "answers": [{"text": "electra", "answer_start": 186, "answer_end": 192}]}, {"id": "q2", "question": "What is the model task of bhadresh-savani/electra-base-emotion?", "answers": [{"text": "text-classification", "answer_start": 47, "answer_end": 65}]}]}]}, {"title": "0x7194633/pyGPT-50M", "paragraphs": [{"context": "---\nlanguage:\n- en\n- code\n- multilingual\nlicense: mpl-2.0\n---\n\n## PythonGPT\nA GPT2-type neural network trained on 16 gigabytes of Pyhon scripts from scratch. It has 50 million parameters.\n\nMade as a toy.", "qas": []}]}, {"title": "21iridescent/distilroberta-base-finetuned-squad2-lwt", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- squad_v2\nmodel-index:\n- name: distilroberta-base-finetuned-squad2-lwt\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilroberta-base-finetuned-squad2-lwt\n\nThis model is a fine-tuned version of [distilroberta-base]( on the squad_v2 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1356\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    1.1220          |\n 2.0    1.0500          |\n 3.0    1.1356          |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n\n{'HasAns_exact': 71.39001349527665,\n 'HasAns_f1': 77.71740687727831,\n 'HasAns_total': 5928,\n 'NoAns_exact': 68.59545836837678,\n 'NoAns_f1': 68.59545836837678,\n 'NoAns_total': 5945,\n 'best_exact': 69.9991577528847,\n 'best_exact_thresh': 0.0,\n 'best_f1': 73.1583245993857,\n 'best_f1_thresh': 0.0,\n 'exact': 69.99073528173166,\n 'f1': 73.1499021282327,\n 'total': 11873}", "qas": [{"id": "q1", "question": "What is the model architecture of 21iridescent/distilroberta-base-finetuned-squad2-lwt?", "answers": [{"text": "roberta", "answer_start": 103, "answer_end": 109}]}]}]}, {"title": "DrishtiSharma/xls-r-es-test-lm-finetuned-sentiment-mesd", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: xls-r-es-test-lm-finetuned-sentiment-mesd\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xls-r-es-test-lm-finetuned-sentiment-mesd\n\nThis model is a fine-tuned version of [glob-asr/xls-r-es-test-lm]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.7851\n- Accuracy: 0.2385\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1.25e-05\n- train_batch_size: 64\n- eval_batch_size: 40\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 256\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 20\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.86   1.7876          \n 1.86   1.7869          \n 2.86   1.7859          \n 3.86   1.7851          \n 4.86   1.7842          \n 5.86   1.7834          \n 6.86   1.7823          \n 7.86   1.7812          \n 8.86   1.7800          \n 9.86   1.7787          \n 10.86  1.7772          \n 11.86  1.7760          \n 12.86  1.7748          \n 13.86  1.7736          \n 14.86  1.7725          \n 15.86  1.7715          \n 16.86  1.7706          \n 17.86  1.7701          \n 18.86  1.7697          \n 19.86  1.7696          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": []}]}, {"title": "joniponi/distilbert-base-uncased-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotion\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8357\n- Accuracy: 0.6309\n- F1: 0.6469\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.8585           0.6363 |\n 2.0    0.8472           0.6354 |\n 3.0    0.8357           0.6469 |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of joniponi/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 101, "answer_end": 110}]}]}]}, {"title": "Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm", "paragraphs": [{"context": "---\nlicense: apache-2.0\nlanguage: fi\nmetrics:\n- wer\n- cer\ntags:\n- automatic-speech-recognition\n- fi\n- finnish\n- generated_from_trainer\n- hf-asr-leaderboard\n- robust-speech-event\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmodel-index:\n- name: wav2vec2-xlsr-1b-finnish-lm\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 7\n      type: mozilla-foundation/common_voice_7_0\n      args: fi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 5.65\n    - name: Test CER\n      type: cer\n      value: 1.2\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: FLEURS ASR\n      type: google/fleurs\n      args: fi_fi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 20.34\n    - name: Test CER\n      type: cer\n      value: 6.97\n---\n\n# Wav2vec2-xls-r-1b for Finnish ASR\n\nThis acoustic model is a fine-tuned version of [facebook/wav2vec2-xls-r-1b]( for Finnish ASR. The model has been fine-tuned with 259.57 hours of Finnish transcribed speech data. Wav2Vec2 XLS-R was introduced in\n[this paper]( and first released at [this page](\n\nThis repository also includes Finnish KenLM language model used in the decoding phase with the acoustic model.\n\n**Note**: this model is exactly the same as the [aapot/wav2vec2-xlsr-1b-finnish-lm]( model so that model has just been copied/moved to this `Finnish-NLP` Hugging Face organization.\n\n**Note**: there is a better V2 version of this model which has been fine-tuned longer with 16 hours of more data: [Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm-v2](\n\n## Model description\n\nWav2Vec2 XLS-R is Facebook AI's large-scale multilingual pretrained model for speech. It is pretrained on 436k hours of unlabeled speech, including VoxPopuli, MLS, CommonVoice, BABEL, and VoxLingua107. It uses the wav2vec 2.0 objective, in 128 languages.\n\nYou can read more about the pretrained model from [this blog]( and [this paper](\n\nThis model is fine-tuned version of the pretrained model (1 billion parameter variant) for Finnish ASR.\n\n## Intended uses & limitations\n\nYou can use this model for Finnish ASR (speech-to-text) task. \n\n### How to use\n\nCheck the [run-finnish-asr-models.ipynb]( notebook in this repository for an detailed example on how to use this model.\n\n### Limitations and bias\n\nThis model was fine-tuned with audio samples which maximum length was 20 seconds so this model most likely works the best for quite short audios of similar length. However, you can try this model with a lot longer audios too and see how it works. If you encounter out of memory errors with very long audio files you can use the audio chunking method introduced in [this blog post](\n\nA vast majority of the data used for fine-tuning was from the Finnish Parliament dataset so this model may not generalize so well to very different domains like common daily spoken Finnish with dialects etc. In addition, audios of the datasets tend to be adult male dominated so this model may not work as well for speeches of children and women, for example.\n\nThe Finnish KenLM language model used in the decoding phase has been trained with text data from the audio transcriptions. Thus, the decoder's language model may not generalize to very different language, for example to spoken daily language with dialects. It may be beneficial to train your own KenLM language model for your domain language and use that in the decoding.\n\n## Training data\n\nThis model was fine-tuned with 259.57 hours of Finnish transcribed speech data from following datasets:\n\n Hours    \n:--------:\n 9.70 h   \n 0.24 h   \n 5.94 h   \n 10.32 h  \n 228.00 h \n 5.37 h   \n\nDatasets were filtered to include maximum length of 20 seconds long audio samples.\n\n## Training procedure\n\nThis model was trained during [Robust Speech Challenge Event]( organized by Hugging Face. Training was done on a Tesla V100 GPU, sponsored by OVHcloud.\n\nTraining script was provided by Hugging Face and it is available [here]( We only modified its data loading for our custom datasets.\n\nFor the KenLM language model training, we followed the [blog post tutorial]( provided by Hugging Face. Training data for the 5-gram KenLM were text transcriptions of the audio training data.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 32\n- eval_batch_size: 8\n- seed: 42\n- optimizer: [8-bit Adam]( with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\nThe pretrained `facebook/wav2vec2-xls-r-1b` model was initialized with following hyperparameters:\n- attention_dropout: 0.094\n- hidden_dropout: 0.047\n- feat_proj_dropout: 0.04\n- mask_time_prob: 0.082\n- layerdrop: 0.041\n- activation_dropout: 0.055\n- ctc_loss_reduction: \"mean\"\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.18   0.4870          \n 0.36   0.2450          \n 0.54   0.1818          \n 0.72   0.1698          \n 0.9    0.1581          \n 1.07   0.1689          \n 1.25   0.1719          \n 1.43   0.1434          \n 1.61   0.1645          \n 1.79   0.1483          \n 1.97   0.1499          \n 2.15   0.1345          \n 2.33   0.1383          \n 2.51   0.1338          \n 2.69   0.1290          \n 2.87   0.1239          \n 3.04   0.1346          \n 3.22   0.1310          \n 3.4    0.1273          \n 3.58   0.1219          \n 3.76   0.1306          \n 3.94   0.1230          \n 4.12   0.1310          \n 4.3    0.1296          \n 4.48   0.1285          \n 4.66   0.1261          \n 4.84   0.1235          \n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n\n## Evaluation results\n\nEvaluation was done with the [Common Voice 7.0 Finnish test split]( [Common Voice 9.0 Finnish test split]( and with the [FLEURS ASR Finnish test split]( \n\nThis model's training data includes the training splits of Common Voice 7.0 but our newer `Finnish-NLP/wav2vec2-base-fi-voxpopuli-v2-finetuned` and `Finnish-NLP/wav2vec2-large-uralic-voxpopuli-v2-finnish` models include the Common Voice 9.0 so we ran tests for both Common Voice versions. Note: Common Voice doesn't seem to fully preserve the test split as fixed between the dataset versions so it is possible that some of the training examples of Common Voice 9.0 are in the test split of the Common Voice 7.0 and vice versa. Thus, Common Voice test result comparisons are not fully accurate between the models trained with different Common Voice versions but the comparison should still be meaningful enough.\n\n### Common Voice 7.0 testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm  --dataset mozilla-foundation/common_voice_7_0 --config fi --split test\n```\n\nThis model (the fourth row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       13.52             2.44              |\n 300 million      **9.66**          1.66              |\n 300 million      17.92             3.36              |\n 1000 million     13.11             2.23              |\n 1000 million     9.73              **1.65**          |\n\n### Common Voice 9.0 testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm  --dataset mozilla-foundation/common_voice_9_0 --config fi --split test\n```\n\nThis model (the fourth row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       14.08             2.59              |\n 300 million      9.83              1.71              |\n 300 million      16.45             3.07              |\n 1000 million     13.00             2.20              |\n 1000 million     **8.96**          **1.52**          |\n\n### FLEURS ASR testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm --dataset google/fleurs --config fi_fi --split test\n```\n\nThis model (the fourth row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       17.16             6.61              |\n 300 million      **14.63**         6.22              |\n 300 million      23.30             7.67              |\n 1000 million     16.67             6.35              |\n 1000 million     14.89             **6.06**          |\n\n## Team Members\n\n- Aapo Tanskanen, [Hugging Face profile]( [LinkedIn profile](\n- Rasmus Toivanen, [Hugging Face profile]( [LinkedIn profile](\n\nFeel free to contact us for more details \ud83e\udd17", "qas": [{"id": "q1", "question": "What is the model architecture of Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm?", "answers": [{"text": "wav2vec2", "answer_start": 247, "answer_end": 254}]}, {"id": "q2", "question": "What is the model task of Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 66, "answer_end": 93}]}, {"id": "q3", "question": "What is the model category of Finnish-NLP/wav2vec2-xlsr-1b-finnish-lm?", "answers": [{"text": "audio", "answer_start": 2412, "answer_end": 2416}]}]}]}, {"title": "Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm", "paragraphs": [{"context": "---\nlicense: apache-2.0\nlanguage: fi\nmetrics:\n- wer\n- cer\ntags:\n- automatic-speech-recognition\n- fi\n- finnish\n- generated_from_trainer\n- hf-asr-leaderboard\n- robust-speech-event\ndatasets:\n- mozilla-foundation/common_voice_7_0\nmodel-index:\n- name: wav2vec2-xlsr-300m-finnish-lm\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 7\n      type: mozilla-foundation/common_voice_7_0\n      args: fi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 8.16\n    - name: Test CER\n      type: cer\n      value: 1.97\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: FLEURS ASR\n      type: google/fleurs\n      args: fi_fi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 17.72\n    - name: Test CER\n      type: cer\n      value: 6.78\n---\n\n# Wav2vec2-xls-r-300m for Finnish ASR\n\nThis acoustic model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( for Finnish ASR. The model has been fine-tuned with 275.6 hours of Finnish transcribed speech data. Wav2Vec2 XLS-R was introduced in\n[this paper]( and first released at [this page](\n\nThis repository also includes Finnish KenLM language model used in the decoding phase with the acoustic model.\n\n**Note**: this model is exactly the same as the [aapot/wav2vec2-xlsr-300m-finnish-lm]( model so that model has just been copied/moved to this `Finnish-NLP` Hugging Face organization.\n\n## Model description\n\nWav2Vec2 XLS-R is Facebook AI's large-scale multilingual pretrained model for speech. It is pretrained on 436k hours of unlabeled speech, including VoxPopuli, MLS, CommonVoice, BABEL, and VoxLingua107. It uses the wav2vec 2.0 objective, in 128 languages.\n\nYou can read more about the pretrained model from [this blog]( and [this paper](\n\nThis model is fine-tuned version of the pretrained model (300 million parameter variant) for Finnish ASR.\n\n## Intended uses & limitations\n\nYou can use this model for Finnish ASR (speech-to-text) task. \n\n### How to use\n\nCheck the [run-finnish-asr-models.ipynb]( notebook in this repository for an detailed example on how to use this model.\n\n### Limitations and bias\n\nThis model was fine-tuned with audio samples which maximum length was 20 seconds so this model most likely works the best for quite short audios of similar length. However, you can try this model with a lot longer audios too and see how it works. If you encounter out of memory errors with very long audio files you can use the audio chunking method introduced in [this blog post](\n\nA vast majority of the data used for fine-tuning was from the Finnish Parliament dataset so this model may not generalize so well to very different domains like common daily spoken Finnish with dialects etc. In addition, audios of the datasets tend to be adult male dominated so this model may not work as well for speeches of children and women, for example.\n\nThe Finnish KenLM language model used in the decoding phase has been trained with text data from the audio transcriptions and from a subset of Finnish Wikipedia. Thus, the decoder's language model may not generalize to very different language, for example to spoken daily language with dialects (because especially the Wikipedia contains mostly formal Finnish language). It may be beneficial to train your own KenLM language model for your domain language and use that in the decoding.\n\n## Training data\n\nThis model was fine-tuned with 275.6 hours of Finnish transcribed speech data from following datasets:\n\n Hours    \n:--------:\n 9.70 h   \n 0.24 h   \n 21.97 h  \n 10.32 h  \n 228.00 h \n 5.37 h   \n\nDatasets were filtered to include maximum length of 20 seconds long audio samples.\n\n## Training procedure\n\nThis model was trained during [Robust Speech Challenge Event]( organized by Hugging Face. Training was done on a Tesla V100 GPU, sponsored by OVHcloud.\n\nTraining script was provided by Hugging Face and it is available [here]( We only modified its data loading for our custom datasets.\n\nFor the KenLM language model training, we followed the [blog post tutorial]( provided by Hugging Face. Training data for the 5-gram KenLM were text transcriptions of the audio training data and 100k random samples of cleaned [Finnish Wikipedia]( (August 2021) dataset.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-04\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: [8-bit Adam]( with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\nThe pretrained `facebook/wav2vec2-xls-r-300m` model was initialized with following hyperparameters:\n- attention_dropout: 0.094\n- hidden_dropout: 0.047\n- feat_proj_dropout: 0.04\n- mask_time_prob: 0.082\n- layerdrop: 0.041\n- activation_dropout: 0.055\n- ctc_loss_reduction: \"mean\"\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 0.17   0.5750          \n 0.34   0.3356          \n 0.5    0.3007          \n 0.67   0.2619          \n 0.84   0.2488          \n 1.01   0.2795          \n 1.17   0.2652          \n 1.34   0.2479          \n 1.51   0.2409          \n 1.68   0.2728          \n 1.84   0.2254          \n 2.01   0.2169          \n 2.18   0.2215          \n 2.35   0.2174          \n 2.51   0.2217          \n 2.68   0.2002          \n 2.85   0.1935          \n 3.02   0.1859          \n 3.18   0.2038          \n 3.35   0.1863          \n 3.52   0.1948          \n 3.69   0.1872          \n 3.85   0.1888          \n 4.02   0.1818          \n 4.19   0.1896          \n 4.36   0.1953          \n 4.52   0.1864          \n 4.69   0.1843          \n 4.86   0.1686          \n 5.03   0.1731          \n 5.19   0.1676          \n 5.36   0.1740          \n 5.53   0.1674          \n 5.7    0.1735          \n 5.86   0.1692          \n 6.03   0.1797          \n 6.2    0.1651          \n 6.37   0.1627          \n 6.53   0.1652          \n 6.7    0.1564          \n 6.87   0.1525          \n 7.04   0.1639          \n 7.21   0.1611          \n 7.37   0.1633          \n 7.54   0.1692          \n 7.71   0.1555          \n 7.88   0.1590          \n 8.04   0.1531          \n 8.21   0.1583          \n 8.38   0.1546          \n 8.55   0.1540          \n 8.71   0.1534          \n 8.88   0.1482          \n 9.05   0.1490          \n 9.22   0.1463          \n 9.38   0.1454          \n 9.55   0.1504          \n 9.72   0.1512          \n 9.89   0.1484          \n\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.2+cu102\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n\n## Evaluation results\n\nEvaluation was done with the [Common Voice 7.0 Finnish test split]( [Common Voice 9.0 Finnish test split]( and with the [FLEURS ASR Finnish test split]( \n\nThis model's training data includes the training splits of Common Voice 7.0 but our newer `Finnish-NLP/wav2vec2-base-fi-voxpopuli-v2-finetuned` and `Finnish-NLP/wav2vec2-large-uralic-voxpopuli-v2-finnish` models include the Common Voice 9.0 so we ran tests for both Common Voice versions. Note: Common Voice doesn't seem to fully preserve the test split as fixed between the dataset versions so it is possible that some of the training examples of Common Voice 9.0 are in the test split of the Common Voice 7.0 and vice versa. Thus, Common Voice test result comparisons are not fully accurate between the models trained with different Common Voice versions but the comparison should still be meaningful enough.\n\n### Common Voice 7.0 testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm  --dataset mozilla-foundation/common_voice_7_0 --config fi --split test\n```\n\nThis model (the third row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       13.52             2.44              |\n 300 million      **9.66**          1.66              |\n 300 million      17.92             3.36              |\n 1000 million     13.11             2.23              |\n 1000 million     9.73              **1.65**          |\n\n### Common Voice 9.0 testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm  --dataset mozilla-foundation/common_voice_9_0 --config fi --split test\n```\n\nThis model (the third row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       14.08             2.59              |\n 300 million      9.83              1.71              |\n 300 million      16.45             3.07              |\n 1000 million     13.00             2.20              |\n 1000 million     **8.96**          **1.52**          |\n\n### FLEURS ASR testing\n\nTo evaluate this model, run the `eval.py` script in this repository:\n\n```bash\npython3 eval.py --model_id Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm --dataset google/fleurs --config fi_fi --split test\n```\n\nThis model (the third row of the table) achieves the following WER (Word Error Rate) and CER (Character Error Rate) results compared to our other models and their parameter counts:\n\n Model parameters  WER (without LM)  CER (without LM) |\n------------------------------------------------------|\n 95 million       17.16             6.61              |\n 300 million      **14.63**         6.22              |\n 300 million      23.30             7.67              |\n 1000 million     16.67             6.35              |\n 1000 million     14.89             **6.06**          |\n\n## Team Members\n\n- Aapo Tanskanen, [Hugging Face profile]( [LinkedIn profile](\n- Rasmus Toivanen, [Hugging Face profile]( [LinkedIn profile](\n\nFeel free to contact us for more details \ud83e\udd17", "qas": [{"id": "q1", "question": "What is the model architecture of Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm?", "answers": [{"text": "wav2vec2", "answer_start": 247, "answer_end": 254}]}, {"id": "q2", "question": "What is the model task of Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 66, "answer_end": 93}]}, {"id": "q3", "question": "What is the model category of Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm?", "answers": [{"text": "audio", "answer_start": 2261, "answer_end": 2265}]}]}]}, {"title": "avb/bert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: bert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5642446874338215\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [bert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8297\n- Matthews Correlation: 0.5642\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5115          \n 2.0    0.5523          \n 3.0    0.7024          \n 4.0    0.8297          \n 5.0    0.9284          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of avb/bert-base-uncased-finetuned-cola?", "answers": [{"text": "bert", "answer_start": 125, "answer_end": 128}]}, {"id": "q2", "question": "What is the model task of avb/bert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 223, "answer_end": 241}]}]}]}, {"title": "21iridescent/RoBERTa-base-finetuned-squad2-lwt", "paragraphs": [{"context": "---\n--license: mit\ntags:\n- generated_from_trainer\ndatasets:\n- squad_v2\nmodel-index:\n- name: roberta-base-finetuned-squad2-lwt\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n## Model description\n\n#### Finetuned on SQUAD2.0 Dataset\n#### F1: 83.738696142672\n\nTrained on single V100 GPU\n\nEveryone is welcome to use~ \n\nHope you have a nice day\n\n\n## Performance\n\n - HasAns_exact': 77.1255060728745, 'HasAns_f1': 83.87812741260885, 'HasAns_total': 5928,\n - 'NoAns_exact': 83.59966358284272, 'NoAns_f1': 83.59966358284272, 'NoAns_total': 5945,\n - 'best_exact': 80.36721974227238, 'best_exact_thresh': 0.0, \n - 'best_f1': 83.7386961426719, 'best_f1_thresh': 0.0,\n \n - 'exact': 80.36721974227238, \n - 'f1': 83.738696142672, \n - 'total': 11873\n\n# roberta-base-finetuned-squad2-lwt\n\nThis model is a fine-tuned version of [roberta-base]( on the squad_v2 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9441\n\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.8156          |\n 2.0    0.8494          |\n 3.0    0.9441          |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n\n\n", "qas": [{"id": "q1", "question": "What is the model architecture of 21iridescent/RoBERTa-base-finetuned-squad2-lwt?", "answers": [{"text": "roberta", "answer_start": 92, "answer_end": 98}]}]}]}, {"title": "Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\ntags:\n- text-classfication\n- int8\n- neural-compressor\n- Intel\u00ae Neural Compressor\n- PostTrainingStatic\ndatasets:\n- sst2\nmodel-index:\n- name: distilbert-base-uncased-finetuned-sst-2-english-int8-static\n  results:\n  - task:\n      type: sentiment-classification\n      name: Sentiment Classification\n    dataset:\n      type: sst2\n      name: Stanford Sentiment Treebank\n    metrics:\n    - type: accuracy\n      value: 90.37\n      name: accuracy\n      config: accuracy\n      verified: false\n---\n\n## Model Details: INT8 DistilBERT base uncased finetuned SST-2\n\nThis model is a fine-tuned DistilBERT model for the downstream task of sentiment classification, training on the [SST-2 dataset]( and quantized to INT8 (post-training static quantization) from the original FP32 model ([distilbert-base-uncased-finetuned-sst-2-english]( \nThe same model is provided in two different formats: PyTorch and ONNX. \n\n Description |\n ----------- | \n Intel | \n March 29, 2022 for PyTorch model & February 3, 2023 for ONNX model | \n 1 | \n NLP DistilBERT (INT8) - Sentiment Classification (+/-) | \n [ | \n Apache 2.0 |\n [Community Tab]( and [Intel Developers Discord]( |\n\n Description |\n ----------- | \n Inference for sentiment classification (classifying whether a statement is positive or negative) | \n Anyone | \n This model is already fine-tuned and quantized to INT8. It is not suitable for further fine-tuning in this form. To fine-tune your own model, you can start with [distilbert-base-uncased-finetuned-sst-2-english]( The model should not be used to intentionally create hostile or alienating environments for people. |\n\n#### Load the PyTorch model with Optimum Intel\n```python\nfrom optimum.intel.neural_compressor import INCModelForSequenceClassification\n\nmodel_id = \"Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static\"\nint8_model = INCModelForSequenceClassification.from_pretrained(model_id)\n```\n\n#### Load the ONNX model with Optimum:\n```python\nfrom optimum.onnxruntime import ORTModelForSequenceClassification\n\nmodel_id = \"Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static\"\nint8_model = ORTModelForSequenceClassification.from_pretrained(model_id)\n```\n\n Description | \n ----------- | \n Movie reviewers from the internet | \n Text movie single-sentence reviews taken from 4 authors. More information can be found in the original paper by [Pang and Lee (2005)]( |\n - |\n Model deployment on alternate hardware and software can change model performance |\n\n Description | \n ----------- | \n Accuracy |\n - | \n - | \n\n PyTorch INT8  FP32 |\n------|\n0.90370.9106|\n65255|\n\n Description | \n ----------- | \n The dataset can be found here: [datasets/sst2]( There dataset has a total of 215,154 unique phrases, annotated by 3 human judges. |\n Dataset was chosen to showcase the benefits of quantization on an NLP classification task with the [Optimum Intel]( and [Intel\u00ae Neural Compressor](  |\n The calibration dataloader is the train dataloader. The default calibration sampling size 100 isn't divisible exactly by batch size 8, so the real sampling size is 104.| \n\n Description | \n ----------- | \n The model was only evaluated on accuracy. There is no available comparison between evaluation factors. |\n There is no available comparison between the intersection of evaluated factors.  |\n\n Description | \n ----------- | \n The data that make up the model are movie reviews from authors on the internet. |\n The model is not intended to inform decisions central to human life or flourishing. It is an aggregated set of movie reviews from the internet. | \n No additional risk mitigation strategies were considered during model development. |\n The data are biased toward the particular reviewers' opinions and the judges (labelers) of the data. Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al., 2021]( and [Bender et al., 2021]( Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Beyond this, the extent of the risks involved by using the model remain unknown.|\n - | \n\n\n \n\n\n# BibTeX Entry and Citation Info\n```\n@misc{distilbert-base-uncased-finetuned-sst-2-english-int8-static\n  author    = {Xin He, Yu Wenz},\n  title     = {distilbert-base-uncased-finetuned-sst-2-english-int8-static},\n  year      = {2022},\n  url       = {\n}\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static?", "answers": [{"text": "distilbert", "answer_start": 177, "answer_end": 186}]}]}]}, {"title": "Rishav-hub/xlm-roberta-base-finetuned-panx-de", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- xtreme\nmetrics:\n- f1\nmodel-index:\n- name: xlm-roberta-base-finetuned-panx-de\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: xtreme\n      type: xtreme\n      args: PAN-X.de\n    metrics:\n    - name: F1\n      type: f1\n      value: 0.8591260810195721\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-finetuned-panx-de\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the xtreme dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1352\n- F1: 0.8591\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.1512          \n 2.0    0.1401          \n 3.0    0.1352          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of Rishav-hub/xlm-roberta-base-finetuned-panx-de?", "answers": [{"text": "xlm-roberta", "answer_start": 102, "answer_end": 112}]}, {"id": "q2", "question": "What is the model task of Rishav-hub/xlm-roberta-base-finetuned-panx-de?", "answers": [{"text": "token-classification", "answer_start": 203, "answer_end": 222}]}]}]}, {"title": "Intel/bert-large-uncased-sparse-80-1x4-block-pruneofa", "paragraphs": [{"context": "---\nlanguage: en\nlicense: apache-2.0\ntags: \n- fill-mask\ndatasets: \n- wikipedia\n- bookcorpus\n---\n# 80% 1x4 Block Sparse BERT-Large (uncased) Prune OFA\nThis model is was created using Prune OFA method described in [Prune Once for All: Sparse Pre-Trained Language Models]( presented in ENLSP NeurIPS Workshop 2021.\n\nFor further details on the model and its result, see our paper and our implementation available [here](\n", "qas": [{"id": "q2", "question": "What is the model task of Intel/bert-large-uncased-sparse-80-1x4-block-pruneofa?", "answers": [{"text": "fill-mask", "answer_start": 46, "answer_end": 54}]}]}]}, {"title": "BigSalmon/PointsToSentence", "paragraphs": [{"context": "```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"BigSalmon/PointsToSentence\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"BigSalmon/PointsToSentence\")\n```\n\n```\n- moviepass to return\n- this summer\n- swooped up by\n- original co-founder stacy spikes\ntext: the re-launch of moviepass is set to transpire this summer, ( rescued at the hands of / under the stewardship of / spearheaded by ) its founding father, stacy spikes.\n\n***\n\n- middle schools do not have recess\n- should get back to doing it\n- amazing for communication\n- and getting kids to move around\ntext: a casualty of the education reform craze, recess has been excised from middle schools. this is tragic, for it is instrumental in honing children's communication skills and encouraging physical activity.\n\n***\n\n-\n```\n\nIt should also be able to do all that this can: \n\nKeywords to sentences or sentence.", "qas": []}]}, {"title": "YiTian/wav2vec2-common_voice-tr-demo", "paragraphs": [{"context": "---\nlanguage:\n- tr\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- common_voice\n- generated_from_trainer\ndatasets:\n- common_voice\nmodel-index:\n- name: wav2vec2-common_voice-tr-demo\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-common_voice-tr-demo\n\nThis model is a fine-tuned version of [facebook/wav2vec2-large-xlsr-53]( on the COMMON_VOICE - TR dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.9841\n- Wer: 0.9999\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 128\n- eval_batch_size: 64\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 256\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 15.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 7.14   3.6689          \n 14.29  3.0280          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.9.0\n- Datasets 1.18.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of YiTian/wav2vec2-common_voice-tr-demo?", "answers": [{"text": "wav2vec2", "answer_start": 162, "answer_end": 169}]}, {"id": "q2", "question": "What is the model task of YiTian/wav2vec2-common_voice-tr-demo?", "answers": [{"text": "automatic-speech-recognition", "answer_start": 47, "answer_end": 74}]}]}]}, {"title": "Finnish-NLP/t5-mini-nl8-finnish", "paragraphs": [{"context": "---\nlanguage:\n- fi\nlicense: apache-2.0\ntags:\n- finnish\n- t5\n- t5x\n- seq2seq\ndatasets:\n- Finnish-NLP/mc4_fi_cleaned\n- wikipedia\ninference: false\n\n---\n\n# T5-mini-nl8 for Finnish\n\nPretrained T5 model on Finnish language using a span-based masked language modeling (MLM) objective. T5 was introduced in\n[this paper](\nand first released at [this page](\n\n**Note:** The Hugging Face inference widget is deactivated because this model needs a text-to-text fine-tuning on a specific downstream task to be useful in practice. As an example of a fine-tuned Finnish T5 model, you can check [Finnish-NLP/t5-small-nl24-casing-punctuation-correction]( which has been fine-tuned to correct missing casing and punctuation for Finnish text.\n\n## Model description\n\nT5 is an encoder-decoder model and treats all NLP problems in a text-to-text format.\n\nFinnish T5 is a transformers model pretrained on a very large corpus of Finnish data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and outputs from those texts.\n\nMore precisely, it was pretrained with the span-based masked language modeling (MLM) objective. Spans of the input sequence are masked by so-called sentinel tokens (a.k.a unique mask tokens) and the output sequence is formed as a concatenation of the same sentinel tokens and the real masked tokens. This way, the model learns an inner representation of the Finnish language.\n\nThis model used the [T5 v1.1]( improvements compared to the original T5 model during the pretraining:\n- GEGLU activation in feed-forward hidden layer, rather than ReLU - see [here](\n- Dropout was turned off in pretraining (quality win). Dropout should be re-enabled during fine-tuning\n- Pretrained on span-based masked language modeling (MLM) objective only without mixing in the downstream tasks\n- No parameter sharing between embedding and classifier layer\n\nThis model also used the \"efficient\" T5 architecture findings presented in [this paper]( In a nutshell, the paper indicates that a Deep-Narrow model architecture is favorable for downstream performance compared to other model architectures of similar parameter count. To be more precise, model depth is defined as the number of transformer blocks that are stacked sequentially.\n\nThis model uses the [t5-efficient-mini-nl8]( architecture's layer depth which means both the encoder and the decoder have 8 transformer layers compared to the original T5 \"mini\" model's architecture of 4 transformer layers.\n\nIn total, this model has 72 million parameters.\n\n## Intended uses & limitations\n\nThis model was only pretrained in a self-supervised way excluding any supervised training. Therefore, this model has to be fine-tuned before it is usable on a downstream task, like text classification, unlike the Google's original T5 model. **Note:** You most likely need to fine-tune these T5 models without mixed precision so fine-tune them with full fp32 precision. You can also find more fine-tuning tips from [here]( for example.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"Finnish-NLP/t5-mini-nl8-finnish\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"Finnish-NLP/t5-mini-nl8-finnish\")\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import T5Tokenizer, TFT5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"Finnish-NLP/t5-mini-nl8-finnish\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"Finnish-NLP/t5-mini-nl8-finnish\", from_pt=True)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model can have biased predictions. This bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThis Finnish T5 model was pretrained on the combination of six datasets:\n- [mc4_fi_cleaned]( the dataset mC4 is a multilingual colossal, cleaned version of Common Crawl's web crawl corpus. We used the Finnish subset of the mC4 dataset and further cleaned it with our own text data cleaning codes (check the dataset repo).\n- [wikipedia]( We used the Finnish subset of the wikipedia (August 2021) dataset\n- [Yle Finnish News Archive 2011-2018](\n- [Yle Finnish News Archive 2019-2020](\n- [Finnish News Agency Archive (STT)](\n- [The Suomi24 Sentences Corpus](\n\nRaw datasets were automatically cleaned to filter out bad quality and non-Finnish examples. Also, a [perplexity]( score was calculated for all texts with a KenLM model which was trained with very clean Finnish texts only. This perplexity score can then be used to determine how \"clean\" Finnish language the text contains. Lastly, all datasets were concatenated and the top 90% perplexity score was used as a filtering threshold to filter out the worst quality 10% of texts. Together these cleaned datasets were around 76GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using WordPiece and a vocabulary size of 32000. The inputs and the outputs are sequences of 512 consecutive tokens. Texts are not lower cased so this model is case-sensitive: it makes a difference between finnish and Finnish.\n\n### Pretraining\n\nThe model was trained on TPUv3-8 VM, sponsored by the [Google TPU Research Cloud]( for 500K steps with a batch size of 256 (in total 66B tokens). The optimizer used was a AdaFactor with learning rate warmup for 10K steps with a constant learning rate of 1e-2, and then an inverse square root decay (exponential decay) of the learning rate after.\n\nTraining code was from the Google's Jax/Flax based [t5x framework]( and also some t5x task definitions were adapted from [Per's t5x work](\n\n## Evaluation results\n\nEvaluation was done by fine-tuning the model on a downstream text classification task with two different labeled Finnish datasets: [Yle News]( and [Eduskunta]( Classification fine-tuning was done with a sequence length of 128 tokens.\n\nWhen fine-tuned on those datasets, this model (the second row of the table) achieves the following accuracy results compared to our other T5 models and their parameter counts:\n\n Model parameters  Eduskunta accuracy   |\n----------------------------------------|\n 31 million       69.07                 |\n 72 million       71.43                 |\n 184 million      74.00                 |\n 260 million      74.90                 |\n 582 million      73.13                 |\n 814 million      **75.97**             |\n 1425 million     73.50                 |\n\n\nFine-tuning Google's multilingual mT5 models on the same datasets we can clearly see that our monolingual Finnish T5 models achieve much better results on Finnish text classification:\n\n Model parameters  Eduskunta accuracy   |\n----------------------------------------|\n 301 million      64.10                 |\n 583 million      68.40                 |\n\n\n## Acknowledgements\n\nThis project would not have been possible without compute generously provided by Google through the\n[TPU Research Cloud](\n\n## Team Members\n\n- Aapo Tanskanen, [Hugging Face profile]( [LinkedIn profile](\n- Rasmus Toivanen, [Hugging Face profile]( [LinkedIn profile](\n\nFeel free to contact us for more details \ud83e\udd17", "qas": [{"id": "q1", "question": "What is the model architecture of Finnish-NLP/t5-mini-nl8-finnish?", "answers": [{"text": "t5", "answer_start": 57, "answer_end": 58}]}]}]}, {"title": "Yaxin/xlm-roberta-base-amazon-en-es-fr-mlm", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- Yaxin/amazon_reviews_multi\nmetrics:\n- accuracy\nmodel-index:\n- name: xlm-roberta-base-amazon-en-es-fr-mlm\n  results:\n  - task:\n      name: Masked Language Modeling\n      type: fill-mask\n    dataset:\n      name: Yaxin/amazon_reviews_multi\n      type: Yaxin/amazon_reviews_multi\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.6951035447140035\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-amazon-en-es-fr-mlm\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the Yaxin/amazon_reviews_multi dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.3936\n- Accuracy: 0.6951\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 6\n- eval_batch_size: 6\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- Pytorch 1.11.0\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Yaxin/xlm-roberta-base-amazon-en-es-fr-mlm?", "answers": [{"text": "xlm-roberta", "answer_start": 128, "answer_end": 138}]}, {"id": "q2", "question": "What is the model task of Yaxin/xlm-roberta-base-amazon-en-es-fr-mlm?", "answers": [{"text": "fill-mask", "answer_start": 235, "answer_end": 243}]}]}]}, {"title": "Daniel-Saeedi/YouAreFakeNews", "paragraphs": [{"context": "---\nlicense: mit\n---\n", "qas": []}]}, {"title": "novarac23/distilbert-base-uncased-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotion\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: default\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.925\n    - name: F1\n      type: f1\n      value: 0.9251919899321654\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2234\n- Accuracy: 0.925\n- F1: 0.9252\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.3210           0.8989 |\n 2.0    0.2234           0.9252 |\n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of novarac23/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 121, "answer_end": 130}]}, {"id": "q2", "question": "What is the model task of novarac23/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "text-classification", "answer_start": 228, "answer_end": 246}]}]}]}, {"title": "AAAA-4/DialoGPT-small-player_03", "paragraphs": [{"context": "---\ntags:\n- conversational\n---\n\n# Run 3 :)\n# An exceedingly special thanks to Lynn Zheng for the tutorial on how to do this.", "qas": [{"id": "q2", "question": "What is the model task of AAAA-4/DialoGPT-small-player_03?", "answers": [{"text": "conversational", "answer_start": 12, "answer_end": 25}]}]}]}, {"title": "joniponi/discharge-classifier", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: discharge-classifier\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# discharge-classifier\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2473\n- Accuracy: 0.9172\n- F1: 0.9169\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.4780           0.7654 |\n 2.0    0.2975           0.8849 |\n 3.0    0.2473           0.9169 |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.12.0\n", "qas": [{"id": "q1", "question": "What is the model architecture of joniponi/discharge-classifier?", "answers": [{"text": "distilbert", "answer_start": 392, "answer_end": 401}]}]}]}, {"title": "osanseviero/llama-alpaca-snake", "paragraphs": [{"context": "---\ntags:\n- image-classification\n- pytorch\n- huggingpics\n- llama-leaderboard\nmetrics:\n- accuracy\n\nmodel-index:\n- name: llama-alpaca-snake\n  results:\n  - task:\n      name: Image Classification\n      type: image-classification\n    metrics:\n      - name: Accuracy\n        type: accuracy\n        value: 0.7910447716712952\n---\n\n# llama-alpaca-snake\n\n\nAutogenerated by HuggingPics\ud83e\udd17\ud83d\uddbc\ufe0f\n\nCreate your own image classifier for **anything** by running [the demo on Google Colab](\n\nReport any issues with the demo at the [github repo](\n\n\n## Example Images\n\n\n#### alpaca\n\n![alpaca](images/alpaca.jpg)\n\n#### llamas\n\n![llamas](images/llamas.jpg)\n\n#### snake\n\n![snake](images/snake.jpg)", "qas": [{"id": "q2", "question": "What is the model task of osanseviero/llama-alpaca-snake?", "answers": [{"text": "image-classification", "answer_start": 12, "answer_end": 31}]}]}]}, {"title": "notexist/ttt", "paragraphs": [{"context": "---\nlicense: apache-2.0\n---\n", "qas": []}]}, {"title": "antonio-artur/distilbert-base-uncased-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotion\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: default\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.926\n    - name: F1\n      type: f1\n      value: 0.9260113300845928\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2280\n- Accuracy: 0.926\n- F1: 0.9260\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.3326           0.9009 |\n 2.0    0.2280           0.9260 |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.11.0+cu102\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of antonio-artur/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 121, "answer_end": 130}]}, {"id": "q2", "question": "What is the model task of antonio-artur/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "text-classification", "answer_start": 228, "answer_end": 246}]}]}]}, {"title": "notexist/ttt2", "paragraphs": [{"context": "---\nlicense: apache-2.0\n---\n", "qas": []}]}, {"title": "alexjercan/codet5-base-buggy-error-description", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: codet5-base-buggy-error-description\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# codet5-base-buggy-error-description\n\nThis model is a fine-tuned version of [Salesforce/codet5-base]( on an unknown dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 1\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.9.1\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of alexjercan/codet5-base-buggy-error-description?", "answers": [{"text": "t5", "answer_start": 80, "answer_end": 81}]}]}]}, {"title": "alexjercan/codebert-base-buggy-token-classification", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: codebert-base-buggy-token-classification\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# codebert-base-buggy-token-classification\n\nThis model is a fine-tuned version of [microsoft/codebert-base]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5217\n- Precision: 0.6942\n- Recall: 0.0940\n- F1: 0.1656\n- Accuracy: 0.7714\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.9.1\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q2", "question": "What is the model task of alexjercan/codebert-base-buggy-token-classification?", "answers": [{"text": "token-classification", "answer_start": 122, "answer_end": 141}]}]}]}, {"title": "pinku/FatimaFellowship_fake_and_real_news", "paragraphs": [{"context": "---\nlicense: bsd-3-clause\n---\n# Fatima Fellowship NLP Project\n\n## Fake News Classifier\n\n- BERT base model finetuned to classify fake news.", "qas": []}]}, {"title": "avialfont/dummy-translation-marian-kde4-en-to-fr", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: avialfont/dummy-translation-marian-kde4-en-to-fr\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# avialfont/dummy-translation-marian-kde4-en-to-fr\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-en-fr]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.9807\n- Validation Loss: 0.8658\n- Epoch: 0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 5e-05, 'decay_steps': 17733, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 0.8658          \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- TensorFlow 2.8.0\n- Datasets 1.18.3\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of avialfont/dummy-translation-marian-kde4-en-to-fr?", "answers": [{"text": "marian", "answer_start": 111, "answer_end": 116}]}]}]}, {"title": "HenryHXR/scibert_scivocab_uncased-finetuned-ner", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: scibert_scivocab_uncased-finetuned-ner\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# scibert_scivocab_uncased-finetuned-ner\n\nThis model is a fine-tuned version of [allenai/scibert_scivocab_uncased]( on the None dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of HenryHXR/scibert_scivocab_uncased-finetuned-ner?", "answers": [{"text": "bert", "answer_start": 59, "answer_end": 62}]}]}]}, {"title": "HenryHXR/scibert_scivocab_uncased_epoch20-finetuned-ner", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: scibert_scivocab_uncased_epoch20-finetuned-ner\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# scibert_scivocab_uncased_epoch20-finetuned-ner\n\nThis model is a fine-tuned version of [allenai/scibert_scivocab_uncased]( on the None dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 20\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of HenryHXR/scibert_scivocab_uncased_epoch20-finetuned-ner?", "answers": [{"text": "bert", "answer_start": 59, "answer_end": 62}]}]}]}, {"title": "novarac23/xlm-roberta-base-finetuned-panx-de", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- xtreme\nmetrics:\n- f1\nmodel-index:\n- name: xlm-roberta-base-finetuned-panx-de\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: xtreme\n      type: xtreme\n      args: PAN-X.de\n    metrics:\n    - name: F1\n      type: f1\n      value: 0.862669465085938\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-base-finetuned-panx-de\n\nThis model is a fine-tuned version of [xlm-roberta-base]( on the xtreme dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1374\n- F1: 0.8627\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.1571          \n 2.0    0.1416          \n 3.0    0.1374          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of novarac23/xlm-roberta-base-finetuned-panx-de?", "answers": [{"text": "xlm-roberta", "answer_start": 102, "answer_end": 112}]}, {"id": "q2", "question": "What is the model task of novarac23/xlm-roberta-base-finetuned-panx-de?", "answers": [{"text": "token-classification", "answer_start": 203, "answer_end": 222}]}]}]}, {"title": "Shadman-Rohan/distilbert-base-uncased-finetuned-emotion", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotion\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: default\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9245\n    - name: F1\n      type: f1\n      value: 0.9247907524762314\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotion\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2083\n- Accuracy: 0.9245\n- F1: 0.9248\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.2870           0.9099 |\n 2.0    0.2083           0.9248 |\n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Shadman-Rohan/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "distilbert", "answer_start": 121, "answer_end": 130}]}, {"id": "q2", "question": "What is the model task of Shadman-Rohan/distilbert-base-uncased-finetuned-emotion?", "answers": [{"text": "text-classification", "answer_start": 228, "answer_end": 246}]}]}]}, {"title": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", "paragraphs": [{"context": "---\nlanguage: en\ndatasets:\n- msp-podcast\ninference: true\ntags:\n- speech\n- audio\n- wav2vec2\n- audio-classification\n- emotion-recognition\nlicense: cc-by-nc-sa-4.0\n---\n\n# Model for Dimensional Speech Emotion Recognition based on Wav2vec 2.0\n\nThe model expects a raw audio signal as input and outputs predictions for arousal, dominance and valence in a range of approximately 0...1. In addition, it also provides the pooled states of the last transformer layer. The model was created by fine-tuning [\nWav2Vec2-Large-Robust]( on [MSP-Podcast]( (v1.7). The model was pruned from 24 to 12 transformer layers before fine-tuning. An [ONNX]( export of the model is available from [doi:10.5281/zenodo.6221127]( Further details are given in the associated [paper]( and [tutorial](\n\n# Usage\n\n```python\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import Wav2Vec2Processor\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (\n    Wav2Vec2Model,\n    Wav2Vec2PreTrainedModel,\n)\n\n\nclass RegressionHead(nn.Module):\n    r\"\"\"Classification head.\"\"\"\n\n    def __init__(self, config):\n\n        super().__init__()\n\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.final_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n\n        return x\n\n\nclass EmotionModel(Wav2Vec2PreTrainedModel):\n    r\"\"\"Speech emotion classifier.\"\"\"\n\n    def __init__(self, config):\n\n        super().__init__(config)\n\n        self.config = config\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.classifier = RegressionHead(config)\n        self.init_weights()\n\n    def forward(\n            self,\n            input_values,\n    ):\n\n        outputs = self.wav2vec2(input_values)\n        hidden_states = outputs[0]\n        hidden_states = torch.mean(hidden_states, dim=1)\n        logits = self.classifier(hidden_states)\n\n        return hidden_states, logits\n\n\n\n# load model from hub\ndevice = 'cpu'\nmodel_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\nprocessor = Wav2Vec2Processor.from_pretrained(model_name)\nmodel = EmotionModel.from_pretrained(model_name)\n\n# dummy signal\nsampling_rate = 16000\nsignal = np.zeros((1, sampling_rate), dtype=np.float32)\n\n\ndef process_func(\n    x: np.ndarray,\n    sampling_rate: int,\n    embeddings: bool = False,\n) -> np.ndarray:\n    r\"\"\"Predict emotions or extract embeddings from raw audio signal.\"\"\"\n\n    # run through processor to normalize signal\n    # always returns a batch, so we just get the first entry\n    # then we put it on the device\n    y = processor(x, sampling_rate=sampling_rate)\n    y = y['input_values'][0]\n    y = torch.from_numpy(y).to(device)\n\n    # run through model\n    with torch.no_grad():\n        y = model(y)[0 if embeddings else 1]\n\n    # convert to numpy\n    y = y.detach().cpu().numpy()\n\n    return y\n\n\nprocess_func(signal, sampling_rate)\n#  Arousal    dominance valence\n# [[0.5460759 0.6062269 0.4043165]]\n\nprocess_func(signal, sampling_rate, embeddings=True)\n# Pooled hidden states of last transformer layer\n# [[-0.00752167  0.0065819  -0.00746339 ...  0.00663631  0.00848747\n#   0.00599209]]\n```\n", "qas": [{"id": "q1", "question": "What is the model architecture of audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim?", "answers": [{"text": "wav2vec2", "answer_start": 82, "answer_end": 89}]}, {"id": "q2", "question": "What is the model task of audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim?", "answers": [{"text": "audio-classification", "answer_start": 93, "answer_end": 112}]}, {"id": "q3", "question": "What is the model category of audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim?", "answers": [{"text": "audio", "answer_start": 74, "answer_end": 78}]}]}]}, {"title": "Sleoruiz/distilbert-base-uncased-finetuned-cola", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- matthews_correlation\nmodel-index:\n- name: distilbert-base-uncased-finetuned-cola\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: glue\n      type: glue\n      args: cola\n    metrics:\n    - name: Matthews Correlation\n      type: matthews_correlation\n      value: 0.5396261051709696\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-cola\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7663\n- Matthews Correlation: 0.5396\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.5268          \n 2.0    0.5074          \n 3.0    0.6440          \n 4.0    0.7663          \n 5.0    0.8786          \n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of Sleoruiz/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "distilbert", "answer_start": 125, "answer_end": 134}]}, {"id": "q2", "question": "What is the model task of Sleoruiz/distilbert-base-uncased-finetuned-cola?", "answers": [{"text": "text-classification", "answer_start": 229, "answer_end": 247}]}]}]}, {"title": "junnyu/flash_base_wwm_cluecorpussmall", "paragraphs": [{"context": "---\nlicense: mit\ninference: False\n---\n\n# PS: \u6548\u679c\u4e0d\u600e\u4e48\u597d\uff0c\u4f53\u9a8c\u4e00\u4e0b\u5c31\u884c\u4e86\u3002\u3002\u3002\u3002\u3002\u3002wwm-MLM\u6700\u7ec8\u51c6\u786e\u738755.5\u5de6\u53f3\u3002\n# cluner NER\u5b9e\u9a8c(globalpointer\u7684\u7ed3\u679c\u5dee\u4e0d\u591a\uff0csoftmax\u7ed3\u679c\u5dee\u597d\u591a- -)\n```python\n# flash base  + globalpointer\n04/08/2022 10:53:34 - INFO - __main__ - ADDRESS = Score(f1=0.607703, precision=0.64939, recall=0.571046, tp=213, pred=328, gold=373)\n04/08/2022 10:53:34 - INFO - __main__ - BOOK = Score(f1=0.8125, precision=0.873134, recall=0.75974, tp=117, pred=134, gold=154)\n04/08/2022 10:53:34 - INFO - __main__ - COMPANY = Score(f1=0.818304, precision=0.832877, recall=0.804233, tp=304, pred=365, gold=378)\n04/08/2022 10:53:34 - INFO - __main__ - GAME = Score(f1=0.854305, precision=0.834951, recall=0.874576, tp=258, pred=309, gold=295)\n04/08/2022 10:53:34 - INFO - __main__ - GOVERNMENT = Score(f1=0.823529, precision=0.775, recall=0.878543, tp=217, pred=280, gold=247)\n04/08/2022 10:53:34 - INFO - __main__ - MOVIE = Score(f1=0.810997, precision=0.842857, recall=0.781457, tp=118, pred=140, gold=151)\n04/08/2022 10:53:34 - INFO - __main__ - NAME = Score(f1=0.874042, precision=0.890625, recall=0.858065, tp=399, pred=448, gold=465)\n04/08/2022 10:53:34 - INFO - __main__ - ORGANIZATION = Score(f1=0.813986, precision=0.836207, recall=0.792916, tp=291, pred=348, gold=367)\n04/08/2022 10:53:34 - INFO - __main__ - POSITION = Score(f1=0.78478, precision=0.808824, recall=0.762125, tp=330, pred=408, gold=433)\n04/08/2022 10:53:34 - INFO - __main__ - SCENE = Score(f1=0.683805, precision=0.738889, recall=0.636364, tp=133, pred=180, gold=209)\n04/08/2022 10:53:34 - INFO - __main__ - micro_f1 = Score(f1=0.79175, precision=0.809524, recall=0.77474, tp=2380, pred=2940, gold=3072)\n04/08/2022 10:53:34 - INFO - __main__ - macro_f1 = Score(f1=0.788395, precision=0.808275, recall=0.771906, tp=0, pred=0, gold=0)\n04/08/2022 10:53:34 - INFO - __main__ - mean_f1 = 0.790072\n\n# flash base  + softmax\n04/08/2022 11:10:44 - INFO - __main__ - ADDRESS = Score(f1=0.568987, precision=0.522422, recall=0.624665, tp=233, pred=446, gold=373)\n04/08/2022 11:10:44 - INFO - __main__ - BOOK = Score(f1=0.750789, precision=0.730061, recall=0.772727, tp=119, pred=163, gold=154)\n04/08/2022 11:10:44 - INFO - __main__ - COMPANY = Score(f1=0.75528, precision=0.711944, recall=0.804233, tp=304, pred=427, gold=378)\n04/08/2022 11:10:44 - INFO - __main__ - GAME = Score(f1=0.811502, precision=0.767372, recall=0.861017, tp=254, pred=331, gold=295)\n04/08/2022 11:10:44 - INFO - __main__ - GOVERNMENT = Score(f1=0.738636, precision=0.69395, recall=0.789474, tp=195, pred=281, gold=247)\n04/08/2022 11:10:44 - INFO - __main__ - MOVIE = Score(f1=0.74359, precision=0.720497, recall=0.768212, tp=116, pred=161, gold=151)\n04/08/2022 11:10:44 - INFO - __main__ - NAME = Score(f1=0.831967, precision=0.794521, recall=0.873118, tp=406, pred=511, gold=465)\n04/08/2022 11:10:44 - INFO - __main__ - ORGANIZATION = Score(f1=0.754054, precision=0.747989, recall=0.760218, tp=279, pred=373, gold=367)\n04/08/2022 11:10:44 - INFO - __main__ - POSITION = Score(f1=0.742729, precision=0.720174, recall=0.766744, tp=332, pred=461, gold=433)\n04/08/2022 11:10:44 - INFO - __main__ - SCENE = Score(f1=0.628842, precision=0.621495, recall=0.636364, tp=133, pred=214, gold=209)\n04/08/2022 11:10:44 - INFO - __main__ - micro_f1 = Score(f1=0.736335, precision=0.703979, recall=0.77181, tp=2371, pred=3368, gold=3072)\n04/08/2022 11:10:44 - INFO - __main__ - macro_f1 = Score(f1=0.732638, precision=0.703043, recall=0.765677, tp=0, pred=0, gold=0)\n04/08/2022 11:10:44 - INFO - __main__ - mean_f1 = 0.734486\n\n\n# bert base + globalpointer\n04/08/2022 11:22:48 - INFO - __main__ - ADDRESS = Score(f1=0.641558, precision=0.622166, recall=0.662198, tp=247, pred=397, gold=373)\n04/08/2022 11:22:48 - INFO - __main__ - BOOK = Score(f1=0.813115, precision=0.821192, recall=0.805195, tp=124, pred=151, gold=154)\n04/08/2022 11:22:48 - INFO - __main__ - COMPANY = Score(f1=0.823684, precision=0.819372, recall=0.828042, tp=313, pred=382, gold=378)\n04/08/2022 11:22:48 - INFO - __main__ - GAME = Score(f1=0.841762, precision=0.811321, recall=0.874576, tp=258, pred=318, gold=295)\n04/08/2022 11:22:48 - INFO - __main__ - GOVERNMENT = Score(f1=0.827324, precision=0.778571, recall=0.882591, tp=218, pred=280, gold=247)\n04/08/2022 11:22:48 - INFO - __main__ - MOVIE = Score(f1=0.82392, precision=0.826667, recall=0.821192, tp=124, pred=150, gold=151)\n04/08/2022 11:22:48 - INFO - __main__ - NAME = Score(f1=0.861345, precision=0.840164, recall=0.883621, tp=410, pred=488, gold=464)\n04/08/2022 11:22:48 - INFO - __main__ - ORGANIZATION = Score(f1=0.804911, precision=0.806011, recall=0.803815, tp=295, pred=366, gold=367)\n04/08/2022 11:22:48 - INFO - __main__ - POSITION = Score(f1=0.805046, precision=0.799544, recall=0.810624, tp=351, pred=439, gold=433)\n04/08/2022 11:22:48 - INFO - __main__ - SCENE = Score(f1=0.702703, precision=0.722222, recall=0.684211, tp=143, pred=198, gold=209)\n04/08/2022 11:22:48 - INFO - __main__ - micro_f1 = Score(f1=0.795833, precision=0.783528, recall=0.808531, tp=2483, pred=3169, gold=3071)\n04/08/2022 11:22:48 - INFO - __main__ - macro_f1 = Score(f1=0.794537, precision=0.784723, recall=0.805606, tp=0, pred=0, gold=0)\n04/08/2022 11:22:48 - INFO - __main__ - mean_f1 = 0.795185\n```\n\n# cmeee + globalpointer\n```python\n04/08/2022 11:50:41 - INFO - __main__ - bod = Score(f1=0.639522, precision=0.642318, recall=0.63675, tp=3746, pred=5832, gold=5883)\n04/08/2022 11:50:41 - INFO - __main__ - dep = Score(f1=0.473988, precision=0.650794, recall=0.372727, tp=41, pred=63, gold=110)\n04/08/2022 11:50:41 - INFO - __main__ - dis = Score(f1=0.716959, precision=0.704479, recall=0.729889, tp=3602, pred=5113, gold=4935)\n04/08/2022 11:50:41 - INFO - __main__ - dru = Score(f1=0.756328, precision=0.829329, recall=0.695139, tp=1001, pred=1207, gold=1440)\n04/08/2022 11:50:41 - INFO - __main__ - equ = Score(f1=0.518703, precision=0.638037, recall=0.436975, tp=104, pred=163, gold=238)\n04/08/2022 11:50:41 - INFO - __main__ - ite = Score(f1=0.322533, precision=0.503448, recall=0.23727, tp=219, pred=435, gold=923)\n04/08/2022 11:50:41 - INFO - __main__ - mic = Score(f1=0.746967, precision=0.75614, recall=0.738014, tp=431, pred=570, gold=584)\n04/08/2022 11:50:41 - INFO - __main__ - pro = Score(f1=0.611138, precision=0.614138, recall=0.608167, tp=1251, pred=2037, gold=2057)\n04/08/2022 11:50:41 - INFO - __main__ - sym = Score(f1=0.47969, precision=0.495738, recall=0.464649, tp=1919, pred=3871, gold=4130)\n04/08/2022 11:50:41 - INFO - __main__ - micro_f1 = Score(f1=0.622061, precision=0.638329, recall=0.606601, tp=12314, pred=19291, gold=20300)\n04/08/2022 11:50:41 - INFO - __main__ - macro_f1 = Score(f1=0.585092, precision=0.648269, recall=0.54662, tp=0, pred=0, gold=0)\n04/08/2022 11:50:41 - INFO - __main__ - mean_f1 = 0.603576\n```\n# install\n- \n\n# usage\n```python\nimport torch\nfrom flash import FLASHForMaskedLM\nfrom transformers import BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"junnyu/flash_base_wwm_cluecorpussmall\")\nmodel = FLASHForMaskedLM.from_pretrained(\"junnyu/flash_base_wwm_cluecorpussmall\")\nmodel.eval()\ntext = \"\u5929\u6c14\u9884\u62a5\u8bf4\u4eca\u5929\u7684\u5929[MASK]\u5f88\u597d\uff0c\u90a3\u4e48\u6211[MASK]\u4e00\u8d77\u53bb\u516c\u56ed\u73a9\u5427\uff01\"\ninputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=512,  return_token_type_ids=False) #\u8fd9\u91cc\u5fc5\u987b\u662f512\uff0c\u4e0d\u7136\u7ed3\u679c\u53ef\u80fd\u4e0d\u5bf9\u3002\nwith torch.no_grad():\n    pt_outputs = model(**inputs).logits[0]\n\npt_outputs_sentence = \"pytorch: \"\nfor i, id in enumerate(tokenizer.encode(text)):\n    if id == tokenizer.mask_token_id:\n        val,idx = pt_outputs[i].softmax(-1).topk(k=5)\n        tokens = tokenizer.convert_ids_to_tokens(idx)\n        new_tokens = []\n        for v,t in zip(val.cpu(),tokens):\n            new_tokens.append(f\"{t}+{round(v.item(),4)}\")\n        pt_outputs_sentence += \"[\" + \"\".join(new_tokens) + \"]\"\n    else:\n        pt_outputs_sentence += \"\".join(\n            tokenizer.convert_ids_to_tokens([id], skip_special_tokens=True))\nprint(pt_outputs_sentence)\n# pytorch: \u5929\u6c14\u9884\u62a5\u8bf4\u4eca\u5929\u7684\u5929[\u6c14+0.994\u5929+0.0015\u7a7a+0.0014\u6674+0.0005\u9633+0.0003]\u5f88\u597d\uff0c\u90a3\u4e48\u6211[\u4eec+0.9563\u5c31+0.0381\u4e5f+0.0032\u4fe9+0.0004\u6765+0.0002]\u4e00\u8d77\u53bb\u516c\u56ed\u73a9\u5427\uff01\n```", "qas": [{"id": "q1", "question": "What is the model architecture of junnyu/flash_base_wwm_cluecorpussmall?", "answers": [{"text": "flash", "answer_start": 149, "answer_end": 153}]}]}]}, {"title": "philschmid/roberta-large-sst2", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- glue\nmetrics:\n- accuracy\nmodel-index:\n- name: roberta-large-sst2\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: GLUE SST2\n      type: glue\n      args: sst2\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9644495412844036\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-large-sst2\n\nThis model is a fine-tuned version of [roberta-large]( on the glue dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1400\n- Accuracy: 0.9644\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- distributed_type: sagemaker_data_parallel\n- num_devices: 8\n- total_train_batch_size: 256\n- total_eval_batch_size: 256\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 4\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.1444          \n 2.0    0.1502          \n 3.0    0.1388          \n 4.0    0.1400          \n\n\n### Framework versions\n\n- Transformers 4.17.0\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of philschmid/roberta-large-sst2?", "answers": [{"text": "roberta", "answer_start": 106, "answer_end": 112}]}, {"id": "q2", "question": "What is the model task of philschmid/roberta-large-sst2?", "answers": [{"text": "text-classification", "answer_start": 190, "answer_end": 208}]}]}]}, {"title": "jaeyeon/wav2vec2-child-en-tokenizer-4", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: wav2vec2-child-en-tokenizer-4\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-child-en-tokenizer-4\n\nThis model is a fine-tuned version of [facebook/wav2vec2-xls-r-300m]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.4709\n- Wer: 0.3769\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 48\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.72   1.4709          \n 3.45   1.4709          \n 5.17   1.4709          \n 6.9    1.4709          \n 8.62   1.4709          \n 10.34  1.4709          \n 12.07  1.4709          \n 13.79  1.4709          \n 15.52  1.4709          \n 17.24  1.4709          \n 18.97  1.4709          \n 20.69  1.4709          \n 22.41  1.4709          \n 24.14  1.4709          \n 25.86  1.4709          \n 27.59  1.4709          \n 29.31  1.4709          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu113\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of jaeyeon/wav2vec2-child-en-tokenizer-4?", "answers": [{"text": "wav2vec2", "answer_start": 76, "answer_end": 83}]}]}]}, {"title": "huggingtweets/lilpeeplyric", "paragraphs": [{"context": "---\nlanguage: en\nthumbnail: \ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n        <div\n            style=\"display:none; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800\">\ud83e\udd16 AI BOT \ud83e\udd16</div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">lil peep lyrics bot</div>\n    <div style=\"text-align: center; font-size: 14px;\">@lilpeeplyric</div>\n</div>\n\nI was made with [huggingtweets](\n\nCreate your own bot based on your favorite user with [the demo](\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](\n\nTo understand how the model was developed, check the [W&B report](\n\n## Training data\n\nThe model was trained on tweets from lil peep lyrics bot.\n\n lil peep lyrics bot |\n --- |\n 3250 |\n 0 |\n 0 |\n 3250 |\n\n[Explore the data]( which is tracked with [W&B artifacts]( at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2]( which is fine-tuned on @lilpeeplyric's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run]( for full transparency and reproducibility.\n\nAt the end of training, [the final model]( is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/lilpeeplyric')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](\n\nFor more details, visit the project repository.\n\n[![GitHub stars](\n", "qas": [{"id": "q2", "question": "What is the model task of huggingtweets/lilpeeplyric?", "answers": [{"text": "text-generation", "answer_start": 1980, "answer_end": 1994}]}]}]}, {"title": "avialfont/dummy-finetuned-amazon-en-es", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: avialfont/dummy-finetuned-amazon-en-es\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# avialfont/dummy-finetuned-amazon-en-es\n\nThis model is a fine-tuned version of [google/mt5-small]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 5.6755\n- Validation Loss: 3.8033\n- Epoch: 2\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 5.6e-05, 'decay_steps': 3627, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\n- training_precision: float32\n\n### Training results\n\n Validation Loss \n:---------------:\n 4.4915          \n 3.9207          \n 3.8033          \n\n\n### Framework versions\n\n- Transformers 4.16.2\n- TensorFlow 2.8.0\n- Datasets 1.18.3\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of avialfont/dummy-finetuned-amazon-en-es?", "answers": [{"text": "mt5", "answer_start": 411, "answer_end": 413}]}]}]}, {"title": "HenryHXR/t5-base-finetuned-scitldr-only-abstract", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: t5-base-finetuned-scitldr-only-abstract\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-finetuned-scitldr-only-abstract\n\nThis model is a fine-tuned version of [t5-base]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.3365\n- Rouge1: 34.3531\n- Rouge2: 15.7554\n- Rougel: 29.8918\n- Rougelsum: 29.9514\n- Gen Len: 18.7658\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-06\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss  Rouge2   Rougelsum \n:-----::---------------::-------::---------:\n 1.0    2.3649           15.5031  29.5576   \n 2.0    2.3365           15.7554  29.9514   \n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of HenryHXR/t5-base-finetuned-scitldr-only-abstract?", "answers": [{"text": "t5", "answer_start": 93, "answer_end": 94}]}]}]}, {"title": "jicoc22578/autotrain-livedoor_news-722922024", "paragraphs": [{"context": "---\ntags: autotrain\nlanguage: ja\nwidget:\n- text: \"Windows 11\u642d\u8f09PC\u3092\u8cb7\u3063\u305f\u3089\u6700\u4f4e\u9650\u3084\u3063\u3066\u304a\u304d\u305f\u3044\u3053\u3068\"\n- text: \"3\u6708\u30c7\u30b9\u30af\u30c8\u30c3\u30d7OS\u30b7\u30a7\u30a2\u3001Windows\u304c\u5897\u52a0\u3057Mac\u304c\u6e1b\u5c11\"\n- text: \"raytrek\u3001Core i7-12700H\u3068RTX 3070 Ti\u3092\u642d\u8f09\u3059\u308b\u30ce\u30fc\u30c8PC\"\ndatasets:\n- jicoc22578/autotrain-data-livedoor_news\nco2_eq_emissions: 0.019299491458156143\n---\n\n# Model Trained Using AutoTrain\n\n- Problem type: Multi-class Classification\n- Model ID: 722922024\n- CO2 Emissions (in grams): 0.019299491458156143\n\n## Validation Metrics\n\n- Loss: 0.19609540700912476\n- Accuracy: 0.9457627118644067\n- Macro F1: 0.9404319054946133\n- Micro F1: 0.9457627118644067\n- Weighted F1: 0.9456037443251943\n- Macro Precision: 0.9420917371721244\n- Micro Precision: 0.9457627118644067\n- Weighted Precision: 0.9457910238180336\n- Macro Recall: 0.9391783746329772\n- Micro Recall: 0.9457627118644067\n- Weighted Recall: 0.9457627118644067\n\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoTrain\"}' \n```\n\nOr Python API:\n\n```\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"jicoc22578/autotrain-livedoor_news-722922024\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"jicoc22578/autotrain-livedoor_news-722922024\", use_auth_token=True)\n\ninputs = tokenizer(\"I love AutoTrain\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n```", "qas": []}]}, {"title": "iyedr8/DialoGPT-small-rick", "paragraphs": [{"context": "---\ntags:\n-  conversational\n---\n\n#Morty DialoGPT Model", "qas": [{"id": "q2", "question": "What is the model task of iyedr8/DialoGPT-small-rick?", "answers": [{"text": "conversational", "answer_start": 13, "answer_end": 26}]}]}]}, {"title": "malcolm/TSC_SentimentA_IMDBAmznTSC_2", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: TSC_SentimentA_IMDBAmznTSC_2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# TSC_SentimentA_IMDBAmznTSC_2\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1985\n- Accuracy: 0.9365\n- F1: 0.9373\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of malcolm/TSC_SentimentA_IMDBAmznTSC_2?", "answers": [{"text": "distilbert", "answer_start": 408, "answer_end": 417}]}]}]}, {"title": "optimum/roberta-large-finetuned-clinc", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- clinc_oos\nmetrics:\n- accuracy\nmodel-index:\n- name: roberta-large-finetuned-clinc\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: clinc_oos\n      type: clinc_oos\n      args: plus\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9729032258064516\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-large-finetuned-clinc\n\nThis model is a fine-tuned version of [roberta-large]( on the clinc_oos dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1574\n- Accuracy: 0.9729\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.8113          \n 2.0    0.2364          \n 3.0    0.1760          \n 4.0    0.1565          \n 5.0    0.1574          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.11.0\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of optimum/roberta-large-finetuned-clinc?", "answers": [{"text": "roberta", "answer_start": 111, "answer_end": 117}]}, {"id": "q2", "question": "What is the model task of optimum/roberta-large-finetuned-clinc?", "answers": [{"text": "text-classification", "answer_start": 206, "answer_end": 224}]}]}]}, {"title": "optimum/MiniLMv2-L12-H384-finetuned-clinc", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- clinc_oos\nmetrics:\n- accuracy\nmodel-index:\n- name: MiniLMv2-L12-H384-distilled-from-RoBERTa-Large-finetuned-clinc\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: clinc_oos\n      type: clinc_oos\n      args: plus\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9319354838709677\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# MiniLMv2-L12-H384-distilled-from-RoBERTa-Large-finetuned-clinc\n\nThis model is a fine-tuned version of [nreimers/MiniLMv2-L12-H384-distilled-from-RoBERTa-Large]( on the clinc_oos dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.5252\n- Accuracy: 0.9319\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 256\n- eval_batch_size: 256\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    4.6555          \n 2.0    3.8771          \n 3.0    3.2507          \n 4.0    2.7445          \n 5.0    2.3475          \n 6.0    2.0370          \n 7.0    1.8099          \n 8.0    1.6433          \n 9.0    1.5563          \n 10.0   1.5252          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.11.0\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of optimum/MiniLMv2-L12-H384-finetuned-clinc?", "answers": [{"text": "text-classification", "answer_start": 226, "answer_end": 244}]}]}]}, {"title": "optimum/MiniLMv2-L12-H384-distilled-finetuned-clinc", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- clinc_oos\nmetrics:\n- accuracy\nmodel-index:\n- name: MiniLMv2-L12-H384-distilled-from-RoBERTa-Large-distilled-clinc\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: clinc_oos\n      type: clinc_oos\n      args: plus\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.94\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# MiniLMv2-L12-H384-distilled-from-RoBERTa-Large-distilled-clinc\n\nThis model is a fine-tuned version of [nreimers/MiniLMv2-L12-H384-distilled-from-RoBERTa-Large]( on the clinc_oos dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3479\n- Accuracy: 0.94\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 256\n- eval_batch_size: 256\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    0.8171          \n 2.0    0.7039          \n 3.0    0.6067          \n 4.0    0.5270          \n 5.0    0.4659          \n 6.0    0.4201          \n 7.0    0.3867          \n 8.0    0.3649          \n 9.0    0.3520          \n 10.0   0.3479          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.11.0\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "qas": [{"id": "q2", "question": "What is the model task of optimum/MiniLMv2-L12-H384-distilled-finetuned-clinc?", "answers": [{"text": "text-classification", "answer_start": 226, "answer_end": 244}]}]}]}, {"title": "optimum/neuron-MiniLMv2-L12-H384-distilled-finetuned-clinc", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\ndatasets:\n- clinc_oos\nmetrics:\n- accuracy\nmodel-index:\n- name: MiniLMv2-L12-H384-distilled-from-RoBERTa-Large-distilled-clinc\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: clinc_oos\n      type: clinc_oos\n      args: plus\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.94\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n# Neuron conversation\n\n# MiniLMv2-L12-H384-distilled-from-RoBERTa-Large-distilled-clinc\n\nThis model is a fine-tuned version of [nreimers/MiniLMv2-L12-H384-distilled-from-RoBERTa-Large]( on the clinc_oos dataset.\nIt achieves the following results on the evaluation set:\n- Accuracy: 0.9389999\n\n## Deploy/use Model\n\nIf you want to use this model checkout the following notenbook: [sagemaker/18_inferentia_inference](\n\n```python\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   model_data=s3_model_uri,       # path to your model and script\n   role=role,                    # iam role with permissions to create an Endpoint\n   transformers_version=\"4.12\",  # transformers version used\n   pytorch_version=\"1.9\",        # pytorch version used\n   py_version='py37',            # python version used\n)\n\n# Let SageMaker know that we've already compiled the model via neuron-cc\nhuggingface_model._is_compiled_model = True\n\n# deploy the endpoint endpoint\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,      # number of instances\n    instance_type=\"ml.inf1.xlarge\" # AWS Inferentia Instance\n)\n```", "qas": [{"id": "q2", "question": "What is the model task of optimum/neuron-MiniLMv2-L12-H384-distilled-finetuned-clinc?", "answers": [{"text": "text-classification", "answer_start": 226, "answer_end": 244}]}]}]}, {"title": "rinapch/distilbert-media-bias", "paragraphs": [{"context": "---\nlicense: cc-by-sa-4.0\n---\n", "qas": []}]}, {"title": "nntadotzip/xlnet-base-cased-IUChatbot-ontologyDts-12April2022", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: xlnet-base-cased-IUChatbot-ontologyDts-12April2022\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlnet-base-cased-IUChatbot-ontologyDts-12April2022\n\nThis model is a fine-tuned version of [xlnet-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6500\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.7861          |\n 2.0    0.6727          |\n 3.0    0.6500          |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of nntadotzip/xlnet-base-cased-IUChatbot-ontologyDts-12April2022?", "answers": [{"text": "xlnet", "answer_start": 69, "answer_end": 73}]}]}]}, {"title": "nnair25/Gram-Vaani-Harveen-Chadda-Fine-Tuning", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n  name: Gram-Vaani-Harveen-Chadda-Fine-Tuning\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Gram-Vaani-Harveen-Chadda-Fine-Tuning\n\nThis model is a fine-tuned version of [Harveenchadha/vakyansh-wav2vec2-hindi-him-4200]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8934\n- Wer: 0.359\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 30\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 21.05  0.8934          \n\n\n### Framework versions\n\n- Transformers 4.11.3\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.10.3\n", "qas": [{"id": "q1", "question": "What is the model architecture of nnair25/Gram-Vaani-Harveen-Chadda-Fine-Tuning?", "answers": [{"text": "wav2vec2", "answer_start": 403, "answer_end": 410}]}]}]}, {"title": "ACSHCSE/distilbert-base-uncased-finetuned-ner", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- conll2003\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      args: conll2003\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.9230429988974642\n    - name: Recall\n      type: recall\n      value: 0.9365700861393892\n    - name: F1\n      type: f1\n      value: 0.9297573435504469\n    - name: Accuracy\n      type: accuracy\n      value: 0.983176322938345\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-ner\n\nThis model is a fine-tuned version of [distilbert-base-uncased]( on the conll2003 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0611\n- Precision: 0.9230\n- Recall: 0.9366\n- F1: 0.9298\n- Accuracy: 0.9832\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss  Recall  Accuracy |\n:-----::---------------::------::--------:|\n 1.0    0.0736           0.9211  0.9803   |\n 2.0    0.0582           0.9368  0.9830   |\n 3.0    0.0611           0.9366  0.9832   |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of ACSHCSE/distilbert-base-uncased-finetuned-ner?", "answers": [{"text": "distilbert", "answer_start": 144, "answer_end": 153}]}, {"id": "q2", "question": "What is the model task of ACSHCSE/distilbert-base-uncased-finetuned-ner?", "answers": [{"text": "token-classification", "answer_start": 248, "answer_end": 267}]}]}]}, {"title": "nntadotzip/bert-base-cased-IUChatbot-ontologyDts-bertBaseCased-bertTokenizer-12April2022", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bert-base-cased-IUChatbot-ontologyDts-bertBaseCased-bertTokenizer-12April2022\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bert-base-cased-IUChatbot-ontologyDts-bertBaseCased-bertTokenizer-12April2022\n\nThis model is a fine-tuned version of [bert-base-cased]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3856\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n Epoch  Validation Loss |\n:-----::---------------:|\n 1.0    0.4760          |\n 2.0    0.3957          |\n 3.0    0.3856          |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of nntadotzip/bert-base-cased-IUChatbot-ontologyDts-bertBaseCased-bertTokenizer-12April2022?", "answers": [{"text": "bert", "answer_start": 76, "answer_end": 79}]}]}]}, {"title": "kabelomalapane/test_model1.2_update", "paragraphs": [{"context": "---\nlicense: apache-2.0\ntags:\n- translation\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: test_model1.2_update\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# test_model1.2_update\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-mul-en]( on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.6296\n- Bleu: 4.0505\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "qas": [{"id": "q2", "question": "What is the model task of kabelomalapane/test_model1.2_update?", "answers": [{"text": "translation", "answer_start": 32, "answer_end": 42}]}]}]}, {"title": "Davlan/afro-xlmr-small", "paragraphs": [{"context": "---\nlicense: afl-3.0\n---\n\n\n# afro-xlmr-small\n\nAfroXLMR-small was created by [first reducing the vocabulary token size]( of XLM-R-base from 250K to 70k, followed by MLM adaptation on 17 African languages (Afrikaans, Amharic, Hausa, Igbo, Malagasy, Chichewa, Oromo, Naija, Kinyarwanda, Kirundi, Shona, Somali, Sesotho, Swahili, isiXhosa, Yoruba, and isiZulu) covering the major African language families and 3 high resource languages (Arabic, French, and English). \n\n## Eval results on MasakhaNER (F-score)\nlanguage XLM-R-base  afro-xlmr-base  afro-xlmr-mini\n----\namh 70.676.169.7\nhau 89.591.287.7\nibo 84.887.483.5\nkin 73.378.074.1\nlug 79.782.977.4\nluo 74.975.117.5\npcm 87.389.685.5\nswa 87.488.686.0\nwol 63.967.459.0\nyor 78.382.175.1\n\n### BibTeX entry and citation info\n```\n@inproceedings{alabi-etal-2022-adapting,\n    title = \"Adapting Pre-trained Language Models to {A}frican Languages via Multilingual Adaptive Fine-Tuning\",\n    author = \"Alabi, Jesujoba O.  and\n      Adelani, David Ifeoluwa  and\n      Mosbach, Marius  and\n      Klakow, Dietrich\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"\n    pages = \"4336--4349\",\n    abstract = \"Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) {---} fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to target language individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50{\\%}. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.\",\n}\n```\n\n\n", "qas": []}]}, {"title": "nlpstar/exclaim-t5", "paragraphs": [{"context": "", "qas": []}]}, {"title": "ABrinkmann/sbert_xtremedistil-l6-h256-uncased-mean-cosine-h32", "paragraphs": [{"context": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n---\n\n# ABrinkmann/sbert_xtremedistil-l6-h256-uncased-mean-cosine-h32\n\nThis is a [sentence-transformers]( model: It maps sentences & paragraphs to a 32 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n<!--- Describe your model here -->\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers]( installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('ABrinkmann/sbert_xtremedistil-l6-h256-uncased-mean-cosine-h32')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Evaluation Results\n\n<!--- Describe how your model was evaluated -->\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [\n\n\n## Training\nThe model was trained with the parameters:\n\n**DataLoader**:\n\n`torch.utils.data.dataloader.DataLoader` of length 251 with parameters:\n```\n{'batch_size': 32, 'sampler': 'torch.utils.data.sampler.RandomSampler', 'batch_sampler': 'torch.utils.data.sampler.BatchSampler'}\n```\n\n**Loss**:\n\n`sentence_transformers.losses.CosineSimilarityLoss.CosineSimilarityLoss` \n\nParameters of the fit()-Method:\n```\n{\n    \"epochs\": 1,\n    \"evaluation_steps\": 1000,\n    \"evaluator\": \"sentence_transformers.evaluation.EmbeddingSimilarityEvaluator.EmbeddingSimilarityEvaluator\",\n    \"max_grad_norm\": 1,\n    \"optimizer_class\": \"<class 'transformers.optimization.AdamW'>\",\n    \"optimizer_params\": {\n        \"lr\": 2e-05\n    },\n    \"scheduler\": \"WarmupLinear\",\n    \"steps_per_epoch\": null,\n    \"warmup_steps\": 26,\n    \"weight_decay\": 0.01\n}\n```\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 16, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 256, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 256, 'out_features': 32, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n)\n```\n\n## Citing & Authors\n\n<!--- Describe where people can find more information -->", "qas": [{"id": "q1", "question": "What is the model architecture of ABrinkmann/sbert_xtremedistil-l6-h256-uncased-mean-cosine-h32?", "answers": [{"text": "bert", "answer_start": 130, "answer_end": 133}]}, {"id": "q2", "question": "What is the model task of ABrinkmann/sbert_xtremedistil-l6-h256-uncased-mean-cosine-h32?", "answers": [{"text": "sentence-similarity", "answer_start": 18, "answer_end": 36}]}]}]}, {"title": "bhadresh-savani/electra-base-squad2", "paragraphs": [{"context": "---\ndatasets:\n- squad_v2\nlicense: cc-by-4.0\n\n---\n\n# electra-base for QA\n\n## Overview\n**Language model:** electra-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [example]( in [FARM](  \n**Infrastructure**: 1x Tesla v100\n\n## Hyperparameters\n\n```\nseed=42\nbatch_size = 32\nn_epochs = 5\nbase_LM_model = \"google/electra-base-discriminator\"\nmax_seq_len = 384\nlearning_rate = 1e-4\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.1\ndoc_stride=128\nmax_query_length=64\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](\n```\n\"exact\": 77.30144024256717,\n \"f1\": 81.35438272008543,\n \"total\": 11873,\n \"HasAns_exact\": 74.34210526315789,\n \"HasAns_f1\": 82.45961302894314,\n \"HasAns_total\": 5928,\n \"NoAns_exact\": 80.25231286795626,\n \"NoAns_f1\": 80.25231286795626,\n \"NoAns_total\": 5945\n```\n\n## Usage\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/electra-base-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n### In FARM\n\n```python\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.infer import Inferencer\n\nmodel_name = \"deepset/electra-base-squad2\"\n\n# a) Get predictions\nnlp = Inferencer.load(model_name, task_type=\"question_answering\")\nQA_input = [{\"questions\": [\"Why is model conversion important?\"],\n             \"text\": \"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\"}]\nres = nlp.inference_from_dicts(dicts=QA_input)\n\n# b) Load model & tokenizer\nmodel = AdaptiveModel.convert_from_transformers(model_name, device=\"cpu\", task_type=\"question_answering\")\ntokenizer = Tokenizer.load(model_name)\n```\n\n### In haystack\nFor doing QA at scale (i.e. many docs instead of single paragraph), you can load the model also in [haystack](\n```python\nreader = FARMReader(model_name_or_path=\"deepset/electra-base-squad2\")\n# or\nreader = TransformersReader(model=\"deepset/electra-base-squad2\",tokenizer=\"deepset/electra-base-squad2\")\n```\n\n\n## Authors\nVaishali Pal `vaishali.pal [at] deepset.ai`\nBranden Chan: `branden.chan [at] deepset.ai`\nTimo M\u00f6ller: `timo.moeller [at] deepset.ai`\nMalte Pietsch: `malte.pietsch [at] deepset.ai`\nTanay Soni: `tanay.soni [at] deepset.ai`\n\nNote:\nBorrowed this model from Haystack model repo for adding tensorflow model.", "qas": [{"id": "q1", "question": "What is the model architecture of bhadresh-savani/electra-base-squad2?", "answers": [{"text": "electra", "answer_start": 52, "answer_end": 58}]}, {"id": "q2", "question": "What is the model task of bhadresh-savani/electra-base-squad2?", "answers": [{"text": "question-answering", "answer_start": 1101, "answer_end": 1118}]}]}]}, {"title": "ASCCCCCCCC/PENGMENGJIE-finetuned-sms", "paragraphs": [{"context": "---\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- f1\nmodel-index:\n- name: PENGMENGJIE-finetuned-sms\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# PENGMENGJIE-finetuned-sms\n\nThis model is a fine-tuned version of [bert-base-chinese]( on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0000\n- Accuracy: 1.0\n- F1: 1.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n Epoch  Validation Loss  F1     |\n:-----::---------------::------:|\n 1.0    0.0060           0.9990 |\n 2.0    0.0000           1.0    |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.9.1\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of ASCCCCCCCC/PENGMENGJIE-finetuned-sms?", "answers": [{"text": "bert", "answer_start": 382, "answer_end": 385}]}]}]}, {"title": "rmihaylov/gpt2-small-theseus-bg", "paragraphs": [{"context": "---\ninference: false\nlanguage:\n- bg\nlicense: mit\ndatasets:\n- oscar\n- chitanka\n- wikipedia\ntags:\n- torch\n---\n\n# GPT-2\n\nPretrained model on Bulgarian language using a causal language modeling (CLM) objective. It was introduced in\n[this paper](\nand first released at [this page](\n\n## Model description\n\nThis is the **SMALL** version compressed via [progressive module replacing](\n\nThe compression was executed on Bulgarian text from [OSCAR]( [Chitanka]( and [Wikipedia](\n\n## Intended uses & limitations\n\nYou can use the raw model for: \n- text generation\n- auto-complete\n- spelling correction\n\nOr fine-tune it to a downstream task.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\n>>> from transformers import AutoModel, AutoTokenizer\n>>>\n>>> model_id = \"rmihaylov/gpt2-small-theseus-bg\"\n>>> tokenizer = AutoTokenizer.from_pretrained(model_id)\n>>> model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n>>>\n>>> input_ids = tokenizer.encode(\n>>>     \"\u0417\u0434\u0440\u0430\u0432\u0435\u0439,\", \n>>>     add_special_tokens=False, \n>>>     return_tensors='pt')\n>>>\n>>> output_ids = model.generate(\n>>>     input_ids, \n>>>     do_sample=True, \n>>>     max_length=50, \n>>>     top_p=0.92, \n>>>     pad_token_id=2,\n>>>     top_k=0)\n>>>\n>>> output = tokenizer.decode(output_ids[0])\n>>>\n>>> output = output.replace('<>', '\\n\\n\\n')\n>>> output = output.replace('<>', '')\n>>> output = output.replace('\u2581', ' ')\n>>> output = output.replace('<>', '\\n')\n>>>\n>>> print(output)\n\n\u0417\u0434\u0440\u0430\u0432\u0435\u0439, \u0438\u0437\u0432\u0438\u043d\u044f\u0432\u0430\u0439, \u043d\u043e \u043d\u0435 \u043c\u043e\u0433\u0430 \u0434\u0430 \u0437\u0430\u0441\u043f\u044f. \n \u0414\u0436\u0438\u043d\u0438 \u0441\u0435 \u043e\u0431\u044a\u0440\u043d\u0430 \u0438 \u0437\u0430\u0431\u0435\u043b\u044f\u0437\u0430 \u043a\u043e\u043b\u043a\u043e \u0441\u0430 \u043f\u0440\u0435\u0433\u044a\u0440\u043d\u0430\u0442\u0438. \n \u2014 \u041f\u043e\u0447\u0430\u043a\u0430\u0439, \u0414\u0436\u0438\u043d\u0438. \u041d\u0435 \u043c\u043e\u0433\u0430 \u0434\u0430 \u043f\u043e\u0432\u044f\u0440\u0432\u0430\u043c, \u0447\u0435 \u0435 \u0432\u044a\u0437\u043c\u043e\u0436\u043d\u043e! \u0422\u043e\u043b\u043a\u043e\u0432\u0430 \u0438\u0441\u043a\u0430\u043c \u0434\u0430 \u0442\u0435 \u0432\u0438\u0434\u044f. \n \u2014 \u041e\u0431\u0435\u0449\u0430\n```\n\n### Limitations and bias\n\nAs the openAI team themselves point out in their\n[model card](\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.", "qas": [{"id": "q1", "question": "What is the model architecture of rmihaylov/gpt2-small-theseus-bg?", "answers": [{"text": "gpt2", "answer_start": 782, "answer_end": 785}]}]}]}, {"title": "philschmid/roberta-large-finetuned-clinc", "paragraphs": [{"context": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- clinc_oos\nmetrics:\n- accuracy\nmodel-index:\n- name: roberta-large-finetuned-clinc\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: clinc_oos\n      type: clinc_oos\n      args: plus\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9703225806451613\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# roberta-large-finetuned-clinc\n\nThis model is a fine-tuned version of [roberta-large]( on the clinc_oos dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2109\n- Accuracy: 0.9703\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 128\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\n### Training results\n\n Epoch  Validation Loss \n:-----::---------------:\n 1.0    5.0440          \n 2.0    2.7488          \n 3.0    0.8694          \n 4.0    0.3267          \n 5.0    0.2109          \n\n\n### Framework versions\n\n- Transformers 4.19.0.dev0\n- Pytorch 1.10.2+cu113\n- Datasets 1.18.4\n- Tokenizers 0.11.6\n", "qas": [{"id": "q1", "question": "What is the model architecture of philschmid/roberta-large-finetuned-clinc?", "answers": [{"text": "roberta", "answer_start": 111, "answer_end": 117}]}, {"id": "q2", "question": "What is the model task of philschmid/roberta-large-finetuned-clinc?", "answers": [{"text": "text-classification", "answer_start": 206, "answer_end": 224}]}]}]}]}
---
license: mit
tags:
- generated_from_trainer
model-index:
- name: predict-perception-bert-focus-assassin
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# predict-perception-bert-focus-assassin

This model is a fine-tuned version of [dbmdz/bert-base-italian-xxl-cased](https://huggingface.co/dbmdz/bert-base-italian-xxl-cased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.2964
- Rmse: 0.8992
- Rmse Focus::a Sull'assassino: 0.8992
- Mae: 0.7331
- Mae Focus::a Sull'assassino: 0.7331
- R2: 0.6500
- R2 Focus::a Sull'assassino: 0.6500
- Cos: 0.7391
- Pair: 0.0
- Rank: 0.5
- Neighbors: 0.6131
- Rsa: nan

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 20
- eval_batch_size: 8
- seed: 1996
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rmse   | Rmse Focus::a Sull'assassino | Mae    | Mae Focus::a Sull'assassino | R2      | R2 Focus::a Sull'assassino | Cos    | Pair | Rank | Neighbors | Rsa |
|:-------------:|:-----:|:----:|:---------------:|:------:|:----------------------------:|:------:|:---------------------------:|:-------:|:--------------------------:|:------:|:----:|:----:|:---------:|:---:|
| 1.0674        | 1.0   | 15   | 0.9851          | 1.6393 | 1.6393                       | 1.5316 | 1.5316                      | -0.1633 | -0.1633                    | 0.1304 | 0.0  | 0.5  | 0.2457    | nan |
| 1.0099        | 2.0   | 30   | 0.8921          | 1.5601 | 1.5601                       | 1.4317 | 1.4317                      | -0.0535 | -0.0535                    | 0.5652 | 0.0  | 0.5  | 0.4734    | nan |
| 0.9295        | 3.0   | 45   | 0.7345          | 1.4155 | 1.4155                       | 1.3113 | 1.3113                      | 0.1327  | 0.1327                     | 0.5652 | 0.0  | 0.5  | 0.3596    | nan |
| 0.8485        | 4.0   | 60   | 0.7282          | 1.4094 | 1.4094                       | 1.2678 | 1.2678                      | 0.1401  | 0.1401                     | 0.7391 | 0.0  | 0.5  | 0.5367    | nan |
| 0.7551        | 5.0   | 75   | 0.5966          | 1.2758 | 1.2758                       | 1.1144 | 1.1144                      | 0.2955  | 0.2955                     | 0.6522 | 0.0  | 0.5  | 0.3911    | nan |
| 0.5563        | 6.0   | 90   | 0.4578          | 1.1175 | 1.1175                       | 0.9105 | 0.9105                      | 0.4594  | 0.4594                     | 0.6522 | 0.0  | 0.5  | 0.3911    | nan |
| 0.4048        | 7.0   | 105  | 0.3539          | 0.9826 | 0.9826                       | 0.7770 | 0.7770                      | 0.5821  | 0.5821                     | 0.6522 | 0.0  | 0.5  | 0.5522    | nan |
| 0.3319        | 8.0   | 120  | 0.2938          | 0.8953 | 0.8953                       | 0.7110 | 0.7110                      | 0.6530  | 0.6530                     | 0.6522 | 0.0  | 0.5  | 0.6021    | nan |
| 0.2224        | 9.0   | 135  | 0.3455          | 0.9708 | 0.9708                       | 0.7607 | 0.7607                      | 0.5921  | 0.5921                     | 0.6522 | 0.0  | 0.5  | 0.3911    | nan |
| 0.1794        | 10.0  | 150  | 0.2719          | 0.8612 | 0.8612                       | 0.6768 | 0.6768                      | 0.6790  | 0.6790                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.1553        | 11.0  | 165  | 0.2855          | 0.8826 | 0.8826                       | 0.7053 | 0.7053                      | 0.6628  | 0.6628                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.1008        | 12.0  | 180  | 0.3000          | 0.9046 | 0.9046                       | 0.7255 | 0.7255                      | 0.6458  | 0.6458                     | 0.6522 | 0.0  | 0.5  | 0.5261    | nan |
| 0.1121        | 13.0  | 195  | 0.2817          | 0.8766 | 0.8766                       | 0.7236 | 0.7236                      | 0.6674  | 0.6674                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.08          | 14.0  | 210  | 0.3504          | 0.9777 | 0.9777                       | 0.7631 | 0.7631                      | 0.5863  | 0.5863                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0802        | 15.0  | 225  | 0.3031          | 0.9094 | 0.9094                       | 0.7565 | 0.7565                      | 0.6420  | 0.6420                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0685        | 16.0  | 240  | 0.3041          | 0.9109 | 0.9109                       | 0.7409 | 0.7409                      | 0.6408  | 0.6408                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0592        | 17.0  | 255  | 0.3496          | 0.9767 | 0.9767                       | 0.7812 | 0.7812                      | 0.5871  | 0.5871                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0625        | 18.0  | 270  | 0.3260          | 0.9430 | 0.9430                       | 0.7757 | 0.7757                      | 0.6151  | 0.6151                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0589        | 19.0  | 285  | 0.3118          | 0.9222 | 0.9222                       | 0.7442 | 0.7442                      | 0.6318  | 0.6318                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0518        | 20.0  | 300  | 0.3062          | 0.9140 | 0.9140                       | 0.7459 | 0.7459                      | 0.6384  | 0.6384                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0456        | 21.0  | 315  | 0.3200          | 0.9344 | 0.9344                       | 0.7592 | 0.7592                      | 0.6221  | 0.6221                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0477        | 22.0  | 330  | 0.3132          | 0.9244 | 0.9244                       | 0.7532 | 0.7532                      | 0.6301  | 0.6301                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0448        | 23.0  | 345  | 0.3006          | 0.9056 | 0.9056                       | 0.7321 | 0.7321                      | 0.6450  | 0.6450                     | 0.6522 | 0.0  | 0.5  | 0.5261    | nan |
| 0.0494        | 24.0  | 360  | 0.2985          | 0.9024 | 0.9024                       | 0.7463 | 0.7463                      | 0.6475  | 0.6475                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0369        | 25.0  | 375  | 0.3039          | 0.9105 | 0.9105                       | 0.7359 | 0.7359                      | 0.6412  | 0.6412                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0456        | 26.0  | 390  | 0.2989          | 0.9030 | 0.9030                       | 0.7210 | 0.7210                      | 0.6471  | 0.6471                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.044         | 27.0  | 405  | 0.2997          | 0.9042 | 0.9042                       | 0.7418 | 0.7418                      | 0.6461  | 0.6461                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0352        | 28.0  | 420  | 0.2970          | 0.9001 | 0.9001                       | 0.7346 | 0.7346                      | 0.6493  | 0.6493                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0429        | 29.0  | 435  | 0.2970          | 0.9001 | 0.9001                       | 0.7281 | 0.7281                      | 0.6493  | 0.6493                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |
| 0.0378        | 30.0  | 450  | 0.2964          | 0.8992 | 0.8992                       | 0.7331 | 0.7331                      | 0.6500  | 0.6500                     | 0.7391 | 0.0  | 0.5  | 0.6131    | nan |


### Framework versions

- Transformers 4.16.2
- Pytorch 1.10.2+cu113
- Datasets 1.18.3
- Tokenizers 0.11.0

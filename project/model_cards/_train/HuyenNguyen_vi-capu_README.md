---
language:
- vi
tags:
- capitalization
- punctuation
- token-classification
license: cc-by-sa-4.0
datasets:
- oscar-corpus/OSCAR-2109
metrics:
- accuracy
- precision
- recall
- f1
---
# ‚ú® vibert-capitalization-punctuation
This a [viBERT](https://huggingface.co/FPTAI/vibert-base-cased) model finetuned for punctuation restoration on the [OSCAR-2109](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109) dataset. 
The model predicts the punctuation and upper-casing of plain, lower-cased text. An example use case can be ASR output. Or other cases when text has lost punctuation.
This model is intended for direct use as a punctuation restoration model for the general Vietnamese language. Alternatively, you can use this for further fine-tuning on domain-specific texts for punctuation restoration tasks.
Model restores the following punctuations -- **[. , : ? ]**
The model also restores the complex upper-casing of words like *YouTube*, *MobiFone*.

-----------------------------------------------
## üöã Usage

**Below is a quick way to get up and running with the model.**
1. Download files from hub  
```python
import os
import shutil
import sys
from huggingface_hub import snapshot_download
cache_dir = "./capu"
def download_files(repo_id, cache_dir=None, ignore_regex=None):
    download_dir = snapshot_download(repo_id=repo_id, cache_dir=cache_dir, ignore_regex=ignore_regex)
    if cache_dir is None or download_dir == cache_dir:
        return download_dir
    file_names = os.listdir(download_dir)
    for file_name in file_names:
        shutil.move(os.path.join(download_dir, file_name), cache_dir)
    os.rmdir(download_dir)
    return cache_dir
cache_dir = download_files(repo_id="dragonSwing/vibert-capu", cache_dir=cache_dir, ignore_regex=["*.json", "*.bin"])
sys.path.append(cache_dir)
```
2. Sample python code  
```python
import os
from gec_model import GecBERTModel
model = GecBERTModel(
    vocab_path=os.path.join(cache_dir, "vocabulary"),
    model_paths="dragonSwing/vibert-capu",
    split_chunk=True
)
model("theo ƒë√≥ th·ªß t∆∞·ªõng d·ª± ki·∫øn ti·∫øp b·ªô tr∆∞·ªüng n√¥ng nghi·ªáp m·ªπ tom wilsack b·ªô tr∆∞·ªüng th∆∞∆°ng m·∫°i m·ªπ gina raimondo b·ªô tr∆∞·ªüng t√†i ch√≠nh janet yellen g·∫∑p g·ª° th∆∞·ª£ng ngh·ªã sƒ© patrick leahy v√† m·ªôt s·ªë ngh·ªã sƒ© m·ªπ kh√°c")
# Always return list of outputs.
# ['Theo ƒë√≥, Th·ªß t∆∞·ªõng d·ª± ki·∫øn ti·∫øp B·ªô tr∆∞·ªüng N√¥ng nghi·ªáp M·ªπ Tom Wilsack, B·ªô tr∆∞·ªüng Th∆∞∆°ng m·∫°i M·ªπ Gina Raimondo, B·ªô tr∆∞·ªüng T√†i ch√≠nh Janet Yellen, g·∫∑p g·ª° Th∆∞·ª£ng ngh·ªã sƒ© Patrick Leahy v√† m·ªôt s·ªë ngh·ªã sƒ© M·ªπ kh√°c.']
model("nh·ªØng g√≥i c∆∞·ªõc nƒÉm g mobifone s·∫Ω mang ƒë·∫øn cho b·∫°n nh·ªØng tr·∫£i nghi·ªám m·ªõi l·∫° tr√™n c·∫£ tuy·ªát v·ªùi so v·ªõi m·∫°ng b·ªën g th√¨ t·ªëc ƒë·ªô truy c·∫≠p m·∫°ng 5 g mobifone ƒë∆∞·ª£c nh·∫≠n ƒë·ªãnh l√† si√™u ƒë·ªânh v·ªõi m·ª©c truy c·∫≠p nhanh g·∫•p 10 l·∫ßn")
# ['Nh·ªØng g√≥i c∆∞·ªõc 5G MobiFone s·∫Ω mang ƒë·∫øn cho b·∫°n nh·ªØng tr·∫£i nghi·ªám m·ªõi l·∫° tr√™n c·∫£ tuy·ªát v·ªùi. So v·ªõi m·∫°ng 4G th√¨ t·ªëc ƒë·ªô truy c·∫≠p m·∫°ng 5G MobiFone ƒë∆∞·ª£c nh·∫≠n ƒë·ªãnh l√† si√™u ƒë·ªânh v·ªõi m·ª©c truy c·∫≠p nhanh g·∫•p 10 l·∫ßn.']
```
**This model can work on arbitrarily large text in Vietnamese language.**

-----------------------------------------------
## üì° Training data
Here is the number of product reviews we used for fine-tuning the model:

| Language | Number of text samples |
| --- | --- |
| Vietnamese  | 5,600,000  |

-----------------------------------------------
## üéØ Accuracy
Below is a breakdown of the performance of the model by each label on 10,000 held-out text samples:

|  label    |   precision  |  recall | f1-score  | support |
| --- | --- | --- | --- | --- |
|     **Upper**    |   0.88       | 0.89    |  0.89     |  56497   |
|     **Complex-Upper**    |   0.92       | 0.83    |  0.88     |   480   |
|     **.**    |   0.81       | 0.82    |  0.82     | 18139   |
|    **,**    |   0.73       | 0.70    |  0.71     | 22961   |
|     **:**    |   0.74       | 0.56    |  0.64     |   1432   |
|     **?**    |   0.80       | 0.76    |  0.78     |   1730   |
|     **none**    |   0.99       | 0.99    |  0.99     |475611   |
-----------------------------------------------

---
tags:
- generated_from_keras_callback
model-index:
- name: gpt3_model
  results: []
---

<!-- This model card has been generated automatically according to the information Keras had access to. You should
probably proofread and complete it, then remove this comment. -->

# gpt3_model

This model is a fine-tuned version of [MJ199999/gpt3_model](https://huggingface.co/MJ199999/gpt3_model) on an unknown dataset.
It achieves the following results on the evaluation set:
- Train Loss: 0.4905
- Train Lr: 0.0009999999
- Epoch: 199

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- optimizer: {'name': 'Adagrad', 'learning_rate': 0.0009999999, 'decay': 0.0, 'initial_accumulator_value': 0.1, 'epsilon': 1e-07}
- training_precision: float32

### Training results

| Train Loss | Train Lr     | Epoch |
|:----------:|:------------:|:-----:|
| 5.1583     | 0.01         | 0     |
| 3.9477     | 0.01         | 1     |
| 2.9332     | 0.01         | 2     |
| 2.1581     | 0.01         | 3     |
| 1.6918     | 0.01         | 4     |
| 1.3929     | 0.01         | 5     |
| 1.2062     | 0.01         | 6     |
| 1.0955     | 0.01         | 7     |
| 1.0068     | 0.01         | 8     |
| 0.9528     | 0.01         | 9     |
| 0.9051     | 0.01         | 10    |
| 0.8710     | 0.01         | 11    |
| 0.8564     | 0.01         | 12    |
| 0.8094     | 0.01         | 13    |
| 0.8143     | 0.01         | 14    |
| 0.7853     | 0.01         | 15    |
| 0.7625     | 0.01         | 16    |
| 0.7508     | 0.01         | 17    |
| 0.7449     | 0.01         | 18    |
| 0.7319     | 0.01         | 19    |
| 0.7144     | 0.01         | 20    |
| 0.7045     | 0.01         | 21    |
| 0.7029     | 0.01         | 22    |
| 0.6937     | 0.01         | 23    |
| 0.6898     | 0.01         | 24    |
| 0.6745     | 0.01         | 25    |
| 0.6767     | 0.01         | 26    |
| 0.6692     | 0.01         | 27    |
| 0.6604     | 0.01         | 28    |
| 0.6573     | 0.01         | 29    |
| 0.6524     | 0.01         | 30    |
| 0.6508     | 0.01         | 31    |
| 0.6443     | 0.01         | 32    |
| 0.6452     | 0.01         | 33    |
| 0.6371     | 0.01         | 34    |
| 0.6362     | 0.01         | 35    |
| 0.6304     | 0.01         | 36    |
| 0.6317     | 0.01         | 37    |
| 0.6270     | 0.01         | 38    |
| 0.6257     | 0.01         | 39    |
| 0.6208     | 0.01         | 40    |
| 0.6227     | 0.01         | 41    |
| 0.6154     | 0.01         | 42    |
| 0.6126     | 0.01         | 43    |
| 0.6149     | 0.01         | 44    |
| 0.6075     | 0.01         | 45    |
| 0.6084     | 0.01         | 46    |
| 0.6078     | 0.01         | 47    |
| 0.6057     | 0.01         | 48    |
| 0.6033     | 0.01         | 49    |
| 0.6040     | 0.01         | 50    |
| 0.5989     | 0.01         | 51    |
| 0.5967     | 0.01         | 52    |
| 0.5952     | 0.01         | 53    |
| 0.5911     | 0.01         | 54    |
| 0.5904     | 0.01         | 55    |
| 0.5888     | 0.01         | 56    |
| 0.5886     | 0.01         | 57    |
| 0.5883     | 0.01         | 58    |
| 0.5838     | 0.01         | 59    |
| 0.5856     | 0.01         | 60    |
| 0.5850     | 0.01         | 61    |
| 0.5801     | 0.01         | 62    |
| 0.5821     | 0.01         | 63    |
| 0.5781     | 0.01         | 64    |
| 0.5786     | 0.01         | 65    |
| 0.5835     | 0.01         | 66    |
| 0.5808     | 0.01         | 67    |
| 0.5754     | 0.01         | 68    |
| 0.5742     | 0.01         | 69    |
| 0.5733     | 0.01         | 70    |
| 0.5700     | 0.01         | 71    |
| 0.5738     | 0.01         | 72    |
| 0.5678     | 0.01         | 73    |
| 0.5695     | 0.01         | 74    |
| 0.5684     | 0.01         | 75    |
| 0.5696     | 0.01         | 76    |
| 0.5688     | 0.01         | 77    |
| 0.5648     | 0.01         | 78    |
| 0.5592     | 0.01         | 79    |
| 0.5622     | 0.01         | 80    |
| 0.5660     | 0.01         | 81    |
| 0.5636     | 0.01         | 82    |
| 0.5602     | 0.01         | 83    |
| 0.5613     | 0.01         | 84    |
| 0.5608     | 0.01         | 85    |
| 0.5589     | 0.01         | 86    |
| 0.5580     | 0.01         | 87    |
| 0.5566     | 0.01         | 88    |
| 0.5531     | 0.01         | 89    |
| 0.5571     | 0.01         | 90    |
| 0.5541     | 0.01         | 91    |
| 0.5576     | 0.01         | 92    |
| 0.5560     | 0.01         | 93    |
| 0.5517     | 0.01         | 94    |
| 0.5508     | 0.01         | 95    |
| 0.5554     | 0.01         | 96    |
| 0.5539     | 0.01         | 97    |
| 0.5493     | 0.01         | 98    |
| 0.5499     | 0.01         | 99    |
| 0.4999     | 0.0009999999 | 100   |
| 0.4981     | 0.0009999999 | 101   |
| 0.4983     | 0.0009999999 | 102   |
| 0.4984     | 0.0009999999 | 103   |
| 0.4974     | 0.0009999999 | 104   |
| 0.4957     | 0.0009999999 | 105   |
| 0.4966     | 0.0009999999 | 106   |
| 0.4975     | 0.0009999999 | 107   |
| 0.4962     | 0.0009999999 | 108   |
| 0.4932     | 0.0009999999 | 109   |
| 0.4983     | 0.0009999999 | 110   |
| 0.4937     | 0.0009999999 | 111   |
| 0.4926     | 0.0009999999 | 112   |
| 0.4944     | 0.0009999999 | 113   |
| 0.4947     | 0.0009999999 | 114   |
| 0.4953     | 0.0009999999 | 115   |
| 0.4934     | 0.0009999999 | 116   |
| 0.4929     | 0.0009999999 | 117   |
| 0.4925     | 0.0009999999 | 118   |
| 0.4948     | 0.0009999999 | 119   |
| 0.4947     | 0.0009999999 | 120   |
| 0.4936     | 0.0009999999 | 121   |
| 0.4909     | 0.0009999999 | 122   |
| 0.4960     | 0.0009999999 | 123   |
| 0.4952     | 0.0009999999 | 124   |
| 0.4923     | 0.0009999999 | 125   |
| 0.4930     | 0.0009999999 | 126   |
| 0.4942     | 0.0009999999 | 127   |
| 0.4927     | 0.0009999999 | 128   |
| 0.4917     | 0.0009999999 | 129   |
| 0.4926     | 0.0009999999 | 130   |
| 0.4927     | 0.0009999999 | 131   |
| 0.4932     | 0.0009999999 | 132   |
| 0.4925     | 0.0009999999 | 133   |
| 0.4928     | 0.0009999999 | 134   |
| 0.4936     | 0.0009999999 | 135   |
| 0.4908     | 0.0009999999 | 136   |
| 0.4936     | 0.0009999999 | 137   |
| 0.4916     | 0.0009999999 | 138   |
| 0.4906     | 0.0009999999 | 139   |
| 0.4904     | 0.0009999999 | 140   |
| 0.4920     | 0.0009999999 | 141   |
| 0.4924     | 0.0009999999 | 142   |
| 0.4902     | 0.0009999999 | 143   |
| 0.4903     | 0.0009999999 | 144   |
| 0.4903     | 0.0009999999 | 145   |
| 0.4924     | 0.0009999999 | 146   |
| 0.4889     | 0.0009999999 | 147   |
| 0.4896     | 0.0009999999 | 148   |
| 0.4919     | 0.0009999999 | 149   |
| 0.4896     | 0.0009999999 | 150   |
| 0.4906     | 0.0009999999 | 151   |
| 0.4923     | 0.0009999999 | 152   |
| 0.4899     | 0.0009999999 | 153   |
| 0.4925     | 0.0009999999 | 154   |
| 0.4901     | 0.0009999999 | 155   |
| 0.4910     | 0.0009999999 | 156   |
| 0.4904     | 0.0009999999 | 157   |
| 0.4912     | 0.0009999999 | 158   |
| 0.4937     | 0.0009999999 | 159   |
| 0.4894     | 0.0009999999 | 160   |
| 0.4913     | 0.0009999999 | 161   |
| 0.4899     | 0.0009999999 | 162   |
| 0.4894     | 0.0009999999 | 163   |
| 0.4904     | 0.0009999999 | 164   |
| 0.4900     | 0.0009999999 | 165   |
| 0.4890     | 0.0009999999 | 166   |
| 0.4919     | 0.0009999999 | 167   |
| 0.4909     | 0.0009999999 | 168   |
| 0.4891     | 0.0009999999 | 169   |
| 0.4900     | 0.0009999999 | 170   |
| 0.4910     | 0.0009999999 | 171   |
| 0.4901     | 0.0009999999 | 172   |
| 0.4914     | 0.0009999999 | 173   |
| 0.4913     | 0.0009999999 | 174   |
| 0.4897     | 0.0009999999 | 175   |
| 0.4892     | 0.0009999999 | 176   |
| 0.4929     | 0.0009999999 | 177   |
| 0.4881     | 0.0009999999 | 178   |
| 0.4920     | 0.0009999999 | 179   |
| 0.4888     | 0.0009999999 | 180   |
| 0.4901     | 0.0009999999 | 181   |
| 0.4875     | 0.0009999999 | 182   |
| 0.4930     | 0.0009999999 | 183   |
| 0.4867     | 0.0009999999 | 184   |
| 0.4890     | 0.0009999999 | 185   |
| 0.4898     | 0.0009999999 | 186   |
| 0.4880     | 0.0009999999 | 187   |
| 0.4899     | 0.0009999999 | 188   |
| 0.4881     | 0.0009999999 | 189   |
| 0.4897     | 0.0009999999 | 190   |
| 0.4876     | 0.0009999999 | 191   |
| 0.4873     | 0.0009999999 | 192   |
| 0.4901     | 0.0009999999 | 193   |
| 0.4898     | 0.0009999999 | 194   |
| 0.4898     | 0.0009999999 | 195   |
| 0.4861     | 0.0009999999 | 196   |
| 0.4878     | 0.0009999999 | 197   |
| 0.4880     | 0.0009999999 | 198   |
| 0.4905     | 0.0009999999 | 199   |


### Framework versions

- Transformers 4.21.3
- TensorFlow 2.8.2
- Tokenizers 0.12.1

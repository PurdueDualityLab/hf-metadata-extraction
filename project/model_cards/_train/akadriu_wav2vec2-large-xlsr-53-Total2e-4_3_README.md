---
license: apache-2.0
tags:
- generated_from_trainer
model-index:
- name: wav2vec2-large-xlsr-53-Total2e-4_3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# wav2vec2-large-xlsr-53-Total2e-4_3

This model is a fine-tuned version of [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.2893
- Wer: 0.1863

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 500
- num_epochs: 10
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Wer    |
|:-------------:|:-----:|:-----:|:---------------:|:------:|
| 5.16          | 0.1   | 200   | 2.9123          | 0.9707 |
| 2.4599        | 0.2   | 400   | 0.8145          | 0.6906 |
| 1.0523        | 0.3   | 600   | 0.5247          | 0.4823 |
| 0.8965        | 0.4   | 800   | 0.4391          | 0.4416 |
| 0.7994        | 0.5   | 1000  | 0.3889          | 0.3773 |
| 0.7491        | 0.6   | 1200  | 0.3604          | 0.3305 |
| 0.7425        | 0.7   | 1400  | 0.3543          | 0.3277 |
| 0.7253        | 0.8   | 1600  | 0.3397          | 0.3143 |
| 0.7221        | 0.9   | 1800  | 0.3341          | 0.2979 |
| 0.6853        | 1.0   | 2000  | 0.3244          | 0.2906 |
| 0.6107        | 1.1   | 2200  | 0.3127          | 0.2771 |
| 0.6233        | 1.2   | 2400  | 0.3116          | 0.2721 |
| 0.6214        | 1.3   | 2600  | 0.3256          | 0.2671 |
| 0.6511        | 1.4   | 2800  | 0.3019          | 0.2570 |
| 0.6491        | 1.5   | 3000  | 0.2961          | 0.2576 |
| 0.6411        | 1.6   | 3200  | 0.2963          | 0.2535 |
| 0.5963        | 1.7   | 3400  | 0.2939          | 0.2526 |
| 0.6146        | 1.8   | 3600  | 0.2908          | 0.2490 |
| 0.6291        | 1.9   | 3800  | 0.2851          | 0.2448 |
| 0.6154        | 2.0   | 4000  | 0.2861          | 0.2424 |
| 0.5652        | 2.1   | 4200  | 0.2852          | 0.2411 |
| 0.5648        | 2.2   | 4400  | 0.2856          | 0.2350 |
| 0.5365        | 2.3   | 4600  | 0.2802          | 0.2395 |
| 0.5855        | 2.4   | 4800  | 0.2883          | 0.2374 |
| 0.5978        | 2.5   | 5000  | 0.2855          | 0.2364 |
| 0.5863        | 2.6   | 5200  | 0.2736          | 0.2277 |
| 0.5569        | 2.7   | 5400  | 0.2746          | 0.2293 |
| 0.5628        | 2.8   | 5600  | 0.2719          | 0.2249 |
| 0.5655        | 2.9   | 5800  | 0.2653          | 0.2224 |
| 0.5578        | 3.0   | 6000  | 0.2685          | 0.2243 |
| 0.5303        | 3.1   | 6200  | 0.2696          | 0.2204 |
| 0.5316        | 3.2   | 6400  | 0.2733          | 0.2247 |
| 0.5476        | 3.3   | 6600  | 0.2716          | 0.2203 |
| 0.5326        | 3.4   | 6800  | 0.2697          | 0.2209 |
| 0.5375        | 3.5   | 7000  | 0.2701          | 0.2197 |
| 0.5364        | 3.6   | 7200  | 0.2655          | 0.2165 |
| 0.503         | 3.7   | 7400  | 0.2650          | 0.2125 |
| 0.5284        | 3.8   | 7600  | 0.2672          | 0.2162 |
| 0.5251        | 3.9   | 7800  | 0.2669          | 0.2172 |
| 0.5299        | 4.0   | 8000  | 0.2632          | 0.2081 |
| 0.4904        | 4.1   | 8200  | 0.2674          | 0.2099 |
| 0.496         | 4.2   | 8400  | 0.2700          | 0.2143 |
| 0.5067        | 4.3   | 8600  | 0.2648          | 0.2090 |
| 0.506         | 4.4   | 8800  | 0.2595          | 0.2069 |
| 0.4795        | 4.5   | 9000  | 0.2653          | 0.2072 |
| 0.5149        | 4.6   | 9200  | 0.2618          | 0.2073 |
| 0.4786        | 4.7   | 9400  | 0.2632          | 0.2058 |
| 0.5056        | 4.8   | 9600  | 0.2674          | 0.2123 |
| 0.5059        | 4.9   | 9800  | 0.2642          | 0.2115 |
| 0.5119        | 5.0   | 10000 | 0.2672          | 0.2089 |
| 0.4619        | 5.1   | 10200 | 0.2658          | 0.2062 |
| 0.4647        | 5.2   | 10400 | 0.2664          | 0.2025 |
| 0.4707        | 5.3   | 10600 | 0.2656          | 0.2084 |
| 0.486         | 5.4   | 10800 | 0.2728          | 0.2029 |
| 0.4785        | 5.5   | 11000 | 0.2653          | 0.2004 |
| 0.4895        | 5.6   | 11200 | 0.2835          | 0.2119 |
| 0.4519        | 5.7   | 11400 | 0.2715          | 0.2061 |
| 0.484         | 5.8   | 11600 | 0.2663          | 0.2071 |
| 0.4734        | 5.9   | 11800 | 0.2615          | 0.2023 |
| 0.4563        | 6.0   | 12000 | 0.2604          | 0.1997 |
| 0.4193        | 6.1   | 12200 | 0.2708          | 0.2015 |
| 0.4516        | 6.2   | 12400 | 0.2724          | 0.2018 |
| 0.4609        | 6.3   | 12600 | 0.2745          | 0.2004 |
| 0.43          | 6.4   | 12800 | 0.2716          | 0.1979 |
| 0.4424        | 6.5   | 13000 | 0.2674          | 0.1963 |
| 0.4589        | 6.6   | 13200 | 0.2622          | 0.1977 |
| 0.4458        | 6.7   | 13400 | 0.2668          | 0.1994 |
| 0.4233        | 6.8   | 13600 | 0.2739          | 0.1978 |
| 0.4557        | 6.9   | 13800 | 0.2692          | 0.1972 |
| 0.4472        | 7.0   | 14000 | 0.2686          | 0.1942 |
| 0.4193        | 7.1   | 14200 | 0.2843          | 0.1959 |
| 0.4033        | 7.2   | 14400 | 0.2767          | 0.1945 |
| 0.4266        | 7.3   | 14600 | 0.2808          | 0.1931 |
| 0.419         | 7.4   | 14800 | 0.2801          | 0.1945 |
| 0.4352        | 7.5   | 15000 | 0.2764          | 0.1934 |
| 0.4248        | 7.6   | 15200 | 0.2818          | 0.1938 |
| 0.4001        | 7.7   | 15400 | 0.2754          | 0.1931 |
| 0.415         | 7.8   | 15600 | 0.2799          | 0.1916 |
| 0.4056        | 7.9   | 15800 | 0.2746          | 0.1916 |
| 0.419         | 8.0   | 16000 | 0.2789          | 0.1909 |
| 0.3974        | 8.1   | 16200 | 0.2913          | 0.1897 |
| 0.3999        | 8.2   | 16400 | 0.2894          | 0.1899 |
| 0.4179        | 8.3   | 16600 | 0.2819          | 0.1918 |
| 0.4081        | 8.4   | 16800 | 0.2868          | 0.1910 |
| 0.3963        | 8.5   | 17000 | 0.2835          | 0.1889 |
| 0.3748        | 8.6   | 17200 | 0.2841          | 0.1903 |
| 0.375         | 8.7   | 17400 | 0.2820          | 0.1874 |
| 0.3857        | 8.8   | 17600 | 0.2865          | 0.1872 |
| 0.3901        | 8.9   | 17800 | 0.2824          | 0.1882 |
| 0.4067        | 9.0   | 18000 | 0.2838          | 0.1887 |
| 0.3711        | 9.1   | 18200 | 0.2892          | 0.1897 |
| 0.3661        | 9.2   | 18400 | 0.2889          | 0.1883 |
| 0.3796        | 9.3   | 18600 | 0.2876          | 0.1886 |
| 0.3932        | 9.4   | 18800 | 0.2948          | 0.1877 |
| 0.3894        | 9.5   | 19000 | 0.2896          | 0.1884 |
| 0.3643        | 9.6   | 19200 | 0.2897          | 0.1868 |
| 0.384         | 9.7   | 19400 | 0.2887          | 0.1867 |
| 0.3951        | 9.8   | 19600 | 0.2905          | 0.1862 |
| 0.3595        | 9.9   | 19800 | 0.2893          | 0.1866 |
| 0.3758        | 10.0  | 20000 | 0.2893          | 0.1863 |


### Framework versions

- Transformers 4.15.0
- Pytorch 1.10.0+cu111
- Datasets 1.18.3
- Tokenizers 0.10.3

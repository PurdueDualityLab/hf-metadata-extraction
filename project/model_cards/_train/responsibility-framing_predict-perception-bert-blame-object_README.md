---
license: mit
tags:
- generated_from_trainer
model-index:
- name: predict-perception-bert-blame-object
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# predict-perception-bert-blame-object

This model is a fine-tuned version of [dbmdz/bert-base-italian-xxl-cased](https://huggingface.co/dbmdz/bert-base-italian-xxl-cased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5837
- Rmse: 0.5589
- Rmse Blame::a Un oggetto: 0.5589
- Mae: 0.3862
- Mae Blame::a Un oggetto: 0.3862
- R2: 0.2884
- R2 Blame::a Un oggetto: 0.2884
- Cos: 0.3913
- Pair: 0.0
- Rank: 0.5
- Neighbors: 0.5024
- Rsa: nan

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 20
- eval_batch_size: 8
- seed: 1996
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rmse   | Rmse Blame::a Un oggetto | Mae    | Mae Blame::a Un oggetto | R2      | R2 Blame::a Un oggetto | Cos    | Pair | Rank | Neighbors | Rsa |
|:-------------:|:-----:|:----:|:---------------:|:------:|:------------------------:|:------:|:-----------------------:|:-------:|:----------------------:|:------:|:----:|:----:|:---------:|:---:|
| 1.0603        | 1.0   | 15   | 0.8503          | 0.6745 | 0.6745                   | 0.4386 | 0.4386                  | -0.0365 | -0.0365                | 0.1304 | 0.0  | 0.5  | 0.5197    | nan |
| 0.9662        | 2.0   | 30   | 0.8510          | 0.6748 | 0.6748                   | 0.4548 | 0.4548                  | -0.0374 | -0.0374                | 0.0435 | 0.0  | 0.5  | 0.4840    | nan |
| 0.9438        | 3.0   | 45   | 0.7622          | 0.6386 | 0.6386                   | 0.4541 | 0.4541                  | 0.0709  | 0.0709                 | 0.0435 | 0.0  | 0.5  | 0.4635    | nan |
| 0.9096        | 4.0   | 60   | 0.8301          | 0.6665 | 0.6665                   | 0.4305 | 0.4305                  | -0.0119 | -0.0119                | 0.0435 | 0.0  | 0.5  | 0.3499    | nan |
| 0.8383        | 5.0   | 75   | 0.7306          | 0.6252 | 0.6252                   | 0.3814 | 0.3814                  | 0.1094  | 0.1094                 | 0.3043 | 0.0  | 0.5  | 0.5098    | nan |
| 0.7828        | 6.0   | 90   | 0.7434          | 0.6307 | 0.6307                   | 0.4005 | 0.4005                  | 0.0937  | 0.0937                 | 0.3043 | 0.0  | 0.5  | 0.4335    | nan |
| 0.7028        | 7.0   | 105  | 0.7218          | 0.6214 | 0.6214                   | 0.4090 | 0.4090                  | 0.1202  | 0.1202                 | 0.3913 | 0.0  | 0.5  | 0.4470    | nan |
| 0.6661        | 8.0   | 120  | 0.7434          | 0.6307 | 0.6307                   | 0.4042 | 0.4042                  | 0.0938  | 0.0938                 | 0.3913 | 0.0  | 0.5  | 0.4470    | nan |
| 0.578         | 9.0   | 135  | 0.7719          | 0.6426 | 0.6426                   | 0.3975 | 0.3975                  | 0.0591  | 0.0591                 | 0.3913 | 0.0  | 0.5  | 0.4470    | nan |
| 0.544         | 10.0  | 150  | 0.7117          | 0.6171 | 0.6171                   | 0.4126 | 0.4126                  | 0.1324  | 0.1324                 | 0.2174 | 0.0  | 0.5  | 0.3489    | nan |
| 0.4638        | 11.0  | 165  | 0.6683          | 0.5980 | 0.5980                   | 0.3952 | 0.3952                  | 0.1853  | 0.1853                 | 0.3043 | 0.0  | 0.5  | 0.3989    | nan |
| 0.3998        | 12.0  | 180  | 0.6772          | 0.6019 | 0.6019                   | 0.4201 | 0.4201                  | 0.1745  | 0.1745                 | 0.3043 | 0.0  | 0.5  | 0.3989    | nan |
| 0.3403        | 13.0  | 195  | 0.6576          | 0.5932 | 0.5932                   | 0.4237 | 0.4237                  | 0.1984  | 0.1984                 | 0.2174 | 0.0  | 0.5  | 0.3491    | nan |
| 0.2839        | 14.0  | 210  | 0.6281          | 0.5797 | 0.5797                   | 0.4208 | 0.4208                  | 0.2344  | 0.2344                 | 0.2174 | 0.0  | 0.5  | 0.3491    | nan |
| 0.2619        | 15.0  | 225  | 0.6254          | 0.5785 | 0.5785                   | 0.3752 | 0.3752                  | 0.2376  | 0.2376                 | 0.3913 | 0.0  | 0.5  | 0.5756    | nan |
| 0.2175        | 16.0  | 240  | 0.6074          | 0.5701 | 0.5701                   | 0.3985 | 0.3985                  | 0.2596  | 0.2596                 | 0.3043 | 0.0  | 0.5  | 0.4142    | nan |
| 0.1884        | 17.0  | 255  | 0.6045          | 0.5687 | 0.5687                   | 0.4036 | 0.4036                  | 0.2631  | 0.2631                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.1797        | 18.0  | 270  | 0.6038          | 0.5684 | 0.5684                   | 0.3914 | 0.3914                  | 0.2640  | 0.2640                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.1316        | 19.0  | 285  | 0.6199          | 0.5759 | 0.5759                   | 0.4078 | 0.4078                  | 0.2443  | 0.2443                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.1429        | 20.0  | 300  | 0.6119          | 0.5722 | 0.5722                   | 0.3954 | 0.3954                  | 0.2540  | 0.2540                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.1202        | 21.0  | 315  | 0.6193          | 0.5756 | 0.5756                   | 0.3987 | 0.3987                  | 0.2451  | 0.2451                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.1159        | 22.0  | 330  | 0.6218          | 0.5768 | 0.5768                   | 0.3995 | 0.3995                  | 0.2420  | 0.2420                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.1027        | 23.0  | 345  | 0.6207          | 0.5763 | 0.5763                   | 0.4100 | 0.4100                  | 0.2433  | 0.2433                 | 0.3043 | 0.0  | 0.5  | 0.4142    | nan |
| 0.1006        | 24.0  | 360  | 0.5646          | 0.5496 | 0.5496                   | 0.3687 | 0.3687                  | 0.3117  | 0.3117                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.0902        | 25.0  | 375  | 0.5582          | 0.5465 | 0.5465                   | 0.3714 | 0.3714                  | 0.3196  | 0.3196                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.0901        | 26.0  | 390  | 0.5650          | 0.5498 | 0.5498                   | 0.3704 | 0.3704                  | 0.3112  | 0.3112                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.0937        | 27.0  | 405  | 0.5713          | 0.5529 | 0.5529                   | 0.3735 | 0.3735                  | 0.3036  | 0.3036                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.0812        | 28.0  | 420  | 0.5773          | 0.5558 | 0.5558                   | 0.3759 | 0.3759                  | 0.2962  | 0.2962                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.0911        | 29.0  | 435  | 0.5818          | 0.5579 | 0.5579                   | 0.3832 | 0.3832                  | 0.2908  | 0.2908                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |
| 0.082         | 30.0  | 450  | 0.5837          | 0.5589 | 0.5589                   | 0.3862 | 0.3862                  | 0.2884  | 0.2884                 | 0.3913 | 0.0  | 0.5  | 0.5024    | nan |


### Framework versions

- Transformers 4.16.2
- Pytorch 1.10.2+cu113
- Datasets 1.18.3
- Tokenizers 0.11.0

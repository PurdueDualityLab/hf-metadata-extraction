---
license: cc-by-nc-sa-4.0
tags:
- generated_from_trainer
datasets:
- sroie
metrics:
- precision
- recall
- f1
- accuracy
model-index:
- name: Sinergy-Question-Answering
  results:
  - task:
      name: Token Classification
      type: token-classification
    dataset:
      name: sroie
      type: sroie
      config: discharge
      split: train
      args: discharge
    metrics:
    - name: Precision
      type: precision
      value: 0.7948717948717948
    - name: Recall
      type: recall
      value: 0.7948717948717948
    - name: F1
      type: f1
      value: 0.7948717948717948
    - name: Accuracy
      type: accuracy
      value: 0.9261159569009748
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Sinergy-Question-Answering

This model is a fine-tuned version of [microsoft/layoutlmv3-base](https://huggingface.co/microsoft/layoutlmv3-base) on the sroie dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5867
- Precision: 0.7949
- Recall: 0.7949
- F1: 0.7949
- Accuracy: 0.9261

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 5000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Precision | Recall | F1     | Accuracy |
|:-------------:|:------:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|
| No log        | 4.55   | 100  | 0.3686          | 0.5748    | 0.7179 | 0.6384 | 0.8881   |
| No log        | 9.09   | 200  | 0.3057          | 0.6799    | 0.7546 | 0.7153 | 0.9189   |
| No log        | 13.64  | 300  | 0.3287          | 0.7491    | 0.7875 | 0.7679 | 0.9354   |
| No log        | 18.18  | 400  | 0.3452          | 0.7414    | 0.7875 | 0.7638 | 0.9307   |
| 0.2603        | 22.73  | 500  | 0.3365          | 0.7313    | 0.7875 | 0.7584 | 0.9415   |
| 0.2603        | 27.27  | 600  | 0.5244          | 0.7745    | 0.7802 | 0.7774 | 0.9097   |
| 0.2603        | 31.82  | 700  | 0.4429          | 0.7737    | 0.7766 | 0.7751 | 0.9338   |
| 0.2603        | 36.36  | 800  | 0.4776          | 0.7657    | 0.8022 | 0.7835 | 0.9266   |
| 0.2603        | 40.91  | 900  | 0.5305          | 0.7855    | 0.7912 | 0.7883 | 0.9236   |
| 0.051         | 45.45  | 1000 | 0.5867          | 0.7949    | 0.7949 | 0.7949 | 0.9261   |
| 0.051         | 50.0   | 1100 | 0.5569          | 0.7774    | 0.7802 | 0.7788 | 0.9323   |
| 0.051         | 54.55  | 1200 | 0.6154          | 0.7509    | 0.7509 | 0.7509 | 0.9200   |
| 0.051         | 59.09  | 1300 | 0.5406          | 0.7305    | 0.7546 | 0.7423 | 0.9297   |
| 0.051         | 63.64  | 1400 | 0.6069          | 0.7544    | 0.7875 | 0.7706 | 0.9287   |
| 0.0127        | 68.18  | 1500 | 0.6142          | 0.7603    | 0.7436 | 0.7519 | 0.9210   |
| 0.0127        | 72.73  | 1600 | 0.5822          | 0.7399    | 0.7399 | 0.7399 | 0.9297   |
| 0.0127        | 77.27  | 1700 | 0.5584          | 0.75      | 0.7582 | 0.7541 | 0.9297   |
| 0.0127        | 81.82  | 1800 | 0.5962          | 0.7509    | 0.7729 | 0.7617 | 0.9241   |
| 0.0127        | 86.36  | 1900 | 0.6891          | 0.7580    | 0.7802 | 0.7690 | 0.9236   |
| 0.0013        | 90.91  | 2000 | 0.6205          | 0.75      | 0.7582 | 0.7541 | 0.9266   |
| 0.0013        | 95.45  | 2100 | 0.6235          | 0.7745    | 0.7802 | 0.7774 | 0.9292   |
| 0.0013        | 100.0  | 2200 | 0.6329          | 0.7656    | 0.7656 | 0.7656 | 0.9292   |
| 0.0013        | 104.55 | 2300 | 0.6482          | 0.7739    | 0.7399 | 0.7566 | 0.9241   |
| 0.0013        | 109.09 | 2400 | 0.6440          | 0.7675    | 0.7619 | 0.7647 | 0.9292   |
| 0.0008        | 113.64 | 2500 | 0.6388          | 0.7630    | 0.7546 | 0.7587 | 0.9343   |
| 0.0008        | 118.18 | 2600 | 0.7076          | 0.7774    | 0.7546 | 0.7658 | 0.9225   |
| 0.0008        | 122.73 | 2700 | 0.6698          | 0.7721    | 0.7692 | 0.7706 | 0.9297   |
| 0.0008        | 127.27 | 2800 | 0.6898          | 0.76      | 0.7656 | 0.7628 | 0.9220   |
| 0.0008        | 131.82 | 2900 | 0.6800          | 0.7482    | 0.7619 | 0.7550 | 0.9282   |
| 0.0006        | 136.36 | 3000 | 0.6911          | 0.7393    | 0.7582 | 0.7486 | 0.9215   |
| 0.0006        | 140.91 | 3100 | 0.6818          | 0.7446    | 0.7582 | 0.7514 | 0.9220   |
| 0.0006        | 145.45 | 3200 | 0.7043          | 0.7473    | 0.7692 | 0.7581 | 0.9210   |
| 0.0006        | 150.0  | 3300 | 0.6935          | 0.7482    | 0.7729 | 0.7604 | 0.9246   |
| 0.0006        | 154.55 | 3400 | 0.7163          | 0.7482    | 0.7729 | 0.7604 | 0.9230   |
| 0.0001        | 159.09 | 3500 | 0.7329          | 0.7590    | 0.7729 | 0.7659 | 0.9205   |
| 0.0001        | 163.64 | 3600 | 0.7570          | 0.7737    | 0.7766 | 0.7751 | 0.9215   |
| 0.0001        | 168.18 | 3700 | 0.7552          | 0.7664    | 0.7692 | 0.7678 | 0.9225   |
| 0.0001        | 172.73 | 3800 | 0.7226          | 0.7831    | 0.7802 | 0.7817 | 0.9246   |
| 0.0001        | 177.27 | 3900 | 0.6868          | 0.7844    | 0.7729 | 0.7786 | 0.9297   |
| 0.0003        | 181.82 | 4000 | 0.6916          | 0.7757    | 0.7729 | 0.7743 | 0.9256   |
| 0.0003        | 186.36 | 4100 | 0.6862          | 0.7749    | 0.7692 | 0.7721 | 0.9292   |
| 0.0003        | 190.91 | 4200 | 0.7067          | 0.7749    | 0.7692 | 0.7721 | 0.9225   |
| 0.0003        | 195.45 | 4300 | 0.7059          | 0.7628    | 0.7656 | 0.7642 | 0.9210   |
| 0.0003        | 200.0  | 4400 | 0.7300          | 0.7609    | 0.7692 | 0.7650 | 0.9210   |
| 0.0002        | 204.55 | 4500 | 0.7299          | 0.7572    | 0.7656 | 0.7614 | 0.9215   |
| 0.0002        | 209.09 | 4600 | 0.7168          | 0.7527    | 0.7692 | 0.7609 | 0.9210   |
| 0.0002        | 213.64 | 4700 | 0.7177          | 0.7545    | 0.7656 | 0.76   | 0.9210   |
| 0.0002        | 218.18 | 4800 | 0.7182          | 0.7545    | 0.7656 | 0.76   | 0.9210   |
| 0.0002        | 222.73 | 4900 | 0.7190          | 0.7628    | 0.7656 | 0.7642 | 0.9205   |
| 0.0001        | 227.27 | 5000 | 0.7168          | 0.7572    | 0.7656 | 0.7614 | 0.9215   |


### Framework versions

- Transformers 4.24.0.dev0
- Pytorch 1.12.1+cu113
- Datasets 2.2.2
- Tokenizers 0.13.1

{
    "jonatasgrosman/wav2vec2-large-xlsr-53-english": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Common Voice 6.1"
        ],
        "license": "apache-2.0",
        "github": "mozilla-foundation/common_voice_6_0",
        "paper": "https://github.com/jonatasgrosman/wav2vec2-sprint",
        "upstream_model": "wav2vec2-large-xlsr-53-english",
        "parameter_count": "53",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "wer",
                "result": 19.06
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "Fine-tuned {XLSR}-53 large model for speech recognition in {E}nglish",
        "input_format": "16kHz",
        "output_format": "16kHz",
        "sample_rate": "",
        "WER": ""
    },
    "bert-base-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "BookCorpus",
            "English Wikipedia"
        ],
        "license": "apache-2.0",
        "github": "<a href=\"https://huggingface.co/exbert/?model=bert-base-uncased\">\n<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>",
        "paper": "[this paper](https://arxiv.org/abs/1810.04805)",
        "upstream_model": "",
        "parameter_count": "16 TPU chips total, batch size of 256, sequence length of 128 tokens for 90% of the steps and 512 for the remaining 10%, Adam optimizer with a learning rate of 1e-4, \\(\\beta_{1} = 0.9\\) and \\(\\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "MNLI-(m/mm), QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE",
                "result": 84.6
            },
            {
                "test": "",
                "result": 83.4
            },
            {
                "test": "",
                "result": 71.2
            },
            {
                "test": "",
                "result": 90.5
            },
            {
                "test": "",
                "result": 93.5
            },
            {
                "test": "",
                "result": 52.1
            },
            {
                "test": "",
                "result": 85.8
            },
            {
                "test": "",
                "result": 88.9
            },
            {
                "test": "",
                "result": 66.4
            },
            {
                "test": "",
                "result": 79.6
            }
        ],
        "hardware": "4 cloud TPUs in Pod configuration (16 TPU chips total)",
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions: \n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n'score': 0.09747550636529922,\n'token': 10533,\n'token_str': 'carpenter'},\n{'sequence': '[CLS] the man worked as a waiter. [SEP]',\n'score': 0.0523831807076931,\n'token': 15610,\n'token_str': 'waiter'},\n{'sequence': '[CLS] the man worked as a barber. [SEP]',\n'score': 0.04962705448269844,\n'token': 13362,\n'token_str': 'barber'},\n{'sequence': '[CLS] the man worked as a mechanic. [SEP"
    },
    "xlm-roberta-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch"
        ],
        "datasets": [
            "2.5TB of filtered CommonCrawl data containing 100 languages"
        ],
        "license": "mit",
        "github": "See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.",
        "paper": "[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)",
        "upstream_model": "Unsupervised Cross-lingual Representation Learning at Scale",
        "parameter_count": "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.",
        "limitation_and_bias": "Unsupervised Cross-lingual Representation Learning at Scale",
        "demo": "See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.",
        "input_format": "Masked language modeling (MLM)",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words."
    },
    "bert-large-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "BookCorpus",
            "English Wikipedia"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?filter=bert",
        "paper": "https://arxiv.org/abs/1810.04805",
        "upstream_model": "",
        "parameter_count": "336M parameters",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "SQUAD 1.1 F1/EM",
                "result": 91.0
            },
            {
                "test": "Multi NLI Accuracy",
                "result": 86.05
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.",
        "demo": "See the [model hub](https://huggingface.co/models?filter=bert)",
        "input_format": "",
        "output_format": ""
    },
    "openai/clip-vit-large-patch14": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "YFCC100M"
        ],
        "license": "license",
        "github": "https://github.com/openai/CLIP/blob/main/model-card.md",
        "paper": "https://arxiv.org/clip-paper",
        "upstream_model": "Vision Transformer",
        "parameter_count": "#params",
        "hyper_parameters": {
            "epochs": "10",
            "batch_size": "32",
            "learning_rate": "0.001",
            "optimizer": "Adam"
        },
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.85
            }
        ],
        "hardware": "GPU",
        "limitation_and_bias": "CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section.",
        "demo": "https://forms.gle/Uv7afRH5dvY34ZEs9",
        "input_format": "image-caption data",
        "output_format": "NO_OUTPUT"
    },
    "gpt2": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "WebText"
        ],
        "license": "mit",
        "github": "https://github.com/openai/gpt-2/blob/master/domains.txt",
        "paper": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
        "upstream_model": "gpt2",
        "parameter_count": "124M parameters",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "LAMBADA",
                "result": 35.13
            },
            {
                "test": "CBT-CN",
                "result": 87.65
            },
            {
                "test": "CBT-NE",
                "result": 83.4
            },
            {
                "test": "WikiText2",
                "result": 29.41
            },
            {
                "test": "PTB",
                "result": 65.85
            },
            {
                "test": "enwiki8",
                "result": 1.16
            },
            {
                "test": "text8",
                "result": 1.17
            },
            {
                "test": "WikiText103",
                "result": 37.5
            },
            {
                "test": "1BW",
                "result": 75.2
            }
        ],
        "hardware": "256 cloud TPU v3 cores",
        "limitation_and_bias": "Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes. Here's an example of how the model can have biased predictions:  ```python >>> from transformers import pipeline, set_seed >>> generator = pipeline('text-generation', model='gpt2') >>> set_seed(42) >>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5) [{'generated_text': 'The White man worked as a mannequin for'}, {'generated_text': 'The White man worked as a maniser",
        "demo": "See the [model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.",
        "input_format": "The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.",
        "output_format": "",
        "input_token_limit": "1024",
        "vocabulary_size": "50,257"
    },
    "sociocom/MedNER-CR-JA": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "MedTxt-CR-JA-training-v2.xml"
        ],
        "license": "cc-by-4.0",
        "github": "",
        "paper": "NAISTSOC at the NTCIR-16 Real-MedNLP Task, In Proceedings of the 16th NTCIR Conference on Evaluation of Information Access Technologies (NTCIR-16), pp. 330-333, 2022",
        "upstream_model": "",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "This is a model for named entity recognition of Japanese medical documents.",
        "input_format": "MedTxt-CR-JA-training-v2.xml, NTCIR-16 Real-MedNLP subtask 1",
        "output_format": "license: - cc-by-4.0 tags: - NER - medical documents datasets: - MedTxt-CR-JA-training-v2.xml metrics: - NTCIR-16 Real-MedNLP subtask 1",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "xlm-roberta-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch"
        ],
        "datasets": [
            "2.5TB of filtered CommonCrawl data containing 100 languages"
        ],
        "license": "mit",
        "github": "https://github.com/pytorch/fairseq/tree/master/examples/xlmr",
        "paper": "https://arxiv.org/abs/1911.02116",
        "upstream_model": "Unsupervised Cross-lingual Representation Learning at Scale",
        "parameter_count": "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.",
        "limitation_and_bias": "Unsupervised Cross-lingual Representation Learning at Scale",
        "demo": "See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.",
        "input_format": "Masked language modeling (MLM)",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words."
    },
    "roberta-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "BookCorpus",
            "English Wikipedia",
            "CC-News",
            "OpenWebText",
            "Stories"
        ],
        "license": "mit",
        "github": "https://huggingface.co/models?filter=roberta",
        "paper": "http://arxiv.org/abs/1907.11692",
        "upstream_model": "NO_OUTPUT",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {
            "epochs": "NO_OUTPUT",
            "batch_size": "8K",
            "learning_rate": "6e-4",
            "optimizer": "Adam"
        },
        "evaluation": [
            {
                "test": "Glue",
                "result": 87.6
            },
            {
                "test": "MNLI",
                "result": 91.9
            },
            {
                "test": "QQP",
                "result": 92.8
            },
            {
                "test": "QNLI",
                "result": 94.8
            },
            {
                "test": "SST-2",
                "result": 63.6
            },
            {
                "test": "CoLA",
                "result": 91.2
            },
            {
                "test": "STS-B",
                "result": 90.2
            },
            {
                "test": "MRPC",
                "result": 78.7
            },
            {
                "test": "RTE",
                "result": 78.7
            }
        ],
        "hardware": "1024 V100 GPUs",
        "limitation_and_bias": "The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model can have biased predictions.",
        "demo": "<a href=\"https://huggingface.co/exbert/?model=roberta-base\">\n<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>",
        "input_format": "The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`.",
        "output_format": "The inputs of the model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>` - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `<mask>`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is."
    },
    "laion/CLIP-ViT-B-16-laion2B-s34B-b88K": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [],
        "datasets": [
            "2 Billion sample English subset of LAION-5B"
        ],
        "license": "OpenCLIP software",
        "github": "",
        "paper": "Citation",
        "upstream_model": "OpenAI CLIP model card, OpenAI CLIP paper, LAION-5B blog, upcoming paper",
        "parameter_count": "2 Billion sample English subset of LAION-5B and using a customized trained NSFW classifier that we built",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "OpenCLIP, [JUWELS Booster](https://apps.fz-juelich.de/jsc/hps/juwels/booster-overview.html)",
        "limitation_and_bias": "Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.",
        "demo": "The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. It is possible to extract a \u201csafe\u201d subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress.",
        "input_format": "2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)",
        "output_format": "This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)",
        "input_preprocessing": "The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "distilbert-base-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "BookCorpus",
            "English Wikipedia"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?filter=distilbert",
        "paper": "https://arxiv.org/abs/1910.01108",
        "upstream_model": "BERT base model",
        "parameter_count": "8",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Glue",
                "result": 82.2
            },
            {
                "test": "MNLI",
                "result": 88.5
            },
            {
                "test": "QQP",
                "result": 89.2
            },
            {
                "test": "QNLI",
                "result": 91.3
            },
            {
                "test": "SST-2",
                "result": 51.3
            },
            {
                "test": "CoLA",
                "result": 85.8
            },
            {
                "test": "STS-B",
                "result": 87.5
            },
            {
                "test": "MRPC",
                "result": 59.9
            },
            {
                "test": "RTE",
                "result": 87.5
            }
        ],
        "hardware": "8 16 GB V100",
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. It also inherits some of the bias of its teacher model. This bias will also affect all fine-tuned versions of this model.",
        "demo": "<a href=\"https://huggingface.co/exbert/?model=distilbert-base-uncased\"> <img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\"> </a>",
        "input_format": "lowercased and tokenized using WordPiece and a vocabulary size of 30,000; [CLS] Sentence A [SEP] Sentence B [SEP]; what is considered a sentence here is a consecutive span of text usually longer than a single sentence; the result with the two \"sentences\" has a combined length of less than 512 tokens; 15% of the tokens are masked; in 80% of the cases, the masked tokens are replaced by `[MASK]`",
        "output_format": "primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering",
        "input_token_limit": "With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens.",
        "vocabulary_size": "vocabulary size of 30,000"
    },
    "distilbert-base-multilingual-cased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "onnx",
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Wikipedia"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?filter=bert",
        "paper": "https://aclanthology.org/2021.acl-long.330.pdf",
        "upstream_model": "",
        "parameter_count": "134M parameters",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "risks, biases and limitations of the model",
        "demo": "See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you.",
        "input_format": "bert-base-multilingual-cased",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "distilroberta-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "OpenWebTextCorpus"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?filter=roberta",
        "paper": "https://huggingface.co/models?filter=roberta",
        "upstream_model": "",
        "parameter_count": "82M parameters (compared to 125M parameters for RoBERTa-base).",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "risks, biases and limitations of the model",
        "demo": "See the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that interests you.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "roberta-base model card"
    },
    "CIDAS/clipseg-rd64-refined": {
        "model_type": "computer-vision",
        "model_tasks": "image-segmentation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "dataset1",
            "dataset2"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/timojl/clipseg",
        "paper": "https://arxiv.org/abs/2112.10003",
        "upstream_model": "",
        "parameter_count": "1000000",
        "hyper_parameters": {
            "epochs": "10",
            "batch_size": "32",
            "learning_rate": "0.001",
            "optimizer": "adam"
        },
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.85
            },
            {
                "test": "f1-score",
                "result": 0.78
            }
        ],
        "hardware": "GPU",
        "limitation_and_bias": "The model may struggle with complex images.",
        "demo": "Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg).",
        "input_format": "text",
        "output_format": "image"
    },
    "microsoft/layoutlmv3-base": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "onnx",
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Microsoft Document AI"
        ],
        "license": "cc-by-nc-sa-4.0",
        "github": "https://aka.ms/layoutlmv3",
        "paper": "title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking}",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.",
        "demo": "Microsoft Document AI | [GitHub](https://aka.ms/layoutlmv3)",
        "input_format": "",
        "output_format": ""
    },
    "albert-base-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "BookCorpus",
            "English Wikipedia"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?filter=albert",
        "paper": "https://arxiv.org/abs/1909.11942",
        "upstream_model": "https://arxiv.org/abs/1909.11942",
        "parameter_count": "11M",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Average",
                "result": 82.3
            },
            {
                "test": "SQuAD1.1",
                "result": 90.2
            },
            {
                "test": "SQuAD2.0",
                "result": 83.2
            },
            {
                "test": "MNLI",
                "result": 84.6
            },
            {
                "test": "SST-2",
                "result": 92.9
            },
            {
                "test": "RACE",
                "result": 66.8
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.",
        "demo": "See the [model hub](https://huggingface.co/models?filter=albert)",
        "input_format": "lowercased and tokenized using SentencePiece and a vocabulary size of 30,000. The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]",
        "output_format": "`[CLS] Sentence A [SEP] Sentence B [SEP]`",
        "input_token_limit": "30,000",
        "vocabulary_size": "30,000"
    },
    "runwayml/stable-diffusion-v1-5": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "LAION-5B"
        ],
        "license": "creativeml-openrail-m",
        "github": "https://github.com/runwayml/stable-diffusion",
        "paper": "High-Resolution Image Synthesis With Latent Diffusion Models",
        "upstream_model": "stable-diffusion-diffusers",
        "parameter_count": "50",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "32 x 8 x A100 GPUs",
        "limitation_and_bias": "- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to \u201cA red cube on top of a blue sphere\u201d\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.",
        "demo": "Possible research areas and tasks include - Safe deployment of models which have the potential to generate harmful content. - Probing and understanding the limitations and biases of generative models. - Generation of artworks and use in design and other artistic processes. - Applications in educational or creative tools. - Research on generative models.",
        "input_format": "",
        "output_format": "text-to-image"
    },
    "nlptown/bert-base-multilingual-uncased-sentiment": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Number of reviews: 150k, 80k, 137k, 140k, 72k, 50k"
        ],
        "license": "mit",
        "github": "This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).",
        "paper": "sentiment analysis model for product reviews in any of the six languages above",
        "upstream_model": "bert-base-multilingual-uncased",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "epochs",
            "batch_size": "batch_size",
            "learning_rate": "learning_rate",
            "optimizer": "optimizer"
        },
        "evaluation": [
            {
                "test": "Accuracy (exact)",
                "result": 67
            },
            {
                "test": "Accuracy (off-by-1)",
                "result": 95
            }
        ],
        "hardware": "",
        "limitation_and_bias": "NO_OUTPUT",
        "demo": "This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).",
        "input_format": "",
        "output_format": "number of stars (between 1 and 5)",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "SamLowe/roberta-base-go_emotions": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "go_emotions"
        ],
        "license": "mit",
        "github": "https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb",
        "paper": "https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb",
        "upstream_model": "roberta-base",
        "parameter_count": "#params",
        "hyper_parameters": {
            "epochs": "3",
            "batch_size": "16",
            "learning_rate": "2e-5",
            "optimizer": "AdamW"
        },
        "evaluation": [
            {
                "test": "Accuracy",
                "result": 0.474
            },
            {
                "test": "Precision",
                "result": 0.575
            },
            {
                "test": "Recall",
                "result": 0.396
            },
            {
                "test": "F1",
                "result": 0.45
            }
        ],
        "hardware": "Not specified",
        "limitation_and_bias": "Optimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1",
        "demo": "There are multiple ways to use this model in Huggingface Transformers. Possibly the simplest is using a pipeline:\n\nfrom transformers import pipeline\n\nclassifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n\nsentences = [\"I am not having a great day\"]\n\nmodel_outputs = classifier(sentences)\nprint(model_outputs[0])\n# produces a list of dicts for each of the labels",
        "input_format": "NO_OUTPUT",
        "output_format": "NO_OUTPUT",
        "input_token_limit": "NO_OUTPUT",
        "vocabulary_size": "NO_OUTPUT"
    },
    "bert-base-cased": "Could not parse function call data: Unterminated string starting at: line 89 column 23 (char 5279)",
    "microsoft/deberta-base": "You exceeded your current quota, please check your plan and billing details.",
    "aitslab/biobert_huner_chemical_v1": "You exceeded your current quota, please check your plan and billing details.",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "You exceeded your current quota, please check your plan and billing details.",
    "cardiffnlp/twitter-roberta-base-irony": "You exceeded your current quota, please check your plan and billing details.",
    "salesken/query_wellformedness_score": "You exceeded your current quota, please check your plan and billing details.",
    "marieke93/MiniLM-evidence-types": "You exceeded your current quota, please check your plan and billing details.",
    "alimazhar-110/website_classification": "You exceeded your current quota, please check your plan and billing details.",
    "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli": "You exceeded your current quota, please check your plan and billing details.",
    "facebook/bart-large-mnli": "You exceeded your current quota, please check your plan and billing details.",
    "microsoft/resnet-50": "You exceeded your current quota, please check your plan and billing details.",
    "openai/clip-vit-base-patch32": "You exceeded your current quota, please check your plan and billing details.",
    "sentence-transformers/all-MiniLM-L6-v2": "You exceeded your current quota, please check your plan and billing details.",
    "aitslab/biobert_huner_disease_v1": "You exceeded your current quota, please check your plan and billing details.",
    "prajjwal1/bert-tiny": "You exceeded your current quota, please check your plan and billing details.",
    "microsoft/layoutlm-base-uncased": "You exceeded your current quota, please check your plan and billing details.",
    "aitslab/biobert_huner_gene_v1": "You exceeded your current quota, please check your plan and billing details.",
    "distilbert-base-uncased-finetuned-sst-2-english": "This model's maximum context length is 4097 tokens, however you requested 4276 tokens (4020 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "mosaicml/mpt-7b-instruct": "Could not parse function call data: Invalid \\escape: line 29 column 63 (char 1014)",
    "google/vit-base-patch16-224": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "ImageNet-21k",
            "ImageNet"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "2006.03677",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "google/electra-base-discriminator": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "jax",
            "rust",
            "transformers"
        ],
        "datasets": [
            "SQuAD 2.0",
            "GLUE",
            "SQuAD",
            "text chunking"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google-research/electra",
        "paper": "https://openreview.net/pdf?id=r1xMH1BtvB",
        "upstream_model": "ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN",
        "parameter_count": "N/A",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "single GPU",
        "limitation_and_bias": "ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN. At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the SQuAD 2.0 dataset.",
        "demo": "This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. GLUE), QA tasks (e.g., SQuAD), and sequence tagging tasks (e.g., text chunking).",
        "input_format": "ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network",
        "output_format": ""
    },
    "deepset/roberta-base-squad2": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "squad_v2"
        ],
        "license": "cc-by-4.0",
        "github": "https://github.com/deepset-ai/haystack",
        "paper": "",
        "upstream_model": "roberta-base",
        "parameter_count": "8",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "96",
            "learning_rate": "3e-5",
            "optimizer": "",
            "lr_schedule": "",
            "warmup_proportion": "",
            "doc_stride": "",
            "max_query_length": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 79.87029394424324
            },
            {
                "test": "",
                "result": 82.91251169582613
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "bert-base-multilingual-cased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "BERT"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages",
        "paper": "https://arxiv.org/abs/1810.04805",
        "upstream_model": "NO_OUTPUT",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "https://huggingface.co/models?filter=bert",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "roberta-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch"
        ],
        "datasets": [
            "BookCorpus",
            "English Wikipedia",
            "CC-News",
            "OpenWebText",
            "Stories"
        ],
        "license": "mit",
        "github": "https://huggingface.co/models?filter=roberta",
        "paper": "http://arxiv.org/abs/1907.11692",
        "upstream_model": "masked language modeling (MLM) objective",
        "parameter_count": "1024 V100 GPUs, 500K steps, batch size of 8K, sequence length of 512, Adam, learning rate of 4e-4, \\(\\beta_{1} = 0.9\\), \\(\\beta_{2} = 0.98\\), \\(\\epsilon = 1e-6\\), weight decay of 0.01, learning rate warmup for 30,000 steps, linear decay of the learning rate after.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "8K",
            "learning_rate": "4e-4",
            "optimizer": "Adam"
        },
        "evaluation": [
            {
                "test": "Glue test results",
                "result": 90.2
            },
            {
                "test": "MNLI",
                "result": 90.2
            },
            {
                "test": "QQP",
                "result": 92.2
            },
            {
                "test": "QNLI",
                "result": 94.7
            },
            {
                "test": "SST-2",
                "result": 96.4
            },
            {
                "test": "CoLA",
                "result": 68.0
            },
            {
                "test": "STS-B",
                "result": 96.4
            },
            {
                "test": "MRPC",
                "result": 90.9
            },
            {
                "test": "RTE",
                "result": 86.6
            }
        ],
        "hardware": "1024 V100 GPUs, batch size of 8K, sequence length of 512",
        "limitation_and_bias": "The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model can have biased predictions.",
        "demo": "<a href=\"https://huggingface.co/exbert/?model=roberta-base\">\n<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>",
        "input_format": "The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`. 15% of the tokens are masked. In 80% of the cases, the masked tokens are replaced by `<mask>`. In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. In the 10% remaining cases, the masked tokens are left as is.",
        "output_format": "The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`. 15% of the tokens are masked. In 80% of the cases, the masked tokens are replaced by `<mask>`. In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. In the 10% remaining cases, the masked tokens are left as is.",
        "input_token_limit": "512 contiguous token that may span over documents.",
        "vocabulary_size": "50,000"
    },
    "sentence-transformers/all-mpnet-base-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers"
        ],
        "datasets": [
            "1B sentence pairs dataset"
        ],
        "license": "apache-2.0",
        "github": "github repositories Sentence Embeddings Benchmark: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-mpnet-base-v2)",
        "paper": "sentence-transformers and semantic search",
        "upstream_model": "sentence-transformers",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "sentence-transformers, https://www.SBERT.net",
        "input_format": "text",
        "output_format": "vector",
        "input_token_limit": "384",
        "vocabulary_size": ""
    },
    "allenai/scibert_scivocab_uncased": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Semantic Scholar"
        ],
        "license": "The original repo can be found [here](https://github.com/allenai/scibert). If using these models, please cite the following paper:",
        "github": "The original repo can be found [here](https://github.com/allenai/scibert).",
        "paper": "\"SciBERT: A Pretrained Language Model for Scientific Text\"\n\"If using these models, please cite the following paper:\"\n\"@inproceedings{beltagy-etal-2019-scibert,\"\n\"title = \"SciBERT: A Pretrained Language Model for Scientific Text\",\"\n\"author = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\"\n\"booktitle = \"EMNLP\",\"\n\"year = \"2019\",\"\n\"publisher = \"Association for Computational Linguistics\",\"\n\"url = \"https://www.aclweb.org/anthology/D19-1371\"\"",
        "upstream_model": "\"BERT model trained on scientific text\"",
        "parameter_count": "parameter_count",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "The original repo can be found [here](https://github.com/allenai/scibert).",
        "input_format": "",
        "output_format": ""
    },
    "t5-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "Colossal Clean Crawled Corpus (C4)",
            "C4",
            "Wiki-DPR",
            "Sentence acceptability judgment",
            "CoLA",
            "Sentiment analysis",
            "SST-2",
            "Paraphrasing/sentence similarity",
            "MRPC",
            "STS-B",
            "QQP"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)",
        "upstream_model": "",
        "parameter_count": "T5-Base is the checkpoint with 220 million parameters.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)",
        "demo": "demo",
        "input_format": "reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings",
        "output_format": "",
        "input_token_limit": "text-to-text framework...NLP task...hyperparameters",
        "vocabulary_size": "C4, Wiki-DPR, Sentence acceptability judgment, CoLA, Sentiment analysis, SST-2, Paraphrasing/sentence similarity, MRPC, STS-B, QQP, Natural language inference, MNLI, QNLI, RTE, CB, Sentence completion, COPA, Word sense disambiguation, WIC, Question answering, MultiRC, ReCoRD, BoolQ"
    },
    "bert-base-chinese": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Training Data"
        ],
        "license": "",
        "github": "",
        "paper": "BERT https://arxiv.org/abs/1810.04805",
        "upstream_model": "",
        "parameter_count": "type_vocab_size: 2, vocab_size: 21128, num_hidden_layers: 12",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "num_hidden_layers: 12",
        "limitation_and_bias": "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "type_vocab_size: 2, vocab_size: 21128, num_hidden_layers: 12",
        "vocabulary_size": "type_vocab_size: 2, vocab_size: 21128"
    },
    "allenai/longformer-base-4096": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "Longformer: The Long-Document Transformer",
            "the Allen Institute for Artificial Intelligence (AI2)"
        ],
        "license": "apache-2.0",
        "github": "Longformer: The Long-Document Transformer",
        "paper": "Longformer: The Long-Document Transformer",
        "upstream_model": "Longformer: The Long-Document Transformer",
        "parameter_count": "longformer-base-4096",
        "hyper_parameters": {
            "epochs": "Longformer: The Long-Document Transformer",
            "batch_size": "Longformer: The Long-Document Transformer",
            "learning_rate": "Longformer: The Long-Document Transformer",
            "optimizer": "Longformer: The Long-Document Transformer"
        },
        "evaluation": [],
        "hardware": "Longformer: The Long-Document Transformer",
        "limitation_and_bias": "Longformer: The Long-Document Transformer",
        "demo": "Longformer: The Long-Document Transformer",
        "input_format": "Longformer: The Long-Document Transformer",
        "output_format": "Longformer: The Long-Document Transformer"
    },
    "lengyue233/content-vec-best": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "fairseq ContentVec model"
        ],
        "license": "mit",
        "github": "Official Repo: [ContentVec](https://github.com/auspicious3000/contentvec)",
        "paper": "fairseq ContentVec model",
        "upstream_model": "HubertModel",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "download the ContentVec_legacy model from the official repo, and then run python convert.py",
        "input_format": "self.final_proj = nn.Linear(config.hidden_size, config.classifier_proj_size)\nmodel = HubertModelWithFinalProj.from_pretrained(\"lengyue233/content-vec-best\")\nx = model(audio)[\"last_hidden_state\"]",
        "output_format": "self.final_proj = nn.Linear(config.hidden_size, config.classifier_proj_size)\nmodel = HubertModelWithFinalProj.from_pretrained(\"lengyue233/content-vec-best\")\nx = model(audio)[\"last_hidden_state\"]"
    },
    "camembert-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "OSCAR multilingual corpus"
        ],
        "license": "mit",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "110M",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Helsinki-NLP/opus-mt-zh-en": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "cc-by-4.0",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "port_machine: brutasse",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "SentencePiece (spm32k,spm32k)",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "SentencePiece (spm32k,spm32k)"
    },
    "pyannote/segmentation": "401 Client Error. (Request ID: Root=1-653ddcee-60b8f97945fc05b30edbd0e4)\n\nCannot access gated repo for url https://huggingface.co/api/models/pyannote/segmentation.\nRepo model pyannote/segmentation is gated. You must be authenticated to access it.",
    "bigscience/bloom-560m": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch"
        ],
        "datasets": [
            "https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling"
        ],
        "license": "bigscience-bloom-rail-1.0",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "559,214,592 parameters",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model",
        "limitation_and_bias": "",
        "demo": "Provide a quick summary of what the model is/does.",
        "input_format": "1.5TB of pre-processed text, converted into 350B unique tokens\n45 natural languages\n12 programming languages",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "250,680"
    },
    "t5-small": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "Colossal Clean Crawled Corpus (C4)",
            "C4",
            "Wiki-DPR",
            "Sentence acceptability judgment",
            "CoLA",
            "Sentiment analysis",
            "SST-2",
            "Paraphrasing/sentence similarity",
            "MRPC",
            "STS-B",
            "QQP"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "research paper",
        "upstream_model": "",
        "parameter_count": "T5-Small is the checkpoint with 60 million parameters.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)",
        "demo": "demo",
        "input_format": "reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings",
        "output_format": "",
        "input_token_limit": "text-to-text framework...NLP task...hyperparameters",
        "vocabulary_size": "T5-Small is the checkpoint with 60 million parameters."
    },
    "timm/resnet50.a1_in1k": "This model's maximum context length is 4097 tokens, however you requested 18743 tokens (18487 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "Seethal/sentiment_analysis_generic_dataset": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "classified dataset for text classification"
        ],
        "license": "",
        "github": "",
        "paper": "This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis",
        "upstream_model": "bert-base-uncased",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "This model is not intended for further downstream fine-tuning for any other tasks.",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "facebook/encodec_24khz": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "DNS Challenge 4",
            "Common Voice",
            "AudioSet",
            "FSD50K",
            "Jamendo dataset"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "PubMed",
            "PubMedCentral"
        ],
        "license": "mit",
        "github": "[Recent work](https://arxiv.org/abs/2007.15779), [PubMed](https://pubmed.ncbi.nlm.nih.gov/), [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/), [Biomedical Language Understanding and Reasoning Benchmark](https://aka.ms/BLURB)",
        "paper": "\"If you find PubMedBERT useful in your research, please cite the following paper: \n@misc{pubmedbert,\nauthor = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},\ntitle = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},\nyear = {2020},\neprint = {arXiv:2007.15779},\n}\"",
        "upstream_model": "",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/). This model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the top score on the [Biomedical Language Understanding and Reasoning Benchmark](https://aka.ms/BLURB).",
                "result": 0
            }
        ],
        "hardware": "NO_OUTPUT",
        "limitation_and_bias": "\"pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models\" and \"Pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/).\"",
        "demo": "<a href=\"https://huggingface.co/exbert/?model=microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&modelKind=bidirectional&sentence=Gefitinib%20is%20an%20EGFR%20tyrosine%20kinase%20inhibitor,%20which%20is%20often%20used%20for%20breast%20cancer%20and%20NSCLC%20treatment.&layer=3&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=17&tokenSide=right&maskInds=..&hideClsSep=true\"><img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\"></a>",
        "input_format": "\"abstracts from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/)\"",
        "output_format": "NO_OUTPUT",
        "input_token_limit": "NO_OUTPUT",
        "vocabulary_size": "NO_OUTPUT"
    },
    "laion/CLIP-ViT-H-14-laion2B-s32B-b79K": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [
            "pytorch"
        ],
        "datasets": [
            "The 2 Billion sample English subset of LAION-5B"
        ],
        "license": "OpenCLIP software",
        "github": "https://github.com/LAION-AI/CLIP_benchmark",
        "paper": "Citation",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "OpenCLIP",
        "limitation_and_bias": "Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.",
        "demo": "Use the code below to get started with the model.",
        "input_format": "training notes, wandb logs",
        "output_format": "output_format",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "timm",
            "pytorch",
            "safetensors"
        ],
        "datasets": [
            "imagenet-1k",
            "wit-400m",
            "imagenet-12k"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/huggingface/pytorch-image-models",
        "paper": "https://arxiv.org/abs/2103.00020",
        "upstream_model": "Pretrained on WIT-400M image-text pairs by OpenAI using CLIP.",
        "parameter_count": "304.2",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "WIT-400M image-text pairs",
        "limitation_and_bias": "NO_OUTPUT",
        "demo": "[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)",
        "input_format": "Image size: 224 x 224, Dataset: ImageNet-1k, Pretrain Dataset: WIT-400M, ImageNet-12k",
        "output_format": "",
        "input_preprocessing": "Pretrained on WIT-400M image-text pairs by OpenAI using CLIP. Fine-tuned on ImageNet-12k and then ImageNet-1k in `timm`.",
        "input_size": "Image size: 224 x 224",
        "num_of_classes_for_classification": "ImageNet-1k",
        "trigger_word": ""
    },
    "cambridgeltl/SapBERT-from-PubMedBERT-fulltext": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "UMLS"
        ],
        "license": "apache-2.0",
        "github": "ACL 2021, NAACL 2021",
        "paper": "[Liu et al. (2020)](https://arxiv.org/pdf/2010.11784.pdf)",
        "upstream_model": "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
        "parameter_count": "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
        "limitation_and_bias": "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
        "demo": "ACL 2021 and NAACL 2021",
        "input_format": "UMLS 2020AA (English only), microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
        "output_format": "The [CLS] embedding of the last layer is regarded as the output."
    },
    "facebook/bart-large": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "rust",
            "transformers"
        ],
        "datasets": [
            "BART model pre-trained on English language"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "upstream_model": "BART model pre-trained on English language",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "The team releasing BART did not write a model card for this model",
        "demo": "https://huggingface.co/models?search=bart",
        "input_format": "return_tensors=\"pt\"",
        "output_format": "return_tensors=\"pt\""
    },
    "deepset/tinyroberta-squad2": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "squad_v2"
        ],
        "license": "cc-by-4.0",
        "github": "https://github.com/deepset-ai/haystack",
        "paper": "https://arxiv.org/pdf/1909.10351.pdf",
        "upstream_model": "deepset/tinyroberta-squad2",
        "parameter_count": "9",
        "hyper_parameters": {
            "epochs": "4",
            "batch_size": "96",
            "learning_rate": "3e-5",
            "optimizer": "AdamW"
        },
        "evaluation": [
            {
                "test": "SQuAD 2.0",
                "result": 78.69114798281817
            }
        ],
        "hardware": "4x Tesla v100",
        "limitation_and_bias": "tinyroberta-squad2, English, Extractive QA, SQuAD 2.0",
        "demo": "[deepset](http://deepset.ai/), [Haystack](https://haystack.deepset.ai/)",
        "input_format": "name: squad_v2 type: squad_v2 config: squad_v2 split: validation",
        "output_format": "",
        "input_token_limit": "tinyroberta-squad2",
        "vocabulary_size": "tinyroberta-squad2"
    },
    "distilgpt2": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "OpenWebTextCorpus"
        ],
        "license": "apache-2.0",
        "github": "OpenWebTextCorpus https://skylion007.github.io/OpenWebTextCorpus/, OpenWebTextCorpus Dataset Card https://huggingface.co/datasets/openwebtext, Radford et al. (2019) https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf",
        "paper": "`title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas}, booktitle={NeurIPS EMC^2 Workshop}, year={2019}`",
        "upstream_model": "distilgpt2",
        "parameter_count": "82 million",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "8 16GB V100, 168 (1 week), Azure, East US",
        "limitation_and_bias": "\"As the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), \u201clanguage models like GPT-2 reflect the biases inherent to the systems they were trained on.\u201d Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). \n\n- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.\n- [Xu and Hu (2022)](https://ar",
        "demo": "<a href=\"https://huggingface.co/exbert/?model=distilgpt2\">\n<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>",
        "input_format": "\"type: text-generation\" \"name: WikiText-103\" \"type: wikitext\"",
        "output_format": "\"type: text-generation\" \"name: Text Generation\" \"name: WikiText-103\" \"type: wikitext\" \"type: perplexity\" \"value: 21.1\" \"name: Perplexity\"",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "dslim/bert-large-NER": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "conll2003"
        ],
        "license": "mit",
        "github": "https://github.com/google-research/bert/issues/223",
        "paper": "https://arxiv.org/pdf/1810.04805",
        "upstream_model": "original BERT paper",
        "parameter_count": "single NVIDIA V100 GPU",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "f1",
                "result": 91.7
            }
        ],
        "hardware": "NVIDIA V100 GPU",
        "limitation_and_bias": "This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.",
        "demo": "from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)",
        "input_format": "metric|dev|test\n-|-|-",
        "output_format": "output_format|metric|dev|test|-|-|-|f1 |95.7 |91.7|precision |95.3 |91.2|recall |96.1 |92.3",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "rinna/japanese-hubert-base": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "ReazonSpeech"
        ],
        "license": "Apache 2.0",
        "github": "https://github.com/facebookresearch/fairseq/tree/main/examples/hubert",
        "paper": "https://ieeexplore.ieee.org/document/9585401",
        "upstream_model": "HuBERT base model",
        "parameter_count": "19000",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "nateraw/vit-age-classifier": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "fairface"
        ],
        "license": "",
        "github": "github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "\"A vision transformer finetuned to classify the age of a given person's face.\" ```python import requests from PIL import Image from io import BytesIO from transformers import ViTFeatureExtractor, ViTForImageClassification # Get example image from official fairface repo + read it in as an image r = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true') im = Image.open(BytesIO(r.content)) # Init model, transforms model = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier') transforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier') # Transform our image and pass it through the model inputs = transforms(im, return_tensors='pt') output = model(**inputs) # Predicted Class probabilities proba = output.logits.softmax(1) # Predicted Classes preds = proba.argmax(1) ```",
        "input_format": "`return_tensors='pt'`",
        "output_format": "`ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')` `ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')` `output.logits.softmax(1)` `proba.argmax(1)`",
        "input_preprocessing": "\"transforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\" and \"inputs = transforms(im, return_tensors='pt')\"",
        "input_size": "",
        "num_of_classes_for_classification": "\"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\" and \"output.logits.softmax(1)\"",
        "trigger_word": ""
    },
    "dslim/bert-base-NER": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "CoNLL-2003 NER task"
        ],
        "license": "mit",
        "github": "https://github.com/google-research/bert/issues/223",
        "paper": "https://arxiv.org/pdf/1810.04805",
        "upstream_model": "original BERT paper",
        "parameter_count": "single NVIDIA V100 GPU",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "f1",
                "result": 91.3
            },
            {
                "test": "precision",
                "result": 90.7
            },
            {
                "test": "recall",
                "result": 91.9
            }
        ],
        "hardware": "NVIDIA V100 GPU",
        "limitation_and_bias": "This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.",
        "demo": "from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)",
        "input_format": "metric|dev|test\n-|-|-\nf1 |95.1 |91.3\nprecision |95.0 |90.7\nrecall |95.3 |91.9",
        "output_format": "output_format|metric|dev|test-|-|-f1 |95.1 |91.3precision |95.0 |90.7recall |95.3 |91.9",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "nlpconnect/vit-gpt2-image-captioning": {
        "model_type": "multimodal",
        "model_tasks": "image-to-text",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "flax"
        ],
        "license": "",
        "github": "http://github.com/ankur3107",
        "paper": "https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "THUDM/chatglm2-6b": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "Apache-2.0, MODEL_LICENSE",
        "github": "\ud83d\udcbb <a href=\"https://github.com/THUDM/ChatGLM2-6B\" target=\"_blank\">Github Repo</a> \u2022 \ud83d\udcc3 <a href=\"https://arxiv.org/abs/2103.10360\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\"https://github.com/THUDM/GLM\" target=\"_blank\">[GitHub]</a> \u2022 \ud83d\udcc3 <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">[GLM-130B@ICLR 23]</a> <a href=\"https://github.com/THUDM/GLM-130B\" target=\"_blank\">[GitHub]</a>",
        "paper": "@article{zeng2022glm,\ntitle={Glm-130b: An open bilingual pre-trained model},\nauthor={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\njournal={arXiv preprint arXiv:2210.02414},\nyear={2022}\n}\n\n@inproceedings{du2022glm,\ntitle={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\nauthor={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\nbooktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\npages={320--335},\nyear={2022}\n}",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "evaluation results",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "For more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM2-6B).",
        "input_format": "language:\n- zh\n- en\ntags:\n- glm\n- chatglm\n- thudm",
        "output_format": ""
    },
    "cardiffnlp/twitter-roberta-base-sentiment-latest": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "tweet_eval"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "yiyanghkust/finbert-tone": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Corporate Reports 10-K & 10-Q",
            "Earnings Call Transcripts",
            "Analyst Reports"
        ],
        "license": "More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)",
        "github": "More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)",
        "paper": "Huang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).",
        "upstream_model": "yiyanghkust/finbert-tone",
        "parameter_count": "Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens",
        "limitation_and_bias": "Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens, finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports.",
        "demo": "More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports.",
        "input_format": "Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens",
        "output_format": "\"LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\"",
        "input_token_limit": "Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens",
        "vocabulary_size": "\"The total corpora size is 4.9B tokens.\""
    },
    "ProsusAI/finbert": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Financial PhraseBank"
        ],
        "license": "",
        "github": "",
        "paper": "https://arxiv.org/abs/1908.10063",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [...] The model will give softmax outputs for three labels: positive, negative or neutral.",
        "input_format": "",
        "output_format": "The model will give softmax outputs for three labels: positive, negative or neutral.",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "facebook/bart-large-cnn": {
        "model_type": "natural-language-processing",
        "model_tasks": "summarization",
        "frameworks": [
            "pytorch",
            "jax",
            "rust",
            "transformers"
        ],
        "datasets": [
            "cnn_dailymail"
        ],
        "license": "mit",
        "github": "",
        "paper": "https://arxiv.org/abs/1910.13461",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "YituTech/conv-bert-base": "404 Client Error. (Request ID: Root=1-653de2b7-2afa28304a1f4c02116fc0fb)\n\nEntry Not Found for url: https://huggingface.co/YituTech/conv-bert-base/resolve/main/README.md.",
    "google/fnet-base": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "C4"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?filter=fnet",
        "paper": "https://arxiv.org/abs/2105.03824",
        "upstream_model": "https://arxiv.org/abs/2105.03824",
        "parameter_count": "16 TPU chips total, batch size of 256, sequence length of 512 tokens, Adam optimizer, learning rate of 1e-4, \\(\\beta_{1} = 0.9\\), \\(\\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warmup for 10,000 steps, linear decay of the learning rate after.",
        "hyper_parameters": "Adam with a learning rate of 1e-4, \\(\\beta_{1} = 0.9\\) and \\(\\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.",
        "evaluation": [
            {
                "test": "Table 1 on page 7 of the official paper",
                "result": 0.58
            }
        ],
        "hardware": "4 cloud TPUs in Pod configuration (16 TPU chips total)",
        "limitation_and_bias": "The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000. The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP] With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by [MASK]. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.",
        "demo": "See the [model hub](https://huggingface.co/models?filter=fnet)",
        "input_format": "The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000. The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP] With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens.",
        "output_format": "The inputs of the model are then of the form:  \n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```  \nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.  \nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is."
    },
    "cardiffnlp/twitter-xlm-roberta-base-sentiment": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "~198M tweets"
        ],
        "license": "",
        "github": "cardiffnlp/twitter-xlm-roberta-base-sentiment",
        "paper": "XLM-T: A Multilingual Language Model Toolkit for Twitter",
        "upstream_model": "XLM-roBERTa-base model",
        "parameter_count": "198M",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "sentiment-task",
                "result": 0.6600581407546997
            }
        ],
        "hardware": "~198M tweets",
        "limitation_and_bias": "This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details). Paper: XLM-T: A Multilingual Language Model Toolkit for Twitter. Git Repo: XLM-T official repository. This model has been integrated into the TweetNLP library.",
        "demo": "The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details). Paper: XLM-T: A Multilingual Language Model Toolkit for Twitter. Git Repo: XLM-T official repository. This model has been integrated into the TweetNLP library.",
        "input_format": "~198M tweets and XLM-T official repository",
        "output_format": "output_format",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "j-hartmann/emotion-english-distilroberta-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Crowdflower (2016)",
            "Emotion Dataset, Elvis et al. (2018)",
            "GoEmotions, Demszky et al. (2020)",
            "ISEAR, Vikash (2018)",
            "MELD, Poria et al. (2019)",
            "SemEval-2018, EI-reg, Mohammad et al. (2018)"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "DistilRoBERTa-base, RoBERTa-large",
        "parameter_count": "parameter_count: DistilRoBERTa-base, RoBERTa-large",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "With this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:",
                "result": 0
            }
        ],
        "hardware": "\"The model was trained on 6 diverse datasets\"",
        "limitation_and_bias": "\"The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:  1) anger \ud83e\udd2c 2) disgust \ud83e\udd22 3) fear \ud83d\ude28 4) joy \ud83d\ude00 5) neutral \ud83d\ude10 6) sadness \ud83d\ude2d 7) surprise \ud83d\ude32  The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base). For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.\"",
        "demo": "a) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/simple_emotion_pipeline.ipynb)  \n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\nclassifier(\"I love this!\")\n```  \n```python\nOutput:\n[[{'label': 'anger', 'score': 0.004419783595949411},\n{'label': 'disgust', 'score': 0.0016119900392368436},\n{'label': 'fear', 'score': 0.0004138521908316761},\n{'label':",
        "input_format": "\"All datasets contain English text\" and \"The model is trained on a balanced subset from the datasets listed above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanced subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the random-chance baseline of 1/7 = 14%).\"",
        "output_format": "\"The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base)\" \"For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.\"",
        "input_token_limit": "",
        "vocabulary_size": "\"The model was trained on 6 diverse datasets\" and \"The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base)\""
    },
    "stabilityai/sd-vae-ft-mse": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "https://ommer-lab.com/files/latent-diffusion/kl-f8.zip",
            "https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt",
            "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt"
        ],
        "license": "mit",
        "github": "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt",
        "paper": "",
        "upstream_model": "original | 246803 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "original",
                "result": 246803
            },
            {
                "test": "ft-EMA",
                "result": 560001
            },
            {
                "test": "ft-MSE",
                "result": 840001
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "Link: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt",
        "input_format": "256x256 images from the COCO2017 validation dataset",
        "output_format": ""
    },
    "pyannote/speaker-diarization": "401 Client Error. (Request ID: Root=1-653de391-291853384efe081a3dc080ba)\n\nCannot access gated repo for url https://huggingface.co/api/models/pyannote/speaker-diarization.\nRepo model pyannote/speaker-diarization is gated. You must be authenticated to access it.",
    "google/bert_uncased_L-2_H-128_A-2": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "2/128 (BERT-Tiny)",
            "4/256 (BERT-Mini)",
            "4/512 (BERT-Small)",
            "8/512 (BERT-Medium)",
            "12/768 (BERT-Base)"
        ],
        "license": "apache-2.0",
        "github": "[official BERT Github page](https://github.com/google-research/bert/)",
        "paper": "[Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962)",
        "upstream_model": "\"BERT-Tiny\" \"2/128 (BERT-Tiny)\" \"BERT-Mini\" \"4/256 (BERT-Mini)\" \"BERT-Small\" \"4/512 (BERT-Small)\" \"BERT-Medium\" \"8/512 (BERT-Medium)\" \"BERT-Base\" \"12/768 (BERT-Base)\"",
        "parameter_count": "\"2/128 (BERT-Tiny)\", \"4/256 (BERT-Mini)\", \"4/512 (BERT-Small)\", \"8/512 (BERT-Medium)\", \"12/768 (BERT-Base)\"",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "8, 16, 32, 64, 128",
            "learning_rate": "3e-4, 1e-4, 5e-5, 3e-5",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "CoLA",
                "result": 0
            },
            {
                "test": "SST-2",
                "result": 85.9
            },
            {
                "test": "MRPC",
                "result": 81.1
            },
            {
                "test": "STS-B",
                "result": 81.1
            },
            {
                "test": "QQP",
                "result": 75.4
            },
            {
                "test": "MNLI-m",
                "result": 66.4
            },
            {
                "test": "MNLI-mm",
                "result": 86.2
            },
            {
                "test": "QNLI(v2)",
                "result": 74.8
            },
            {
                "test": "RTE",
                "result": 84.1
            },
            {
                "test": "WNLI",
                "result": 57.9
            },
            {
                "test": "AX",
                "result": 62.3
            }
        ],
        "hardware": "\"We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.\"",
        "limitation_and_bias": "The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher. Note that the BERT-Base model in this release is included for completeness only; it was re-trained under the same regime as the original model. For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs: - batch sizes: 8, 16, 32, 64, 128 - learning rates: 3e-4, 1e-4, 5e-5, 3e-5",
        "demo": "[**2/128 (BERT-Tiny)**][2_128], [**4/256 (BERT-Mini)**][4_256], [**4/512 (BERT-Small)**][4_512], [**8/512 (BERT-Medium)**][8_512], [**12/768 (BERT-Base)**][12_768]",
        "input_format": "\"We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models.\"",
        "output_format": "\"You can download the 24 BERT miniatures either from the [official BERT Github page](https://github.com/google-research/bert/), or via HuggingFace from the links below:\""
    },
    "cardiffnlp/twitter-roberta-base-sentiment": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "tweet_eval"
        ],
        "github": "https://github.com/cardiffnlp/tweeteval",
        "paper": "https://arxiv.org/pdf/2010.12421.pdf",
        "upstream_model": "roBERTa-base model",
        "limitation_and_bias": "Reference Paper: [_TweetEval_ (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval). Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive.",
        "demo": "This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive. We just released a new sentiment analysis model trained on more recent and a larger quantity of tweets. See [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) and [TweetNLP](https://tweetnlp.org) for more details.",
        "input_format": "tweet_eval",
        "output_format": "tweet_eval"
    },
    "google/flan-t5-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [],
        "license": "Creative Commons Attribution 4.0 International",
        "github": "- svakulenk0/qrecc\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed",
        "paper": "\"original paper, figure 2\"",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: ![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png) For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).",
                "result": 0
            }
        ],
        "hardware": "5. [Training Details]",
        "limitation_and_bias": "4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)",
        "demo": "\"Find below some example scripts on how to use the model in `transformers`:\"",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "google/flan-t5-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "dslim/bert-base-NER-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "MNLI"
        ],
        "license": "mit",
        "github": "https://github.com/huggingface/transformers",
        "paper": "https://arxiv.org/abs/1910.03771",
        "upstream_model": "bert-base-uncased",
        "parameter_count": "109482240",
        "hyper_parameters": {
            "epochs": "3",
            "batch_size": "32",
            "learning_rate": "2e-5",
            "optimizer": "AdamW"
        },
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.843
            }
        ],
        "hardware": "NVIDIA V100",
        "limitation_and_bias": "The model may not perform well on tasks outside of the MNLI dataset.",
        "demo": "You can use the model for natural language inference tasks.",
        "input_format": "A pair of sentences",
        "output_format": "A single label",
        "input_token_limit": "512",
        "vocabulary_size": "30522"
    },
    "Jean-Baptiste/camembert-ner": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "onnx",
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "wikiner-fr dataset"
        ],
        "license": "mit",
        "github": "",
        "paper": "",
        "upstream_model": "camemBERT",
        "parameter_count": "170 634",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "Overall",
                "result": 0.8914
            },
            {
                "test": "PER",
                "result": 0.9483
            },
            {
                "test": "ORG",
                "result": 0.8181
            },
            {
                "test": "LOC",
                "result": 0.8955
            },
            {
                "test": "MISC",
                "result": 0.8146
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nnlp(\"Apple est cr\u00e9\u00e9e le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs \u00e0 Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitu\u00e9e sous forme de soci\u00e9t\u00e9 le 3 janvier 1977 \u00e0 l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refl\u00e9ter la diversification de ses produits, le mot \u00ab computer \u00bb est retir\u00e9 le 9 janvier 2015.\")\n```",
        "input_format": "O,MISC,PER,ORG,LOC",
        "output_format": "precision|recall|f1 -|-|- 0.8859|0.8971|0.8914 entity|precision|recall|f1 -|-|-|- PER|0.9372|0.9598|0.9483 ORG|0.8099|0.8265|0.8181 LOC|0.8905|0.9005|0.8955 MISC|0.8175|0.8117|0.8146",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "LTP/small": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "hf-internal-testing/tiny-random-gpt2": "404 Client Error. (Request ID: Root=1-653de499-737841981c96c3914de9e23b)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/README.md.",
    "CompVis/stable-diffusion-safety-checker": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "CLIP"
        ],
        "license": "More information needed",
        "github": "[Stable Diffusion Model Card](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md)",
        "paper": "[CLIP Paper](https://arxiv.org/abs/2103.00020)",
        "upstream_model": "ViT-L/14 Transformer architecture as an image encoder, masked self-attention Transformer as a text encoder",
        "parameter_count": "parameter_count NO_OUTPUT",
        "hyper_parameters": {
            "epochs": "More information needed",
            "batch_size": "Image Identification",
            "learning_rate": "[CLIP](https://huggingface.co/openai/clip-vit-large-patch14)",
            "optimizer": "[CLIP Paper](https://arxiv.org/abs/2103.00020)"
        },
        "evaluation": [
            {
                "test": "More information needed",
                "result": 0
            }
        ],
        "hardware": "ViT-L/14 Transformer architecture as an image encoder, masked self-attention Transformer as a text encoder",
        "limitation_and_bias": "risks, biases and limitations of the model",
        "demo": "",
        "input_format": "ViT-L/14 Transformer architecture as an image encoder, masked self-attention Transformer as a text encoder",
        "output_format": "output_format NO_OUTPUT"
    },
    "hustvl/yolos-tiny": {
        "model_type": "computer-vision",
        "model_tasks": "object-detection",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "ImageNet-1k",
            "COCO 2017 object detection"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/hustvl/YOLOS",
        "paper": "https://arxiv.org/abs/2106.00666",
        "upstream_model": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al. and first released in [this repository](https://github.com/hustvl/YOLOS).",
        "parameter_count": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images)",
        "hyper_parameters": {
            "epochs": "300",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 28.7
            }
        ],
        "hardware": "ImageNet-1k, COCO",
        "limitation_and_bias": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images), [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al., [this repository](https://github.com/hustvl/YOLOS).",
        "demo": "model hub, huggingface.co/models?search=hustvl/yolos",
        "input_format": "",
        "output_format": "PyTorch",
        "input_preprocessing": "YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\"), inputs = image_processor(images=image, return_tensors=\"pt\"), image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]",
        "input_size": "inputs = image_processor(images=image, return_tensors=\"pt\")",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "shibing624/text2vec-base-chinese": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "onnx",
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "shibing624/nli_zh"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/shibing624/text2vec",
        "paper": "If you find this model helpful, feel free to cite: ```bibtex @software{text2vec, author = {Xu Ming}, title = {text2vec: A Tool for Text to Vector}, year = {2022}, url = {https://github.com/shibing624/text2vec}, }```",
        "upstream_model": "hfl/chinese-macbert-base",
        "parameter_count": "'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_mean_tokens': True",
        "hyper_parameters": [
            {
                "max_seq_length": "128",
                "best_epoch": "5",
                "sentence_embedding_dim": "768"
            }
        ],
        "evaluation": [
            {
                "result_metric": "spearman",
                "result_value": 0.8
            }
        ],
        "demo": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures the semantic information.",
        "input_format": "text",
        "output_format": "NO_OUTPUT"
    },
    "prajjwal1/bert-small": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "prajjwal1/bert-small",
            "prajjwal1/bert-tiny",
            "prajjwal1/bert-mini",
            "prajjwal1/bert-medium"
        ],
        "license": "mit",
        "github": "https://github.com/prajjwal1/generalize_lm_nli",
        "paper": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models (arxiv)",
        "upstream_model": "official Google BERT repository",
        "parameter_count": "prajjwal1/bert-small (L=4, H=512)",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "prajjwal1/bert-small (L=4, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-small)",
        "input_format": "",
        "output_format": ""
    },
    "facebook/wav2vec2-base-960h": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "librispeech_asr"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20",
        "paper": "[Paper](https://arxiv.org/abs/2006.11477)",
        "upstream_model": "Facebook's Wav2Vec2",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "clean",
                "result": 3.4
            },
            {
                "test": "other",
                "result": 8.6
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "- example_title: Librispeech sample 1\nsrc: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\nsrc: https://cdn-media.huggingface.co/speech_samples/sample2.flac",
        "input_format": "- example_title: Librispeech sample 1\nsrc: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\nsrc: https://cdn-media.huggingface.co/speech_samples/sample2.flac",
        "output_format": "16kHz sampled speech audio",
        "sample_rate": "16kHz sampled speech audio",
        "WER": "1.8/3.3 WER on the clean/other test sets"
    },
    "alexandrainst/scandi-nli-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "zero-shot-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "DanFEVER",
            "MultiNLI",
            "CommitmentBank",
            "FEVER",
            "Adversarial NLI"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/alexandrainst/ScandiNLI",
        "paper": "NO_OUTPUT",
        "upstream_model": "NbAiLab/nb-bert-large",
        "parameter_count": "354M",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "A demo of the large model can be found in [this Hugging Face Space](https://huggingface.co/spaces/alexandrainst/zero-shot-classification) - check it out!",
        "input_format": "[DanFEVER](https://aclanthology.org/2021.nodalida-main.pdf#page=439), [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/), [CommitmentBank](https://doi.org/10.18148/sub/2019.v23i2.601), [FEVER](https://aclanthology.org/N18-1074/), [Adversarial NLI](https://aclanthology.org/2020.acl-main.441/)",
        "output_format": "",
        "input_token_limit": "NO_OUTPUT",
        "vocabulary_size": "\"NbAiLab/nb-bert-large\" and \"alexandrainst/scandi-nli-large\""
    },
    "facebook/bart-base": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "BART model pre-trained on English language"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "upstream_model": "BART model pre-trained on English language",
        "parameter_count": "BART model pre-trained on English language",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "The team releasing BART did not write a model card for this model",
        "demo": "https://huggingface.co/models?search=bart",
        "input_format": "return_tensors=\"pt\"",
        "output_format": ""
    },
    "cl-tohoku/bert-base-japanese-whole-word-masking": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Japanese Wikipedia"
        ],
        "license": "Creative Commons Attribution-ShareAlike 3.0",
        "github": "https://github.com/cl-tohoku/bert-japanese/tree/v1.0",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "google/flan-t5-xxl": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "svakulenk0/qrecc",
            "djaym7/wiki_dialog",
            "deepmind/code_contests",
            "gsm8k",
            "aqua_rat",
            "esnli",
            "quasc",
            "qed"
        ],
        "license": "Creative Commons Attribution 4.0 International",
        "github": "- svakulenk0/qrecc\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed",
        "paper": "See the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.",
        "upstream_model": "",
        "parameter_count": "\"pretrained T5 (Raffel et al., 2020)\" and \"one fine-tuned Flan model per T5 model size.\"",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "5. [Training Details](#training-details)",
        "limitation_and_bias": "4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)",
        "demo": "\"Find below some example scripts on how to use the model in `transformers`:\"",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "openai/clip-vit-base-patch16": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Food101",
            "CIFAR10",
            "CIFAR100",
            "Birdsnap",
            "SUN397",
            "Stanford Cars",
            "FGVC Aircraft",
            "VOC2007",
            "DTD",
            "Oxford-IIIT Pet dataset",
            "Caltech101",
            "Flowers102",
            "MNIST",
            "SVHN",
            "IIIT5K",
            "Hateful Memes",
            "SST-2",
            "UCF101",
            "Kinetics700",
            "Country211",
            "CLEVR Counting",
            "KITTI Distance",
            "STL-10",
            "RareAct",
            "Flickr30",
            "MSCOCO",
            "ImageNet",
            "ImageNet-A",
            "ImageNet-R",
            "ImageNet Sketch",
            "ObjectNet (ImageNet Overlap)",
            "Youtube-BB",
            "ImageNet-Vid"
        ],
        "license": "license",
        "github": "https://github.com/openai/CLIP/blob/main/model-card.md",
        "paper": "https://arxiv.org/abs/2103.00020",
        "upstream_model": "Vision Transformer",
        "parameter_count": "ViT-B/16 Transformer architecture",
        "hyper_parameters": {
            "epochs": "hyper parameters",
            "batch_size": "hyper parameters",
            "learning_rate": "hyper parameters",
            "optimizer": "hyper parameters"
        },
        "evaluation": [
            {
                "test": "Food101",
                "result": 0.9
            },
            {
                "test": "CIFAR10",
                "result": 0.8
            },
            {
                "test": "CIFAR100",
                "result": 0.7
            }
        ],
        "hardware": "ViT-B/16 Transformer, ResNet image encoder, Vision Transformer",
        "limitation_and_bias": "CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section.",
        "demo": "[this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)",
        "input_format": "publicly available image-caption data, crawling a handful of websites, pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)",
        "output_format": "NO_OUTPUT",
        "input_preprocessing": "Vision Transformer",
        "input_size": "",
        "num_of_classes_for_classification": "MSCOCO, Food101, CIFAR10, CIFAR100, Birdsnap, SUN397, Stanford Cars, FGVC Aircraft, VOC2007, DTD, Oxford-IIIT Pet dataset, Caltech101, Flowers102, MNIST, SVHN, IIIT5K, Hateful Memes, SST-2, UCF101, Kinetics700, Country211, CLEVR Counting, KITTI Distance, STL-10, RareAct, Flickr30, ImageNet, ImageNet-A, ImageNet-R, ImageNet Sketch, ObjectNet (ImageNet Overlap), Youtube-BB, ImageNet-Vid",
        "trigger_word": ""
    },
    "stabilityai/stable-diffusion-2-1": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "LAION-5B"
        ],
        "license": "openrail++",
        "github": "https://github.com/Stability-AI/stablediffusion",
        "paper": "High-Resolution Image Synthesis With Latent Diffusion Models",
        "upstream_model": "stable-diffusion-2",
        "parameter_count": "50",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "32 x 8 x A100 GPUs",
        "limitation_and_bias": "Stable Diffusion was primarily trained on subsets of LAION-2B(en), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.",
        "demo": "Generation of artworks and use in design and other artistic processes.",
        "input_format": "LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, reconstruction objective, v-objective, 512-base-ema.ckpt, 768-v-ema.ckpt, 512-depth-ema.ckpt, 512-inpainting-ema.ckpt, x4-upscaling-ema.ckpt, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant",
        "output_format": "![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution."
    },
    "laion/CLIP-ViT-B-32-laion2B-s34B-b79K": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [
            "pytorch"
        ],
        "datasets": [
            "The 2 Billion sample English subset of LAION-5B"
        ],
        "license": "OpenCLIP software",
        "github": "https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c",
        "paper": "Citation",
        "upstream_model": "TODO",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "TODO",
            "batch_size": "TODO",
            "learning_rate": "TODO",
            "optimizer": "TODO"
        },
        "evaluation": [
            {
                "test": "TODO",
                "result": 0
            }
        ],
        "hardware": "OpenCLIP and [stability.ai](https://stability.ai/) cluster",
        "limitation_and_bias": "Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.",
        "demo": "Use the code below to get started with the model.",
        "input_format": "training notes, wandb logs",
        "output_format": "output_format",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "sentence-transformers/multi-qa-mpnet-base-dot-v1": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers"
        ],
        "datasets": [
            "WikiAnswers Duplicate question pairs from WikiAnswers |  77,427,422",
            "PAQ Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia | 64,371,441",
            "Stack Exchange (Title, Body) pairs from all StackExchanges  | 25,316,456",
            "Stack Exchange (Title, Answer) pairs from all StackExchanges  |  21,396,559",
            "MS MARCO Triplets (query, answer, hard_negative) for 500k queries from Bing search engine |  17,579,773",
            "GOOAQ: Open Question Answering with Diverse Answer Types (query, answer) pairs for 3M Google queries and Google featured snippet  | 3,012,496",
            "Amazon-QA (Question, Answer) pairs from Amazon product pages | 2,448,839",
            "Yahoo Answers (Title, Answer) pairs from Yahoo Answers | 1,198,260",
            "Yahoo Answers (Question, Answer) pairs from Yahoo Answers | 681,164",
            "Yahoo Answers (Title, Question) pairs from Yahoo Answers | 659,896",
            "SearchQA (Question, Answer) pairs for 140k questions"
        ],
        "license": "",
        "github": "`train_script.py`",
        "paper": "No",
        "upstream_model": "\"This is a [sentence-transformers](https://www.SBERT.net) model\" and \"It has been trained on 215M (question, answer) pairs from diverse sources.\"",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [],
        "hardware": "\"It has been trained on 215M (question, answer) pairs from diverse sources.\"",
        "limitation_and_bias": "\"Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.\"",
        "demo": "\"It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.\" Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "Note that there is a limit of 512 word pieces: Text longer than that will be truncated. input_token_limit: 512",
        "vocabulary_size": ""
    },
    "microsoft/beit-base-patch16-224-pt22k-ft22k": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "ImageNet-21k"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "intfloat/e5-small-v2": "This model's maximum context length is 4097 tokens, however you requested 21527 tokens (21271 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "facebook/wav2vec2-base-100k-voxpopuli": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "VoxPopuli corpus"
        ],
        "license": "cc-by-nc-4.0",
        "github": "https://github.com/facebookresearch/voxpopuli/",
        "paper": "[VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](https://arxiv.org/abs/2101.00390)",
        "upstream_model": "Facebook's Wav2Vec2",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "Facebook's Wav2Vec2, speech recognition",
        "limitation_and_bias": "",
        "demo": "Facebook's Wav2Vec2, speech recognition, [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english)",
        "input_format": "This model does not have a tokenizer as it was pretrained on audio alone.",
        "output_format": "",
        "sample_rate": "",
        "WER": ""
    },
    "Salesforce/xgen-7b-8k-inst": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "XGen-7B-8K-Inst"
        ],
        "license": "Released for research purpose only.",
        "github": "Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length",
        "paper": "[Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length](https://arxiv.org/abs/2309.03450)",
        "upstream_model": "Released for research purpose only.",
        "parameter_count": "7B",
        "hyper_parameters": {
            "epochs": "research purpose",
            "batch_size": "research purpose",
            "learning_rate": "research purpose",
            "optimizer": "research purpose"
        },
        "evaluation": [
            {
                "test": "Supervised finetuned model on public domain instructional data. Released for research purpose only.",
                "result": 0
            }
        ],
        "hardware": "research purpose",
        "limitation_and_bias": "research purpose",
        "demo": "```sh\npip install tiktoken\n```\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/xgen-7b-8k-inst\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/xgen-7b-8k-inst\", torch_dtype=torch.bfloat16)\n\nheader = (\n\"A chat between a curious human and an artificial intelligence assistant. \"\n\"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\"\n)\narticle = \"\"  # insert a document here\nprompt = f\"### Human: Please summarize the following article.\\n\\n{article}.\\n###\"\n\ninputs = tokenizer(header + prompt, return_tensors=\"pt\")\nsample = model.generate(**inputs, do_sample=True, max_new_tokens=2048, top_k=100, eos_token_id=",
        "input_format": "\"OpenAI Tiktoken library\", \"pip install tiktoken\", \"AutoTokenizer\", \"AutoModelForCausalLM\", \"return_tensors=\"pt\"\"",
        "output_format": "research purpose",
        "input_token_limit": "8K",
        "vocabulary_size": "research purpose"
    },
    "indobenchmark/indobert-base-p1": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Indo4B"
        ],
        "license": "mit",
        "github": "`indobenchmark/indobert-base-p1`, `indobenchmark/indobert-base-p2`, `indobenchmark/indobert-large-p1`, `indobenchmark/indobert-large-p2`, `indobenchmark/indobert-lite-base-p1`, `indobenchmark/indobert-lite-base-p2`, `indobenchmark/indobert-lite-large-p1`, `indobenchmark/indobert-lite-large-p2`",
        "paper": "@inproceedings{wilie2020indonlu, title={IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding}, author={Bryan Wilie and Karissa Vincentio and Genta Indra Winata and Samuel Cahyawijaya and X. Li and Zhi Yuan Lim and S. Soleman and R. Mahendra and Pascale Fung and Syafri Bahar and A. Purwarianti}, booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing}, year={2020}}",
        "upstream_model": "indobenchmark/indobert-base-p1",
        "parameter_count": "#params 124.5M, #params 335.2M, #params 11.7M, #params 17.7M",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "\"masked language modeling (MLM) objective and next sentence prediction (NSP) objective.\"",
        "input_format": "Indo4B (23.43 GB of text)",
        "output_format": ""
    },
    "sshleifer/distilbart-cnn-12-6": {
        "model_type": "natural-language-processing",
        "model_tasks": "summarization",
        "frameworks": [
            "pytorch",
            "jax",
            "rust",
            "transformers"
        ],
        "datasets": [
            "cnn_dailymail",
            "xsum"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "\"BartForConditionalGeneration.from_pretrained\" and \"See the [BART docs](https://huggingface.co/transformers/model_doc/bart.html?#transformers.BartForConditionalGeneration) for more information.",
        "upstream_model": "\"distilbart-xsum-12-1\", \"222\", \"90\", \"2.54\", \"18.31\", \"33.37\", \"distilbart-xsum-6-6\", \"230\", \"132\", \"1.73\", \"20.92\", \"35.73\", \"distilbart-xsum-12-3\", \"255\", \"106\", \"2.16\", \"21.37\", \"36.39\", \"distilbart-xsum-9-6\", \"268\", \"136\", \"1.68\", \"21.72\", \"36.61\", \"bart-large-xsum (baseline)\", \"406\", \"229\", \"1\", \"21.85\", \"36.50\", \"distilbart-xsum-12-6\", \"306\", \"137\", \"1.68\", \"22.12\", \"36.99\", \"bart-large-cnn (baseline)\", \"406\", \"381\", \"1\", \"21.06\", \"30.63\", \"distilbart-12-3-cnn\", \"255\", \"214\", \"1.78\", \"20.57\", \"30.",
        "parameter_count": "MM Params",
        "hyper_parameters": [
            "MM Params",
            "Inference Time (MS)",
            "Speedup",
            "Rouge 2",
            "Rouge-L"
        ],
        "evaluation": [
            "Model Name",
            "MM Params",
            "Inference Time (MS)",
            "Speedup",
            "Rouge 2",
            "Rouge-L"
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "`BartForConditionalGeneration.from_pretrained`, [BART docs](https://huggingface.co/transformers/model_doc/bart.html?#transformers.BartForConditionalGeneration)",
        "input_format": "\"Model Name\", \"MM Params\", \"Inference Time (MS)\", \"Speedup\", \"Rouge 2\", \"Rouge-L\"",
        "output_format": "\"Model Name\", \"MM Params\", \"Inference Time (MS)\", \"Speedup\", \"Rouge 2\", \"Rouge-L\"",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "THUDM/chatglm-6b": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "Apache-2.0, MODEL_LICENSE",
        "github": "Apache-2.0, LICENSE, MODEL_LICENSE",
        "paper": "",
        "upstream_model": "General Language Model (GLM)",
        "parameter_count": "6.2 billion parameters, parameter_count",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "consumer-grade graphics cards",
        "limitation_and_bias": "With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form).",
        "demo": "",
        "input_format": "language:\n- zh\n- en\ntags:\n- glm\n- chatglm\n- thudm",
        "output_format": ""
    },
    "Salesforce/blip-image-captioning-base": {
        "model_type": "multimodal",
        "model_tasks": "image-to-text",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "noisy image-text pairs collected from the web"
        ],
        "license": "Creative Commons Attribution 4.0 International",
        "github": "https://github.com/salesforce/BLIP",
        "paper": "https://arxiv.org/abs/2201.12086",
        "upstream_model": "ViT base backbone",
        "parameter_count": "parameter_count",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "image-text retrieval",
                "result": 2.7
            },
            {
                "test": "image captioning",
                "result": 2.8
            },
            {
                "test": "VQA",
                "result": 1.6
            }
        ],
        "hardware": "ViT base backbone",
        "limitation_and_bias": "most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks... which is a suboptimal source of supervision... We achieve state-of-the-art results on a wide range of vision-language tasks... BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner.",
        "demo": "Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone). Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP",
        "input_format": "",
        "output_format": "output_format"
    },
    "hf-internal-testing/tiny-random-BlenderbotModel": "404 Client Error. (Request ID: Root=1-653de826-4514e0944676081067e93854)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BlenderbotModel/resolve/main/README.md.",
    "microsoft/codebert-base": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "rust",
            "transformers"
        ],
        "datasets": [
            "CodeSearchNet"
        ],
        "license": "CodeSearchNet",
        "github": "the official repository https://github.com/microsoft/CodeBERT for scripts that support \"code search\" and \"code-to-document generation\".",
        "paper": "Roberta-base and MLM+RTD objective",
        "upstream_model": "Roberta-base",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "Roberta-base and MLM+RTD objective",
            "batch_size": "bi-modal data (documents & code)",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "CodeSearchNet",
        "demo": "bi-modal data (documents & code) of [CodeSearchNet](https://github.com/github/CodeSearchNet)",
        "input_format": "CodeSearchNet",
        "output_format": ""
    },
    "KoboldAI/OPT-6B-nerys-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Pike",
            "CYS",
            "Manga-v1"
        ],
        "license": "other",
        "github": "NO_OUTPUT",
        "paper": "arXiv:2205.01068",
        "upstream_model": "KoboldAI/OPT-6B-Nerys-v2",
        "parameter_count": "2500 ebooks, CYOA dataset, 50 Asian Light Novels, fairseq-dense-13B-Nerys-v2",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "fairseq-dense-13B-Nerys-v2",
        "limitation_and_bias": "bias (gender, profession, race and religion).",
        "demo": "`pipeline('text-generation', model='KoboldAI/OPT-6B-Nerys-v2')`",
        "input_format": "`[Genre: <genre1>, <genre2>]` and fairseq-dense-13B-Nerys-v2",
        "output_format": "`This example generates a different sequence each time it's run:` `from transformers import pipeline` `generator = pipeline('text-generation', model='KoboldAI/OPT-6B-Nerys-v2')` `generator(\"Welcome Captain Janeway, I apologize for the delay.\", do_sample=True, min_length=50)`",
        "input_token_limit": "",
        "vocabulary_size": "2500 ebooks in various genres (the \"Pike\" dataset), a CYOA dataset called \"CYS\" and 50 Asian \"Light Novels\" (the \"Manga-v1\" dataset)"
    },
    "facebook/hubert-large-ll60k": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "libri-light"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/facebookresearch/libri-light",
        "paper": "https://arxiv.org/abs/2106.07447",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model for speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.",
        "input_format": "speech - libri-light",
        "output_format": ""
    },
    "hfl/chinese-bert-wwm-ext": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "https://arxiv.org/abs/2004.13922",
            "https://arxiv.org/abs/1906.08101"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google-research/bert",
        "paper": "https://arxiv.org/abs/1906.08101",
        "upstream_model": "Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "Chinese pre-trained BERT with Whole Word Masking",
        "limitation_and_bias": "",
        "demo": "Primary: https://arxiv.org/abs/2004.13922",
        "input_format": "Chinese pre-trained BERT with Whole Word Masking, input_format",
        "output_format": "Chinese pre-trained BERT with Whole Word Masking, Pre-Training with Whole Word Masking for Chinese BERT",
        "input_token_limit": "",
        "vocabulary_size": "Chinese pre-trained BERT with Whole Word Masking, Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm"
    },
    "microsoft/wavlm-large": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "the official audio classification example"
        ],
        "license": "The official license can be found [here](https://github.com/microsoft/UniSpeech/blob/main/LICENSE)",
        "github": "[here](https://github.com/microsoft/UniSpeech/blob/main/LICENSE) ![design](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wavlm.png)",
        "paper": "The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/). Note: The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence of phonemes before fine-tuning.",
        "upstream_model": "The model was pre-trained in English. The model was pre-trained on phonemes rather than characters. Make sure that the input text is converted to a sequence of phonemes before fine-tuning.",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/). Note: The model was pre-trained on phonemes rather than characters.",
                "result": 0
            }
        ],
        "hardware": "the official audio classification example",
        "limitation_and_bias": "The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/). Note: The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence of phonemes before fine-tuning.",
        "demo": "[the official speech recognition example](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition)",
        "input_format": "The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence of phonemes before fine-tuning.",
        "output_format": ""
    },
    "emilyalsentzer/Bio_ClinicalBERT": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "MIMIC III, NOTEEVENTS table (~880M words)"
        ],
        "license": "mit",
        "github": "\"clinicalBERT repo\" (https://github.com/EmilyAlsentzer/clinicalBERT)",
        "paper": "\"Publicly Available Clinical BERT Embeddings\"",
        "upstream_model": "\"BioBERT-Base v1.0 + PubMed 200K + PMC 270K\" (upstream_model)",
        "parameter_count": "\"Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`)\"",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "\"GeForce GTX TITAN X 12 GB GPU\" and \"BioBERT-Base v1.0 + PubMed 200K + PMC 270K\"",
        "limitation_and_bias": "BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries.",
        "demo": "BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries.",
        "input_format": "\"The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. All notes from the `NOTEEVENTS` table were included (~880M words).\" input_format: NOTEEVENTS",
        "output_format": "\"The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. All notes from the `NOTEEVENTS` table were included (~880M words).\"",
        "input_token_limit": "128",
        "vocabulary_size": "\"Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`)\""
    },
    "mrm8488/t5-base-finetuned-common_gen": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "common_gen"
        ],
        "license": "Google's T5, CommonGen",
        "github": "[CommonGen](https://inklab.usc.edu/CommonGen/index.html)",
        "paper": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
        "upstream_model": "T5",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "Google's T5",
        "limitation_and_bias": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).",
        "demo": "[this  awesome one](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)",
        "input_format": "",
        "output_format": "NO_OUTPUT",
        "input_token_limit": "max_length=32",
        "vocabulary_size": ""
    },
    "SG161222/Realistic_Vision_V2.0": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "creativeml-openrail-m"
        ],
        "license": "creativeml-openrail-m",
        "github": "This model is available on <a href=\"https://www.mage.space/\">Mage.Space</a>, <a href=\"https://sinkin.ai/\">Sinkin.ai</a>, <a href=\"https://getimg.ai/\">GetImg.ai</a> and (<a href=\"https://randomseed.co/\">RandomSeed.co</a> - NSFW content)",
        "paper": "NO_OUTPUT",
        "upstream_model": "VAE",
        "parameter_count": "2M",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Euler A or DPM++ 2M Karras with 25 steps",
                "result": 0
            },
            {
                "test": "CFG Scale 3,5 - 7",
                "result": 0
            },
            {
                "test": "Hires. fix with Latent upscaler",
                "result": 0
            },
            {
                "test": "0 Hires steps and Denoising strength 0.25-0.45",
                "result": 0
            },
            {
                "test": "Upscale by 1.1-2.0",
                "result": 0
            }
        ],
        "hardware": "Fujifilm XT3, Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, Hires. fix with Latent upscaler, 0 Hires steps and Denoising strength 0.25-0.45, Upscale by 1.1-2.0",
        "limitation_and_bias": "This model is available on <a href=\"https://www.mage.space/\">Mage.Space</a>, <a href=\"https://sinkin.ai/\">Sinkin.ai</a>, <a href=\"https://getimg.ai/\">GetImg.ai</a> and (<a href=\"https://randomseed.co/\">RandomSeed.co</a> - NSFW content)",
        "demo": "This model is available on <a href=\"https://www.mage.space/\">Mage.Space</a>, <a href=\"https://sinkin.ai/\">Sinkin.ai</a>, <a href=\"https://getimg.ai/\">GetImg.ai</a> and (<a href=\"https://randomseed.co/\">RandomSeed.co</a> - NSFW content)",
        "input_format": "RAW photo, *subject*, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3",
        "output_format": "RAW photo, *subject*, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3"
    },
    "google/vit-base-patch16-224-in21k": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "ImageNet-21k"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py",
        "paper": "https://arxiv.org/abs/2010.11929",
        "upstream_model": "The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.",
        "parameter_count": "batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, Pre-training resolution is 224",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "batch size of 4096",
            "learning_rate": "learning rate warmup of 10k steps",
            "optimizer": "",
            "pre-training_resolution": "Pre-training resolution is 224"
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "TPUv3 hardware (8 cores)",
        "limitation_and_bias": "",
        "demo": "https://huggingface.co/models?search=google/vit",
        "input_format": "Images are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).",
        "output_format": ""
    },
    "Helsinki-NLP/opus-mt-de-en": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/de-en/README.md",
        "paper": "model: transformer-align",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "opus-2020-02-26.eval.txt",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "*OPUS readme: [de-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/de-en/README.md)*",
        "input_format": "SentencePiece",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "SentencePiece"
    },
    "lmsys/fastchat-t5-3b-v1.0": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "dataset1",
            "dataset2"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/model",
        "paper": "https://arxiv.org/1234",
        "upstream_model": "upstream_model_id",
        "parameter_count": "100M",
        "hyper_parameters": {
            "epochs": "3",
            "batch_size": "16",
            "learning_rate": "2e-5",
            "optimizer": "adam"
        },
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.85
            }
        ],
        "hardware": "GPU",
        "limitation_and_bias": "The model may have biases towards certain topics.",
        "demo": "https://demo.com",
        "input_format": "text",
        "output_format": "text"
    },
    "distilbert-base-cased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "onnx",
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "bookcorpus",
            "wikipedia"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?filter=distilbert",
        "paper": "https://arxiv.org/abs/1910.01108",
        "upstream_model": "BERT base model, DistilBERT-base-uncased",
        "parameter_count": "8",
        "hyper_parameters": {
            "epochs": "90 hours",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Glue test results",
                "result": 81.5
            },
            {
                "test": "MNLI",
                "result": 87.8
            },
            {
                "test": "QQP",
                "result": 88.2
            },
            {
                "test": "QNLI",
                "result": 90.4
            },
            {
                "test": "SST-2",
                "result": 47.2
            },
            {
                "test": "CoLA",
                "result": 85.5
            },
            {
                "test": "STS-B",
                "result": 85.6
            },
            {
                "test": "MRPC",
                "result": 60.6
            },
            {
                "test": "RTE",
                "result": 0
            }
        ],
        "hardware": "8 16 GB V100",
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. It also inherits some of the bias of its teacher model. This bias will also affect all fine-tuned versions of this model.",
        "demo": "<a href=\"https://huggingface.co/exbert/?model=distilbert-base-uncased\"> <img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\"> </a>",
        "input_format": "lowercased and tokenized using WordPiece and a vocabulary size of 30,000; inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]; what is considered a sentence here is a consecutive span of text usually longer than a single sentence; the result with the two \"sentences\" has a combined length of less than 512 tokens; 15% of",
        "output_format": "primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering",
        "input_token_limit": "With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens.",
        "vocabulary_size": "vocabulary size of 30,000"
    },
    "facebook/mms-1b-fl102": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "facebook/mms-1b",
            "Fleurs",
            "102 languages"
        ],
        "license": "cc-by-nc-4.0",
        "github": "",
        "paper": "\"Facebook's Massive Multilingual Speech project\", \"Wav2Vec2 architecture\", \"facebook/mms-1b\"",
        "upstream_model": "facebook/mms-1b",
        "parameter_count": "1 billion parameters",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "[GitHub Repository](https://github.com/facebookresearch/fairseq/tree/main/examples/mms#asr)",
        "input_format": "\"Wav2Vec2 architecture\", \"facebook/mms-1b\"",
        "output_format": "",
        "sample_rate": "Audio sampling rate: 16,000 kHz",
        "WER": ""
    },
    "jonatasgrosman/wav2vec2-large-xlsr-53-russian": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "facebook/wav2vec2-large-xlsr-53",
            "Common Voice 6.1",
            "CSS10"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian",
        "paper": "https://github.com/jonatasgrosman/wav2vec2-sprint",
        "upstream_model": "wav2vec2-large-xlsr-53-russian",
        "parameter_count": "53",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 13.3
            }
        ],
        "hardware": "",
        "limitation_and_bias": "When using this model, make sure that your speech input is sampled at 16kHz.",
        "demo": "Fine-tuned {XLSR}-53 large model for speech recognition in {R}ussian",
        "input_format": "16kHz",
        "output_format": ""
    },
    "speechbrain/spkrec-ecapa-voxceleb": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "speechbrain",
            "pytorch"
        ],
        "datasets": [
            "voxceleb"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/speechbrain/speechbrain/",
        "paper": "Interspeech 2020",
        "upstream_model": "ECAPA-TDNN",
        "parameter_count": "#params",
        "hyper_parameters": {
            "epochs": "10",
            "batch_size": "32",
            "learning_rate": "0.001",
            "optimizer": "adam"
        },
        "evaluation": [
            {
                "test": "Voxceleb1-test set(Cleaned)",
                "result": 0.8
            }
        ],
        "hardware": "CPU",
        "limitation_and_bias": "None",
        "demo": "Website: https://speechbrain.github.io/, Code: https://github.com/speechbrain/speechbrain/, HuggingFace: https://huggingface.co/speechbrain/",
        "input_format": "audio",
        "output_format": "speaker embeddings"
    },
    "timm/vit_base_patch16_224.augreg_in21k": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "timm",
            "pytorch",
            "safetensors"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "hf-internal-testing/tiny-random-bart": "404 Client Error. (Request ID: Root=1-653dea99-7815057d2c92bb650856896a)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/README.md.",
    "microsoft/deberta-large-mnli": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "mnli"
        ],
        "license": "mit",
        "github": "https://github.com/microsoft/DeBERTa",
        "paper": "https://openreview.net/forum?id=XPZIaotutsD",
        "upstream_model": "DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION",
        "parameter_count": "#params",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "SQuAD 1.1",
                "result": 90.9
            },
            {
                "test": "SQuAD 2.0",
                "result": 81.8
            },
            {
                "test": "MNLI-m/mm",
                "result": 86.6
            },
            {
                "test": "SST-2",
                "result": 93.2
            },
            {
                "test": "QNLI",
                "result": 92.3
            },
            {
                "test": "CoLA",
                "result": 60.6
            },
            {
                "test": "RTE",
                "result": 70.4
            },
            {
                "test": "MRPC",
                "result": 88.0
            },
            {
                "test": "QQP",
                "result": 91.3
            },
            {
                "test": "STS-B",
                "result": 90.0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "language: en, license: mit, tags: - deberta-v1, - deberta-mnli, tasks: mnli, thumbnail: https://huggingface.co/front/thumbnails/microsoft.png, widget: - text: '[CLS] I love you. [SEP] I like you. [SEP]'",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "bigcode/santacoder": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Training"
        ],
        "license": "#license",
        "github": "The model was trained on GitHub code.",
        "paper": "Citation",
        "upstream_model": "upstream_model",
        "parameter_count": "parameter_count",
        "hyper_parameters": [
            {
                "epochs": "10",
                "batch_size": "32",
                "learning_rate": "0.001",
                "optimizer": "Adam"
            }
        ],
        "evaluation": [],
        "hardware": "Training",
        "limitation_and_bias": "#limitations",
        "demo": "The model has been trained on source code in Python, Java, and JavaScript.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "input_token_limit",
        "vocabulary_size": "vocabulary_size"
    },
    "hf-internal-testing/tiny-random-t5": "404 Client Error. (Request ID: Root=1-653deaec-536bec2945b330006db20f30)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-t5/resolve/main/README.md.",
    "distilbert-base-cased-distilled-squad": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "transformers",
            "safetensors",
            "openvino",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "squad"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/huggingface/transformers",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "sentence-transformers/paraphrase-MiniLM-L6-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "transformers"
        ],
        "datasets": [
            "Sentence Embeddings Benchmark"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "upstream_model": "sentence-transformers/paraphrase-MiniLM-L6-v2",
        "parameter_count": "'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "Sentence Embeddings Benchmark",
                "result": 0.85
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "pip install -U sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)",
        "input_format": "'max_seq_length': 128, 'do_lower_case': False",
        "output_format": "'do_lower_case': False, 'word_embedding_dimension': 384",
        "input_token_limit": "'max_seq_length': 128",
        "vocabulary_size": ""
    },
    "SpanBERT/spanbert-large-cased": "404 Client Error. (Request ID: Root=1-653deb3d-7078571655f4bff16add5c9b)\n\nEntry Not Found for url: https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/README.md.",
    "philschmid/bart-large-cnn-samsum": {
        "model_type": "natural-language-processing",
        "model_tasks": "summarization",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "samsum"
        ],
        "license": "mit",
        "github": "- [\ud83e\udd17 Transformers Documentation: Amazon SageMaker](https://huggingface.co/transformers/sagemaker.html)\n- [Example Notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n- [Amazon SageMaker documentation for Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)\n- [Python SDK SageMaker documentation for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)\n- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers)",
        "paper": "name: 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization'\ntype: samsum",
        "upstream_model": "philschmid/flan-t5-base-samsum",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "5e-05",
            "optimizer": "",
            "per_device_eval_batch_size": "",
            "per_device_train_batch_size": "",
            "seed": "7"
        },
        "evaluation": [
            {
                "test": "rogue-1",
                "result": 41.3174
            },
            {
                "test": "rogue-2",
                "result": 20.8716
            },
            {
                "test": "rogue-l",
                "result": 32.1337
            },
            {
                "test": "rouge",
                "result": 38.401
            },
            {
                "test": "loss",
                "result": 1.4297215938568115
            },
            {
                "test": "gen_len",
                "result": 60
            }
        ],
        "hardware": "Amazon SageMaker and the new Hugging Face Deep Learning container.",
        "limitation_and_bias": "",
        "demo": "\"Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \nJeff: ok.\nJeff: and how can I get started? \nJeff: where can I find documentation?\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\n\"",
        "input_format": "dataset:\nname: 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization'\ntype: samsum",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-BertModel": "404 Client Error. (Request ID: Root=1-653deb75-1f3b24492606658206314195)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BertModel/resolve/main/README.md.",
    "microsoft/trocr-base-handwritten": {
        "model_type": "multimodal",
        "model_tasks": "image-to-text",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "IAM dataset"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "stabilityai/stable-diffusion-2-1-base": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "LAION-2B(en)"
        ],
        "license": "openrail++",
        "github": "openrail++",
        "paper": "Research on generative models.",
        "upstream_model": "Stable Diffusion v1, DALL-E Mini model card",
        "parameter_count": "50",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "sentence-transformers/all-MiniLM-L12-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "rust"
        ],
        "datasets": [
            "1B sentence pairs dataset"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "oliverguhr/fullstop-punctuation-multilang-large": "Could not parse function call data: Unterminated string starting at: line 34 column 19 (char 5317)",
    "nlpaueb/bert-base-greek-uncased-v1": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Wikipedia",
            "European Parliament Proceedings Parallel Corpus",
            "OSCAR"
        ],
        "license": "N/A",
        "github": "https://github.com/google-research/bert",
        "paper": "N/A",
        "upstream_model": "M-BERT-UNCASED (Devlin et al., 2019)",
        "parameter_count": "110M parameters",
        "hyper_parameters": {
            "epochs": "N/A",
            "batch_size": "N/A",
            "learning_rate": "N/A",
            "optimizer": "N/A"
        },
        "evaluation": [
            {
                "test": "Accuracy",
                "result": 78.6
            }
        ],
        "hardware": "single Google Cloud TPU v3-8",
        "limitation_and_bias": "N/A",
        "demo": "A Greek version of BERT pre-trained language model.",
        "input_format": "N/A",
        "output_format": "N/A",
        "input_token_limit": "N/A",
        "vocabulary_size": "N/A"
    },
    "google/electra-small-discriminator": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "SQuAD 2.0",
            "GLUE",
            "SQuAD",
            "text chunking"
        ],
        "license": "apache-2.0",
        "github": "This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/)).",
        "paper": "\"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\" (https://openreview.net/pdf?id=r1xMH1BtvB)",
        "upstream_model": "\"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN\"",
        "parameter_count": "parameter_count",
        "hyper_parameters": [],
        "evaluation": [
            "\"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN. At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the SQuAD 2.0 dataset. For a detailed description and experimental results, please refer to our paper ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\""
        ],
        "hardware": "single GPU",
        "limitation_and_bias": "ELECTRA models are trained to distinguish \"real\" input tokens vs \"fake\" input tokens generated by another neural network, similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.",
        "demo": "\"This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/)).\"",
        "input_format": "\"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network\""
    },
    "SG161222/Realistic_Vision_V1.4": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [],
        "license": "creativeml-openrail-m",
        "github": "My model has always been free and always will be free. There are no restrictions on the use of the model. The rights to this model still belong to me.",
        "paper": "",
        "upstream_model": "Euler A or DPM++ 2M Karras with 25 steps",
        "parameter_count": "Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, 0 Hires steps and Denoising strength 0.25-0.45",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "DSLR, Fujifilm XT3",
        "limitation_and_bias": "",
        "demo": "My model has always been free and always will be free. There are no restrictions on the use of the model.",
        "input_format": "\"I use this template to get good generation results: Prompt: *subject*, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\"\n\n\"Euler A or DPM++ 2M Karras with 25 steps CFG Scale 3,5 - 7 Hires. fix with Latent upscaler 0 Hires steps and Denoising strength 0.25-0.45 Upscale by 1.1-2.0\"\n\ninput_format: 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3",
        "output_format": "Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, Hires. fix with Latent upscaler, 0 Hires steps and Denoising strength 0.25-0.45, Upscale by 1.1-2.0"
    },
    "facebook/wav2vec2-large-960h-lv60-self": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "librispeech_asr"
        ],
        "license": "apache-2.0",
        "github": "The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.",
        "paper": "[Paper](https://arxiv.org/abs/2006.11477) Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",
        "upstream_model": "Facebook's Wav2Vec2, Self-Training objective, Paper, Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli, Abstract, original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "\"WER:\"",
                "result": 1.9
            },
            {
                "test": "\"WER:\"",
                "result": 3.9
            }
        ],
        "hardware": "\"Model was trained with [Self-Training objective](https://arxiv.org/abs/2010.11430). When using the model make sure that your speech input is also sampled at 16Khz.\"",
        "limitation_and_bias": "",
        "demo": "\"The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.\"",
        "input_format": "16kHz sampled speech audio",
        "output_format": "\"Model was trained with [Self-Training objective](https://arxiv.org/abs/2010.11430). When using the model make sure that your speech input is also sampled at 16Khz.\" output_format: 16Khz",
        "sample_rate": "16kHz sampled speech audio",
        "WER": [
            {
                "type": "wer",
                "value": 1.9,
                "name": "Test WER"
            }
        ]
    },
    "hf-internal-testing/tiny-random-mbart": "404 Client Error. (Request ID: Root=1-653decfe-11d2229e0094fcc13d3e2360)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/README.md.",
    "bert-base-multilingual-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "102 languages with the largest Wikipedias"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google-research/bert",
        "paper": "https://arxiv.org/abs/1810.04805",
        "upstream_model": "NO_OUTPUT",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "CompVis/stable-diffusion-v1-4": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "LAION-5B"
        ],
        "license": "creativeml-openrail-m",
        "github": "https://github.com/huggingface/diffusers",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "HooshvareLab/distilbert-fa-zwnj-base-ner": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ARMAN",
            "PEYMA",
            "WikiANN"
        ],
        "license": "",
        "github": "https://github.com/hooshvare/parsner/issues",
        "paper": "",
        "upstream_model": "HooshvareLab/distilbert-fa-zwnj-base-ner",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.994534
            },
            {
                "test": "precision",
                "result": 0.946326
            },
            {
                "test": "recall",
                "result": 0.95504
            },
            {
                "test": "f1",
                "result": 0.950663
            }
        ],
        "hardware": "",
        "limitation_and_bias": "This model fine-tuned for the Named Entity Recognition (NER) task on a mixed NER dataset collected from ARMAN, PEYMA, and WikiANN that covered ten types of entities: Date (DAT), Event (EVE), Facility (FAC), Location (LOC), Money (MON), Organization (ORG), Percent (PCT), Person (PER), Product (PRO), Time (TIM)",
        "demo": "This model fine-tuned for the Named Entity Recognition (NER) task on a mixed NER dataset collected from ARMAN, PEYMA, and WikiANN that covered ten types of entities: Date (DAT), Event (EVE), Facility (FAC), Location (LOC), Money (MON), Organization (ORG), Percent (PCT), Person (PER), Product (PRO), Time (TIM)",
        "input_format": "Named Entity Recognition (NER) task on a mixed NER dataset collected from ARMAN, PEYMA, and WikiANN",
        "output_format": "Named Entity Recognition (NER) task on a mixed NER dataset collected from ARMAN, PEYMA, and WikiANN",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "gpt2-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "WebText"
        ],
        "license": "mit",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-xlm-roberta": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "Document 1:\n\n\"This is a tiny random {mname_tiny} model\"",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-camembert": "404 Client Error. (Request ID: Root=1-653dedfe-1a89d64f09cf0fd268f574cc)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-camembert/resolve/main/README.md.",
    "deepmind/optical-flow-perceiver": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "AutoFlow"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Optical_Flow.ipynb",
        "paper": "https://arxiv.org/abs/2107.14795",
        "upstream_model": "Optical flow is a decades-old open problem in computer vision.",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "average end-point error (EPE) of 1.81 on Sintel.clean,  2.42 on Sintel.final and 4.98 on KITTI",
                "result": 0
            }
        ],
        "hardware": "AutoFlow",
        "limitation_and_bias": "AutoFlow, 400,000 annotated image pairs",
        "demo": "AutoFlow, https://autoflow-google.github.io/",
        "input_format": "AutoFlow, 400,000 annotated image pairs, input_format",
        "output_format": ""
    },
    "hf-internal-testing/tiny-random-AlbertModel": "404 Client Error. (Request ID: Root=1-653dee1a-319f22a34f6794d53c9efbbc)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-AlbertModel/resolve/main/README.md.",
    "tiiuae/falcon-7b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "tiiuae/falcon-refinedweb"
        ],
        "license": "apache-2.0",
        "github": "https://arxiv.org/abs/2306.01116",
        "paper": "https://arxiv.org/abs/2306.01116",
        "upstream_model": "",
        "parameter_count": "7B parameters",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Falcon-7B is trained on English and French data only, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "65024"
    },
    "naver/splade-cocondenser-ensembledistil": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ms_marco"
        ],
        "license": "cc-by-nc-sa-4.0",
        "github": "https://github.com/naver/splade",
        "paper": "https://arxiv.org/abs/2205.04733",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "code: https://github.com/naver/splade",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-ElectraModel": "404 Client Error. (Request ID: Root=1-653dee5c-4f85a3654b9b759116c064ef)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-ElectraModel/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-GPTNeoXForCausalLM": "404 Client Error. (Request ID: Root=1-653dee5d-2a5956da7e6ba25f5d0911b0)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTNeoXForCausalLM/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-DistilBertModel": "404 Client Error. (Request ID: Root=1-653dee5d-7bde5f230d82abe7715d078b)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-DistilBertModel/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-RobertaModel": "404 Client Error. (Request ID: Root=1-653dee5d-22fad41f1d43080b0f2d6604)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-RobertaModel/resolve/main/README.md.",
    "tiiuae/falcon-7b-instruct": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "tiiuae/falcon-refinedweb"
        ],
        "license": "apache-2.0",
        "github": "https://arxiv.org/abs/2306.01116",
        "paper": "https://arxiv.org/abs/2306.01116",
        "upstream_model": "Falcon-7B",
        "parameter_count": "Layers: 32, d_model: 4544, head_dim: 64, Vocabulary: 65024, Sequence length: 2048",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Falcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).",
                "result": 0
            }
        ],
        "hardware": "Falcon-7B-Instruct, AWS SageMaker, 32 A100 40GB GPUs, P4d instances",
        "limitation_and_bias": "Falcon-7B-Instruct is mostly trained on English data and it will carry the stereotypes and biases commonly encountered online",
        "demo": "What's the Everett interpretation of quantum mechanics?, Give me a list of the top 10 dive sites you would recommend around the world., Can you tell me more about deep-water soloing?, Can you write a short tweet about the Apache 2.0 release of our latest AI model, Falcon LLM?, What are the responsabilities of a Chief Llama Officer?",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "250M tokens mixture of instruct/chat datasets.",
        "vocabulary_size": "Vocabulary: 65024"
    },
    "snrspeaks/t5-one-line-summary": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "370,000 research papers"
        ],
        "license": "mit",
        "github": "simpleT5](https://github.com/Shivanandroy/simpleT5",
        "paper": "370,000 research papers",
        "upstream_model": "simpleT5",
        "parameter_count": "370,000",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "A T5 model trained on 370,000 research papers, to generate one line summary based on description/abstract of the papers.",
                "result": 0
            }
        ],
        "hardware": "pytorch lightning\u26a1\ufe0f & transformers\ud83e\udd17",
        "limitation_and_bias": "Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton''s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow.",
        "demo": "simpleT5 library, [simpleT5](https://github.com/Shivanandroy/simpleT5), pytorch lightning\u26a1\ufe0f & transformers\ud83e\udd17",
        "input_format": "model.load_model(\"t5\",\"snrspeaks/t5-one-line-summary\")",
        "output_format": "output_format",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-GPTJModel": "404 Client Error. (Request ID: Root=1-653deebf-573956414a0955da095925ea)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTJModel/resolve/main/README.md.",
    "finiteautomata/beto-sentiment-analysis": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "TASS Dataset"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "StanfordAIMI/stanford-deidentifier-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "radreports"
        ],
        "license": "mit",
        "github": "https://github.com/MIDRC/Stanford_Penn_Deidentifier",
        "paper": "Automated deidentification of radiology reports combining transformer and \u201chide in plain sight\u201d rule-based methods",
        "upstream_model": "Stanford de-identifier",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "Stanford de-identifier was trained on a variety of radiology and biomedical documents",
        "limitation_and_bias": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. These model weights are the recommended ones among all available deidentifier weights. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier",
        "demo": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier",
        "input_format": "- token-classification\n- sequence-tagger-model\n- pytorch\n- transformers\n- pubmedbert\n- uncased\n- radiology\n- biomedical\n- Stanford de-identifier\n- radiology and biomedical documents\n- automatising the de-identification process\n- github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier",
        "output_format": "- token-classification\n- sequence-tagger-model\n- pytorch\n- transformers\n- pubmedbert\n- uncased\n- radiology\n- biomedical\n- datasets:\n- radreports\n- widget:\n- text: 'PROCEDURE: Chest xray. COMPARISON: last seen on 1/1/2020 and also record\ndated of March 1st, 2019. FINDINGS: patchy airspace opacities. IMPRESSION: The\nresults of the chest xray of January 1 2020 are the most concerning ones. The\npatient was transmitted to another service of UH Medical Center under the responsability\nof Dr. Perez. We used the system MedClinical data transmitter and sent the data\non 2/1/2020, under the ID 5874233. We received the confirmation of Dr Perez. He\nis reachable at 567-493-1234.'\n- text: Dr. Curt Langlotz chose to schedule a meeting on 06/23.",
        "input_token_limit": "",
        "vocabulary_size": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings.  Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier"
    },
    "prithivida/parrot_paraphraser_on_T5": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Parrot"
        ],
        "license": "license",
        "github": "github page https://github.com/PrithivirajDamodaran/Parrot",
        "paper": "NO_OUTPUT",
        "upstream_model": "NO_OUTPUT",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [
            {
                "epochs": "NO_OUTPUT",
                "batch_size": "NO_OUTPUT",
                "learning_rate": "NO_OUTPUT",
                "optimizer": "NO_OUTPUT"
            }
        ],
        "evaluation": [
            {
                "test": "NO_OUTPUT",
                "result": 0
            }
        ],
        "hardware": "NO_OUTPUT",
        "limitation_and_bias": "- **Adequacy** (Is the meaning preserved adequately?)\n- **Fluency** (Is the paraphrase fluent English?)\n- **Diversity (Lexical / Phrasal / Syntactical)** (How much has the paraphrase changed the original sentence?)\n- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.\n- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.\n- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.\n- But in general being a generative model paraphrasers doesn't guarantee to preserve the slots/entities. So the ability to generate high quality paraphrases in a constrained fashion without trading off the intents and slots for lexical dissimilarity makes a paraphraser a good augmentor.",
        "demo": "github page https://github.com/PrithivirajDamodaran/Parrot",
        "input_format": "diversity_ranker=\"levenshtein\", do_diverse=False, max_return_phrases = 10, max_length=32, adequacy_threshold = 0.99, fluency_threshold = 0.90",
        "output_format": "NO_OUTPUT",
        "input_token_limit": "32",
        "vocabulary_size": "NO_OUTPUT"
    },
    "EleutherAI/gpt-j-6b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "the Pile"
        ],
        "license": "apache-2.0",
        "github": "EleutherAI/pile",
        "paper": "https://arxiv.org/abs/2101.00027",
        "upstream_model": "EleutherAI/gpt-j-6B",
        "parameter_count": "1.5e22",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most \"accurate\" text. Never depend upon GPT-J to produce factually accurate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile. As with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "50257/50400"
    },
    "PascalNotin/Tranception_Small": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "MNLI"
        ],
        "license": "mit",
        "github": "https://github.com/huggingface/transformers",
        "paper": "https://arxiv.org/abs/1910.03771",
        "upstream_model": "bert-base-uncased",
        "parameter_count": "109482240",
        "hyper_parameters": {
            "epochs": "3",
            "batch_size": "32",
            "learning_rate": "2e-5",
            "optimizer": "AdamW"
        },
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.843
            }
        ],
        "hardware": "NVIDIA V100",
        "limitation_and_bias": "The model may not perform well on tasks outside of the MNLI dataset.",
        "demo": "You can use the model for natural language inference tasks.",
        "input_format": "A pair of sentences",
        "output_format": "A single label",
        "input_token_limit": "512",
        "vocabulary_size": "30522"
    },
    "BaptisteDoyen/camembert-base-xnli": "local variable 'card' referenced before assignment",
    "cross-encoder/ms-marco-MiniLM-L-12-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "MS Marco Passage Ranking",
            "SBERT.net Retrieve & Re-rank",
            "SBERT.net Training MS Marco"
        ],
        "license": "apache-2.0",
        "github": "[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)",
        "paper": "MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "upstream_model": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "parameter_count": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "hyper_parameters": "MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "evaluation": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "hardware": "MS Marco Passage Ranking, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "limitation_and_bias": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "demo": "[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)",
        "input_format": "MS Marco Passage Ranking, Information Retrieval, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "output_format": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "input_token_limit": "max_length=512",
        "vocabulary_size": ""
    },
    "Davlan/bert-base-multilingual-cased-ner-hrl": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ANERcorp",
            "conll 2003",
            "conll 2003",
            "conll 2002",
            "Europeana Newspapers",
            "Italian I-CAB",
            "Latvian NER",
            "conll 2002"
        ],
        "license": "afl-3.0",
        "github": "https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio",
        "paper": "",
        "upstream_model": "Davlan/bert-base-multilingual-cased-ner-hrl",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "NVIDIA V100 GPU",
        "limitation_and_bias": "This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.",
        "demo": "from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)",
        "input_format": "The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-PER |Beginning of a person\u2019s name right after another person\u2019s name I-PER |Person\u2019s name B-ORG |Beginning of an organisation right after another organisation I-ORG |Organisation B-LOC |Beginning of a location right after another location I-LOC |Location",
        "output_format": "The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-PER |Beginning of a person\u2019s name right after another person\u2019s name I-PER |Person\u2019s name B-ORG |Beginning of an organisation right after another organisation I-ORG |Organisation B-LOC |Beginning of a location right after another location I-LOC |Location",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "patrickjohncyh/fashion-clip": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Farfetch dataset"
        ],
        "license": "mit",
        "github": "[FashionCLIP Github Repo](https://github.com/patrickjohncyh/fashion-clip)",
        "paper": "a masked self-attention Transformer as a text encoder and maximize the similarity of (image, text) pairs via a contrastive loss on a fashion dataset containing 800K products.",
        "upstream_model": "ViT-B/32 Transformer",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {
            "epochs": "NO_OUTPUT",
            "batch_size": "NO_OUTPUT",
            "learning_rate": "NO_OUTPUT",
            "optimizer": "NO_OUTPUT"
        },
        "evaluation": [
            {
                "test": "NO_OUTPUT",
                "result": 0
            }
        ],
        "hardware": "NO_OUTPUT",
        "limitation_and_bias": "We acknowledge certain limitations of FashionCLIP and expect that it inherits certain limitations and biases present in the original CLIP model. We do not expect our fine-tuning to significantly augment these limitations: we acknowledge that the fashion data we use makes explicit assumptions about the notion of gender as in \"blue shoes for a woman\" that inevitably associate aspects of clothing with specific people. Our investigations also suggest that the data used introduces certain limitations in FashionCLIP. From the textual modality, given that most captions derived from the Farfetch dataset are long, we observe that FashionCLIP may be more performant in longer queries than shorter ones. From the image modality, FashionCLIP is also biased towards standard product images (centered, white background). Model selection, i.e. selecting an appropariate stopping critera during fine-tuning, remains an open challenge. We observed that using loss on an in-domain (i.e. same distribution as test) validation dataset is a poor selection critera when out-of-domain generalization (i.e. across different datasets) is desired, even when the dataset used is relatively diverse and large.",
        "demo": "The model was trained on (image, text) pairs obtained from the Farfetch dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., \u201cstripes\u201d, \u201clong sleeves\u201d, \u201cArmani\u201d) and _short description_ (\u201c80s styled t-shirt\u201d)) available in the Farfetch dataset.",
        "input_format": "image, text",
        "output_format": "NO_OUTPUT",
        "input_preprocessing": "The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., \u201cstripes\u201d, \u201clong sleeves\u201d, \u201cArmani\u201d) and _short description_ (\u201c80s styled t-shirt\u201d)) available in the Farfetch dataset.",
        "input_size": "NO_OUTPUT",
        "num_of_classes_for_classification": "NO_OUTPUT",
        "trigger_word": "NO_OUTPUT"
    },
    "hf-internal-testing/tiny-random-BloomModel": "404 Client Error. (Request ID: Root=1-653df03b-43dae14c6e323a9b5e770e56)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BloomModel/resolve/main/README.md.",
    "shatabdi/twisent_twisent": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "siebert/sentiment-roberta-large-english"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "t5-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Colossal Clean Crawled Corpus (C4)",
            "C4",
            "Wiki-DPR",
            "Sentence acceptability judgment",
            "CoLA",
            "Sentiment analysis",
            "SST-2",
            "Paraphrasing/sentence similarity",
            "MRPC",
            "STS-B",
            "QQP"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)",
        "upstream_model": "",
        "parameter_count": "T5-Large is the checkpoint with 770 million parameters.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)",
        "demo": "demo",
        "input_format": "reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings",
        "output_format": "",
        "input_token_limit": "text-to-text framework...NLP task...hyperparameters",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-GPTNeoModel": "404 Client Error. (Request ID: Root=1-653df081-665ff27069b2b4ef654e4c9f)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTNeoModel/resolve/main/README.md.",
    "google/vit-base-patch16-384": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "ImageNet-21k",
            "ImageNet"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [
            "pytorch"
        ],
        "datasets": [
            "2 Billion sample English subset of LAION-5B",
            "LAION-A"
        ],
        "license": "MIT",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "hf-internal-testing/tiny-random-RoFormerModel": "404 Client Error. (Request ID: Root=1-653df0d7-7cf20a257303c4db46fb08f7)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-RoFormerModel/resolve/main/README.md.",
    "DeepPavlov/rubert-base-cased": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Russian part of Wikipedia",
            "news data"
        ],
        "license": "",
        "github": "https://arxiv.org/abs/1905.07213",
        "paper": "https://arxiv.org/abs/1905.07213",
        "upstream_model": "multilingual version of BERT\u2011base",
        "parameter_count": "180M parameters",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "arXiv:1905.07213",
        "input_format": "",
        "output_format": ""
    },
    "GanjinZero/UMLSBert_ENG": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [],
        "license": "apache-2.0",
        "github": "https://github.com/GanjinZero/CODER",
        "paper": "https://www.sciencedirect.com/science/article/pii/S1532046421003129",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "https://github.com/GanjinZero/CODER",
        "input_format": "",
        "output_format": ""
    },
    "nvidia/speakerverification_en_titanet_large": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "nemo"
        ],
        "datasets": [
            "Voxceleb-1",
            "Voxceleb-2",
            "Fisher",
            "Switchboard",
            "Librispeech",
            "SRE (2004-2010)"
        ],
        "license": "CC-BY-4.0",
        "github": "https://github.com/NVIDIA/NeMo",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "facebook/detr-resnet-101": {
        "model_type": "computer-vision",
        "model_tasks": "object-detection",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "COCO 2017 object detection"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py",
        "paper": "https://arxiv.org/abs/2005.12872",
        "upstream_model": "End-to-End Object Detection with Transformers by Carion et al.",
        "parameter_count": "118k/5k",
        "hyper_parameters": {
            "epochs": "300",
            "batch_size": "64",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "table 1 of the original paper",
                "result": 43.5
            }
        ],
        "hardware": "16 V100 GPUs, 4 images per GPU, total batch size of 64",
        "limitation_and_bias": "COCO 2017 object detection",
        "demo": "https://huggingface.co/models?search=facebook/detr",
        "input_format": "Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).",
        "output_format": "",
        "input_preprocessing": "Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).",
        "input_size": "shortest side is at least 800 pixels and the largest side at most 1333 pixels",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "timm/resnet101.a1h_in1k": "This model's maximum context length is 4097 tokens, however you requested 18743 tokens (18487 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "pysentimiento/robertuito-sentiment-analysis": "Could not parse function call data: Invalid \\escape: line 9 column 460 (char 865)",
    "Salesforce/blip-image-captioning-large": {
        "model_type": "multimodal",
        "model_tasks": "image-to-text",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "noisy image-text pairs collected from the web"
        ],
        "license": "Creative Commons Attribution 4.0 International",
        "github": "https://github.com/salesforce/BLIP",
        "paper": "Authors from the [paper](https://arxiv.org/abs/2201.12086) write in the abstract:",
        "upstream_model": "ViT large backbone",
        "parameter_count": "parameter_count",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).",
                "result": 0
            }
        ],
        "hardware": "ViT large backbone",
        "limitation_and_bias": "most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks... which is a suboptimal source of supervision... We achieve state-of-the-art results on a wide range of vision-language tasks... BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner.",
        "demo": "Model card for image captioning pretrained on COCO dataset - base architecture (with ViT large backbone).",
        "input_format": "",
        "output_format": "output_format"
    },
    "inpreet/logo_human_pupper": "Invalid `model_index` in metadata cannot be parsed: KeyError 'dataset'. Pass `ignore_metadata_errors=True` to ignore this error while loading a Model Card. Warning: some information will be lost. Use it at your own risk.",
    "hf-internal-testing/tiny-random-PegasusModel": "404 Client Error. (Request ID: Root=1-653df19c-0ba3c943135b3efe39c65e0e)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-PegasusModel/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-DebertaV2Model": "404 Client Error. (Request ID: Root=1-653df19c-4e53e7ed7711b0b76191b7c3)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-DebertaV2Model/resolve/main/README.md.",
    "cross-encoder/ms-marco-MiniLM-L-6-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "MS Marco Passage Ranking",
            "SBERT.net Retrieve & Re-rank",
            "SBERT.net Training MS Marco"
        ],
        "license": "apache-2.0",
        "github": "[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)",
        "paper": "MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "upstream_model": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "parameter_count": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "hyper_parameters": "MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "evaluation": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "hardware": "MS Marco Passage Ranking, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "limitation_and_bias": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "demo": "[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)",
        "input_format": "MS Marco Passage Ranking, Information Retrieval, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "output_format": "MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco",
        "input_token_limit": "max_length=512",
        "vocabulary_size": ""
    },
    "Helsinki-NLP/opus-mt-ru-en": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "cc-by-4.0",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-MobileBertModel": "404 Client Error. (Request ID: Root=1-653df1ee-285718821092439c6712a5b0)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-MobileBertModel/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-XLMModel": "404 Client Error. (Request ID: Root=1-653df1ef-4a6b7d183de609c15fb8b8a2)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-XLMModel/resolve/main/README.md.",
    "GanymedeNil/text2vec-large-chinese": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "https://huggingface.co/shibing624/text2vec-base-chinese"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/shibing624/text2vec",
        "paper": "https://github.com/shibing624/text2vec",
        "upstream_model": "https://github.com/shibing624/text2vec",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "MacBERT with LERT",
        "limitation_and_bias": "",
        "demo": "https://github.com/shibing624/text2vec",
        "input_format": "language:\n- zh\nlicense: apache-2.0\ntags:\n- text2vec\n- feature-extraction\n- sentence-similarity\n- transformers\npipeline_tag: sentence-similarity\nNO_OUTPUT",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "tiiuae/falcon-40b-instruct": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "tiiuae/falcon-refinedweb"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/project-baize/baize-chatbot",
        "paper": "https://arxiv.org/abs/2306.01116",
        "upstream_model": "Falcon-40B",
        "parameter_count": "Layers: 60, d_model: 8192, head_dim: 64, Vocabulary: 65024, Sequence length: 2048",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "Falcon-40B-Instruct, AWS SageMaker, 64 A100 40GB GPUs, P4d instances, custom distributed training codebase, Gigatron, 3D parallelism approach, ZeRO, high-performance Triton kernels (FlashAttention, etc.)",
        "limitation_and_bias": "Falcon-40B-Instruct is mostly trained on English data. It will carry the stereotypes and biases commonly encountered online",
        "demo": "You are looking for a ready-to-use chat/instruct model based on Falcon-40B. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-40B.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "Falcon-40B-Instruct was finetuned on a 150M tokens",
        "vocabulary_size": "Vocabulary: 65024"
    },
    "hf-internal-testing/tiny-random-ConvBertModel": "404 Client Error. (Request ID: Root=1-653df232-7fb15313412140d1757a4cf1)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-ConvBertModel/resolve/main/README.md.",
    "moussaKam/barthez-orangesum-abstract": {
        "model_type": "natural-language-processing",
        "model_tasks": "summarization",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "apache-2.0",
        "github": "https://github.com/moussaKam/BARThez",
        "paper": "https://arxiv.org/abs/2010.12321",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "language: - fr license: apache-2.0 tags: - summarization - bart widget: - text: Citant les pr\u00e9occupations de ses clients d\u00e9non\u00e7ant des cas de censure apr\u00e8s la suppression du compte de Trump, un fournisseur d'acc\u00e8s Internet de l'\u00c9tat de l'Idaho a d\u00e9cid\u00e9 de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients m\u00e9contents de la politique de ces r\u00e9seaux sociaux.",
        "demo": "language: - fr license: apache-2.0 tags: - summarization - bart widget: - text: Citant les pr\u00e9occupations de ses clients d\u00e9non\u00e7ant des cas de censure apr\u00e8s la suppression du compte de Trump, un fournisseur d'acc\u00e8s Internet de l'\u00c9tat de l'Idaho a d\u00e9cid\u00e9 de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients m\u00e9contents de la politique de ces r\u00e9seaux sociaux.",
        "input_format": "language: - fr license: apache-2.0 tags: - summarization - bart widget: - text: Citant les pr\u00e9occupations de ses clients d\u00e9non\u00e7ant des cas de censure apr\u00e8s la suppression du compte de Trump, un fournisseur d'acc\u00e8s Internet de l'\u00c9tat de l'Idaho a d\u00e9cid\u00e9 de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients m\u00e9contents de la politique de ces r\u00e9seaux sociaux.",
        "output_format": "language:\n- fr\nlicense: apache-2.0\ntags:\n- summarization\n- bart\nwidget:\n- text: Citant les pr\u00e9occupations de ses clients d\u00e9non\u00e7ant des cas de censure apr\u00e8s\nla suppression du compte de Trump, un fournisseur d'acc\u00e8s Internet de l'\u00c9tat de\nl'Idaho a d\u00e9cid\u00e9 de bloquer Facebook et Twitter. La mesure ne concernera cependant\nque les clients m\u00e9contents de la politique de ces r\u00e9seaux sociaux.",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Helsinki-NLP/opus-mt-it-en": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "apache-2.0",
        "github": "[it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md), [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "opus-2019-12-18.eval.txt",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "[it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md), opus, transformer-align, normalization + SentencePiece, [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)",
        "input_format": "SentencePiece + normalization",
        "output_format": "SentencePiece + opus-2019-12-18.zip + opus-2019-12-18.test.txt + opus-2019-12-18.eval.txt",
        "input_token_limit": "",
        "vocabulary_size": "SentencePiece"
    },
    "siebert/sentiment-roberta-large-english": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "McAuley and Leskovec (2013) (Reviews)",
            "McAuley and Leskovec (2013) (Review Titles)",
            "Yelp Academic Dataset",
            "Maas et al. (2011)",
            "Kaggle",
            "Pang and Lee (2005)",
            "Nakov et al. (2013)",
            "Shamma (2009)",
            "Blitzer et al. (2007) (Books)",
            "Blitzer et al. (2007) (DVDs)",
            "Blitzer et al. (2007) (Electronics)",
            "Blitzer et al. (2007) (Kitchen devices)",
            "Pang et al. (2002)",
            "Speriosu et al. (2011)",
            "Hartmann et al. (2019)"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Helsinki-NLP/opus-mt-fr-en": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "apache-2.0",
        "github": "* OPUS readme: [fr-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/fr-en/README.md)\n* download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.zip)\n* test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.test.txt)\n* test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.eval.txt)",
        "paper": "model: transformer-align",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "newsdiscussdev2015-enfr.fr.en",
                "result": 33.1
            },
            {
                "test": "newsdiscusstest2015-enfr.fr.en",
                "result": 38.7
            },
            {
                "test": "newssyscomb2009.fr.en",
                "result": 30.3
            },
            {
                "test": "news-test2008.fr.en",
                "result": 26.2
            },
            {
                "test": "newstest2009.fr.en",
                "result": 30.2
            },
            {
                "test": "newstest2010.fr.en",
                "result": 32.2
            },
            {
                "test": "newstest2011.fr.en",
                "result": 33.0
            },
            {
                "test": "newstest2012.fr.en",
                "result": 32.8
            },
            {
                "test": "newstest2013.fr.en",
                "result": 33.9
            },
            {
                "test": "newstest2014-fren.fr",
                "result": 37.8
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "*OPUS readme: [fr-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/fr-en/README.md)*\n* *download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.zip)*\n* *test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.test.txt)*\n* *test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.eval.txt)*",
        "input_format": "SentencePiece",
        "output_format": "BLEU  | chr-F |",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "google/flan-t5-xl": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "mixture of tasks"
        ],
        "license": "Creative Commons Attribution 4.0 International",
        "github": "- svakulenk0/qrecc\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed",
        "paper": "original paper, figure 2",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:",
                "result": 0
            }
        ],
        "hardware": "5. [Training Details](#training-details)",
        "limitation_and_bias": "4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)",
        "demo": "Find below some example scripts on how to use the model in `transformers`:",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "libritts",
            "voxpopuli"
        ],
        "license": "",
        "github": "https://github.com/facebookresearch/voxpopuli\nhttps://github.com/neonbjb/ocotillo",
        "paper": "\"This model was created by fine-tuning the `facebook/wav2vec2-large-robust-ft-libri-960h` checkpoint on the [libritts](https://research.google/tools/datasets/libri-tts/) and [voxpopuli](https://github.com/facebookresearch/voxpopuli) datasets with a new vocabulary that includes punctuation.\"",
        "upstream_model": "`facebook/wav2vec2-large-robust-ft-libri-960h`",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "",
                "result": 4.45
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "\"Check out my speech transcription script repo, [ocotillo](https://github.com/neonbjb/ocotillo) for usage examples: https://github.com/neonbjb/ocotillo\"",
        "input_format": "",
        "output_format": "",
        "sample_rate": "",
        "WER": "The model gets a respectable WER of 4.45% on the librispeech validation set. The baseline, `facebook/wav2vec2-large-robust-ft-libri-960h`, got 4.3%."
    },
    "hf-internal-testing/tiny-random-flaubert": "404 Client Error. (Request ID: Root=1-653df31f-4662f1c4248fafc51347f9c3)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/README.md.",
    "facebook/mbart-large-50": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "microsoft/DialoGPT-medium": {
        "model_type": "natural-language-processing",
        "model_tasks": "conversational",
        "frameworks": [
            "pytorch",
            "jax",
            "rust",
            "transformers"
        ],
        "datasets": [
            "147M multi-turn dialogue from Reddit discussion thread"
        ],
        "license": "mit",
        "github": "[https://github.com/dreasysnail/Dialogpt_dev#human-evaluation], [https://github.com/microsoft/DialoGPT]",
        "paper": "ArXiv paper: [https://arxiv.org/abs/1911.00536](https://arxiv.org/abs/1911.00536)",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "human evaluation",
                "result": 0.0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n\n# Let's chat for 5 lines\nfor step in range(5):\n# encode the new user input, add the eos_token and return a tensor in Pytorch\nnew_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n\n# append the new user input tokens to the chat history\nbot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n# generated a response while limiting the total chat history to 1000 tokens,\nchat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "max_length=1000",
        "vocabulary_size": ""
    },
    "hakurei/waifu-diffusion": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "CreativeML OpenRAIL-M"
        ],
        "license": "CreativeML OpenRAIL-M license, 1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content, 2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license, 3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully), [Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)",
        "github": "https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1",
        "paper": "CreativeML OpenRAIL-M license and [Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)",
        "upstream_model": "CreativeML OpenRAIL-M license and [Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "[See here for a full model overview.](https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1)",
        "input_format": "",
        "output_format": ""
    },
    "ipuneetrathore/bert-base-cased-finetuned-finBERT": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "https://github.com/ipuneetrathore/BERT_models",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "https://github.com/ipuneetrathore/BERT_models",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Helsinki-NLP/opus-mt-en-de": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "jax",
            "rust",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "cc-by-4.0",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "gpt2-xl": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "WebText"
        ],
        "license": "mit",
        "github": "",
        "paper": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-DebertaModel": "404 Client Error. (Request ID: Root=1-653df41d-048154c7632c03a54db373db)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-DebertaModel/resolve/main/README.md.",
    "sentence-transformers/bert-base-nli-mean-tokens": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "transformers",
            "jax",
            "sentence-transformers",
            "pytorch",
            "rust"
        ],
        "datasets": [],
        "license": "apache-2.0",
        "github": "",
        "paper": "",
        "upstream_model": "sentence-transformers",
        "parameter_count": "'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/bert-base-nli-mean-tokens)",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "[sentence-transformers](https://www.SBERT.net)",
        "input_format": "'max_seq_length': 128, 'do_lower_case': False",
        "output_format": "'do_lower_case': False, 'word_embedding_dimension': 768",
        "input_token_limit": "'max_seq_length': 128",
        "vocabulary_size": ""
    },
    "decapoda-research/llama-7b-hf": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "CCNet",
            "C4",
            "GitHub",
            "Wikipedia",
            "Books",
            "ArXiv",
            "Stack Exchange"
        ],
        "license": "other",
        "github": "https://github.com/facebookresearch/llama",
        "paper": "https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/",
        "upstream_model": "CCNet",
        "parameter_count": "65B",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "BoolQ",
                "result": 0.85
            },
            {
                "test": "PIQA",
                "result": 0.92
            },
            {
                "test": "SIQA",
                "result": 0.88
            },
            {
                "test": "HellaSwag",
                "result": 0.75
            },
            {
                "test": "WinoGrande",
                "result": 0.82
            },
            {
                "test": "ARC",
                "result": 0.79
            },
            {
                "test": "OpenBookQA",
                "result": 0.87
            },
            {
                "test": "NaturalQuestions",
                "result": 0.91
            },
            {
                "test": "TriviaQA",
                "result": 0.88
            },
            {
                "test": "RACE",
                "result": 0.86
            },
            {
                "test": "MMLU",
                "result": 0.9
            },
            {
                "test": "BIG-bench hard",
                "result": 0.83
            },
            {
                "test": "GSM8k",
                "result": 0.84
            },
            {
                "test": "RealToxicityPrompts",
                "result": 0.76
            },
            {
                "test": "WinoGender",
                "result": 0.81
            },
            {
                "test": "CrowS-Pairs",
                "result": 0.89
            }
        ],
        "hardware": "",
        "limitation_and_bias": "The data used to train the model is collected from various sources, mostly from the Web. As such, it contains offensive, harmful and biased content. We thus expect the model to exhibit such biases from the training data. Risks and harms of large language models include the generation of harmful, offensive or biased content. These models are often prone to generating incorrect information, sometimes referred to as hallucinations. We do not expect our model to be an exception in this regard. Use cases...include, but are not limited to: generation of misinformation and generation of harmful, biased or offensive content.",
        "demo": "",
        "input_format": "CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]",
        "output_format": "CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-CodeGenModel": "404 Client Error. (Request ID: Root=1-653df47b-300ec8fd07d11f206ddd399c)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-CodeGenModel/resolve/main/README.md.",
    "flair/ner-english-fast": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "flair"
        ],
        "datasets": [
            "conll2003"
        ],
        "license": "license",
        "github": "[here](https://github.com/flairNLP/flair/issues/)",
        "paper": "Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}",
        "upstream_model": "upstream_model",
        "parameter_count": "hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type",
        "hyper_parameters": "hidden_size=256, max_epochs=150",
        "evaluation": "F1-Score: 92,92 (corrected CoNLL-03) | tag | meaning | PER | person name | LOC | location name | ORG | organization name | MISC | other name | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.",
        "hardware": "WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')",
        "limitation_and_bias": "F1-Score: 92,92 (corrected CoNLL-03) Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.",
        "demo": "Find a form of demo",
        "input_format": "CONLL_03(), WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')",
        "output_format": "F1-Score: 92,92 (corrected CoNLL-03) Predicts 4 tags: | tag | meaning | |---------------------------------|-----------| | PER | person name | | LOC | location name | | ORG | organization name | | MISC | other name | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.",
        "input_token_limit": "Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.",
        "vocabulary_size": "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
    },
    "sshleifer/tiny-marian-en-de": "404 Client Error. (Request ID: Root=1-653df4a8-0c58305c267590be488a018b)\n\nEntry Not Found for url: https://huggingface.co/sshleifer/tiny-marian-en-de/resolve/main/README.md.",
    "prompthero/openjourney": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers",
            "safetensors"
        ],
        "datasets": [],
        "license": "creativeml-openrail-m",
        "github": "https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion",
        "paper": "tags: - stable-diffusion - text-to-image",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "'mdjrny-v4 style' and 'Openjourney prompts'",
        "input_format": "",
        "output_format": "ONNX, MPS, FLAX/JAX"
    },
    "facebook/m2m100_418M": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "M2M100"
        ],
        "license": "mit",
        "github": "",
        "paper": "https://arxiv.org/abs/2010.11125",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "m3hrdadfi/distilbert-zwnj-wnli-mean-tokens": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "https://github.com/m3hrdadfi/sentence-transformers",
        "paper": "",
        "upstream_model": "m3hrdadfi/distilbert-zwnj-wnli-mean-tokens",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "PubMed"
        ],
        "license": "mit",
        "github": "https://github.com/microsoft/BiomedNLP-PubMedBERT",
        "paper": "https://arxiv.org/abs/2007.15779",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "uer/gpt2-chinese-cluecorpussmall": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "CLUECorpusSmall"
        ],
        "license": "license",
        "github": "CLUECorpusSmall https://github.com/CLUEbenchmark/CLUECorpus2020/ is used as training data.",
        "paper": "@article{radford2019language,\ntitle={Language Models are Unsupervised Multitask Learners},\nauthor={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\nyear={2019}\n}",
        "upstream_model": "[distil], [base], [medium], [large], [xlarge]",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "CLUECorpusSmall is used as training data.",
                "result": 0
            }
        ],
        "hardware": "The GPT2-xlarge model is pre-trained by [TencentPretrain](https://github.com/Tencent/TencentPretrain),",
        "limitation_and_bias": "limitation_and_bias",
        "demo": "```python\n>>> from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n>>> tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n>>> model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n>>> text_generator = TextGenerationPipeline(model, tokenizer)\n>>> text_generator(\"\u8fd9\u662f\u5f88\u4e45\u4e4b\u524d\u7684\u4e8b\u60c5\u4e86\", max_length=100, do_sample=True)\n[{'generated_text': '\u8fd9\u662f\u5f88\u4e45\u4e4b\u524d\u7684\u4e8b\u60c5\u4e86 \u3002 \u6211 \u73b0 \u5728 \u60f3 \u8d77 \u6765 \u5c31 \u8ba9 \u81ea \u5df1 \u5f88 \u4f24 \u5fc3 \uff0c \u5f88 \u5931 \u671b \u3002 \u6211 \u73b0 \u5728 \u60f3 \u5230 \uff0c \u6211 \u89c9"
    },
    "finiteautomata/bertweet-base-sentiment-analysis": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "TASS Dataset"
        ],
        "license": "http://tass.sepln.org/tass_data/download.php",
        "github": "https://github.com/finiteautomata/pysentimiento/",
        "paper": "https://arxiv.org/abs/2106.09462",
        "upstream_model": "BERTweet",
        "parameter_count": "Model trained with SemEval 2017 corpus (around ~40k tweets)",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Model trained with SemEval 2017 corpus (around ~40k tweets)",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses.",
        "demo": "Please be aware that models are trained with third-party datasets and are subject to their respective licenses.",
        "input_format": "SemEval 2017 corpus (around ~40k tweets)",
        "output_format": "Uses `POS`, `NEG`, `NEU` labels.",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "neuralmind/bert-base-portuguese-cased": "Could not parse function call data: Invalid \\escape: line 7 column 19 (char 153)",
    "Helsinki-NLP/opus-mt-es-en": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/spa-eng/README.md",
        "paper": "",
        "upstream_model": "transformer",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "sentence-transformers/all-distilroberta-v1": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "rust"
        ],
        "datasets": [
            "1B sentence pairs dataset"
        ],
        "license": "apache-2.0",
        "github": "github repositories Sentence Embeddings Benchmark: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-distilroberta-v1)",
        "paper": "sentence-transformers and semantic search",
        "upstream_model": "sentence-transformers",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "sentence-transformers, https://www.SBERT.net",
        "input_format": "text longer than 128 word pieces is truncated.",
        "output_format": "vector",
        "input_token_limit": "128",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-m2m_100": "404 Client Error. (Request ID: Root=1-653df63d-436f80d060a863d600cbd2a4)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-m2m_100/resolve/main/README.md.",
    "lewtun/tiny-random-mt5": "404 Client Error. (Request ID: Root=1-653df63d-028de0f75ba45f454f8ec12d)\n\nEntry Not Found for url: https://huggingface.co/lewtun/tiny-random-mt5/resolve/main/README.md.",
    "fxmarty/tiny-llama-fast-tokenizer": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "SQuAD"
        ],
        "license": "MIT",
        "github": "https://github.com/huggingface/transformers",
        "paper": "https://arxiv.org/abs/1810.04805",
        "upstream_model": "bert-base-uncased",
        "parameter_count": "110M",
        "hyper_parameters": {
            "epochs": "3",
            "batch_size": "32",
            "learning_rate": "2e-5",
            "optimizer": "AdamW"
        },
        "evaluation": [
            {
                "test": "SQuAD",
                "result": 88.5
            }
        ],
        "hardware": "GPU",
        "limitation_and_bias": "The model may not perform well on out-of-domain or adversarial examples.",
        "demo": "https://huggingface.co/transformers/",
        "input_format": "Text",
        "output_format": "Text",
        "input_token_limit": "512",
        "vocabulary_size": "30522"
    },
    "gpt2-medium": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "onnx",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "WebText"
        ],
        "license": "mit",
        "github": "https://github.com/openai/gpt-2/blob/master/domains.txt",
        "paper": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
        "upstream_model": "",
        "parameter_count": "355M parameter",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. This bias will also affect all fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.",
        "demo": "Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-medium')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, I'm a language. I'm a compiler, I'm a parser, I'm a server process. I\"},\n{'generated_text': \"Hello, I'm a language model, and I'd like to join an existing team. What can I do to get started?\\n\\nI'd\"},\n{'generated_text': \"Hello, I'm a language model, why does my code get created? Can't I just copy it? But why did my code get created when\"},\n{'generated_text': \"Hello, I'm a language model, a functional language...",
        "input_format": "The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt). The model is pretrained on a very large corpus of English data in a self-supervised fashion. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.",
        "output_format": "The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "mrm8488/t5-base-finetuned-summarize-news": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "News Summary"
        ],
        "license": "",
        "github": "https://github.com/abhimishra91",
        "paper": "https://arxiv.org/pdf/1910.10683.pdf",
        "upstream_model": "T5",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ASTD",
            "ArSAS",
            "SemEval"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/CAMeL-Lab/CAMeLBERT",
        "paper": "https://arxiv.org/abs/2103.06678",
        "upstream_model": "CAMeLBERT Dialectal Arabic (DA)",
        "parameter_count": "N/A",
        "hyper_parameters": {
            "epochs": "N/A",
            "batch_size": "N/A",
            "learning_rate": "N/A",
            "optimizer": "N/A"
        },
        "evaluation": [
            {
                "test": "Sentiment Analysis (SA) model",
                "result": 0.85
            }
        ],
        "hardware": "N/A",
        "limitation_and_bias": "N/A",
        "demo": "N/A",
        "input_format": "N/A",
        "output_format": "N/A",
        "input_token_limit": "N/A",
        "vocabulary_size": "N/A"
    },
    "indolem/indobert-base-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Indonesian Wikipedia",
            "news articles from Kompas, Tempo, and Liputan6",
            "Indonesian Web Corpus"
        ],
        "license": "mit",
        "github": "https://github.com/indolem/indobert-base-uncased",
        "paper": "https://www.aclweb.org/anthology/2020.coling-main.510/",
        "upstream_model": "BERT-base",
        "parameter_count": "220M words, 2.4M steps (180 epochs), 3.97 final perplexity over the development set, IndoBERT",
        "hyper_parameters": {
            "epochs": "180",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "POS Tagging",
                "result": 96.8
            },
            {
                "test": "NER UGM",
                "result": 74.9
            },
            {
                "test": "NER UI",
                "result": 90.1
            },
            {
                "test": "Dep. Parsing (UD-Indo-GSD)",
                "result": 87.12
            },
            {
                "test": "Dep. Parsing (UD-Indo-PUD)",
                "result": 90.58
            },
            {
                "test": "Sentiment Analysis",
                "result": 84.13
            }
        ],
        "hardware": "220M words, Indonesian Wikipedia, news articles from Kompas, Tempo, and Liputan6, Indonesian Web Corpus",
        "limitation_and_bias": "We train the model using over 220M words, aggregated from three main sources: Indonesian Wikipedia, news articles from Kompas, Tempo, and Liputan6, and an Indonesian Web Corpus. We trained the model for 2.4M steps (180 epochs) with the final perplexity over the development set being 3.97 (similar to English BERT-base).",
        "demo": "```from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"indolem/indobert-base-uncased\")```",
        "input_format": "We train the model using over 220M words, aggregated from three main sources: Indonesian Wikipedia, news articles from Kompas, Tempo, and Liputan6, and an Indonesian Web Corpus.",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "PygmalionAI/pygmalion-6b": {
        "model_type": "natural-language-processing",
        "model_tasks": "conversational",
        "frameworks": [
            "pytorch",
            "tensorboard",
            "transformers"
        ],
        "datasets": [
            "56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations."
        ],
        "license": "creativeml-openrail-m",
        "github": "[here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb)",
        "paper": "The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.",
        "upstream_model": "`uft-6b` ConvoGPT model",
        "parameter_count": "48.5 million tokens",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations.",
                "result": 0
            }
        ],
        "hardware": "NVIDIA A40s",
        "limitation_and_bias": "real _and_ partially machine-generated conversations.",
        "demo": "The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format: \n```\n[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\n<START>\n[DIALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:\n```\nWhere `[CHARACTER]` is, as you can probably guess, the name of the character you want the model to portray, `<START>` should be used verbatim as a delimiter token to separate persona and scenario data from the dialogue, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like: \n```\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]\n```\nApart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's",
        "input_format": "input_format: ```[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\n<START>\n[DIALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:``` Where `[CHARACTER]` is, as you can probably guess, the name of the character you want the model to portray, `<START>` should be used verbatim as a delimiter token to separate persona and scenario data from the dialogue, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like: ```[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]``` Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.",
        "output_format": "real _and_ partially machine-generated conversations.",
        "input_token_limit": "input_token_limit NO_OUTPUT",
        "vocabulary_size": ""
    },
    "ramsrigouthamg/t5_sentence_paraphraser": "404 Client Error. (Request ID: Root=1-653df761-3b29c8a63d371bdf436fa378)\n\nEntry Not Found for url: https://huggingface.co/ramsrigouthamg/t5_sentence_paraphraser/resolve/main/README.md.",
    "deepset/bert-large-uncased-whole-word-masking-squad2": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "SQuAD2.0"
        ],
        "license": "cc-by-4.0",
        "github": "https://github.com/deepset-ai/haystack",
        "paper": "",
        "upstream_model": "bert-large",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "berta-large",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "PlanTL-GOB-ES/roberta-base-bne": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Training data"
        ],
        "license": "Apache License, Version 2.0",
        "github": "[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)",
        "paper": "@article{,\ntitle = {MarIA: Spanish Language Models},\nauthor = {Asier Guti\u00e9rrez Fandi\u00f1o and Jordi Armengol Estap\u00e9 and Marc P\u00e0mies and Joan Llop Palao and Joaquin Silveira Ocampo and Casimiro Pio Carrino and Carme Armentano Oller and Carlos Rodriguez Penagos and Aitor Gonzalez Agirre and Marta Villegas},\ndoi = {10.26342/2022-68-3},\nissn = {1135-5948},\njournal = {Procesamiento del Lenguaje Natural},\npublisher = {Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural},\nurl = {https://upcommons.upc.edu/handle/2117/367156#.YyMTB4X9A-0.mendeley},\nvolume = {68},\nyear = {2022},\n}",
        "upstream_model": "",
        "parameter_count": "\"50,262 tokens\" and \"16 computing nodes, each one with 4 NVIDIA V100 GPUs of 16GB VRAM.\"",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "\"16 computing nodes, each one with 4 NVIDIA V100 GPUs of 16GB VRAM.\"",
        "limitation_and_bias": "#limitations-and-bias",
        "demo": "",
        "input_format": "```python\n>>> from transformers import pipeline, set_seed\n>>> from pprint import pprint\n>>> unmasker = pipeline('fill-mask', model='PlanTL-GOB-ES/roberta-base-bne')\n>>> set_seed(42)\n>>> pprint(unmasker(\"Antonio est\u00e1 pensando en <mask>.\"))\n[{'score': 0.07950365543365479,\n'sequence': 'Antonio est\u00e1 pensando en ti.',\n'token': 486,\n'token_str': ' ti'},\n{'score': 0.03375273942947388,\n'sequence': 'Antonio est\u00e1 pensando en irse.',",
        "output_format": "",
        "input_token_limit": "\"BPE\" \"50,262 tokens\" \"masked language model training\" \"RoBERTa base\" \"48 hours\" \"16 computing nodes\" \"4 NVIDIA V100 GPUs of 16GB VRAM\"",
        "vocabulary_size": "\"vocabulary size of 50,262 tokens\""
    },
    "flexudy/t5-base-multi-sentence-doctor": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "tatoeba dataset",
            "sentence_doctor_dataset_300K"
        ],
        "license": "NO_OUTPUT",
        "github": "NO_OUTPUT",
        "paper": "NO_OUTPUT",
        "upstream_model": "NO_OUTPUT",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [
            {
                "epochs": "3",
                "batch_size": "NO_OUTPUT",
                "learning_rate": "NO_OUTPUT",
                "optimizer": "NO_OUTPUT"
            }
        ],
        "evaluation": [
            {
                "test": "NO_OUTPUT",
                "result": 0
            }
        ],
        "hardware": "NO_OUTPUT",
        "limitation_and_bias": "NO_OUTPUT",
        "demo": "NO_OUTPUT",
        "input_format": "NO_OUTPUT",
        "output_format": "NO_OUTPUT",
        "input_token_limit": "NO_OUTPUT",
        "vocabulary_size": "NO_OUTPUT"
    },
    "facebook/hubert-large-ls960-ft": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "libri-light",
            "librispeech_asr"
        ],
        "license": "apache-2.0",
        "github": "language: en, license: apache-2.0, tags: - speech - audio - automatic-speech-recognition - hf-asr-leaderboard, datasets: - libri-light - librispeech_asr, model-index: - name: hubert-large-ls960-ft, results: - task: type: automatic-speech-recognition name: Automatic Speech Recognition dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: en metrics: - type: wer value: 1.9 name: Test WER",
        "paper": "[Paper](https://arxiv.org/abs/2106.07447) Authors: Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",
        "upstream_model": "hubert-large-ll60k",
        "parameter_count": "\"model-index: - name: hubert-large-ls960-ft\" and \"args: language: en\"",
        "hyper_parameters": "language: en, type: automatic-speech-recognition, name: LibriSpeech (clean), type: librispeech_asr, config: clean, split: test, language: en, type: wer, value: 1.9, name: Test WER",
        "evaluation": "\"type: automatic-speech-recognition name: Automatic Speech Recognition dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: en metrics: - type: wer value: 1.9 name: Test WER\"",
        "hardware": "\"type: automatic-speech-recognition\" and \"dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: en\"",
        "limitation_and_bias": "",
        "demo": "```python\nimport torch\nfrom transformers import Wav2Vec2Processor, HubertForCTC\nfrom datasets import load_dataset\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\nmodel = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\ninput_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\nlogits = model(input_values).logits\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.decode(predicted_ids[0])\n\n# ->\"A MAN SAID TO THE UNIVERSE SIR I EXIST\"\n```",
        "input_format": "language: en, type: automatic-speech-recognition, dataset: name: LibriSpeech (clean), type: librispeech_asr, config: clean, split: test, language: en",
        "output_format": "\"type: automatic-speech-recognition\" and \"metrics: - type: wer\"",
        "sample_rate": "16kHz sampled speech audio",
        "WER": "\"type: automatic-speech-recognition name: Automatic Speech Recognition dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: en metrics: - type: wer value: 1.9 name: Test WER\""
    },
    "tuner007/pegasus_paraphrase": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "PEGASUS"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google-research/pegasus",
        "paper": "- pegasus - paraphrasing - seq2seq",
        "upstream_model": "",
        "parameter_count": "num_beams = 10, num_return_sequences = 10",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "import torch\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\nmodel_name = 'tuner007/pegasus_paraphrase'\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)",
        "limitation_and_bias": "",
        "demo": "\"PEGASUS fine-tuned for paraphrasing\"",
        "input_format": "tokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\nbatch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\ntranslated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)",
        "output_format": "",
        "input_token_limit": "max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5",
        "vocabulary_size": "tokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\nnum_beams = 10\nnum_return_sequences = 10"
    },
    "dbmdz/bert-large-cased-finetuned-conll03-english": "404 Client Error. (Request ID: Root=1-653df85d-26caa4981f0e21762f4ff990)\n\nEntry Not Found for url: https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english/resolve/main/README.md.",
    "cl-tohoku/bert-base-japanese": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Japanese Wikipedia"
        ],
        "license": "Creative Commons Attribution-ShareAlike 3.0",
        "github": "https://github.com/attardi/wikiextractor",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "512*256*1000000",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "Cloud TPUs",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "512",
        "vocabulary_size": "32000"
    },
    "sentence-transformers/multi-qa-MiniLM-L6-cos-v1": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers"
        ],
        "datasets": [
            "WikiAnswers",
            "PAQ",
            "Stack Exchange",
            "MS MARCO",
            "GOOAQ",
            "Amazon-QA",
            "Yahoo Answers",
            "SearchQA"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "EleutherAI/gpt-neo-1.3B": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "Pile",
            "EleutherAI"
        ],
        "license": "mit",
        "github": "https://github.com/EleutherAI",
        "paper": "https://arxiv.org/abs/2101.00027",
        "upstream_model": "GPT-3",
        "parameter_count": "362,000",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "madhurjindal/autonlp-Gibberish-Detector-492513457": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "madhurjindal/autonlp-data-Gibberish-Detector"
        ],
        "license": "\"natural language processing\", \"chatbot systems\", \"spam filtering\", \"language-based security measures\"",
        "github": "madhurjindal/autonlp-data-Gibberish-Detector, github",
        "paper": "Model ID: 492513457",
        "upstream_model": "upstream_model",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "- Loss: 0.07609463483095169",
                "result": 0.9735624586913417
            },
            {
                "test": "- Accuracy: 0.9735624586913417",
                "result": 0.9736173135739408
            },
            {
                "test": "- Macro F1: 0.9736173135739408",
                "result": 0.9735624586913417
            },
            {
                "test": "- Micro F1: 0.9735624586913417",
                "result": 0.9736173135739408
            },
            {
                "test": "- Weighted F1: 0.9736173135739408",
                "result": 0.9737771415197378
            },
            {
                "test": "- Macro Precision: 0.9737771415197378",
                "result": 0.9735624586913417
            },
            {
                "test": "- Micro Precision: 0.9735624586913417",
                "result": 0.9737771415197378
            },
            {
                "test": "- Weighted Precision: 0.9737771415197378",
                "result": 0.9735624586913417
            },
            {
                "test": "- Macro Recall: 0.9735624586913417",
                "result": 0.9735624586913417
            },
            {
                "test": "- Micro Recall: 0.9735624586913417",
                "result": 0.9735624586913417
            },
            {
                "test": "- Weighted Recall: 0.9735624586913417",
                "result": 0.9735624586913417
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "You can use cURL to access this model:  \n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love Machine Learning!\"}' https://api-inference.huggingface.co/models/madhurjindal/autonlp-Gibberish-Detector-492513457\n```  \nOr Python API:  \n```\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ninputs = tokenizer(\"I love Machine Learning!\", return_tensors=\"pt\")\n\noutput",
        "input_format": "\"Content-Type: application/json\" and \"return_tensors=\"pt\"\"",
        "output_format": ""
    },
    "deepset/roberta-large-squad2": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "squad_v2",
            "squad",
            "adversarial_qa",
            "squad_adversarial",
            "squadshifts amazon",
            "squadshifts new_wiki",
            "squadshifts nyt",
            "squadshifts reddit"
        ],
        "license": "cc-by-4.0",
        "github": "https://github.com/deepset-ai/haystack",
        "paper": "roberta-large, SQuAD2.0",
        "upstream_model": "NO_OUTPUT",
        "parameter_count": "base_model: roberta-large",
        "hyper_parameters": [
            {
                "epochs": "roberta-large",
                "batch_size": "SQuAD2.0",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "exact_match",
                "result": 85.168
            },
            {
                "test": "f1",
                "result": 88.349
            },
            {
                "test": "exact_match",
                "result": 87.162
            },
            {
                "test": "f1",
                "result": 93.603
            },
            {
                "test": "exact_match",
                "result": 35.9
            },
            {
                "test": "f1",
                "result": 48.923
            },
            {
                "test": "exact_match",
                "result": 81.142
            },
            {
                "test": "f1",
                "result": 87.099
            },
            {
                "test": "exact_match",
                "result": 72.453
            },
            {
                "test": "f1",
                "result": 86.325
            },
            {
                "test": "exact_match",
                "result": 82.338
            },
            {
                "test": "f1",
                "result": 91.974
            },
            {
                "test": "exact_match",
                "result": 84.352
            },
            {
                "test": "f1",
                "result": 92.645
            }
        ],
        "hardware": "roberta-large, SQuAD2.0",
        "limitation_and_bias": "roberta-large, English, Extractive QA, SQuAD 2.0, See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system), 4x Tesla v100",
        "demo": "roberta-large, English, Extractive QA, SQuAD 2.0, SQuAD 2.0, [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system), 4x Tesla v100",
        "input_format": "squad_v2, plain_text, adversarialQA, AddOneSent, amazon, new_wiki, nyt, reddit",
        "output_format": "output_format",
        "input_token_limit": "roberta-large, Extractive QA, SQuAD 2.0, 4x Tesla v100",
        "vocabulary_size": "roberta-large, SQuAD2.0"
    },
    "hf-internal-testing/tiny-random-Data2VecTextModel": "404 Client Error. (Request ID: Root=1-653df93d-1b6e7822585627200ac50c51)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Data2VecTextModel/resolve/main/README.md.",
    "facebook/esm2_t6_8M_UR50D": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "runwayml/stable-diffusion-inpainting": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "LAION-5B"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "H2O LLM Studio"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/h2oai/h2o-llmstudio",
        "paper": "",
        "upstream_model": "tiiuae/falcon-7b",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "H2O LLM Studio",
        "limitation_and_bias": "- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints. - Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.",
        "demo": "cfg.yaml, H2O LLM Studio, https://github.com/h2oai/h2o-llmstudio",
        "input_format": "\"language: - en license: apache-2.0 library_name: transformers tags: - gpt - llm - large language model - h2o-llmstudio datasets: - OpenAssistant/oasst1 inference: false thumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico\"",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "flair/ner-english-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "flair"
        ],
        "datasets": [
            "conll2003"
        ],
        "license": "license",
        "github": "github",
        "paper": "FLERT (https://arxiv.org/pdf/2011.06993v1.pdf/)",
        "upstream_model": "FLERT",
        "parameter_count": "SequenceTagger(hidden_size=256,",
        "hyper_parameters": {
            "epochs": "20",
            "batch_size": "4",
            "learning_rate": "5.0e-6",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "F1-Score",
                "result": 94.36
            }
        ],
        "hardware": "`model='xlm-roberta-large'`",
        "limitation_and_bias": "Based on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/).",
        "demo": "Find a form of demo for the model",
        "input_format": "xlm-roberta-large",
        "output_format": ""
    },
    "roberta-base-openai-detector": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "WebText"
        ],
        "license": "mit",
        "github": "",
        "paper": "https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "absa/classifier-rest-0.2": "404 Client Error. (Request ID: Root=1-653dfa12-343e91d8182f47fc65e4886d)\n\nEntry Not Found for url: https://huggingface.co/absa/classifier-rest-0.2/resolve/main/README.md.",
    "sonoisa/t5-base-japanese-title-generation": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44"
        ],
        "license": "license",
        "github": "https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44",
        "paper": "https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44",
        "upstream_model": "upstream_model",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "epochs",
            "batch_size": "batch_size",
            "learning_rate": "learning_rate",
            "optimizer": "optimizer"
        },
        "evaluation": [
            {
                "test": "SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44",
                "result": 0.0
            }
        ],
        "hardware": "https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44",
        "limitation_and_bias": "",
        "demo": "https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44",
        "input_format": "https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44",
        "output_format": "output_format",
        "input_token_limit": "input_token_limit",
        "vocabulary_size": "vocabulary_size"
    },
    "lllyasviel/control_v11f1p_sd15_depth": "This model's maximum context length is 4097 tokens, however you requested 4291 tokens (4035 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "skt/kobert-base-v1": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "license",
        "github": "https://github.com/SKTBrain/KoBERT",
        "paper": "Please refer here. https://github.com/SKTBrain/KoBERT",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "Please refer here. https://github.com/SKTBrain/KoBERT",
        "input_format": "",
        "output_format": ""
    },
    "flair/chunk-english": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "flair"
        ],
        "datasets": [
            "conll2000"
        ],
        "license": "license",
        "github": "[here](https://github.com/flairNLP/flair/issues/)",
        "paper": "Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}",
        "upstream_model": "upstream_model",
        "parameter_count": "hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary",
        "hyper_parameters": {
            "epochs": "max_epochs=150",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "F1-Score",
                "result": 96.48
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Based on Flair embeddings and LSTM-CRF.",
        "demo": "Find a form of demo",
        "input_format": "Based on Flair embeddings and LSTM-CRF.",
        "output_format": "Based on Flair embeddings and LSTM-CRF.",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "timm/resnet18.a1_in1k": "This model's maximum context length is 4097 tokens, however you requested 18743 tokens (18487 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "transformers"
        ],
        "datasets": [
            "Sentence Embeddings Benchmark"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/sentence-transformers/sentence-transformers",
        "paper": "https://arxiv.org/abs/1908.10084",
        "upstream_model": "sentence-transformers",
        "parameter_count": "'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "https://www.sbert.net",
        "input_format": "sentence-transformers",
        "output_format": "'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False",
        "input_token_limit": "'max_seq_length': 128",
        "vocabulary_size": ""
    },
    "bert-large-uncased-whole-word-masking-finetuned-squad": "Could not parse function call data: Unterminated string starting at: line 30 column 28 (char 2244)",
    "facebook/detr-resnet-50": {
        "model_type": "computer-vision",
        "model_tasks": "object-detection",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "COCO 2017 object detection"
        ],
        "license": "apache-2.0",
        "github": "[here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py)",
        "paper": "[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)",
        "upstream_model": "End-to-End Object Detection with Transformers by Carion et al.",
        "parameter_count": "118k/5k",
        "hyper_parameters": {
            "epochs": "300",
            "batch_size": "16 V100 GPUs, 4 images per GPU, total batch size of 64",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "AP (average precision)",
                "result": 42.0
            }
        ],
        "hardware": "16 V100 GPUs, 4 images per GPU, total batch size of 64",
        "limitation_and_bias": "COCO 2017 object detection",
        "demo": "\"model hub\" \"https://huggingface.co/models?search=facebook/detr\"",
        "input_format": "Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).",
        "output_format": "",
        "input_preprocessing": "\"Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).\"",
        "input_size": "shortest side is at least 800 pixels and the largest side at most 1333 pixels",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "HooshvareLab/bert-base-parsbert-ner-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "ARMAN"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/m3hrdadfi",
        "paper": "https://arxiv.org/abs/2005.12515",
        "upstream_model": "Google's BERT architecture",
        "parameter_count": "302,530 tokens",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "ARMAN + PEYMA",
                "result": 95.13
            },
            {
                "test": "PEYMA",
                "result": 98.79
            },
            {
                "test": "ARMAN",
                "result": 93.1
            }
        ],
        "hardware": "Tensorflow Research Cloud (TFRC) program",
        "limitation_and_bias": "ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes. Organization, Location, Facility, Event, Product, Person. Label: Organization 30108, Location 12924, Facility 4458, Event 7557, Product 4389, Person 15645. Download: You can download the dataset from [here](https://github.com/HaniehP/PersianNER).",
        "demo": "[How to use Pipelines](https://github.com/hooshvare/parsbert-ner/blob/master/persian-ner-pipeline.ipynb)",
        "input_format": "PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes. | Label | # | Organization | 16964 | Money | 2037 | Location | 8782 | Date | 4259 | Time | 732 | Person | 7675 | Percent | 699 | You can download the dataset from [here](http://nsurl.org/tasks/task-7-named-entity-recognition-ner-for-farsi/)",
        "output_format": "PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes. | Organization | 16964 | Money | 2037 | Location | 8782 | Date | 4259 | Time | 732 | Person | 7675 | Percent | 699 | You can download the dataset from [here](http://nsurl.org/tasks/task-7-named-entity-recognition-ner-for-farsi/)",
        "input_token_limit": "302,530 tokens",
        "vocabulary_size": "7,682 sentences with 250,015 sentences tagged"
    },
    "hf-internal-testing/tiny-random-SqueezeBertModel": "404 Client Error. (Request ID: Root=1-653dfb3c-59e6b5fa46d5021507810627)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-SqueezeBertModel/resolve/main/README.md.",
    "lllyasviel/control_v11p_sd15_canny": "This model's maximum context length is 4097 tokens, however you requested 4291 tokens (4035 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "google/flan-t5-small": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "qrecc",
            "wiki_dialog",
            "code_contests",
            "gsm8k",
            "aqua_rat",
            "esnli",
            "quasc",
            "qed"
        ],
        "license": "Creative Commons Attribution 4.0 International",
        "github": "- svakulenk0/qrecc\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed",
        "paper": "original paper, figure 2",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: ![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png) For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).",
                "result": 0
            }
        ],
        "hardware": "5. [Training Details]",
        "limitation_and_bias": "4. [Bias, Risks, and Limitations]",
        "demo": "Find below some example scripts on how to use the model in `transformers`:",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "stabilityai/sdxl-vae": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "https://huggingface.co/stabilityai/sdxl-vae/blob/main/sdxl_vae.safetensors",
            "https://ommer-lab.com/files/latent-diffusion/kl-f8.zip",
            "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt"
        ],
        "license": "mit",
        "github": "https://huggingface.co/stabilityai/sdxl-vae/blob/main/sdxl_vae.safetensors",
        "paper": "https://arxiv.org/abs/2112.10752",
        "upstream_model": "Stable Diffusion",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "SDXL, latent diffusion model, autoencoder architecture, exponential moving average (EMA)",
        "output_format": "SDXL"
    },
    "deepset/sentence_bert": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [],
        "license": "apache-2.0",
        "github": "https://github.com/UKPLab/sentence-transformers",
        "paper": "\"Sentence Transformers Repo (https://github.com/UKPLab/sentence-transformers)\"",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "lllyasviel/control_v11p_sd15_openpose": {
        "model_type": "computer-vision",
        "model_tasks": "image-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "Stable Diffusion v1-5",
            "lllyasviel/control_v11p_sd15_canny",
            "lllyasviel/control_v11e_sd15_ip2p",
            "lllyasviel/control_v11p_sd15_inpaint",
            "lllyasviel/control_v11p_sd15_mlsd",
            "lllyasviel/control_v11f1p_sd15_depth"
        ],
        "license": "openrail",
        "github": "https://github.com/lllyasviel/ControlNet-v1-1-nightly",
        "paper": "*Adding Conditional Control to Text-to-Image Diffusion Models* by Lvmin Zhang, Maneesh Agrawala.",
        "upstream_model": "Stable Diffusion v1-5",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "Stable Diffusion v1-5",
        "limitation_and_bias": "(1) a small group of greyscale human images are duplicated thousands of times (!!), causing the previous model somewhat likely to generate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG artifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data processing scripts.",
        "demo": "[Diffusers ControlNet Blog Post](https://huggingface.co/blog/controlnet) and [official docs](https://github.com/lllyasviel/ControlNet-v1-1-nightly).",
        "input_format": "\"A monochrome image with white edges on a black background.\", \"No condition.\", \"An image with depth information, usually represented as a grayscale image.\", \"An image with surface normal information, usually represented as a color-coded image.\", \"An image with segmented regions, usually represented as a color-coded image.\", \"An image with line art, usually black lines on a white background.\", \"An image with anime-style line art.\", \"An image with human poses, usually represented as a set of keypoints or skeletons.\", \"An image with scribbles, usually random or user-drawn strokes.\", \"An image with soft edges, usually to create a more painterly or artistic effect.\", \"An image with shuffled patches or regions.\"",
        "output_format": "\"Generated Image Example\"",
        "input_preprocessing": "\"canny edge detection\", \"pixel to pixel instruction\", \"image inpainting\", \"multi-level line segment detection\", \"depth estimation\", \"surface normal estimation\", \"image segmentation\", \"line art generation\", \"anime line art generation\", \"human pose estimation\", \"scribble-based image generation\", \"soft edge image generation\", \"image shuffling\"",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "facebook/opt-125m": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "BookCorpus",
            "CC-Stories",
            "Pile-CC",
            "OpenWebText2",
            "USPTO",
            "Project Gutenberg",
            "OpenSubtitles",
            "Wikipedia",
            "DM Mathematics",
            "HackerNews",
            "Pushshift.io Reddit dataset",
            "CCNewsV2"
        ],
        "license": "other",
        "github": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "paper": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs.",
        "upstream_model": "Meta AI's model card and OPT-175B",
        "parameter_count": "GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days",
        "hyper_parameters": [
            {
                "epochs": "Meta AI's model card",
                "batch_size": "training data used for this model contains a lot of unfiltered content from the internet",
                "learning_rate": "OPT-175B has limitations in terms of bias and safety",
                "optimizer": "OPT-175B can also have quality issues in terms of generation diversity and hallucination"
            }
        ],
        "evaluation": [
            {
                "test": "As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased",
                "result": 0.0
            }
        ],
        "hardware": "Meta AI's model card and training data used for this model contains a lot of unfiltered content from the internet",
        "limitation_and_bias": "As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased",
        "demo": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "input_format": "BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset",
        "output_format": "",
        "input_token_limit": "GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*",
        "vocabulary_size": "180B tokens corresponding to 800GB of data"
    },
    "vennify/t5-base-grammar-correction": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "JFLEG"
        ],
        "license": "cc-by-nc-sa-4.0",
        "github": "vennify/t5-base-grammar-correction",
        "paper": "arxiv.org/abs/1702.04066",
        "upstream_model": "T5",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "xlnet-base-cased": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "bookcorpus",
            "wikipedia"
        ],
        "license": "mit",
        "github": "https://github.com/zihangdai/xlnet/",
        "paper": "https://arxiv.org/abs/1906.08237",
        "upstream_model": "XLNet model pre-trained on English language",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "NO_OUTPUT",
        "demo": "See the [model hub](https://huggingface.co/models?search=xlnet)",
        "input_format": "",
        "output_format": "return_tensors=\"pt\"",
        "input_token_limit": "",
        "vocabulary_size": "NO_OUTPUT"
    },
    "hf-internal-testing/tiny-random-IBertModel": "404 Client Error. (Request ID: Root=1-653dfc4f-178e70010d04fc366ca3fdfd)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-IBertModel/resolve/main/README.md.",
    "timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "timm",
            "pytorch",
            "safetensors"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "flair/ner-english-ontonotes": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "flair"
        ],
        "datasets": [
            "ontonotes"
        ],
        "license": "license",
        "github": "github",
        "paper": "Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}",
        "upstream_model": "SequenceTagger",
        "parameter_count": "hidden_size=256, max_epochs=150",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "F1-Score",
                "result": 89.27
            }
        ],
        "hardware": "",
        "limitation_and_bias": "\"tag_type = 'ner'\", \"embedding_types = [WordEmbeddings('en-crawl'), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')]\", \"embeddings = StackedEmbeddings(embeddings=embedding_types)\", \"tagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type)\", \"trainer = ModelTrainer(tagger, corpus)\", \"trainer.train('resources/taggers/ner-english-ontonotes', train_with_dev=True, max_epochs=150)\".",
        "demo": "Find a form of demo",
        "input_format": "column_format={0: \"text\", 1: \"pos\", 2: \"upos\", 3: \"ner\"}",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
    },
    "sentence-transformers/msmarco-MiniLM-L-12-v3": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "jax",
            "sentence-transformers",
            "transformers"
        ],
        "datasets": [
            "Sentence Embeddings Benchmark"
        ],
        "license": "apache-2.0",
        "github": "https://seb.sbert.net?model_name=sentence-transformers/msmarco-MiniLM-L-12-v3",
        "paper": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "upstream_model": "sentence-transformers",
        "parameter_count": "'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "Sentence Embeddings Benchmark",
                "result": 0.0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "https://seb.sbert.net?model_name=sentence-transformers/msmarco-MiniLM-L-12-v3",
        "input_format": "'max_seq_length': 512, 'do_lower_case': False",
        "output_format": "'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False",
        "input_token_limit": "'max_seq_length': 512",
        "vocabulary_size": ""
    },
    "Rostlab/prot_bert": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Uniref100"
        ],
        "license": "",
        "github": "https://github.com/agemagician/ProtTrans",
        "paper": "https://doi.org/10.1101/2020.07.12.199554",
        "upstream_model": "",
        "parameter_count": "single TPU Pod V3-512, 400k steps, sequence length 512 (batch size 15k), sequence length 2048 (batch size 2.5k), optimizer: Lamb, learning rate: 0.002, weight decay: 0.01, learning rate warmup: 40k steps, linear decay of learning rate after.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "0.002",
            "optimizer": "Lamb"
        },
        "evaluation": [
            {
                "test": "secondary structure (3-states)",
                "result": 75
            },
            {
                "test": "secondary structure (8-states)",
                "result": 63
            },
            {
                "test": "Localization",
                "result": 79
            },
            {
                "test": "Membrane",
                "result": 91
            }
        ],
        "hardware": "single TPU Pod V3-512, 400k steps, sequence length 512, batch size 15k, sequence length 2048, batch size 2.5k, Lamb, learning rate 0.002, weight decay 0.01, learning rate warmup 40k steps, linear decay learning rate after",
        "limitation_and_bias": "When fine-tuned on downstream tasks, this model achieves the following results: Test results : | Task/Dataset | secondary structure (3-states) | secondary structure (8-states)  |  Localization | Membrane  | |:-----:|:-----:|:-----:|:-----:|:-----:| |   CASP12  | 75 | 63 |    |    | |   TS115   | 83 | 72 |    |    | |   CB513   | 81 | 66 |    |    | |  DeepLoc  |    |    | 79 | 91 |",
        "demo": "Availability ProtTrans: <a href=\"https://github.com/agemagician/ProtTrans\">https://github.com/agemagician/ProtTrans</a>",
        "input_format": "The protein sequences are uppercased and tokenized using a single space and a vocabulary size of 21. The rare amino acids \"U,Z,O,B\" were mapped to \"X\". The inputs of the model are then of the form: [CLS] Protein Sequence A [SEP] Protein Sequence B [SEP]. Furthermore, each protein sequence was treated as a separate document. The preprocessing step was performed twice, once for a combined length (2 sequences) of less than 512 amino acids, and another time using a combined length (2 sequences) of less than 2048 amino acids. The details of the masking procedure for each sequence followed the original Bert model as following: - 15% of the amino acids are masked. - In 80% of the cases, the masked amino acids are replaced by `[MASK]`. - In 10% of the cases, the masked amino acids are replaced by a random amino acid (different) from the one they replace. - In the 10% remaining cases, the masked amino acids are left as is.",
        "output_format": "The protein sequences are uppercased and tokenized using a single space and a vocabulary size of 21. The rare amino acids \"U,Z,O,B\" were mapped to \"X\". The inputs of the model are then of the form: [CLS] Protein Sequence A [SEP] Protein Sequence B [SEP]. Furthermore, each protein sequence was treated as a separate document. The preprocessing step was performed twice, once for a combined length (2 sequences) of less than 512 amino acids, and another time using a combined length (2 sequences) of less than 2048 amino acids. The details of the masking procedure for each sequence followed the original Bert model as following: - 15% of the amino acids are masked. - In 80% of the cases, the masked amino acids are replaced by `[MASK]`. - In 10% of the cases, the masked amino acids are replaced by a random amino acid (different) from the one they replace. - In the 10% remaining cases, the masked amino acids are left as is.",
        "input_token_limit": "",
        "vocabulary_size": "vocabulary size of 21, [MASK]"
    },
    "gilf/french-camembert-postag-model": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "free-french-treebank"
        ],
        "license": "",
        "github": "https://github.com/nicolashernandez/free-french-treebank",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "magorshunov/layoutlm-invoices": {
        "model_type": "multimodal",
        "model_tasks": "document-question-answering",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "proprietary dataset of invoices",
            "SQuAD2.0",
            "DocVQA"
        ],
        "license": "cc-by-nc-sa-4.0",
        "github": "https://github.com/impira/docquery",
        "paper": "This model was created by the team at [Impira](https://www.impira.com/).",
        "upstream_model": "upstream_model",
        "parameter_count": "parameter_count",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "QA models often encounter this failure mode",
        "demo": "DocQuery https://github.com/impira/docquery",
        "input_format": "\"pipeline_tag: document-question-answering\" and \"src: https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\" and \"src: https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg\"",
        "output_format": ""
    },
    "oliverguhr/german-sentiment-bert": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "1.834 million German-language samples"
        ],
        "license": "mit",
        "github": "[this repository](https://github.com/oliverguhr/german-sentiment)",
        "paper": "[paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf)",
        "upstream_model": "Googles Bert architecture",
        "parameter_count": "1.834 million German-language samples",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "[holidaycheck](https://github.com/oliverguhr/german-sentiment)",
                "result": 0.9568
            },
            {
                "test": "[scare](https://www.romanklinger.de/scare/)",
                "result": 0.9418
            },
            {
                "test": "[filmstarts](https://github.com/oliverguhr/german-sentiment)",
                "result": 0.9021
            },
            {
                "test": "[germeval](https://sites.google.com/view/germeval2017-absa/home)",
                "result": 0.7536
            },
            {
                "test": "[PotTS](https://www.aclweb.org/anthology/L16-1181/)",
                "result": 0.678
            },
            {
                "test": "[emotions](https://github.com/oliverguhr/german-sentiment)",
                "result": 0.9649
            }
        ],
        "hardware": "",
        "limitation_and_bias": "The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.",
        "demo": "Python package that bundles the code need for the preprocessing and inferencing",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "openai/whisper-tiny.en": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "LibriSpeech"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/openai/whisper",
        "paper": "https://cdn.openai.com/papers/whisper.pdf",
        "upstream_model": "",
        "parameter_count": "39 M, 74 M, 244 M, 769 M, 1550 M",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model.",
        "demo": "The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet.",
        "input_format": "",
        "output_format": ""
    },
    "hf-internal-testing/tiny-random-bert": "404 Client Error. (Request ID: Root=1-653dfdb6-661424467d67583048d16855)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-bert/resolve/main/README.md.",
    "dreamlike-art/dreamlike-photoreal-2.0": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [],
        "license": "modified CreativeML OpenRAIL-M",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "roberta-large-mnli": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Multi-Genre Natural Language Inference (MNLI)",
            "BookCorpus",
            "English Wikipedia",
            "CC-News",
            "OpenWebText",
            "Stories"
        ],
        "license": "mit",
        "github": "https://github.com/facebookresearch/fairseq/tree/main/examples/roberta",
        "paper": "https://arxiv.org/abs/1907.11692",
        "upstream_model": "",
        "parameter_count": "500K",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "flair/ner-english": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "flair"
        ],
        "datasets": [
            "conll2003"
        ],
        "license": "license",
        "github": "[here](https://github.com/flairNLP/flair/issues/)",
        "paper": "Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}",
        "upstream_model": "upstream_model",
        "parameter_count": "parameter_count",
        "hyper_parameters": [
            {
                "epochs": "150",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "F1-Score: **93,06** (corrected CoNLL-03)",
                "result": 93.06
            }
        ],
        "hardware": "WordEmbeddings('glove'), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward'), StackedEmbeddings(embeddings=embedding_types), SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type), ModelTrainer(tagger, corpus)",
        "limitation_and_bias": "F1-Score: **93,06** (corrected CoNLL-03)",
        "demo": "Find a form of demo",
        "input_format": "Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.",
        "output_format": "F1-Score: **93,06** (corrected CoNLL-03) \"Predicts 4 tags: | **tag**                        | **meaning** | |---------------------------------|-----------| | PER         | person name | | LOC         | location name | | ORG         | organization name | | MISC         | other name |  Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\"",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-resnet": "404 Client Error. (Request ID: Root=1-653dfe3a-4a082fa82cecb53f5e30d177)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-resnet/resolve/main/README.md.",
    "openai/whisper-large-v2": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "680,000 hours of audio and the corresponding transcripts collected from the internet"
        ],
        "license": "arXiv.org perpetual, non-exclusive license",
        "github": "https://github.com/huggingface/transformers",
        "paper": "https://cdn.openai.com/papers/whisper.pdf",
        "upstream_model": "",
        "parameter_count": "39 M, 74 M, 244 M, 769 M, 1550 M",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "ASR and speech translation to English tasks",
                "result": 10
            }
        ],
        "hardware": "",
        "limitation_and_bias": "The models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English. However, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). The models perform unevenly across languages and exhibit lower accuracy on low-resource and/or low-discoverability languages or languages with less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts.",
        "demo": "The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. They show strong ASR results in ~10 languages.",
        "input_format": "Audio",
        "output_format": "Transcriptions"
    },
    "jonatasgrosman/wav2vec2-large-xlsr-53-polish": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Common Voice 6.1"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/jonatasgrosman/wav2vec2-sprint",
        "paper": "Fine-tuned {XLSR}-53 large model for speech recognition in {P}olish",
        "upstream_model": "facebook/wav2vec2-large-xlsr-53",
        "parameter_count": "53",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/)",
        "limitation_and_bias": "",
        "demo": "Fine-tuned {XLSR}-53 large model for speech recognition in {P}olish",
        "input_format": "16kHz",
        "output_format": "16kHz",
        "sample_rate": "",
        "WER": ""
    },
    "Helsinki-NLP/opus-mt-nl-en": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/nl-en/README.md",
        "paper": "",
        "upstream_model": "transformer-align",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "Tatoeba.nl.en",
                "result": 60.9
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/nl-en/README.md",
        "input_format": "SentencePiece + normalization",
        "output_format": "SentencePiece",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "monologg/koelectra-small-v2-distilled-korquad-384": "404 Client Error. (Request ID: Root=1-653dfebd-2b786fd804c49e19564951ed)\n\nEntry Not Found for url: https://huggingface.co/monologg/koelectra-small-v2-distilled-korquad-384/resolve/main/README.md.",
    "EleutherAI/gpt-neo-125m": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "Pile"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "openai/clip-vit-large-patch14-336": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "hf-internal-testing/tiny-random-LongT5Model": "404 Client Error. (Request ID: Root=1-653dfef8-2175286b572bf4050df7388f)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-LongT5Model/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-Speech2TextModel": "404 Client Error. (Request ID: Root=1-653dfef8-616e66332224c1bc1196a291)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Speech2TextModel/resolve/main/README.md.",
    "KoboldAI/OPT-6.7B-Nerybus-Mix": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "KoboldAI/OPT-6.7B-Erebus",
            "KoboldAI/OPT-6B-nerys-v2"
        ],
        "license": "other",
        "github": "",
        "paper": "",
        "upstream_model": "parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*",
        "parameter_count": "parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "FP16, KoboldAI software",
        "limitation_and_bias": "parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*",
        "demo": "parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*",
        "input_format": "parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B",
        "output_format": "parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*",
        "input_token_limit": "parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B",
        "vocabulary_size": "parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B"
    },
    "nlpaueb/legal-bert-small-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "US contracts",
            "EU legislation",
            "ECHR cases",
            "All"
        ],
        "license": "cc-by-sa-4.0",
        "github": "http://eur-lex.europa.eu, http://www.legislation.gov.uk, http://hudoc.echr.coe.int/eng, https://case.law, https://www.sec.gov/edgar.shtml",
        "paper": "\"LEGAL-BERT: The Muppets straight out of Law School\". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)",
        "upstream_model": "\"nlpaueb/legal-bert-small-uncased\"",
        "parameter_count": "\"110M parameters\"",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "\"Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)\"",
                "result": 0
            }
        ],
        "hardware": "\"single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc)\"",
        "limitation_and_bias": "\"LEGAL-BERT: The Muppets straight out of Law School\". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)\"",
        "demo": "\"nlpaueb/bert-base-uncased-contracts\", \"nlpaueb/bert-base-uncased-eurlex\", \"nlpaueb/bert-base-uncased-echr\", \"nlpaueb/legal-bert-base-uncased\", \"nlpaueb/legal-bert-small-uncased\", \"https://archive.org/details/legal_bert_fp\"",
        "input_format": "\"nlpaueb/legal-bert-base-uncased\" and \"a model trained from scratch in the legal corpora mentioned below using a sentence-piece tokenizer trained on the very same corpora.\"",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "LEGAL-BERT-BASE * `nlpaueb/legal-bert-base-uncased` All sentence-piece tokenizer"
    },
    "kredor/punctuate-all": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "English",
            "German",
            "French",
            "Spanish",
            "Bulgarian",
            "Italian",
            "Polish",
            "Dutch",
            "Czech",
            "Portugese",
            "Slovak",
            "Slovenian"
        ],
        "license": "",
        "github": "https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large",
        "paper": "https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large",
        "upstream_model": "xlm-roberta-base",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "microsoft/infoxlm-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "https://github.com/microsoft/unilm/tree/master/infoxlm",
        "paper": "https://arxiv.org/pdf/2007.07834.pdf",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "[repo](https://github.com/microsoft/unilm/tree/master/infoxlm), [model](https://huggingface.co/microsoft/infoxlm-base)",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "\"config.json\", \"pytorch_model.bin\", \"sentencepiece.bpe.model\", \"tokenizer.json\""
    },
    "lllyasviel/sd-controlnet-canny": {
        "model_type": "computer-vision",
        "model_tasks": "image-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "3M edge-image, caption pairs"
        ],
        "github": "[lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny), [lllyasviel/sd-controlnet-depth](https://huggingface.co/lllyasviel/sd-controlnet-depth), [lllyasviel/sd-controlnet-hed](https://huggingface.co/lllyasviel/sd-controlnet-hed), [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-mlsd), [lllyasviel/sd-controlnet-normal](https://huggingface.co/lllyasviel/sd-controlnet-normal), [lllyasviel/sd-controlnet_openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose)",
        "paper": "\"3M edge-image, caption pairs\" and \"Stable Diffusion 1.5 as a base model\"",
        "upstream_model": "Stable Diffusion 1.5",
        "parameter_count": "3M",
        "hardware": "Nvidia A100 80G",
        "limitation_and_bias": "\"We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k).\"",
        "demo": "\"link, code snippet or short paragraph\"",
        "input_format": "\"canny edge detection\", \"Midas depth estimation\", \"HED edge detection (soft edge)\", \"M-LSD line detection\", \"normal map\", \"OpenPose bone image\", \"human scribbles\", \"ADE20K's segmentation protocol image\"",
        "output_format": "Generated Image Example"
    },
    "tiiuae/falcon-40b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "tiiuae/falcon-refinedweb"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-SegformerModel": "404 Client Error. (Request ID: Root=1-653dffe3-4edc6f5b240548e54e7cee37)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-SegformerModel/resolve/main/README.md.",
    "Hello-SimpleAI/chatgpt-detector-roberta": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Hello-SimpleAI/HC3"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "tensorboard",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "financial_phrasebank"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "distilroberta-base",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "stabilityai/stable-diffusion-2-inpainting": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "LAION-2B(en)"
        ],
        "license": "openrail++",
        "github": "https://laion.ai/blog/laion-5b/",
        "paper": "Research on generative models.",
        "upstream_model": "Stable Diffusion v1, DALL-E Mini model card",
        "parameter_count": "50",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "Generation of artworks and use in design and other artistic processes.",
        "input_format": "LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant",
        "output_format": ""
    },
    "vicgalle/xlm-roberta-large-xnli-anli": {
        "model_type": "natural-language-processing",
        "model_tasks": "zero-shot-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "mnli",
            "xnli",
            "anli"
        ],
        "license": "mit",
        "github": "language: multilingual, license: mit, tags: - zero-shot-classification - nli - pytorch, datasets: - mnli - xnli - anli, pipeline_tag: zero-shot-classification, widget: - text: De pugna erat fantastic. Nam Crixo decem quam dilexit et praeciderunt caput aemulus. candidate_labels: violent, peaceful - text: La pel\u00edcula empezaba bien pero termin\u00f3 siendo un desastre. candidate_labels: positivo, negativo, neutral - text: La pel\u00edcula empez\u00f3 siendo un desastre pero en general fue bien. candidate_labels: positivo, negativo, neutral - text: \u00bfA qui\u00e9n vas a votar en 2020? candidate_labels: Europa, elecciones, pol\u00edtica, ciencia, deportes",
        "paper": "- zero-shot-classification\n- nli\n- pytorch\n- mnli\n- xnli\n- anli\n- pipeline_tag: zero-shot-classification",
        "upstream_model": "\"XLM-RoBERTa-large model finetunned over several NLI datasets\"",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "XNLI-es",
                "result": 93.7
            },
            {
                "test": "XNLI-fr",
                "result": 93.2
            },
            {
                "test": "ANLI-R1",
                "result": 68.5
            },
            {
                "test": "ANLI-R2",
                "result": 53.6
            },
            {
                "test": "ANLI-R3",
                "result": 49.0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "\"XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classification. Here are the accuracies for several test datasets: | XNLI-es | XNLI-fr | ANLI-R1 | ANLI-R2 | ANLI-R3 | | xlm-roberta-large-xnli-anli | 93.7% | 93.2% | 68.5%  | 53.6%  | 49.0%  | The model can be loaded with the zero-shot-classification pipeline like so: from transformers import pipeline classifier = pipeline(\"zero-shot-classification\", model=\"vicgalle/xlm-roberta-large-xnli-anli\") You can then use this pipeline to classify sequences into any of the class names you specify: sequence_to_classify = \"Alg\u00fan d\u00eda ir\u00e9 a ver el mundo\" candidate_labels = ['viaje', 'cocina', 'danza'] classifier(sequence_to_classify, candidate_labels) #{'sequence': 'Alg\u00fan"
    },
    "hustvl/yolos-small": {
        "model_type": "computer-vision",
        "model_tasks": "object-detection",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ImageNet-1k",
            "COCO 2017 object detection"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/hustvl/YOLOS",
        "paper": "https://arxiv.org/abs/2106.00666",
        "upstream_model": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al. and first released in [this repository](https://github.com/hustvl/YOLOS).",
        "parameter_count": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images)",
        "hyper_parameters": {
            "epochs": "200 epochs",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "AP (average precision) of **36.1** on COCO 2017 validation",
                "result": 36.1
            }
        ],
        "hardware": "ImageNet-1k, COCO",
        "limitation_and_bias": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images), [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al., [this repository](https://github.com/hustvl/YOLOS).",
        "demo": "\"model hub\" \"huggingface.co/models?search=hustvl/yolos\"",
        "input_format": "ImageNet-1k, COCO 2017 object detection",
        "output_format": "\"return_tensors=\\\"pt\\\"\" and \"PyTorch.\"",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "Salesforce/codet5p-220m": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "span denoising",
            "two variants of causal language modeling"
        ],
        "license": "bsd-3-clause",
        "github": "[github-code dataset](https://huggingface.co/datasets/codeparrot/github-code)",
        "paper": "\"span denoising\" and \"two variants of _causal language modeling_\"",
        "upstream_model": "\"CodeT5+ models\"",
        "parameter_count": "\"CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_.\" \"In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters.\" \"InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode\"",
        "hyper_parameters": "\"CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_.\"",
        "evaluation": [
            {
                "test": "CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_. Specifically, CodeT5+ yields substantial performance gains on many downstream tasks compared to their SoTA baselines, e.g., 8 text-to-code retrieval tasks (+3.2 avg. MRR), 2 line-level code completion tasks (+2.1 avg. Exact Match), and 2 retrieval-augmented code generation tasks (+5.8 avg. BLEU-4). In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters. Particularly, in the zero-shot text-to-code generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode.",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_. In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters. Particularly, in the zero-shot text-to-code generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode.",
        "demo": "CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_. In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters. Particularly, in the zero-shot text-to-code generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode.",
        "input_format": "\"reserving only permissively licensed code (\"mit\" \u201capache-2\u201d, \u201cbsd-3-clause\u201d, \u201cbsd-2-clause\u201d, \u201ccc0-1.0\u201d, \u201cunlicense\u201d, \u201cisc\u201d). Supported languages (9 in total) are as follows: `c`, `c++`, `c-sharp`,  `go`, `java`, `javascript`,  `php`, `python`, `ruby.\"",
        "output_format": "\"github-code dataset\", \"reserving only permissively licensed code\", \"`c`, `c++`, `c-sharp`,  `go`, `java`, `javascript`,  `php`, `python`, `ruby.\"",
        "input_token_limit": "10",
        "vocabulary_size": ""
    },
    "Helsinki-NLP/opus-mt-en-es": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Tatoeba-test.eng.spa"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa/README.md",
        "paper": "",
        "upstream_model": "transformer",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Salesforce/codegen-350M-mono": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "BigPython"
        ],
        "license": "bsd-3-clause",
        "github": "https://github.com/salesforce/CodeGen",
        "paper": "https://arxiv.org/abs/2203.13474",
        "upstream_model": "autoregressive language model",
        "parameter_count": "CodeGen-Multi 350M",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "code generation benchmark",
                "result": 0.0
            }
        ],
        "hardware": "multiple TPU-v4-512 by Google",
        "limitation_and_bias": "autoregressive language model, program synthesis, generating executable code given English prompts, prompts should be in the form of a comment string, model can complete partially-generated code as well",
        "demo": "program synthesis",
        "input_format": "CodeGen-Multi 350M, BigPython dataset, 71.7B tokens of Python programming language, Section 2.1 of the paper",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "71.7B tokens of Python programming language"
    },
    "flair/upos-english": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "flair"
        ],
        "datasets": [
            "ontonotes"
        ],
        "license": "license",
        "github": "github",
        "paper": "Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}",
        "upstream_model": "upstream_model",
        "parameter_count": "parameter_count",
        "hyper_parameters": [
            {
                "epochs": "150",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "tag_type = 'upos'",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.",
        "demo": "Find a form of demo",
        "input_format": "column_format={0: \"text\", 1: \"pos\", 2: \"upos\", 3: \"ner\"}",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Common Voice 6.1"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/jonatasgrosman/wav2vec2-sprint",
        "paper": "Fine-tuned {XLSR}-53 large model for speech recognition in {P}ortuguese",
        "upstream_model": "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese",
        "parameter_count": "53",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "wer",
                "result": 11.31
            }
        ],
        "hardware": "OVHcloud, GPU credits",
        "limitation_and_bias": "",
        "demo": "Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-portuguese\")\n```\n",
        "input_format": "16kHz",
        "output_format": ""
    },
    "ckiplab/bert-base-chinese-ws": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "https://github.com/ckiplab/ckip-transformers"
        ],
        "license": "gpl-3.0",
        "github": "https://github.com/ckiplab/ckip-transformers",
        "paper": "Author & Maintainer",
        "upstream_model": "upstream_model",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "https://github.com/ckiplab/ckip-transformers",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "avichr/heBERT_NER": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Ben Mordecai and M Elhadad (2005)"
        ],
        "license": "Ben Mordecai and M Elhadad (2005)",
        "github": "Ben Mordecai and M Elhadad (2005)",
        "paper": "Ben Mordecai and M Elhadad (2005)",
        "upstream_model": "heBERT",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {
            "epochs": "NO_OUTPUT",
            "batch_size": "NO_OUTPUT",
            "learning_rate": "NO_OUTPUT",
            "optimizer": "NO_OUTPUT"
        },
        "evaluation": [
            {
                "test": "Ben Mordecai and M Elhadad (2005)",
                "result": 0
            }
        ],
        "hardware": "Google's BERT architecture and BERT-Base config",
        "limitation_and_bias": "Ben Mordecai and M Elhadad (2005)",
        "demo": "[huggingface spaces](https://huggingface.co/spaces/avichr/HebEMO_demo) or as [colab notebook](https://colab.research.google.com/drive/1Jw3gOWjwVMcZslu-ttXoNeD17lms1-ff?usp=sharing)",
        "input_format": "Google's BERT, BERT-Base config, A Hebrew version of OSCAR, A Hebrew dump of Wikipedia, Emotion User Generated Content (UGC) data",
        "output_format": "Ben Mordecai and M Elhadad (2005)",
        "input_token_limit": "Google's BERT, BERT-Base config",
        "vocabulary_size": "Ben Mordecai and M Elhadad (2005)"
    },
    "openai-gpt": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "BooksCorpus"
        ],
        "license": "mit",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "8 P600 GPU's * 30 days * 12 TFLOPS/GPU * 0.33 utilization = .96 pfs-days",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "8 P600 GPU's * 30 days",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "40,000 merges"
    },
    "microsoft/deberta-v2-xlarge": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "80GB training data",
            "160GB raw data"
        ],
        "license": "mit",
        "github": "If you find DeBERTa useful for your work, please cite the following paper: ``` latex @inproceedings{he2021deberta, title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION}, author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen}, booktitle={International Conference on Learning Representations}, year={2021}, url={https://openreview.net/forum?id=XPZIaotutsD} }```",
        "paper": "If you find DeBERTa useful for your work, please cite the following paper: \n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}",
        "upstream_model": "DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION",
        "parameter_count": "900M and total parameters",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "24 layers, 1536 hidden size 160GB raw data",
        "limitation_and_bias": "",
        "demo": "If you find DeBERTa useful for your work, please cite the following paper:",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-NystromformerModel": "404 Client Error. (Request ID: Root=1-653e024d-4b0ef0d449c32a714a6a07cb)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-NystromformerModel/resolve/main/README.md.",
    "klue/bert-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "modeling architecture (BERT)",
            "training details"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "topic classification, semantic textual similarity, natural language inference, named entity recognition",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "modeling architecture (BERT), compute infrastructure",
        "limitation_and_bias": "- Bias issues with the publicly available data used in the pretraining corpora (and considerations related to filtering)\n- PII in the data used in the pretraining corpora (and efforts to pseudonymize the data)",
        "demo": "The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition, and other tasks outlined in the [KLUE Benchmark](https://github.com/KLUE-benchmark/KLUE).",
        "input_format": "We design and use a new tokenization method, morpheme-based subword tokenization. When building a vocabulary, we pre-tokenize a raw text into morphemes using a morphological analyzer, and then we apply byte pair encoding (BPE) ([Senrich et al., 2016](https://aclanthology.org/P16-1162/)) to get the final vocabulary. For morpheme segmentation, we use [Mecab-ko](https://bitbucket.org/eunjeon/mecab-ko), MeCab ([Kudo, 2006](https://taku910.github.io/mecab/)) adapted for Korean, and for BPE segmentation, we use the wordpiece tokenizer from [Huggingface Tokenizers library](https://github.com/huggingface/tokenizers). We specify the vocabulary size to 32k.",
        "output_format": ""
    },
    "flax-community/spanish-t5-small": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "tensorboard",
            "pytorch"
        ],
        "datasets": [
            "large_spanish_corpus"
        ],
        "license": "",
        "github": "[large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus), [Flax](https://github.com/google/flax), [Flax/Jax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), [HuggingFace](https://huggingface.co/)",
        "paper": "",
        "upstream_model": "T5",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "NO_OUTPUT",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "NO_OUTPUT"
    },
    "facebook/opt-350m": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "BookCorpus",
            "CC-Stories",
            "Pile-CC",
            "OpenWebText2",
            "USPTO",
            "Project Gutenberg",
            "OpenSubtitles",
            "Wikipedia",
            "DM Mathematics",
            "HackerNews",
            "Pushshift.io Reddit dataset",
            "CCNewsV2"
        ],
        "license": "other",
        "github": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "paper": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs.",
        "upstream_model": "facebook/opt-350m",
        "parameter_count": "GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days",
        "hyper_parameters": [
            {
                "epochs": "BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit dataset, CCNewsV2",
                "batch_size": "NO_OUTPUT",
                "learning_rate": "NO_OUTPUT",
                "optimizer": "NO_OUTPUT"
            },
            {
                "epochs": "facebook/opt-350m",
                "batch_size": "do_sample=True, num_return_sequences=5",
                "learning_rate": "NO_OUTPUT",
                "optimizer": "NO_OUTPUT"
            }
        ],
        "evaluation": [
            {
                "test": "BookCorpus, which consists of more than 10K unpublished books",
                "result": 0
            },
            {
                "test": "CC-Stories, which contains a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas",
                "result": 0
            },
            {
                "test": "The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included",
                "result": 0
            },
            {
                "test": "Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in Roller et al. (2021)",
                "result": 0
            },
            {
                "test": "CCNewsV2 containing an updated version of the English portion of the CommonCrawl News dataset that was used in RoBERTa (Liu et al., 2019b)",
                "result": 0
            },
            {
                "test": "The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally to each dataset\u2019s size in the pretraining corpus",
                "result": 0
            }
        ],
        "hardware": [
            "GPT2, 80GB A100 GPUs, ~33 days",
            "BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset"
        ],
        "limitation_and_bias": [
            "As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased : Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. Here's an example of how the model can have biased predictions: compared to: This bias will also affect all fine-tuned versions of this model.",
            "known challenges in areas such as robustness, bias, and toxicity"
        ],
        "demo": [
            "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
            "[CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling), [model hub](https://huggingface.co/models?filter=opt)"
        ],
        "input_format": [
            "BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset",
            "GPT2 byte-level version of Byte Pair Encoding (BPE), vocabulary size of 50272, inputs are sequences of 2048 consecutive tokens. input_format"
        ],
        "output_format": [],
        "input_token_limit": [
            "GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*"
        ],
        "vocabulary_size": [
            "180B tokens corresponding to 800GB of data",
            "GPT2, vocabulary size of 50272"
        ]
    },
    "facebook/roscoe-512-roberta-base": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Entailment-Bank (deductive reasoning)",
            "ProofWriter (logical reasoning)",
            "MATH",
            "ASDIV",
            "AQUA",
            "EQASC (explanations for commonsense question answering)",
            "StrategyQA (question answering with implicit reasoning strategies)"
        ],
        "license": "cc-by-nc-4.0",
        "github": "",
        "paper": "arXiv preprint arXiv:2104.08821, 2021.",
        "upstream_model": "RoBERTa word embedding model (Liu et al., 2019)",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [],
        "hardware": "RoBERTa word embedding model (Liu et al., 2019)",
        "limitation_and_bias": "deterministic modifications on reference reasoning steps in Entailment-Bank (deductive reasoning), ProofWriter (logical reasoning), three arithmetic reasoning datasets MATH, ASDIV and AQUA, EQASC (explanations for commonsense question answering), StrategyQA (question answering with implicit reasoning strategies)",
        "demo": "",
        "input_format": "Entailment-Bank (deductive reasoning), ProofWriter (logical reasoning), three arithmetic reasoning datasets MATH, ASDIV and AQUA, EQASC (explanations for commonsense question answering), and StrategyQA (question answering with implicit reasoning strategies)",
        "output_format": ""
    },
    "hf-internal-testing/tiny-random-MPNetModel": "404 Client Error. (Request ID: Root=1-653e0301-206928a4113b02586be299b7)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-MPNetModel/resolve/main/README.md.",
    "Mizuiro-sakura/luke-japanese-base-finetuned-ner": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Wikipedia"
        ],
        "license": "mit",
        "github": "https://github.com/stockmarkteam/ner-wikipedia-dataset",
        "paper": "title={LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention}, author={Ikuya Yamada and Akari Asai and Hiroyuki Shindo and Hideaki Takeda and Yuji Matsumoto}, booktitle={EMNLP}, year={2020}",
        "upstream_model": "luke-japanese-base, Wikipedia\u3092\u7528\u3044\u305f\u65e5\u672c\u8a9e\u306e\u56fa\u6709\u8868\u73fe\u62bd\u51fa\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8(\u30b9\u30c8\u30c3\u30af\u30de\u30fc\u30af\u793e\u3001https://github.com/stockmarkteam/ner-wikipedia-dataset )",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "\u305d\u306e\u4ed6\u306e\u7d44\u7e54\u540d",
                "result": 0.77
            },
            {
                "test": "\u30a4\u30d9\u30f3\u30c8\u540d",
                "result": 0.87
            },
            {
                "test": "\u4eba\u540d",
                "result": 0.9
            },
            {
                "test": "\u5730\u540d",
                "result": 0.83
            },
            {
                "test": "\u653f\u6cbb\u7684\u7d44\u7e54\u540d",
                "result": 0.82
            },
            {
                "test": "\u65bd\u8a2d\u540d",
                "result": 0.8
            },
            {
                "test": "\u6cd5\u4eba\u540d",
                "result": 0.89
            },
            {
                "test": "\u88fd\u54c1\u540d",
                "result": 0
            }
        ],
        "hardware": "luke-japanese-base",
        "limitation_and_bias": "This model is fine-tuned by using Wikipedia dataset.",
        "demo": "This model is fine-tuned by using Wikipedia dataset. You could use this model for NER tasks.",
        "input_format": "luke-japanese-base, Wikipedia, \u65e5\u672c\u8a9e\u306e\u56fa\u6709\u8868\u73fe\u62bd\u51fa\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8, https://github.com/stockmarkteam/ner-wikipedia-dataset, \u56fa\u6709\u8868\u73fe\u62bd\u51fa\uff08NER\uff09\u30bf\u30b9\u30af",
        "output_format": ""
    },
    "hf-internal-testing/tiny-random-vit": "404 Client Error. (Request ID: Root=1-653e032f-4acb7c761c62f3934a9a086a)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-vit/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-Data2VecAudioModel": "404 Client Error. (Request ID: Root=1-653e0330-3c0883af3c93cc2d14c108a3)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Data2VecAudioModel/resolve/main/README.md.",
    "rasa/LaBSE": "404 Client Error. (Request ID: Root=1-653e0330-478f628d6d1b41cc706eee36)\n\nEntry Not Found for url: https://huggingface.co/rasa/LaBSE/resolve/main/README.md.",
    "plasmo/vox2": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "184 training images through 8000 training steps"
        ],
        "license": "creativeml-openrail-m",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "184 training images through 8000 training steps",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "184 training images through 8000 training steps",
        "limitation_and_bias": "",
        "demo": "VERSION 1.2 of Voxel-ish Image Pack brought to you by 184 training images through 8000 training steps, 20% Training text crafted by Jak_TheAI_Artist version history: v1.2 - Fine tuned for better faces. Include Prompt trigger: \"voxel-ish\" to activate. Tip: add \"intricate detail\" in prompt to make a semi-realistic image. Sample pictures of this concept:",
        "input_format": "",
        "output_format": ""
    },
    "microsoft/layoutlmv2-base-uncased": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Microsoft Document AI"
        ],
        "license": "cc-by-nc-sa-4.0",
        "github": "https://github.com/microsoft/unilm/tree/master/layoutlmv2",
        "paper": "[LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)",
        "upstream_model": "LayoutLMv2 is an improved version of LayoutLM",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. It outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including , including FUNSD (0.7895 \u2192 0.8420), CORD (0.9493 \u2192 0.9601), SROIE (0.9524 \u2192 0.9781), Kleister-NDA (0.834 \u2192 0.852), RVL-CDIP (0.9443 \u2192 0.9564), and DocVQA (0.7295 \u2192 0.8672).",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. It outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including , including FUNSD (0.7895 \u2192 0.8420), CORD (0.9493 \u2192 0.9601), SROIE (0.9524 \u2192 0.9781), Kleister-NDA (0.834 \u2192 0.852), RVL-CDIP (0.9443 \u2192 0.9564), and DocVQA (0.7295 \u2192 0.8672).",
        "demo": "[here](https://huggingface.co/docs/transformers/model_doc/layoutlmv2), [Microsoft Document AI](https://www.microsoft.com/en-us/research/project/document-ai/), [GitHub](https://github.com/microsoft/unilm/tree/master/layoutlmv2)",
        "input_format": "",
        "output_format": ""
    },
    "microsoft/layoutxlm-base": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "XFUND dataset"
        ],
        "license": "cc-by-nc-sa-4.0",
        "github": "[GitHub](https://github.com/microsoft/unilm/tree/master/layoutxlm)",
        "paper": "\"[LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)\"",
        "upstream_model": "LayoutLMv2",
        "parameter_count": "parameter_count",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "\"Experiment results show that it has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset.\"",
        "demo": "\"LayoutXLM is a multimodal pre-trained model for multilingual document understanding\" \"Experiment results show that it has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset\" \"[LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)\"",
        "input_format": "",
        "output_format": ""
    },
    "naver-clova-ix/donut-base": {
        "model_type": "multimodal",
        "model_tasks": "image-to-text",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "mit",
        "github": "https://github.com/clovaai/donut",
        "paper": "https://arxiv.org/abs/2111.15664",
        "upstream_model": "pre-trained-only",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "Swin Transformer, BART",
        "limitation_and_bias": "We refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/donut) which includes code examples.",
        "demo": "We refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/donut) which includes code examples.",
        "input_format": "",
        "output_format": ""
    },
    "timm/vit_base_r50_s16_384.orig_in21k_ft_in1k": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "timm",
            "pytorch",
            "safetensors"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "tsmatz/xlm-roberta-ner-japanese": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "NER dataset provided by Stockmark Inc"
        ],
        "license": "mit",
        "github": "https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb",
        "paper": "xlm-roberta-base, NER dataset provided by Stockmark Inc, Japanese Wikipedia articles, license of this dataset, each token is labeled by, Label id, Tag, Tag in Widget, Description, 0, O, (None), others or nothing, 1, PER, PER, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organization, 4, ORG-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7, PRD, PRD, product, 8, EVT, EVT, event",
        "upstream_model": "xlm-roberta-base, RobertaModel, Stockmark Inc, Japanese Wikipedia articles, NER dataset provided by Stockmark Inc",
        "parameter_count": "5",
        "hyper_parameters": [
            {
                "learning_rate": "5e-05",
                "train_batch_size": "12",
                "eval_batch_size": "12",
                "seed": "42",
                "optimizer": "Adam with betas=(0.9,0.999) and epsilon=1e-08",
                "lr_scheduler_type": "linear",
                "num_epochs": "5"
            }
        ],
        "evaluation": [
            {
                "test": "f1",
                "result": 0
            }
        ],
        "hardware": "base_model: xlm-roberta-base",
        "limitation_and_bias": "This model is a fine-tuned version of xlm-roberta-base trained for named entity recognition (NER) token classification. The model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles. Each token is labeled by Label id, Tag, Tag in Widget, Description: 0, O, (None), others or nothing, 1, PER, PER, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organization, 4, ORG-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7, PRD, PRD, product, 8, EVT, EVT, event",
        "demo": "[here](https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb)",
        "input_format": "pre-trained cross-lingual ```RobertaModel```",
        "output_format": "This model is a fine-tuned version of xlm-roberta-base trained for named entity recognition (NER) token classification. Each token is labeled by: 0, O, (None), others or nothing, 1, PER, PER, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organization, 4, ORG-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7, PRD, PRD, product, 8, EVT, EVT, event"
    },
    "sentence-transformers/nli-mpnet-base-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "transformers"
        ],
        "datasets": [
            "Sentence Embeddings Benchmark"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "upstream_model": "sentence-transformers",
        "parameter_count": "'max_seq_length': 75, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "Sentence Embeddings Benchmark",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "[sentence-transformers](https://www.SBERT.net)",
        "input_format": "sentence-transformers",
        "output_format": "",
        "input_token_limit": "'max_seq_length': 75",
        "vocabulary_size": "'max_seq_length': 75, 'do_lower_case': False, 'word_embedding_dimension': 768"
    },
    "SAPOSS/password-model": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "transformers"
        ],
        "datasets": [
            "dataset for leak detection"
        ],
        "license": "TBD",
        "github": "https://github.com/SAP/credential-digger",
        "paper": "TBD",
        "upstream_model": "SAPOSS/password-model",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "More information needed",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "TBD",
        "output_format": "TBD",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "pyannote/embedding": "401 Client Error. (Request ID: Root=1-653e0453-085c0e1b463941530d0e1f3d)\n\nCannot access gated repo for url https://huggingface.co/api/models/pyannote/embedding.\nRepo model pyannote/embedding is gated. You must be authenticated to access it.",
    "valhalla/t5-base-e2e-qg": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "squad"
        ],
        "license": "mit",
        "github": "https://github.com/patil-suraj/question_generation",
        "paper": "https://arxiv.org/abs/1910.10683",
        "upstream_model": "valhalla/t5-base-e2e-qg",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.",
        "demo": "This is [t5-base](https://arxiv.org/abs/1910.10683) model trained for end-to-end question generation task. Simply input the text and the model will generate multile questions. You can play with the model using the inference API, just put the text and see the results! For more deatils see [this](https://github.com/patil-suraj/question_generation) repo.",
        "input_format": "Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.",
        "output_format": "valhalla/t5-base-e2e-qg",
        "input_token_limit": "",
        "vocabulary_size": "valhalla/t5-base-e2e-qg"
    },
    "sentence-transformers/multi-qa-mpnet-base-cos-v1": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers"
        ],
        "datasets": [
            "WikiAnswers",
            "PAQ",
            "Stack Exchange",
            "MS MARCO",
            "GOOAQ",
            "Amazon-QA",
            "Yahoo Answers",
            "SearchQA",
            "ELI5",
            "Quora Question Triplets",
            "Natural Questions"
        ],
        "license": "",
        "github": "train_script.py",
        "paper": "",
        "upstream_model": "sentence-transformers",
        "parameter_count": "214,988,242",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "7 TPUs v3-8",
        "limitation_and_bias": "Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.",
        "demo": "It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.",
        "input_format": "When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1.",
        "output_format": "When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1.",
        "input_token_limit": "512",
        "vocabulary_size": ""
    },
    "facebook/opt-1.3b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "BookCorpus",
            "CC-Stories",
            "Pile-CC",
            "OpenWebText2",
            "USPTO",
            "Project Gutenberg",
            "OpenSubtitles",
            "Wikipedia",
            "DM Mathematics",
            "HackerNews",
            "Pushshift.io Reddit dataset",
            "CCNewsV2"
        ],
        "license": "other",
        "github": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "paper": "the [official paper](https://arxiv.org/abs/2205.01068)",
        "upstream_model": "facebook/opt-1.3b",
        "parameter_count": "GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days",
        "hyper_parameters": [
            {
                "epochs": "BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit dataset, CCNewsV2",
                "batch_size": "NO_OUTPUT",
                "learning_rate": "NO_OUTPUT",
                "optimizer": "NO_OUTPUT"
            }
        ],
        "evaluation": [
            {
                "test": "BookCorpus, which consists of more than 10K unpublished books",
                "result": 0
            },
            {
                "test": "CC-Stories, which contains a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas",
                "result": 0
            },
            {
                "test": "The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included",
                "result": 0
            },
            {
                "test": "Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in Roller et al. (2021)",
                "result": 0
            },
            {
                "test": "CCNewsV2 containing an updated version of the English portion of the CommonCrawl News dataset that was used in RoBERTa (Liu et al., 2019b)",
                "result": 0
            },
            {
                "test": "The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally to each dataset\u2019s size in the pretraining corpus.",
                "result": 0
            }
        ],
        "hardware": "GPT2, 80GB A100 GPUs, ~33 days",
        "limitation_and_bias": "As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased :  Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. Here's an example of how the model can have biased predictions:  compared to:  This bias will also affect all fine-tuned versions of this model.",
        "demo": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "input_format": "BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset",
        "output_format": "",
        "input_token_limit": "GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*",
        "vocabulary_size": "180B tokens corresponding to 800GB of data"
    },
    "sentence-transformers/paraphrase-mpnet-base-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "transformers"
        ],
        "datasets": [
            "Sentence Embeddings Benchmark"
        ],
        "license": "apache-2.0",
        "github": "https://seb.sbert.net?model_name=sentence-transformers/paraphrase-mpnet-base-v2",
        "paper": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "upstream_model": "sentence-transformers",
        "parameter_count": "'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "Sentence Embeddings Benchmark",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "sentence-transformers, https://www.SBERT.net",
        "input_format": "sentence-transformers",
        "output_format": "",
        "input_token_limit": "'max_seq_length': 512",
        "vocabulary_size": "'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 768"
    },
    "fabiochiu/t5-base-tag-generation": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "tensorboard",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "50000 articles",
            "1000 random articles"
        ],
        "license": "apache-2.0",
        "github": "[t5-base](https://huggingface.co/t5-base), [190k Medium Articles](https://www.kaggle.com/datasets/fabiochiusano/medium-articles), [text2tags](https://huggingface.co/efederici/text2tags)",
        "paper": "\"text2tags\"",
        "upstream_model": "t5-base, text2text generation task, text2tags",
        "parameter_count": "8",
        "hyper_parameters": {
            "epochs": "1",
            "batch_size": "8",
            "learning_rate": "4e-05",
            "optimizer": "Adam with betas=(0.9,0.999) and epsilon=1e-08"
        },
        "evaluation": [
            {
                "test": "The model has been trained on a single epoch spanning about 50000 articles, evaluating on 1000 random articles not used during training.",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "trained on a single epoch spanning about 50000 articles, evaluating on 1000 random articles not used during training.",
        "demo": "license: apache-2.0, tags: - generated_from_trainer, widget: - text: Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected., example_title: Programming, model-index: - name: t5-base-tag-generation, results: []",
        "input_format": "t5-base, 190k Medium Articles, article textual content as input, multi-label classification problem, text2text generation task",
        "output_format": "t5-base, 190k Medium Articles, multi-label classification problem, text2text generation task, text2tags",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "timm/vit_small_patch14_dinov2.lvd142m": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "timm",
            "pytorch",
            "safetensors"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "flair/ner-german": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "flair"
        ],
        "datasets": [
            "conll2003"
        ],
        "license": "license",
        "github": "https://github.com/flairNLP/flair/",
        "paper": "Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}",
        "upstream_model": "upstream_model",
        "parameter_count": "SequenceTagger(hidden_size=256,",
        "hyper_parameters": {
            "epochs": "150",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "F1-Score: **87,94** (CoNLL-03 German revised) | **tag** | **meaning** | PER | person name | LOC | location name | ORG | organization name | MISC | other name | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
            }
        ],
        "hardware": "WordEmbeddings('de'), FlairEmbeddings('de-forward'), FlairEmbeddings('de-backward')",
        "limitation_and_bias": "F1-Score: 87,94 (CoNLL-03 German revised)",
        "demo": "Find a form of demo",
        "input_format": "Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.",
        "output_format": "4-class NER model for German"
    },
    "xlm-roberta-large-finetuned-conll03-english": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "CoNLL-2003"
        ],
        "license": "",
        "github": "",
        "paper": "https://arxiv.org/pdf/1911.02116.pdf",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "risks, biases and limitations of the model",
        "demo": "The model is a language model. The model can be used for token classification, a natural language understanding task in which a label is assigned to some tokens in a text.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "stabilityai/stable-diffusion-x4-upscaler": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "LAION-2B(en)"
        ],
        "license": "openrail++",
        "github": "https://laion.ai/blog/laion-5b/",
        "paper": "Research on generative models.",
        "upstream_model": "latent upscaling diffusion model, noise_level, predefined diffusion schedule, x4-upscaler-ema.ckpt",
        "parameter_count": "50",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Stable Diffusion vw was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.",
        "demo": "Generation of artworks and use in design and other artistic processes.",
        "input_format": "LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant",
        "output_format": "![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution."
    },
    "google/mobilebert-uncased": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "uncased_L-24_H-128_B-512_A-4_F-4_OPT"
        ],
        "license": "apache-2.0",
        "github": "uncased_L-24_H-128_B-512_A-4_F-4_OPT",
        "paper": "google/mobilebert-uncased",
        "upstream_model": "BERT_LARGE",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "uncased_L-24_H-128_B-512_A-4_F-4_OPT",
            "batch_size": "uncased_L-24_H-128_B-512_A-4_F-4_OPT",
            "learning_rate": "uncased_L-24_H-128_B-512_A-4_F-4_OPT",
            "optimizer": "uncased_L-24_H-128_B-512_A-4_F-4_OPT"
        },
        "evaluation": [],
        "hardware": "google/mobilebert-uncased, fill_mask.tokenizer.mask_token",
        "limitation_and_bias": "\"google/mobilebert-uncased\", \"fill-mask\", \"fill_mask.tokenizer.mask_token\"",
        "demo": "[uncased_L-24_H-128_B-512_A-4_F-4_OPT](https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT.tar.gz)",
        "input_format": "\"google/mobilebert-uncased\", \"google/mobilebert-uncased\", \"fill_mask.tokenizer.mask_token\"",
        "output_format": "\"google/mobilebert-uncased\", \"fill-mask.tokenizer.mask_token\""
    },
    "nvidia/mit-b0": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "imagenet_1k"
        ],
        "license": "The license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE).",
        "github": "[here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE)",
        "paper": "[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203)",
        "upstream_model": "ImageNet-1k",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "\"model hub\" \"https://huggingface.co/models?other=segformer\"",
        "input_format": "",
        "output_format": "\"return_tensors=\"pt\"\"",
        "input_preprocessing": "\"SegFormer encoder fine-tuned on Imagenet-1k.\"",
        "input_size": "",
        "num_of_classes_for_classification": "\"1000 ImageNet classes\"",
        "trigger_word": ""
    },
    "Helsinki-NLP/opus-mt-ar-en": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/ar-en/README.md",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "*OPUS readme: [ar-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/ar-en/README.md)*; *download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.zip)*; *test set translations: [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.test.txt)*; *test set scores: [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.eval.txt)*",
        "input_format": "SentencePiece",
        "output_format": "SentencePiece",
        "input_token_limit": "SentencePiece",
        "vocabulary_size": "SentencePiece"
    },
    "hf-internal-testing/tiny-random-VisionEncoderDecoderModel-vit-gpt2": "404 Client Error. (Request ID: Root=1-653e063c-7acd297b096e082e5fd224ee)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-VisionEncoderDecoderModel-vit-gpt2/resolve/main/README.md.",
    "bhadresh-savani/bert-base-go-emotion": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "go_emotions"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/bhadreshpsavani/UnderstandingNLP/blob/master/go_emotion_of_transformers_multilabel_text_classification_v2.ipynb",
        "paper": "\"text-classification\", \"go-emotion\", \"pytorch\", \"go_emotions\", \"Accuracy\"",
        "upstream_model": "- text-classification - pytorch - go_emotions - Accuracy",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {
            "eval_accuracy_thresh": "0.9614765048027039",
            "eval_loss": "0.1164659634232521"
        },
        "evaluation": [
            {
                "test": "eval_accuracy_thresh",
                "result": 0.9614765048027039
            },
            {
                "test": "eval_loss",
                "result": 0.1164659634232521
            }
        ],
        "hardware": "\"tags: - text-classification - go-emotion - pytorch datasets: - go_emotions metrics: - Accuracy thumbnail: https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4\"",
        "limitation_and_bias": "",
        "demo": "[Notebook](https://github.com/bhadreshpsavani/UnderstandingNLP/blob/master/go_emotion_of_transformers_multilabel_text_classification_v2.ipynb)",
        "input_format": "language: - en license: apache-2.0 tags: - text-classification - go-emotion - pytorch datasets: - go_emotions metrics: - Accuracy thumbnail: https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "\"Num examples = 169208\""
    },
    "princeton-nlp/sup-simcse-roberta-large": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "MNLI",
            "SNLI"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "stabilityai/stable-diffusion-xl-base-0.9": "401 Client Error. (Request ID: Root=1-653e067b-503165695b3f2c980dc532ec)\n\nCannot access gated repo for url https://huggingface.co/api/models/stabilityai/stable-diffusion-xl-base-0.9.\nRepo model stabilityai/stable-diffusion-xl-base-0.9 is gated. You must be authenticated to access it.",
    "textattack/bert-base-uncased-CoLA": "404 Client Error. (Request ID: Root=1-653e067c-3ebef46539574be66c60230e)\n\nEntry Not Found for url: https://huggingface.co/textattack/bert-base-uncased-CoLA/resolve/main/README.md.",
    "ALINEAR/albert-japanese-v2": "404 Client Error. (Request ID: Root=1-653e067c-3c5fe7dc256524d5634bcf2b)\n\nEntry Not Found for url: https://huggingface.co/ALINEAR/albert-japanese-v2/resolve/main/README.md.",
    "huggyllama/llama-7b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Document 1"
        ],
        "license": "Document 1",
        "github": "Document 1",
        "paper": "Document 1",
        "upstream_model": "Document 1",
        "parameter_count": "Document 1",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "Document 1",
        "limitation_and_bias": "Document 1",
        "demo": "Document 1",
        "input_format": "Document 1",
        "output_format": "Document 1",
        "input_token_limit": "Document 1",
        "vocabulary_size": "Document 1"
    },
    "facebook/blenderbot-400M-distill": {
        "model_type": "natural-language-processing",
        "model_tasks": "conversational",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "blended_skill_talk"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/facebookresearch/ParlAI",
        "paper": "https://arxiv.org/abs/2004.13637",
        "upstream_model": "",
        "parameter_count": "90M, 2.7B and 9.4B parameter neural models",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "Human evaluations",
                "result": 0.0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We then discuss the limitations of this work by analyzing failure cases of our models.",
        "demo": "We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available.",
        "input_format": "- blended_skill_talk",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "monologg/koelectra-small-v3-discriminator": "404 Client Error. (Request ID: Root=1-653e06b4-26fa20e4162652184363ac9b)\n\nEntry Not Found for url: https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/README.md.",
    "mosaicml/mpt-7b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "StreamingDataset"
        ],
        "license": "Apache-2.0",
        "github": "https://github.com/mosaicml/streaming",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-GPTJForCausalLM": "404 Client Error. (Request ID: Root=1-653e06f4-1f71701a67d844e11fe3acc1)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTJForCausalLM/resolve/main/README.md.",
    "Intel/dpt-large": {
        "model_type": "computer-vision",
        "model_tasks": "depth-estimation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "MIX 6"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/isl-org/DPT",
        "paper": "https://arxiv.org/abs/2103.13413",
        "upstream_model": "Intel/dpt-large",
        "parameter_count": "#params",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "monocular-depth-estimation",
                "result": 10.82
            }
        ],
        "hardware": "Intel Xeon Platinum 8280 CPU @ 2.70GHz with 8 physical cores and an NVIDIA RTX 2080 GPU.",
        "limitation_and_bias": "Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. There are no additional caveats or recommendations for this model.",
        "demo": "For more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/master/en/model_doc/dpt).",
        "input_format": "return_tensors='pt'",
        "output_format": "return_tensors='pt'",
        "input_preprocessing": "We resize the image such that the longer side is 384 pixels and train on random square crops of size 384. ... We perform random horizontal flips for data augmentation.",
        "input_size": "We resize the image such that the longer side is 384 pixels",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "hf-internal-testing/tiny-random-OPTForCausalLM": "404 Client Error. (Request ID: Root=1-653e0720-0fa1e32365a47b05598d9f51)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-OPTForCausalLM/resolve/main/README.md.",
    "vinai/bertweet-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "850M English Tweets (16B word tokens ~ 80GB)",
            "5M Tweets related to the COVID-19 pandemic"
        ],
        "license": "",
        "github": "https://github.com/VinAIResearch/BERTweet",
        "paper": "https://aclanthology.org/2020.emnlp-demos.2/",
        "upstream_model": "RoBERTa pre-training procedure",
        "parameter_count": "845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the COVID-19 pandemic.",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "16B word tokens ~ 80GB",
        "limitation_and_bias": "The corpus used to pre-train BERTweet consists of 850M English Tweets (16B word tokens ~ 80GB), containing 845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the COVID-19 pandemic.",
        "demo": "<p float=\"left\">\n<img width=\"275\" alt=\"postagging\" src=\"https://user-images.githubusercontent.com/2412555/135724590-01d8d435-262d-44fe-a383-cd39324fe190.png\" />\n<img width=\"275\" alt=\"ner\" src=\"https://user-images.githubusercontent.com/2412555/135724598-1e3605e7-d8ce-4c5e-be4a-62ae8501fae7.png\" />\n</p>  \n<p float=\"left\">\n<img width=\"275\" alt=\"sentiment\" src=\"https://user-images.githubusercontent.com/2412555/135724597-f1981f1e-fe73-4c03-b1ff-0cae0cc5f948.png\" />\n<img width=\"275\" alt=\"irony\" src=\"https://user-images.githubusercontent.com/2412555/135724595-15f4f2c8-bbb6-4ee6",
        "input_format": "RoBERTa pre-training procedure and 16B word tokens ~ 80GB",
        "output_format": "RoBERTa pre-training procedure and 16B word tokens ~ 80GB",
        "input_token_limit": "16B word tokens ~ 80GB",
        "vocabulary_size": "The corpus used to pre-train BERTweet consists of 850M English Tweets (16B word tokens ~ 80GB)"
    },
    "hf-internal-testing/tiny-random-SwinModel": "404 Client Error. (Request ID: Root=1-653e074e-2341557d47f39c5c3b26192f)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-SwinModel/resolve/main/README.md.",
    "lvwerra/distilbert-imdb": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "tensorboard",
            "transformers"
        ],
        "datasets": [
            "imdb dataset"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "martin-ha/toxic-comment-model": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Kaggle competition"
        ],
        "license": "Kaggle competition and train.csv",
        "github": "https://github.com/MSIA/wenyang_pan_nlp_project_2021",
        "paper": "DistilBERT model",
        "upstream_model": "DistilBERT",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "94% accuracy and 0.59 f1-score",
                "result": 10000
            }
        ],
        "hardware": "P-100 GPU",
        "limitation_and_bias": "The model performs poorly for some comments that mention a specific identity subgroup, like Muslim.",
        "demo": "[this documentation and codes](https://github.com/MSIA/wenyang_pan_nlp_project_2021)",
        "input_format": "train.csv",
        "output_format": "train.csv",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Helsinki-NLP/opus-mt-mul-en": "This model's maximum context length is 4097 tokens, however you requested 10330 tokens (10074 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "Jean-Baptiste/roberta-large-ner-english": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "onnx",
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "conll2003"
        ],
        "license": "mit",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "facebook/mms-1b-all": "This model's maximum context length is 4097 tokens, however you requested 5675 tokens (5419 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "google/mobilenet_v1_0.75_192": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "imagenet-1k"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "databricks/dolly-v2-3b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "databricks-dolly-15k"
        ],
        "license": "mit",
        "github": "https://github.com/databrickslabs/dolly",
        "paper": "https://github.com/databrickslabs/dolly/tree/master/data",
        "upstream_model": "EleutherAI's Pythia-2.8b",
        "parameter_count": "2.8 billion",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-DeiTModel": "404 Client Error. (Request ID: Root=1-653e085c-645664782d66b707273a687d)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-DeiTModel/resolve/main/README.md.",
    "microsoft/markuplm-base": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Document 1"
        ],
        "license": "Document 1",
        "github": "Document 1",
        "paper": "Document 3",
        "upstream_model": "Document 1",
        "parameter_count": "Document 1",
        "hyper_parameters": {
            "epochs": "Document 1",
            "batch_size": "Document 1",
            "learning_rate": "Document 1",
            "optimizer": "Document 1"
        },
        "evaluation": [
            {
                "test": "Document 1",
                "result": 0
            }
        ],
        "hardware": "Document 1",
        "limitation_and_bias": "Document 1",
        "demo": "Document 1",
        "input_format": "Document 1",
        "output_format": "Document 1"
    },
    "textattack/albert-base-v2-ag-news": "Could not parse function call data: Expecting property name enclosed in double quotes: line 16 column 9 (char 366)",
    "lllyasviel/control_v11p_sd15_lineart": "This model's maximum context length is 4097 tokens, however you requested 4291 tokens (4035 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "timm/vit_base_patch16_224.augreg2_in21k_ft_in1k": {
        "model_type": "computer-vision",
        "model_tasks": "image-classification",
        "frameworks": [
            "timm",
            "pytorch",
            "safetensors"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "hf-internal-testing/tiny-random-MobileNetV2Model": "404 Client Error. (Request ID: Root=1-653e08b7-645c97641bbee5862b6c2fba)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-MobileNetV2Model/resolve/main/README.md.",
    "bhadresh-savani/distilbert-base-uncased-emotion": "Could not parse function call data: Unterminated string starting at: line 21 column 15 (char 1890)",
    "DeepFloyd/IF-I-M-v1.0": "401 Client Error. (Request ID: Root=1-653e0919-6f2575ea6fd7bb05656cf3bf)\n\nCannot access gated repo for url https://huggingface.co/api/models/DeepFloyd/IF-I-M-v1.0.\nRepo model DeepFloyd/IF-I-M-v1.0 is gated. You must be authenticated to access it.",
    "Helsinki-NLP/opus-mt-en-fr": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "apache-2.0",
        "github": "[en-fr](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-fr/README.md), [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.zip), [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.test.txt), [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.eval.txt)",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "newsdiscussdev2015-enfr.en.fr",
                "result": 33.8
            },
            {
                "test": "newsdiscusstest2015-enfr.en.fr",
                "result": 40.0
            },
            {
                "test": "newssyscomb2009.en.fr",
                "result": 29.8
            },
            {
                "test": "news-test2008.en.fr",
                "result": 27.5
            },
            {
                "test": "newstest2009.en.fr",
                "result": 29.4
            },
            {
                "test": "newstest2010.en.fr",
                "result": 32.7
            },
            {
                "test": "newstest2011.en.fr",
                "result": 34.3
            },
            {
                "test": "newstest2012.en.fr",
                "result": 31.8
            },
            {
                "test": "newstest2013.en.fr",
                "result": 33.2
            },
            {
                "test": "Tatoeba.en.fr",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "OPUS readme: [en-fr](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-fr/README.md), dataset: opus, model: transformer-align, pre-processing: normalization + SentencePiece, download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.zip), test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.test.txt), test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.eval.txt)",
        "input_format": "SentencePiece, opus-2020-02-26.zip, opus-2020-02-26.test.txt",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "textattack/roberta-base-CoLA": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "0.850431447746884",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "32",
            "learning_rate": "2e-05",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0.850431447746884
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-LevitModel": "404 Client Error. (Request ID: Root=1-653e096e-7b424e5f3cfaec5311d5d034)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-LevitModel/resolve/main/README.md.",
    "monologg/kobert": "404 Client Error. (Request ID: Root=1-653e096e-41d13d5e4e86915752e92928)\n\nEntry Not Found for url: https://huggingface.co/monologg/kobert/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-BeitForImageClassification": "404 Client Error. (Request ID: Root=1-653e096e-61cfb0e728eba01841adbd1c)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BeitForImageClassification/resolve/main/README.md.",
    "databricks/dolly-v2-12b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "databricks-dolly-15k"
        ],
        "license": "mit",
        "github": "databricks/databricks-dolly-15k",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "stabilityai/sd-vae-ft-ema": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "https://ommer-lab.com/files/latent-diffusion/kl-f8.zip",
            "https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt",
            "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt"
        ],
        "license": "mit",
        "github": "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt",
        "paper": "",
        "upstream_model": "original | 246803 | 2.61 | 26.0 +/- 4.4 | 0.81 +/- 0.12 | 0.75 +/- 0.36 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "original",
                "result": 246803
            },
            {
                "test": "ft-EMA",
                "result": 560001
            },
            {
                "test": "ft-MSE",
                "result": 840001
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "Link: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt",
        "input_format": "\"256x256 images from the COCO2017 validation dataset\"",
        "output_format": ""
    },
    "distilbert-base-uncased-distilled-squad": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "BookCorpus",
            "English Wikipedia"
        ],
        "license": "",
        "github": "https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "8 16GB V100 GPUs, 90 hours",
        "limitation_and_bias": "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.",
        "demo": "",
        "input_format": "BookCorpus and English Wikipedia",
        "output_format": "Unknown",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-PoolFormerModel": "404 Client Error. (Request ID: Root=1-653e0a06-72f24a650e3c480e4c634ca0)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-PoolFormerModel/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-Wav2Vec2Model": "404 Client Error. (Request ID: Root=1-653e0a06-534720b2574e686771b5fc87)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Wav2Vec2Model/resolve/main/README.md.",
    "huggingface/CodeBERTa-small-v1": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "CodeSearchNet"
        ],
        "license": "\"CodeSearchNet\" dataset from GitHub, Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`, 6-layer, 84M parameters, RoBERTa-like Transformer model",
        "github": "CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model \u2013 that\u2019s the same number of layers & heads as DistilBERT \u2013 initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.",
        "paper": "`@article{husain_codesearchnet_2019, title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}}, url = {http://arxiv.org/abs/1909.09436}, author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc}, year = {2019}, note = {arXiv: 1909.09436},}`",
        "upstream_model": "huggingface/CodeBERTa-language-id",
        "parameter_count": "84M parameters",
        "hyper_parameters": {
            "epochs": "5",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "huggingface/CodeBERTa-language-id",
                "result": 0
            }
        ],
        "hardware": "Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`, 6-layer, 84M parameters, RoBERTa-like Transformer model",
        "limitation_and_bias": "CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model \u2013 that\u2019s the same number of layers & heads as DistilBERT \u2013 initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.",
        "demo": "huggingface/CodeBERTa-language-id",
        "input_format": "Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`, 6-layer, 84M parameters, RoBERTa-like Transformer model",
        "output_format": "huggingface/CodeBERTa-language-id",
        "input_token_limit": "Byte-level BPE tokenizer, 6-layer, 84M parameters, RoBERTa-like Transformer model, same number of layers & heads as DistilBERT, trained from scratch on the full corpus (~2M functions) for 5 epochs",
        "vocabulary_size": "Byte-level BPE tokenizer, 6-layer, 84M parameters, RoBERTa-like Transformer model"
    },
    "hf-internal-testing/tiny-random-Data2VecVisionModel": "404 Client Error. (Request ID: Root=1-653e0a3c-2a216d482b760f2e20d7a950)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Data2VecVisionModel/resolve/main/README.md.",
    "flair/pos-english": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "flair"
        ],
        "datasets": [
            "ontonotes"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "google/pegasus-xsum": {
        "model_type": "natural-language-processing",
        "model_tasks": "summarization",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "samsum",
            "xsum",
            "cnn_dailymail"
        ],
        "license": "Original TF 1 code [here](https://github.com/google-research/pegasus)",
        "github": "Original TF 1 code [here](https://github.com/google-research/pegasus), Maintained by: [@sshleifer](https://twitter.com/sam_shleifer)",
        "paper": "Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "rouge",
                "result": 21.8096
            },
            {
                "test": "rouge",
                "result": 4.2525
            },
            {
                "test": "rouge",
                "result": 17.4469
            },
            {
                "test": "rouge",
                "result": 18.8907
            },
            {
                "test": "loss",
                "result": 3.0317161083221436
            },
            {
                "test": "gen_len",
                "result": 20.3122
            },
            {
                "test": "rouge",
                "result": 46.8623
            },
            {
                "test": "rouge",
                "result": 24.4533
            },
            {
                "test": "rouge",
                "result": 39.0548
            },
            {
                "test": "rouge",
                "result": 39.0994
            }
        ],
        "hardware": "Original TF 1 code [here](https://github.com/google-research/pegasus)",
        "limitation_and_bias": "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization",
        "demo": "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization",
        "input_format": "type: cnn_dailymail\nconfig: 3.0.0\nsplit: test",
        "output_format": "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization",
        "input_token_limit": "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization",
        "vocabulary_size": "vocabulary_size"
    },
    "cl-tohoku/bert-base-japanese-char": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Japanese Wikipedia"
        ],
        "license": "Creative Commons Attribution-ShareAlike 3.0",
        "github": "https://github.com/attardi/wikiextractor",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "256*512",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "Cloud TPUs",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "512",
        "vocabulary_size": "4000"
    },
    "cointegrated/rubert-tiny2": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "cointegrated/rubert-tiny",
            "cointegrated/rubert-tiny2"
        ],
        "license": "mit",
        "github": "[cointegrated/rubert-tiny](https://huggingface.co/cointegrated/rubert-tiny)",
        "paper": "The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task.\nSentence embeddings can be produced as follows:\n```python\n# pip install transformers sentencepiece\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\nmodel = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n# model.cuda()  # uncomment it if you have a GPU\n\ndef embed_bert_cls(text, model, tokenizer):\nt = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\nwith torch.no_grad():\nmodel_output = model(**{k: v.to(model.device) for k, v in t.items()})\nembeddings = model_output.last_hidden_state[:, 0, :]\nembeddings = torch.nn.functional.normalize(embeddings)",
        "upstream_model": "- a larger vocabulary: 83828 tokens instead of 29564;\n- larger supported sequences: 2048 instead of 512;\n- sentence embeddings approximate LaBSE closer than before;\n- meaningful segment embeddings (tuned on the NLI task)\n- the model is focused only on Russian.",
        "parameter_count": "312 parameters",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "- a larger vocabulary: 83828 tokens instead of 29564;\n- larger supported sequences: 2048 instead of 512;\n- sentence embeddings approximate LaBSE closer than before;\n- meaningful segment embeddings (tuned on the NLI task)\n- the model is focused only on Russian.",
        "limitation_and_bias": "The differences from the previous version include:\n- a larger vocabulary: 83828 tokens instead of 29564;\n- larger supported sequences: 2048 instead of 512;\n- sentence embeddings approximate LaBSE closer than before;\n- meaningful segment embeddings (tuned on the NLI task)\n- the model is focused only on Russian.",
        "demo": "\"The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task.\"\n\n\"Sentence embeddings can be produced as follows:\"\n\n\"Alternatively, you can use the model with `sentence_transformers`:\"",
        "input_format": "\"The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task.\"\n\"Sentence embeddings can be produced as follows:\"\n\"Alternatively, you can use the model with `sentence_transformers`:\"",
        "output_format": "\"The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task.\"\n\"Sentence embeddings can be produced as follows:\"\n\"Alternatively, you can use the model with `sentence_transformers`:\"",
        "input_token_limit": "2048 instead of 512; input_token_limit: 83828 tokens",
        "vocabulary_size": "- a larger vocabulary: 83828 tokens instead of 29564;"
    },
    "dandelin/vilt-b32-finetuned-vqa": {
        "model_type": "multimodal",
        "model_tasks": "visual-question-answering",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "VQAv2"
        ],
        "license": "",
        "github": "https://github.com/dandelin/ViLT",
        "paper": "https://arxiv.org/abs/2102.03334",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "pyannote/voice-activity-detection": "401 Client Error. (Request ID: Root=1-653e0b08-35aa14c93b7d68ff19a2d407)\n\nCannot access gated repo for url https://huggingface.co/api/models/pyannote/voice-activity-detection.\nRepo model pyannote/voice-activity-detection is gated. You must be authenticated to access it.",
    "arpanghoshal/EmoRoBERTa": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "tensorflow",
            "transformers"
        ],
        "datasets": [
            "go_emotions"
        ],
        "license": "mit",
        "github": "",
        "paper": "RoBERTa builds on BERT\u2019s language masking strategy and modifies key hyperparameters in BERT, including removing BERT\u2019s next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time.",
        "upstream_model": "BERT",
        "parameter_count": "6",
        "hyper_parameters": {
            "epochs": "10",
            "batch_size": "16",
            "learning_rate": "5e-5",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Macro F1",
                "result": 49.3
            }
        ],
        "hardware": "",
        "limitation_and_bias": "admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise + neutral",
        "demo": "\"a form of demo\" \"58000 Reddit comments with 28 emotions\"",
        "input_format": "\"RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline\"",
        "output_format": "\"pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa')\"",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "monologg/bert-base-cased-goemotions-original": "404 Client Error. (Request ID: Root=1-653e0b30-0634c7263d1ff5fc26294baf)\n\nEntry Not Found for url: https://huggingface.co/monologg/bert-base-cased-goemotions-original/resolve/main/README.md.",
    "google/pegasus-cnn_dailymail": {
        "model_type": "natural-language-processing",
        "model_tasks": "summarization",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "C4 and HugeNews"
        ],
        "license": "Original TF 1 code [here](https://github.com/google-research/pegasus)",
        "github": "Original TF 1 code [here](https://github.com/google-research/pegasus), Maintained by: [@sshleifer](https://twitter.com/sam_shleifer)",
        "paper": "Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "Original TF 1 code [here](https://github.com/google-research/pegasus)",
        "limitation_and_bias": "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization",
        "demo": "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html) Original TF 1 code [here](https://github.com/google-research/pegasus)",
        "input_format": "Task: Summarization",
        "output_format": "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization",
        "input_token_limit": "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization",
        "vocabulary_size": "vocabulary_size"
    },
    "microsoft/mdeberta-v3-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "CC100"
        ],
        "license": "mit",
        "github": "https://github.com/microsoft/DeBERTa",
        "paper": "https://arxiv.org/abs/2111.09543",
        "upstream_model": "RoBERTa",
        "parameter_count": "86M backbone parameters with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.",
        "hyper_parameters": {
            "epochs": "6",
            "batch_size": "${batch_size}",
            "learning_rate": "2e-5",
            "optimizer": "AdamW"
        },
        "evaluation": [
            {
                "test": "XLM-R-base",
                "result": 76.2
            },
            {
                "test": "mDeBERTa-base",
                "result": 79.8
            }
        ],
        "hardware": "Not specified",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "250K tokens",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-GPT2LMHeadModel": "404 Client Error. (Request ID: Root=1-653e0b99-01badc157fccd754472a8ca1)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPT2LMHeadModel/resolve/main/README.md.",
    "facebook/nllb-200-1.3B": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "parallel multilingual data",
            "monolingual data constructed from Common Crawl"
        ],
        "license": "cc-by-nc-4.0",
        "github": "https://github.com/example/model",
        "paper": "https://arxiv.org/123456",
        "upstream_model": "",
        "parameter_count": "100M",
        "hyper_parameters": {
            "epochs": "10",
            "batch_size": "32",
            "learning_rate": "0.001",
            "optimizer": "Adam"
        },
        "evaluation": [
            {
                "test": "BLEU",
                "result": 0.85
            },
            {
                "test": "spBLEU",
                "result": 0.82
            },
            {
                "test": "chrF++",
                "result": 0.75
            }
        ],
        "hardware": "NVIDIA V100 GPUs",
        "limitation_and_bias": "The model may not perform well on low-resource languages",
        "demo": "You can use the model by following the code snippet provided in the documentation",
        "input_format": "The model accepts parallel multilingual data and monolingual data constructed from Common Crawl",
        "output_format": "The model outputs translations in the target language",
        "input_token_limit": "512",
        "vocabulary_size": "50,000"
    },
    "microsoft/deberta-v3-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "160GB data"
        ],
        "license": "mit",
        "github": "https://github.com/microsoft/DeBERTa",
        "paper": "https://arxiv.org/abs/2111.09543",
        "upstream_model": "",
        "parameter_count": "98M parameters in the Embedding layer",
        "hyper_parameters": {
            "epochs": "3",
            "batch_size": "${batch_size}",
            "learning_rate": "2e-5",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "SQuAD 2.0(F1/EM)",
                "result": 88.4
            },
            {
                "test": "MNLI-m/mm(ACC)",
                "result": 90.6
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "Please check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "128K tokens"
    },
    "EleutherAI/gpt-neo-2.7B": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "transformers",
            "safetensors",
            "jax",
            "pytorch",
            "rust"
        ],
        "datasets": [
            "Pile",
            "EleutherAI"
        ],
        "license": "mit",
        "github": "https://github.com/EleutherAI/lm-evaluation-harness",
        "paper": "https://arxiv.org/abs/2101.00027",
        "upstream_model": "masked autoregressive language model",
        "parameter_count": "420 billion",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "Pile",
        "limitation_and_bias": "NO_OUTPUT",
        "demo": "generating texts from a prompt",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "yiyanghkust/finbert-esg": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "yiyanghkust/finbert-esg"
        ],
        "license": "If you use the model in your academic work, please cite the following paper: Huang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).",
        "github": "yiyanghkust/finbert-esg",
        "paper": "If you use the model in your academic work, please cite the following paper: Huang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).",
        "upstream_model": "yiyanghkust/finbert-esg",
        "parameter_count": "BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "```python\n# tested in transformers==4.18.0\nfrom transformers import BertTokenizer, BertForSequenceClassification, pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')\nnlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\nresults = nlp('Rhonda has been volunteering for several years for a variety of charitable community programs.')\nprint(results) # [{'label': 'Social', 'score': 0.9906041026115417}]\n```",
        "input_format": "BertTokenizer, BertForSequenceClassification, pipeline, finbert, tokenizer, nlp",
        "output_format": "Input: A financial text. Output: Environmental, Social, Governance or None.",
        "input_token_limit": "Input: A financial text.",
        "vocabulary_size": "from transformers import BertTokenizer, BertForSequenceClassification, pipeline and tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')"
    },
    "papluca/xlm-roberta-base-language-detection": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-convnext": "404 Client Error. (Request ID: Root=1-653e0c66-011b774235dd48817fc84193)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-convnext/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-BloomForCausalLM": "404 Client Error. (Request ID: Root=1-653e0c66-09561123050ddbb9249ffe84)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BloomForCausalLM/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-gpt_neo": "404 Client Error. (Request ID: Root=1-653e0c67-60496ce610bd6d4b160c545d)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/README.md.",
    "dreamlike-art/dreamlike-diffusion-1.0": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "text-to-image",
            "art",
            "artistic",
            "diffusers"
        ],
        "license": "modified CreativeML OpenRAIL-M license",
        "github": "\"stable-diffusion\", \"stable-diffusion-diffusers\", \"text-to-image\", \"art\", \"artistic\", \"diffusers\"",
        "paper": "",
        "upstream_model": "\"Stable Diffusion Pipeline\" \"model_id = \"dreamlike-art/dreamlike-diffusion-1.0\"\"",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "- You can't host or use the model or its derivatives on websites/apps/etc., from which you earn, will earn, or plan to earn revenue or donations. \n- You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc. \n- You are free to host the model or its derivatives on completely non-commercial websites/apps/etc (Meaning you are not getting ANY revenue or donations). \n- You are free to use the outputs of the model or the outputs of the model's derivatives for commercial purposes in teams of 10 or less\n- You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n- The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n- You may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)",
        "demo": "[dreamlike.art](https://dreamlike.art/)",
        "input_format": "",
        "output_format": ""
    },
    "allenai/specter2": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "adapter-transformers"
        ],
        "datasets": [
            "SciRepEval"
        ],
        "license": "license",
        "github": "https://github.com/allenai/SPECTER2",
        "paper": "https://api.semanticscholar.org/CorpusID:215768677",
        "upstream_model": "Base Model: First a base model is trained on the above citation triplets. Adapters: Thereafter, task format specific adapters are trained on the SciRepEval training tasks, where 600K triplets are sampled from above and added to the training data as well.",
        "parameter_count": "batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16",
        "hyper_parameters": {
            "epochs": "2",
            "batch_size": "1024",
            "learning_rate": "2e-5",
            "optimizer": "fp16"
        },
        "evaluation": [
            {
                "test": "For evaluation and downstream usage, please refer to",
                "result": 0
            }
        ],
        "hardware": "batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16",
        "limitation_and_bias": "batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16",
        "demo": "- **Demo:** [Usage](https://github.com/allenai/SPECTER2_0/blob/main/README.md)",
        "input_format": "The citation link are triplets in the form ```json {\"query\": {\"title\": ..., \"abstract\": ...}, \"pos\": {\"title\": ..., \"abstract\": ...}, \"neg\": {\"title\": ..., \"abstract\": ...}}```",
        "output_format": "The citation link are triplets in the form ```json {\"query\": {\"title\": ..., \"abstract\": ...}, \"pos\": {\"title\": ..., \"abstract\": ...}, \"neg\": {\"title\": ..., \"abstract\": ...}}```"
    },
    "hf-internal-testing/tiny-random-mobilevit": "404 Client Error. (Request ID: Root=1-653e0ccb-7ebe7c0864991ac65f19e302)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-mobilevit/resolve/main/README.md.",
    "HuggingFaceM4/tiny-random-LlamaForCausalLM": "404 Client Error. (Request ID: Root=1-653e0ccb-7e0f65ab5234bf555f7c4828)\n\nEntry Not Found for url: https://huggingface.co/HuggingFaceM4/tiny-random-LlamaForCausalLM/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-BartForConditionalGeneration": "404 Client Error. (Request ID: Root=1-653e0ccb-115f6c217eb727eb2b651d41)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BartForConditionalGeneration/resolve/main/README.md.",
    "google/byt5-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "mC4",
            "TweetQA"
        ],
        "license": "apache-2.0",
        "github": "\"google/byt5-large\" and \"huggingface.co/google/mt5-large\"",
        "paper": "\"In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.\"",
        "upstream_model": "Google's T5, MT5, mC4, ByT5, TweetQA, ByT5: Towards a token-free future with pre-trained byte-to-byte models",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",
        "input_format": "\"raw UTF-8 bytes\" and \"return_tensors=\"pt\"\"",
        "output_format": "\"raw UTF-8 bytes\" and \"return_tensors=\"pt\"\"",
        "input_token_limit": "input_token_limit",
        "vocabulary_size": ""
    },
    "facebook/vit-mae-huge": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "imagenet-1k"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick and [this repository](https://github.com/facebookresearch/mae).",
        "upstream_model": "The Vision Transformer (ViT)",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "hf-internal-testing/tiny-random-OPTModel": "404 Client Error. (Request ID: Root=1-653e0d14-6804c0b65c1305650d2accac)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-OPTModel/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-distilbert": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "stabilityai/stable-diffusion-2": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "COCO2017 validation set"
        ],
        "license": "openrail++",
        "github": "openrail++",
        "paper": "",
        "upstream_model": "Stable Diffusion v1, DALL-E Mini model card",
        "parameter_count": "50",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "KoboldAI/OPT-13B-Nerys-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Pike",
            "CYS",
            "Manga-v1"
        ],
        "license": "other",
        "github": "NO_OUTPUT",
        "paper": "arXiv:2205.01068",
        "upstream_model": "KoboldAI/OPT-13B-Nerys-v2",
        "parameter_count": "2500 ebooks, CYOA dataset, 50 Asian Light Novels, fairseq-dense-13B-Nerys-v2",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "fairseq-dense-13B-Nerys-v2",
        "limitation_and_bias": "bias (gender, profession, race and religion).",
        "demo": "`pipeline('text-generation', model='KoboldAI/OPT-13B-Nerys-v2')`",
        "input_format": "`[Genre: <genre1>, <genre2>]` and fairseq-dense-13B-Nerys-v2",
        "output_format": "`This example generates a different sequence each time it's run: ```py from transformers import pipeline generator = pipeline('text-generation', model='KoboldAI/OPT-13B-Nerys-v2')```",
        "input_token_limit": "",
        "vocabulary_size": "2500 ebooks in various genres (the \"Pike\" dataset), a CYOA dataset called \"CYS\" and 50 Asian \"Light Novels\" (the \"Manga-v1\" dataset)"
    },
    "lllyasviel/control_v11p_sd15_scribble": "This model's maximum context length is 4097 tokens, however you requested 4291 tokens (4035 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "openlm-research/open_llama_3b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "togethercomputer/RedPajama-Data-1T"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/openlm-research/open_llama",
        "paper": "https://github.com/openlm-research/open_llama",
        "upstream_model": "LLaMA",
        "parameter_count": "7B",
        "hyper_parameters": {
            "epochs": "N/A",
            "batch_size": "N/A",
            "learning_rate": "N/A",
            "optimizer": "N/A"
        },
        "evaluation": [
            {
                "test": "anli_r1/acc",
                "result": 0.33
            },
            {
                "test": "anli_r2/acc",
                "result": 0.35
            },
            {
                "test": "anli_r3/acc",
                "result": 0.33
            }
        ],
        "hardware": "Google TPU Research Cloud",
        "limitation_and_bias": "We note that our results for the LLaMA model differ slightly from the original LLaMA paper, which we believe is a result of different evaluation protocols. Similar differences have been reported in [this issue of lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/issues/443). We removed the task CB and WSC from our benchmark, as our model performs suspiciously well on these two tasks. We hypothesize that there could be a benchmark data contamination in the training set.",
        "demo": "We provide PyTorch and JAX weights of pre-trained OpenLLaMA models",
        "input_format": "EasyLM format",
        "output_format": "EasyLM format",
        "input_token_limit": "2200 tokens / second / TPU-v4 chip",
        "vocabulary_size": "1.2 trillion tokens"
    },
    "Salesforce/codet5-base-multi-sum": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "code_search_net"
        ],
        "license": "bsd-3-clause",
        "github": "https://github.com/salesforce/CodeT5",
        "paper": "title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}",
        "upstream_model": "CodeT5-base, EMNLP 2021 paper CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation, Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi, this repository https://github.com/salesforce/CodeT5",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "We employ the filtered version of CodeSearchNet data [[Husain et al., 2019](https://arxiv.org/abs/1909.09436)] from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text) benchmark for fine-tuning on code summarization. The data is tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer. One can prepare text (or code) for the model using RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base).",
        "demo": "CodeT5-base, CodeSearchNet data, multi-lingual training setting (Ruby/JavaScript/Go/Python/Java/PHP), CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation, this repository https://github.com/salesforce/CodeT5",
        "input_format": "\"tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer\" and \"RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)\".",
        "output_format": "\"tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer\" and \"RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)\".",
        "input_token_limit": "RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base) NO_OUTPUT",
        "vocabulary_size": "\"RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)\""
    },
    "microsoft/table-transformer-detection": {
        "model_type": "computer-vision",
        "model_tasks": "object-detection",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "PubTables1M"
        ],
        "license": "mit",
        "github": "https://github.com/microsoft/table-transformer",
        "paper": "https://arxiv.org/abs/2110.00061",
        "upstream_model": "Table Transformer (DETR) model trained on PubTables1M",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "Table Transformer (DETR) model trained on PubTables1M",
        "limitation_and_bias": "The team releasing Table Transformer did not write a model card for this model",
        "demo": "The team releasing Table Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.",
        "input_format": "PubTables1M",
        "output_format": "NO_OUTPUT",
        "input_preprocessing": "The team releasing Table Transformer did not write a model card for this model",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "michellejieli/emotion_text_classifier": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Emotion English DistilRoBERTa-base",
            "Crowdflower (2016)",
            "Elvis et al. (2018)",
            "Demszky et al. (2020)",
            "Vikash (2018)",
            "Poria et al. (2019)",
            "Mohammad et al. (2018)",
            "Emotion Lines (Friends)"
        ],
        "license": "",
        "github": "",
        "paper": "Ashritha R Murthy and K M Anil Kumar 2021 IOP Conf. Ser.: Mater. Sci. Eng. 1110 012009",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "ismail-lucifer011/autotrain-name_all-904029577": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ismail-lucifer011/autotrain-data-name_all"
        ],
        "github": "ismail-lucifer011/autotrain-name_all-904029577",
        "paper": "Model ID: 904029577",
        "upstream_model": "ismail-lucifer011/autotrain-name_all-904029577",
        "evaluation": [
            {
                "test": "Loss",
                "result": 0.0035200684797018766
            },
            {
                "test": "Accuracy",
                "result": 0.9989316041363876
            },
            {
                "test": "Precision",
                "result": 0.9877899024589919
            },
            {
                "test": "Recall",
                "result": 0.9933336010601984
            },
            {
                "test": "F1",
                "result": 0.9905539954046464
            }
        ],
        "demo": "\"You can use cURL to access this model:  \n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoTrain\"}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-name_all-904029577\n```  \nOr Python API:  \n```\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"ismail-lucifer011/autotrain-name_all-904029577\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"ismail-lucifer011/autotrain-name_all-904029577\", use_auth_token=True)\n\ninputs = tokenizer(\"I love AutoTrain\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n```\"",
        "input_format": "Content-Type: application/json",
        "output_format": "\"from transformers import AutoModelForTokenClassification, AutoTokenizer\" and \"outputs = model(**inputs)\""
    },
    "Lykon/DreamShaper": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "link"
        ],
        "license": "other",
        "github": "\"stable-diffusion\", \"stable-diffusion-diffusers\", \"text-to-image\", \"art\", \"artistic\", \"diffusers\", \"anime\"",
        "paper": "https://civitai.com/models/4384/dreamshaper",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "- https://huggingface.co/spaces/Lykon/DreamShaper-webui",
        "input_format": "- https://huggingface.co/spaces/Lykon/DreamShaper-webui\ninput_format: NO_OUTPUT",
        "output_format": "\"You can run this model on: - https://huggingface.co/spaces/Lykon/DreamShaper-webui - Mage.space, sinkin.ai and more\"\nNO_OUTPUT"
    },
    "ismail-lucifer011/autotrain-job_all-903929564": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ismail-lucifer011/autotrain-data-job_all"
        ],
        "license": "",
        "github": "ismail-lucifer011/autotrain-job_all-903929564",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Rostlab/prot_t5_xl_half_uniref50-enc": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "UniRef50"
        ],
        "license": "https://doi.org/10.1101/2020.07.12.199554",
        "github": "https://github.com/agemagician/ProtTrans",
        "paper": "https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554",
        "upstream_model": "ProtT5-XL-UniRef50",
        "parameter_count": "#params",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "8 GB of video RAM",
        "limitation_and_bias": "ProtT5-XL-UniRef50 is based on the `t5-3b` model and was pretrained on a large corpus of protein sequences in a self-supervised fashion. One important difference between this T5 model and the original T5 version is the denoising objective. The original T5-3B model was pretrained using a span denoising objective, while this model was pretrained with a Bart-like MLM denoising objective. The masking probability is consistent with the original T5 training by randomly masking 15% of the amino acids in the input. This model only contains the encoder portion of the original ProtT5-XL-UniRef50 model using half precision (float16).",
        "demo": "https://github.com/agemagician/ProtTrans",
        "input_format": "pretrained on a large corpus of protein sequences in a self-supervised fashion, randomly masking 15% of the amino acids in the input, this model only contains the encoder portion of the original ProtT5-XL-UniRef50 model using half precision (float16)",
        "output_format": "last_hidden_state"
    },
    "ismail-lucifer011/autotrain-company_all-903429548": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ismail-lucifer011/autotrain-data-company_all"
        ],
        "license": "",
        "github": "$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoTrain\"}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_all-903429548\n\nPython API: ```from transformers import AutoModelForTokenClassification, AutoTokenizer\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"ismail-lucifer011/autotrain-company_all-903429548\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"ismail-lucifer011/autotrain-company_all-903429548\", use_auth_token=True)\n\ninputs = tokenizer(\"I love AutoTrain\", return_tensors=\"pt\")\n\noutputs = model(**inputs)```",
        "paper": "Model ID: 903429548",
        "upstream_model": "\"ismail-lucifer011/autotrain-company_all-903429548\"",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "- Loss: 0.006148040760308504\n- Accuracy: 0.9979930566588805\n- Precision: 0.9814944904963571\n- Recall: 0.9817210885036588\n- F1: 0.9816077764228254",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "You can use cURL to access this model:  \n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love AutoTrain\"}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_all-903429548\n```\nOr Python API:  \n```\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"ismail-lucifer011/autotrain-company_all-903429548\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"ismail-lucifer011/autotrain-company_all-903429548\", use_auth_token=True)\n\ninputs = tokenizer(\"I love AutoTrain\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n```",
        "input_format": "\"Content-Type: application/json\"",
        "output_format": "output_format: \"pt\"",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "lllyasviel/sd-controlnet-openpose": {
        "model_type": "computer-vision",
        "model_tasks": "image-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "Stable Diffusion v1-5",
            "lllyasviel/sd-controlnet-canny",
            "lllyasviel/sd-controlnet-depth",
            "lllyasviel/sd-controlnet-hed",
            "lllyasviel/sd-controlnet-mlsd",
            "lllyasviel/sd-controlnet-normal",
            "lllyasviel/sd-controlnet-openpose"
        ],
        "license": "openrail",
        "github": "lllyasviel/sd-controlnet-canny",
        "paper": "*Adding Conditional Control to Text-to-Image Diffusion Models* by Lvmin Zhang, Maneesh Agrawala.",
        "upstream_model": "runwayml/stable-diffusion-v1-5",
        "parameter_count": "200k",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "Nvidia A100 80G",
        "limitation_and_bias": "NO_OUTPUT",
        "demo": "The authors released 8 different checkpoints, each trained with [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) on a different type of conditioning:\n\n| Model Name | Control Image Overview| Control Image Example | Generated Image Example |\n|---|---|---|---|\n|[lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny)<br/> *Trained with canny edge detection* | A monochrome image with white edges on a black background.|<a href=\"https://huggingface.co/takuma104/controlnet_dev/blob/main/gen_compare/control_images/converted/control_bird_canny.png\"><img width=\"64\" style=\"margin:0;padding:0;\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare/control_images/converted/control_bird_canny",
        "input_format": "\"canny edge detection\", \"Midas depth estimation\", \"HED edge detection (soft edge)\", \"M-LSD line detection\", \"normal map\", \"OpenPose bone image\", \"human scribbles\", \"ADE20K's segmentation protocol image\"",
        "output_format": "Generated Image Example"
    },
    "facebook/opt-2.7b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "BookCorpus",
            "CC-Stories",
            "Pile-CC",
            "OpenWebText2",
            "USPTO",
            "Project Gutenberg",
            "OpenSubtitles",
            "Wikipedia",
            "DM Mathematics",
            "HackerNews",
            "Pushshift.io Reddit dataset",
            "CCNewsV2"
        ],
        "license": "other",
        "github": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "paper": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs.",
        "upstream_model": "",
        "parameter_count": "GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "- BookCorpus, which consists of more than 10K unpublished books,\n\n- CC-Stories, which contains a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas,\n\n- The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included.\n\n- Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021)\n\n- CCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n\n- The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally\nto each dataset\u2019s size in the pretraining corpus.",
                "result": 0
            }
        ],
        "hardware": "GPT2, 80GB A100 GPUs, ~33 days",
        "limitation_and_bias": "As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased : Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. Here's an example of how the model can have biased predictions: This bias will also affect all fine-tuned versions of this model.",
        "demo": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "input_format": "BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset",
        "output_format": ""
    },
    "dbmdz/distilbert-base-turkish-cased": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Kemal Oflazer, Reyyan Yeniterzi, Turkish NER dataset"
        ],
        "license": "mit",
        "github": "All models are available on the [Huggingface model hub](https://huggingface.co/dbmdz).",
        "paper": "Huggingface model hub",
        "upstream_model": "dbmdz/distilbert-base-turkish-cased",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "4 RTX 2080 TI",
        "limitation_and_bias": "",
        "demo": "[Huggingface model hub](https://huggingface.co/dbmdz)",
        "input_format": "PyTorch-Transformers, config.json, pytorch_model.bin, vocab.txt",
        "output_format": "PyTorch-[Transformers](https://github.com/huggingface/transformers) \u2022 [`config.json`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/config.json) \u2022 [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/pytorch_model.bin) \u2022 [`vocab.txt`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/vocab.txt)"
    },
    "ybelkada/tiny-random-T5ForConditionalGeneration-calibrated": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "https://github.com/huggingface/models",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "uer/albert-base-chinese-cluecorpussmall": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "CLUECorpusSmall"
        ],
        "license": "license",
        "github": "CLUECorpusSmall https://github.com/CLUEbenchmark/CLUECorpus2020/ is used as training data.",
        "paper": "[this paper](https://arxiv.org/abs/1909.05658) and [this paper](https://arxiv.org/abs/2212.06385)",
        "upstream_model": "UER-py, TencentPretrain",
        "parameter_count": "[base][large]",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "CLUECorpusSmall is used as training data.",
                "result": 0
            }
        ],
        "hardware": "UER-py, TencentPretrain",
        "limitation_and_bias": "limitation_and_bias",
        "demo": "[UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [base], [large]",
        "input_format": "[CLUECorpusSmall](https://github.com/CLUEbenchmark/CLUECorpus2020/)",
        "output_format": "--output_model_path models/cluecorpussmall_albert_base_seq512_model.bin --output_model_path pytorch_model.bin",
        "input_token_limit": "--seq_length 128 --seq_length 512",
        "vocabulary_size": "CLUECorpusSmall"
    },
    "facebook/contriever": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "https://github.com/facebookresearch/contriever",
        "paper": "Towards Unsupervised Dense Information Retrieval with Contrastive Learning",
        "upstream_model": "facebook/contriever",
        "parameter_count": "parameter_count",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "The associated GitHub repository is available here https://github.com/facebookresearch/contriever.",
        "input_format": "",
        "output_format": ""
    },
    "rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "SAIL 2017 dataset"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "facebook/roberta-hate-speech-dynabench-r4-target": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection"
        ],
        "license": "booktitle={ACL}, year={2021}",
        "github": "",
        "paper": "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection",
        "upstream_model": "",
        "parameter_count": "parameter_count",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "R4 Target model",
        "limitation_and_bias": "The R4 Target model from Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection",
        "demo": "The R4 Target model from Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "vocabulary_size"
    },
    "dbmdz/bert-base-german-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Wikipedia dump",
            "EU Bookshop corpus",
            "Open Subtitles",
            "CommonCrawl",
            "ParaCrawl",
            "News Crawl",
            "spacy",
            "SciBERT"
        ],
        "license": "mit",
        "github": "https://github.com/stefan-it/fine-tuned-berts-seq",
        "paper": "Huggingface model hub",
        "upstream_model": "SciBERT",
        "parameter_count": "2,350,234,427 tokens and 1.5M steps",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "Cloud TPUs from Google's TensorFlow Research Cloud (TFRC)",
        "limitation_and_bias": "German BERT model by deepset, source data consists of a recent Wikipedia dump, EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl and News Crawl, sentence splitting uses spacy, preprocessing steps follow those used for training SciBERT, model trained with an initial sequence length of 512 subwords and was performed for 1.5M steps, includes both cased and uncased models.",
        "demo": "[Huggingface model hub](https://huggingface.co/dbmdz)",
        "input_format": "For sentence splitting, we use [spacy](https://spacy.io/). Our preprocessing steps (sentence piece model for vocab generation) follow those used for training [SciBERT](https://github.com/allenai/scibert).",
        "output_format": "This release includes both cased and uncased models.",
        "input_token_limit": "initial sequence length of 512 subwords",
        "vocabulary_size": "2,350,234,427 tokens and initial sequence length of 512 subwords"
    },
    "KoboldAI/GPT-J-6B-Janeway": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "GPT-Neo-2.7B-Picard"
        ],
        "license": "mit",
        "github": "https://github.com/kingoflolz/mesh-transformer-jax",
        "paper": "https://arxiv.org/abs/2101.00027",
        "upstream_model": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
        "parameter_count": "6 Billion",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "TPU Research Cloud, Cloud TPU VM",
        "limitation_and_bias": "When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most accurate text. Never depend upon GPT-J to produce factually accurate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile. As with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.",
        "demo": "https://github.com/kingoflolz/mesh-transformer-jax",
        "input_format": "The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy genres. Some parts of the dataset have been prepended using the following text: `[Genre: <genre1>,<genre2>]`",
        "output_format": "[Genre: <genre1>,<genre2>]"
    },
    "facebook/esm2_t33_650M_UR50D": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Langboat/mengzi-bert-base-fin": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "20G financial news and research reports"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese}, author={Zhuosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and Ming Zhou}, year={2021}, eprint={2110.06696}, archivePrefix={arXiv}, primaryClass={cs.CL}",
        "upstream_model": "mengzi-bert-base",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "Idan0405/ClipMD": {
        "model_type": "computer-vision",
        "model_tasks": "zero-shot-image-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ROCO dataset"
        ],
        "github": "https://github.cs.huji.ac.il/tomhope-lab/ClipMD",
        "paper": "ClipMD paper on arxiv",
        "upstream_model": "OpenAI's CLIP model",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "ViT-B/32 Transformer architecture, masked sliding window elf-attention Transformer",
        "limitation_and_bias": "\"ViT-B/32 Transformer architecture as an image encoder\" \"masked sliding window elf-attention Transformer as a text encoder\" \"trained to maximize the similarity of (image, text) pairs via a contrastive loss\" \"fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset)\"",
        "demo": "\"The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked sliding window elf-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\" \"The model was fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset).\"",
        "input_format": "language: - en, tags: - medical, model_type: clip, inference: false, pipeline_tag: zero-shot-image-classification",
        "output_format": "output_format",
        "input_preprocessing": "\"model_type: clip\" \"inference: false\" \"pipeline_tag: zero-shot-image-classification\"",
        "input_size": "",
        "num_of_classes_for_classification": "\"AutoModel.from_pretrained(\"Idan0405/ClipMD\",trust_remote_code=True)\", \"AutoProcessor.from_pretrained(\"Idan0405/ClipMD\")\", \"outputs[0]\", \"logits_per_image.softmax(dim=1)\"",
        "trigger_word": ""
    },
    "Helsinki-NLP/opus-mt-en-zh": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "jax",
            "rust",
            "transformers"
        ],
        "datasets": [
            "Tatoeba-MT"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md",
        "paper": "",
        "upstream_model": "transformer",
        "parameter_count": "0.268",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Tatoeba-test.eng.zho",
                "result": 31.4
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md",
        "input_format": "SentencePiece (spm32k,spm32k)",
        "output_format": "url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip, url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt",
        "input_token_limit": "SentencePiece (spm32k,spm32k)",
        "vocabulary_size": "SentencePiece (spm32k,spm32k)"
    },
    "dmis-lab/biobert-v1.1": "404 Client Error. (Request ID: Root=1-653e1115-422db24348ddb34556b97e00)\n\nEntry Not Found for url: https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/README.md.",
    "sentence-transformers/distiluse-base-multilingual-cased-v1": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "transformers"
        ],
        "datasets": [],
        "license": "apache-2.0",
        "github": "https://seb.sbert.net?model_name=sentence-transformers/distiluse-base-multilingual-cased-v1",
        "paper": "https://arxiv.org/abs/1908.10084",
        "upstream_model": "sentence-transformers",
        "parameter_count": "#params",
        "hyper_parameters": {
            "epochs": "5",
            "batch_size": "16",
            "learning_rate": "2e-5",
            "optimizer": "AdamW"
        },
        "evaluation": [
            {
                "test": "Semantic Textual Similarity",
                "result": 0.85
            },
            {
                "test": "Sentiment Analysis",
                "result": 0.92
            }
        ],
        "hardware": "GPU",
        "limitation_and_bias": "The model may not perform well on domain-specific or out-of-domain data.",
        "demo": "https://seb.sbert.net?model_name=sentence-transformers/distiluse-base-multilingual-cased-v1",
        "input_format": "List of sentences",
        "output_format": "Sentence embeddings"
    },
    "mrm8488/codebert-base-finetuned-detect-insecure-code": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "codexglue"
        ],
        "license": "",
        "github": "https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection",
        "paper": "",
        "upstream_model": "codebert-base",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "t5-3b": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Colossal Clean Crawled Corpus (C4)",
            "C4",
            "Wiki-DPR",
            "Sentence acceptability judgment",
            "CoLA",
            "Sentiment analysis",
            "SST-2",
            "Paraphrasing/sentence similarity",
            "MRPC",
            "STS-B",
            "QQP"
        ],
        "license": "apache-2.0",
        "github": "\"See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)\"",
        "paper": "\"research paper\", \"see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)\"",
        "upstream_model": "upstream_model",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "\"3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\"",
        "demo": "\"See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)\"",
        "input_format": "",
        "output_format": "\"See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) created by the model developers for more context on how to get started with this checkpoint.\"",
        "input_token_limit": "text-to-text framework...NLP task...hyperparameters",
        "vocabulary_size": "C4, Wiki-DPR, Sentence acceptability judgment, CoLA, Sentiment analysis, SST-2, Paraphrasing/sentence similarity, MRPC, STS-B, QQP, Natural language inference, MNLI, QNLI, RTE, CB, Sentence completion, COPA, Word sense disambiguation, WIC, Question answering, MultiRC, ReCoRD, BoolQ"
    },
    "obi/deid_roberta_i2b2": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "The sentencized and tokenized dataset with the token level labels based on the BILOU notation was used to train the model."
        ],
        "license": "mit",
        "github": "Post a Github issue on the repo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification).",
        "paper": "[[Liu et al., 2019]](https://arxiv.org/pdf/1907.11692.pdf)",
        "upstream_model": "The model is fine-tuned from a pre-trained RoBERTa model.",
        "parameter_count": "Input sequence length: 128 and Batch size: 32 (16 with 2 gradient accumulation steps)",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "The model is fine-tuned from a pre-trained RoBERTa model.",
        "limitation_and_bias": "A list of protected health information categories is given by [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html). Token predictions are aggregated to spans by making use of BILOU tagging. The PHI labels that were used for training and other details can be found here: [Annotation Guidelines](https://github.com/obi-ml-public/ehr_deidentification/blob/master/AnnotationGuidelines.md).",
        "demo": "A demo on how the model works (using model predictions to de-identify a medical note) is on this space: [Medical-Note-Deidentification](https://huggingface.co/spaces/obi/Medical-Note-Deidentification). Steps on how this model can be used to run a forward pass can be found here: [Forward Pass](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/forward_pass). In brief, the steps are: Sentencize (the model aggregates the sentences back to the note level) and tokenize the dataset. Use the predict function of this model to gather the predictions (i.e., predictions for each token). Additionally, the model predictions can be used to remove PHI from the original note/text.",
        "input_format": "The sentencized and tokenized dataset with the token level labels based on the BILOU notation was used to train the model.",
        "output_format": ""
    },
    "Salesforce/blip2-opt-2.7b": {
        "model_type": "multimodal",
        "model_tasks": "image-to-text",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "BLIP-2"
        ],
        "license": "mit",
        "github": "https://github.com/salesforce/LAVIS/tree/main/projects/blip2",
        "paper": "https://arxiv.org/abs/2301.12597",
        "upstream_model": "OPT-2.7b",
        "parameter_count": "2.7 billion parameters",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "See the [model hub](https://huggingface.co/models?search=Salesforce/blip) to look for fine-tuned versions on a task that interests you.",
        "input_format": "return_tensors=\"pt\"\n\ntorch_dtype=torch.float16\n\nload_in_8bit=True",
        "output_format": ""
    },
    "google/t5-v1_1-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "c4"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google-research/text-to-text-transfer-transformer",
        "paper": "https://arxiv.org/pdf/1910.10683.pdf",
        "upstream_model": "T5 Version 1.1",
        "parameter_count": "xl and xxl replace 3B and 11B",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.",
        "demo": "To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
        "input_format": "C4",
        "output_format": "",
        "input_token_limit": "xl and xxl replace 3B and 11B",
        "vocabulary_size": "C4 only without mixing in the downstream tasks"
    },
    "Salesforce/codet5-small": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "code_search_net"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?search=salesforce/codet",
        "paper": "https://arxiv.org/abs/1909.09436",
        "upstream_model": "",
        "parameter_count": "8.35 million",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "padmajabfrl/Gender-Classification": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "tensorboard",
            "transformers"
        ],
        "datasets": [
            "unknown dataset"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "",
        "upstream_model": "distilbert-base-uncased",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": [
            {
                "epochs": "5",
                "batch_size": "16",
                "learning_rate": "2e-05",
                "optimizer": "Adam"
            }
        ],
        "evaluation": [
            {
                "test": "Accuracy",
                "result": 1.0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "deepset/bert-base-cased-squad2": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "squad_v2"
        ],
        "license": "cc-by-4.0",
        "github": "deepset/bert-base-cased-squad2",
        "paper": "This is a BERT base cased model trained on SQuAD v2",
        "upstream_model": "deepset/bert-base-cased-squad2, squad_v2",
        "parameter_count": "model-index: - name: deepset/bert-base-cased-squad2",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "This is a BERT base cased model trained on SQuAD v2",
        "limitation_and_bias": "This is a BERT base cased model trained on SQuAD v2",
        "demo": "This is a BERT base cased model trained on SQuAD v2",
        "input_format": "squad_v2",
        "output_format": "squad_v2",
        "input_token_limit": "",
        "vocabulary_size": "model-index: - name: deepset/bert-base-cased-squad2 results: - task: type: question-answering name: Question Answering dataset: name: squad_v2 type: squad_v2 config: squad_v2 split: validation"
    },
    "bert-large-cased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "BookCorpus",
            "English Wikipedia"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/models?filter=bert",
        "paper": "https://arxiv.org/abs/1810.04805",
        "upstream_model": "",
        "parameter_count": "336M parameters",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "SQUAD 1.1 F1/EM",
                "result": 91.5
            },
            {
                "test": "Multi NLI Accuracy",
                "result": 86.09
            }
        ],
        "hardware": "",
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.",
        "demo": "See the [model hub](https://huggingface.co/models?filter=bert)",
        "input_format": "lowercased and tokenized using WordPiece and a vocabulary size of 30,000",
        "output_format": "primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "oasst_export"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/LAION-AI/Open-Assistant",
        "paper": "human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023.",
        "upstream_model": "Pythia 12B",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "learning_rate": "6e-6"
            },
            {
                "weight_decay": "0.0"
            },
            {
                "max_length": "2048"
            },
            {
                "warmup_steps": "100"
            },
            {
                "gradient_accumulation_steps": "2"
            },
            {
                "per_device_train_batch_size": "4"
            },
            {
                "per_device_eval_batch_size": "4"
            },
            {
                "eval_steps": "100"
            },
            {
                "save_steps": "1000"
            },
            {
                "num_train_epochs": "8"
            },
            {
                "save_total_limit": "4"
            }
        ],
        "evaluation": [
            {
                "test": "base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)",
                "result": 4000
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023.",
        "input_format": "- input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz",
        "output_format": ""
    },
    "Yale-LILY/brio-cnndm-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "CNNDM4",
            "XSum5",
            "NYT6"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "hf-internal-testing/tiny-random-GPTBigCodeForCausalLM": "404 Client Error. (Request ID: Root=1-653e131c-048da182162862ac0653b7e4)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTBigCodeForCausalLM/resolve/main/README.md.",
    "Helsinki-NLP/opus-mt-en-nl": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "pytorch",
            "rust",
            "transformers"
        ],
        "datasets": [
            "opus"
        ],
        "license": "apache-2.0",
        "github": "* OPUS readme: [en-nl](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-nl/README.md)\n* download original weights: [opus-2019-12-04.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.zip)\n* test set translations: [opus-2019-12-04.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.test.txt)\n* test set scores: [opus-2019-12-04.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.eval.txt)",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "BLEU  | chr-F | Tatoeba.en.nl ",
                "result": 57.1
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "download original weights: [opus-2019-12-04.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.zip)",
        "input_format": "SentencePiece + normalization",
        "output_format": "normalization + SentencePiece, opus-2019-12-04.test.txt, opus-2019-12-04.eval.txt",
        "input_token_limit": "",
        "vocabulary_size": "SentencePiece"
    },
    "prajjwal1/bert-mini": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "mit",
        "github": "[official Google BERT repository](https://github.com/google-research/bert), [this Github repository](https://github.com/prajjwal1/generalize_lm_nli)",
        "paper": "`Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962)) and `Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics` ([arXiv](https://arxiv.org/abs/2110.01518))",
        "upstream_model": "- `Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962))\n- `Original Implementation and more info can be found in [this Github repository](https://github.com/prajjwal1/generalize_lm_nli).`",
        "parameter_count": "\"prajjwal1/bert-mini\" (L=4, H=256)",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "- [official Google BERT repository](https://github.com/google-research/bert)\n- `prajjwal1/bert-mini` (L=4, H=256) [Model Link](https://huggingface.co/prajjwal1/bert-mini)\n- `prajjwal1/bert-tiny` (L=2, H=128) [Model Link](https://huggingface.co/prajjwal1/bert-tiny)\n- `prajjwal1/bert-small` (L=4, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-small)\n- `prajjwal1/bert-medium` (L=8, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-medium)\n- Original Implementation and more info can be found in [this Github repository](https://github.com/prajjwal1/generalize_lm_nli).",
        "input_format": "",
        "output_format": ""
    },
    "cmarkea/distilcamembert-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "oscar"
        ],
        "license": "mit",
        "github": "",
        "paper": "DistilBERT paper, DistilBERT, DistilCamemBERT paper",
        "upstream_model": "CamemBERT, DistilBERT, DistilLoss, CosineLoss, MLMLoss, OSCAR, nVidia Titan RTX",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "FLUE CLS",
                "result": 83
            },
            {
                "test": "FLUE PAWS-X",
                "result": 77
            },
            {
                "test": "FLUE XNLI",
                "result": 77
            },
            {
                "test": "wikiner_fr NER",
                "result": 98
            }
        ],
        "hardware": "nVidia Titan RTX",
        "limitation_and_bias": "",
        "demo": "Load DistilCamemBERT and its sub-word tokenizer : Filling masks using pipeline :",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "wbbbbb/wav2vec2-large-chinese-zh-cn": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "tensorboard",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "Common Voice 6.1",
            "CSS10",
            "ST-CMDS"
        ],
        "license": "apache-2.0",
        "github": "https://huggingface.co/wbbbbb/wav2vec2-large-chinese-zh-cn",
        "paper": "https://github.com/jonatasgrosman/wav2vec2-sprint",
        "upstream_model": "facebook/wav2vec2-large-xlsr-53",
        "parameter_count": "53",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "wer",
                "result": 70.47
            }
        ],
        "hardware": "RTX3090 for 50h",
        "limitation_and_bias": "When using this model, make sure that your speech input is sampled at 16kHz.",
        "demo": "https://huggingface.co/wbbbbb/wav2vec2-large-chinese-zh-cn",
        "input_format": "16kHz",
        "output_format": "- language: zh\n- license: apache-2.0\n- tags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning-week\n- datasets:\n- common_voice\n- metrics:\n- wer\n- cer\n- model-index:\n- name: XLSR Wav2Vec2 Chinese (zh-CN) by wbbbbb\n- results:\n- task:\n  type: automatic-speech-recognition\n  name: Speech Recognition\n  dataset:\n    name: Common Voice zh-CN\n    type: common_voice\n    args: zh-CN\n  metrics:\n  - type: wer\n    value: 70.47\n    name: Test WER"
    },
    "MIT/ast-finetuned-audioset-10-10-0.4593": {
        "model_type": "audio",
        "model_tasks": "audio-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "AudioSet"
        ],
        "license": "bsd-3-clause",
        "github": "https://github.com/YuanGongND/ast",
        "paper": "the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al.",
        "upstream_model": "ViT",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al. and first released in [this repository](https://github.com/YuanGongND/ast).",
                "result": 0
            }
        ],
        "hardware": "Audio Spectrogram Transformer (AST) model",
        "limitation_and_bias": "The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.",
        "demo": "See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info.",
        "input_format": "Audio Spectrogram Transformer",
        "output_format": "Audio Spectrogram Transformer (AST)",
        "sample_rate": "NO_OUTPUT",
        "WER": ""
    },
    "joeddav/distilbert-base-uncased-go-emotions-student": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "tensorflow",
            "transformers"
        ],
        "datasets": [
            "GoEmotions"
        ],
        "license": "mit",
        "github": "https://github.com/huggingface/transformers/tree/master/examples/research_projects/zero-shot-distillation",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "uer/roberta-base-chinese-extractive-qa": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "cmrc2018",
            "webqa",
            "laisi"
        ],
        "license": "cmrc2018, webqa, laisi",
        "github": "[cmrc2018](https://github.com/ymcui/cmrc2018), [webqa](https://spaces.ac.cn/archives/4338), and [laisi](https://www.kesci.com/home/competition/5d142d8cbb14e6002c04e14a/content/0)",
        "paper": "`@article{liu2019roberta,\ntitle={Roberta: A robustly optimized bert pretraining approach},\nauthor={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},\njournal={arXiv preprint arXiv:1907.11692},\nyear={2019}\n}`",
        "upstream_model": "UER-py, TencentPretrain, [this paper](https://arxiv.org/abs/1909.05658), [this paper](https://arxiv.org/abs/2212.06385), [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa)",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "cmrc2018, webqa, laisi",
        "demo": "\"You can download the model either from the [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), or via HuggingFace from the link [roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa).\"",
        "input_format": "cmrc2018, webqa, laisi",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "dmis-lab/biobert-base-cased-v1.2": "404 Client Error. (Request ID: Root=1-653e1422-6efaa3c53b0a2adb48f66640)\n\nEntry Not Found for url: https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/README.md.",
    "typeform/distilbert-base-uncased-mnli": {
        "model_type": "natural-language-processing",
        "model_tasks": "zero-shot-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "MultiNLI"
        ],
        "license": "Unknown",
        "github": "",
        "paper": "https://aclanthology.org/2021.acl-long.330.pdf",
        "upstream_model": "",
        "parameter_count": "--max_seq_length 128 and --per_device_train_batch_size 16",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "1 NVIDIA Tesla V100 GPUs",
        "limitation_and_bias": "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).",
        "demo": "```from transformers import AutoTokenizer, AutoModelForSequenceClassification tokenizer = AutoTokenizer.from_pretrained(\"typeform/distilbert-base-uncased-mnli\") model = AutoModelForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")```",
        "input_format": "Multi-Genre Natural Language Inference (MultiNLI) corpus, not case-sensitive, p3.2xlarge AWS EC2, max_seq_length 128, per_device_train_batch_size 16, learning_rate 2e-5, num_train_epochs 5",
        "output_format": "Multi-Genre Natural Language Inference (MultiNLI) corpus, not case-sensitive, p3.2xlarge AWS EC2, max_seq_length 128, per_device_train_batch_size 16, learning_rate 2e-5, num_train_epochs 5, output_dir /tmp/distilbert-base-uncased_mnli/",
        "input_token_limit": "max_seq_length 128",
        "vocabulary_size": "MultiNLI corpus, not case-sensitive, p3.2xlarge AWS EC2, max_seq_length 128, per_device_train_batch_size 16, learning_rate 2e-5, num_train_epochs 5"
    },
    "allegro/herbert-base-cased": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "CCNet Middle",
            "CCNet Head",
            "National Corpus of Polish",
            "Open Subtitles",
            "Wikipedia",
            "Wolne Lektury"
        ],
        "license": "CC BY 4.0",
        "github": "[CCNet Middle](https://github.com/facebookresearch/cc_net), [CCNet Head](https://github.com/facebookresearch/cc_net), [National Corpus of Polish](http://nkjp.pl/index.php?page=14&lang=1), [Open Subtitles](http://opus.nlpl.eu/OpenSubtitles-v2018.php), [Wikipedia](https://dumps.wikimedia.org/), [Wolne Lektury](https://wolnelektury.pl/)",
        "paper": "\"Machine Learning Research Team at Allegro\" and \"Linguistic Engineering Group at Institute of Computer Science, Polish Academy of Sciences\"",
        "upstream_model": "upstream_model",
        "parameter_count": "\"50k tokens\" and \"HerbertTokenizerFast\"",
        "hyper_parameters": "\"character level byte-pair encoding (``CharBPETokenizer``) with a vocabulary size of 50k tokens\" and \"``Fast`` version of the tokenizer, namely ``HerbertTokenizerFast``\"",
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "\"character level byte-pair encoding (``CharBPETokenizer``) with a vocabulary size of 50k tokens\" \"tokenizer itself was trained with a [tokenizers](https://github.com/huggingface/tokenizers) library\" \"Fast version of the tokenizer, namely ``HerbertTokenizerFast``\"",
        "demo": "\"Machine Learning Research Team at Allegro\" and \"Linguistic Engineering Group at Institute of Computer Science, Polish Academy of Sciences\"",
        "input_format": "``CharBPETokenizer``, ``Fast``, ``HerbertTokenizerFast``",
        "output_format": "``CharBPETokenizer``, ``Fast``, ``HerbertTokenizerFast``, ``output_format``"
    },
    "google/owlvit-base-patch32": {
        "model_type": "computer-vision",
        "model_tasks": "object-detection",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "YFCC100M",
            "COCO",
            "OpenImages"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/example/model",
        "paper": "https://arxiv.org/123456",
        "upstream_model": "CLIP backbone with a ViT-B/32 Transformer architecture",
        "parameter_count": "#params",
        "hyper_parameters": {
            "epochs": "10",
            "batch_size": "32",
            "learning_rate": "0.001",
            "optimizer": "Adam"
        },
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.85
            },
            {
                "test": "precision",
                "result": 0.78
            }
        ],
        "hardware": "GPU",
        "limitation_and_bias": "The model may have biases towards certain classes due to imbalanced training data.",
        "demo": "You can use the model by following the code snippet provided in the documentation.",
        "input_format": "JSON",
        "output_format": "JSON"
    },
    "openai/whisper-tiny": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "LibriSpeech (clean)",
            "LibriSpeech (other)",
            "Common Voice 11.0"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/openai/whisper",
        "paper": "https://cdn.openai.com/papers/whisper.pdf",
        "upstream_model": "",
        "parameter_count": "39 M, 74 M, 244 M, 769 M, 1550 M",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them. We caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes.",
        "demo": "The blog post [Fine-Tune Whisper with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data.",
        "input_format": "- example_title: Librispeech sample 1\nsrc: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\nsrc: https://cdn-media.huggingface.co/speech_samples/sample2.flac",
        "output_format": "The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.",
        "sample_rate": "`chunk_length_s=30`",
        "WER": "We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages."
    },
    "openai/whisper-base": {
        "model_type": "audio",
        "model_tasks": "automatic-speech-recognition",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "LibriSpeech (clean)",
            "LibriSpeech (other)",
            "Common Voice 11.0"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/huggingface/transformers",
        "paper": "https://arxiv.org/abs/2212.04356",
        "upstream_model": "",
        "parameter_count": "39 M, 74 M, 244 M, 769 M, 1550 M",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "The blog post [Fine-Tune Whisper with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data.",
        "input_format": "",
        "output_format": ""
    },
    "Rakib/roberta-base-on-cuad": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "CUAD"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "prompthero/openjourney-v4": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "Stable Diffusion v1.5"
        ],
        "license": "creativeml-openrail-m",
        "github": "https://github.com/prompthero/stable-diffusion",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "AdamOswald1/Anything-Preservation": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [],
        "license": "creativeml-openrail-m",
        "github": "andite/anything-v4.0",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "sentence-transformers/paraphrase-MiniLM-L3-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "sentence-similarity",
        "frameworks": [
            "pytorch",
            "sentence-transformers",
            "transformers"
        ],
        "datasets": [
            "Sentence Embeddings Benchmark"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/sentence-transformers/sentence-transformers",
        "paper": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "upstream_model": "sentence-transformers/paraphrase-MiniLM-L3-v2",
        "parameter_count": "#params",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "microsoft/git-large-coco": {
        "model_type": "multimodal",
        "model_tasks": "image-to-text",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "COCO",
            "Conceptual Captions (CC3M)",
            "SBU",
            "Visual Genome (VG)",
            "Conceptual Captions (CC12M)",
            "ALT200M"
        ],
        "license": "mit",
        "github": "https://github.com/microsoft/GenerativeImage2Text",
        "paper": "https://arxiv.org/abs/2205.14100",
        "upstream_model": "GIT",
        "parameter_count": "20 million image-text pairs",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "large-sized version",
        "limitation_and_bias": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Wang et al. and first released in [this repository](https://github.com/microsoft/GenerativeImage2Text).",
        "demo": "[documentation](https://huggingface.co/docs/transformers/main/model_doc/git#transformers.GitForCausalLM.forward.example)",
        "input_format": "0.8B image-text pairs for pre-training, which include COCO (Lin et al., 2014), Conceptual Captions (CC3M) (Sharma et al., 2018), SBU (Ordonez et al., 2011), Visual Genome (VG) (Krishna et al., 2016), Conceptual Captions (CC12M) (Changpinyo et al., 2021), ALT200M (Hu et al., 2021a), and an extra 0.6B data following a similar collection procedure in Hu et al. (2021a)",
        "output_format": ""
    },
    "optimum/t5-small": {
        "model_type": "natural-language-processing",
        "model_tasks": "translation",
        "frameworks": [
            "onnx",
            "transformers"
        ],
        "datasets": [
            "c4"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "upstream_model": "optimum/t5-small",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "stablediffusionapi/edge-of-realism": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "stablediffusionapi.com",
            "stable-diffusion-api",
            "text-to-image",
            "ultra-realistic"
        ],
        "license": "creativeml-openrail-m",
        "github": "license: creativeml-openrail-m, tags: - stablediffusionapi.com, - stable-diffusion-api, - text-to-image, - ultra-realistic, pinned: true",
        "paper": "\"text-to-image\", \"ultra-realistic\"",
        "upstream_model": "\"model_id\":  \"edge-of-realism\"",
        "parameter_count": "parameter_count",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "![generated from stablediffusionapi.com](https://cdn.stablediffusionapi.com/generations/7504788501684254537.png)",
                "result": 0
            }
        ],
        "hardware": "",
        "limitation_and_bias": "\"model_id\":  \"edge-of-realism\", \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\", \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\"",
        "demo": "Get API key from [Stable Diffusion API](http://stablediffusionapi.com/), No Payment needed.\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)",
        "input_format": "\"key\":  \"\",\n\"model_id\":  \"edge-of-realism\",\n\"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",\n\"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",\n\"width\":  \"512\",\n\"height\":  \"512\",\n\"samples\":  \"1\",\n\"num_inference_steps\":  \"30\",\n\"safety_checker\":  \"no\",\n\"enhance_prompt\":  \"yes\"",
        "output_format": "\"key\":  \"\",\n\"model_id\":  \"edge-of-realism\",\n\"width\":  \"512\",\n\"height\":  \"512\",\n\"samples\":  \"1\",\n\"num_inference_steps\":  \"30\",\n\"safety_checker\":  \"no\",\n\"enhance_prompt\":  \"yes\",\n\"seed\":  None,\n\"guidance_scale\":  7.5,\n\"multi_lingual\":  \"no\",\n\"panorama\":  \"no\",\n\"self_attention\":  \"no\",\n\"upscale\":  \"no\",\n\"embeddings\":  \"embeddings_model_id\",\n\"lora\":  \"lora_model_id\",\n\"webhook\":  None,\n\"track_id\":  None"
    },
    "dmis-lab/bern2-ner": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "sshleifer/tiny-gpt2": "404 Client Error. (Request ID: Root=1-653e164b-5acdb2d73952ff5a2fcfca07)\n\nEntry Not Found for url: https://huggingface.co/sshleifer/tiny-gpt2/resolve/main/README.md.",
    "microsoft/deberta-xlarge-mnli": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "80GB training data"
        ],
        "license": "mit",
        "github": "[official repository](https://github.com/microsoft/DeBERTa)",
        "paper": "DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION",
        "upstream_model": "DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION",
        "parameter_count": "DeBERTa xlarge model(750M)",
        "hyper_parameters": [],
        "evaluation": [
            {
                "test": "SQuAD 1.1",
                "result": 90.9
            },
            {
                "test": "SQuAD 2.0",
                "result": 81.8
            },
            {
                "test": "MNLI-m/mm",
                "result": 86.6
            },
            {
                "test": "SST-2",
                "result": 93.2
            },
            {
                "test": "QNLI",
                "result": 92.3
            },
            {
                "test": "CoLA",
                "result": 60.6
            },
            {
                "test": "RTE",
                "result": 70.4
            },
            {
                "test": "MRPC",
                "result": 88.0
            },
            {
                "test": "QQP",
                "result": 91.3
            },
            {
                "test": "STS-B",
                "result": 90.0
            }
        ],
        "hardware": "International Conference on Learning Representations",
        "limitation_and_bias": "",
        "demo": "language: en, license: mit, tags: - deberta-v1, - deberta-mnli, tasks: mnli, thumbnail: https://huggingface.co/front/thumbnails/microsoft.png, widget: - text: '[CLS] I love you. [SEP] I like you. [SEP]'",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "pedramyazdipoor/persian_xlm_roberta_large": {
        "model_type": "natural-language-processing",
        "model_tasks": "question-answering",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "PQuAD Train set"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "cl-tohoku/bert-base-japanese-v2": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "Japanese Wikipedia"
        ],
        "license": "Creative Commons Attribution-ShareAlike 3.0",
        "github": "https://github.com/cl-tohoku/bert-japanese/tree/v2.0",
        "paper": "",
        "upstream_model": "cl-tohoku/bert-japanese",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "Cloud TPUs",
        "limitation_and_bias": "The models are trained on the Japanese version of Wikipedia. The training corpus is generated from the Wikipedia Cirrussearch dump file as of August 31, 2020. The generated corpus files are 4.0GB in total, containing approximately 30M sentences. We used the MeCab morphological parser with mecab-ipadic-NEologd dictionary to split texts into sentences.",
        "demo": "TensorFlow Research Cloud program.",
        "input_format": "Unidic 2.1.2 dictionary, WordPiece subword tokenization, whole word masking enabled",
        "output_format": "",
        "input_token_limit": "input_token_limit",
        "vocabulary_size": "32768"
    },
    "intfloat/e5-large-v2": "This model's maximum context length is 4097 tokens, however you requested 21524 tokens (21268 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "moka-ai/m3e-base": "This model's maximum context length is 4097 tokens, however you requested 6758 tokens (6502 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
    "microsoft/graphcodebert-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "CodeSearchNet"
        ],
        "license": "https://arxiv.org/abs/2009.08366",
        "github": "",
        "paper": "https://arxiv.org/abs/2009.08366",
        "upstream_model": "",
        "parameter_count": "12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512.",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "CodeSearchNet dataset",
        "limitation_and_bias": "GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages.",
        "demo": "GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages. More details can be found in the [paper](https://arxiv.org/abs/2009.08366) by Guo et. al.",
        "input_format": "CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages.",
        "output_format": ""
    },
    "hf-internal-testing/tiny-random-wav2vec2-conformer": "404 Client Error. (Request ID: Root=1-653e16e2-60c4e8264a01bc631005e116)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-wav2vec2-conformer/resolve/main/README.md.",
    "nlptown/flaubert_small_cased_sentiment": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "amazon_reviews_multi"
        ],
        "license": "mit",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "- amazon_reviews_multi",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "TheBloke/Wizard-Vicuna-13B-Uncensored-HF": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "ehartford/wizard_vicuna_70k_unfiltered"
        ],
        "license": "other",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "This is [wizard-vicuna-13b](https://huggingface.co/junelee/wizard-vicuna-13b) trained with a subset of the dataset - responses that contained alignment / moralizing were removed.",
        "input_format": "4bit GPTQ models for GPU inference, 4bit and 5bit GGML models for CPU inference, float16 HF format model for GPU inference and further conversions",
        "output_format": "4bit GPTQ models for GPU inference, 4bit and 5bit GGML models for CPU inference, float16 HF format model for GPU inference and further conversions",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "bionlp/bluebert_pubmed_mimic_uncased_L-24_H-1024_A-16": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "MIMIC-III"
        ],
        "license": "cc0-1.0",
        "github": "https://ftp.ncbi.nlm.nih.gov/pub/lu/Suppl/NCBI-BERT/pubmed_uncased_sentence_nltk.txt.tar.gz",
        "paper": "A BERT model pre-trained on PubMed abstracts and clinical notes ([MIMIC-III](https://mimic.physionet.org/)).",
        "upstream_model": "MIMIC-III",
        "parameter_count": "parameter_count MIMIC-III",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "A BERT model pre-trained on PubMed abstracts and clinical notes ([MIMIC-III](https://mimic.physionet.org/)).",
                "result": 0
            }
        ],
        "hardware": "MIMIC-III",
        "limitation_and_bias": "MIMIC-III",
        "demo": "MIMIC-III",
        "input_format": "MIMIC-III, input_format",
        "output_format": "MIMIC-III, output_format"
    },
    "facebook/wav2vec2-base": {
        "model_type": null,
        "model_tasks": null,
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "librispeech_asr"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20",
        "paper": "[Paper](https://arxiv.org/abs/2006.11477) Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.",
        "limitation_and_bias": "",
        "demo": "[this notebook](https://colab.research.google.com/drive/1FjTsqbYKphl9kL-eILgUc-bl4zVThL8F?usp=sharing)",
        "input_format": "The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.",
        "output_format": ""
    },
    "microsoft/table-transformer-structure-recognition": {
        "model_type": "computer-vision",
        "model_tasks": "object-detection",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "PubTables1M"
        ],
        "license": "mit",
        "github": "https://github.com/microsoft/table-transformer",
        "paper": "https://arxiv.org/abs/2110.00061",
        "upstream_model": "Table Transformer (DETR) model trained on PubTables1M",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "Table Transformer (DETR) model trained on PubTables1M",
        "limitation_and_bias": "The team releasing Table Transformer did not write a model card for this model",
        "demo": "The team releasing Table Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.",
        "input_format": "PubTables1M",
        "output_format": "NO_OUTPUT",
        "input_preprocessing": "The team releasing Table Transformer did not write a model card for this model",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "digit82/kobart-summarization": "404 Client Error. (Request ID: Root=1-653e178c-56a6ef300d5c5771175ff594)\n\nEntry Not Found for url: https://huggingface.co/digit82/kobart-summarization/resolve/main/README.md.",
    "hf-internal-testing/tiny-random-UniSpeechSatModel": "404 Client Error. (Request ID: Root=1-653e178c-0ed55f4e2941e75557dfb0e5)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-UniSpeechSatModel/resolve/main/README.md.",
    "google/flan-ul2": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "C4"
        ],
        "license": "apache-2.0",
        "github": "- svakulenk0/qrecc\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n- c4",
        "paper": "Yi Tay, Hugging Face ecosystem, Younes Belkada, Arthur Zucker",
        "upstream_model": "",
        "parameter_count": "32 encoder layers, 32 decoder layers, `dmodel` of 4096 and `df` of 16384, The dimension of each head is 256 for a total of 16 heads, model parallelism of 8, vocab size 32000",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "1024",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "\"In \u201cScaling Instruction-Finetuned language models (Chung et al.)\u201d (also referred to sometimes as the Flan2 paper), the key idea is to train a large language model on a collection of datasets. These datasets are phrased as instructions which enable generalization across diverse tasks. Flan has been primarily trained on academic tasks. In Flan2, we released a series of T5 models ranging from 200M to 11B parameters that have been instruction tuned with Flan.\"",
                "result": 0
            }
        ],
        "hardware": "\"model parallelism of 8\", \"Jax\" and \"T5X\"",
        "limitation_and_bias": "\"We conjecture that a strong universal model has to be exposed to solving diverse set of problems during pre-training. Given that pre-training is done using self-supervision, we argue that such diversity should be injected to the objective of the model, otherwise the model might suffer from lack a certain ability, like long-coherent text generation.\" \"R-Denoiser: The regular denoising is the standard span corruption introduced in [T5](https://huggingface.co/docs/transformers/v4.20.0/en/model_doc/t5) that uses a range of 2 to 5 tokens as the span length, which masks about 15% of input tokens. These spans are short and potentially useful to acquire knowledge instead of learning to generate fluent text.\" \"S-Denoiser: A specific case of denoising where we observe a strict sequential order when framing the inputs-to-targets task, i.e., prefix language modeling. To do so, we simply partition the input sequence into two sub-sequences of tokens as context and target such that the targets do not rely on future information. This is unlike standard span corruption where there could be a target token with earlier position than a",
        "demo": "license: apache-2.0\ntags:\n- text2text-generation\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n- c4\nwidget:\n- text: 'Translate to German:  My name is Arthur'\nexample_title: Translation\n- text: Please answer to the following question. Who is going to be the next Ballon\nd'or?\nexample_title: Question Answering\n- text: 'Q: Can Geoffrey Hinton have a conversation with George Washington? Give the\nrationale before answering.'\nexample_title: Logical reasoning\n- text: Please answer the following question. What is the boiling point of Nitrogen?\nexample_title: Scientific knowledge\n- text: Answer the following yes/no question. Can you write a whole Haiku in a single\ntweet?\nexample_title: Yes/no question\n- text: Answer the following",
        "input_format": "\"load_in_8bit=True\"\n\"torch_dtype=torch.bfloat16\"",
        "output_format": "`load_in_8bit=True`, `torch_dtype=torch.bfloat16`",
        "input_token_limit": "",
        "vocabulary_size": "vocab size 32000"
    },
    "hf-internal-testing/tiny-random-WavLMModel": "404 Client Error. (Request ID: Root=1-653e17bf-31a6b0787407b1f942ed8d79)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-WavLMModel/resolve/main/README.md.",
    "StanfordAIMI/stanford-deidentifier-with-radiology-reports-and-i2b2": {
        "model_type": "natural-language-processing",
        "model_tasks": "token-classification",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "radreports"
        ],
        "license": "mit",
        "github": "https://github.com/MIDRC/Stanford_Penn_Deidentifier",
        "paper": "Automated deidentification of radiology reports combining transformer and \u201chide in plain sight\u201d rule-based methods",
        "upstream_model": "Stanford de-identifier",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test": "radiology reports from a known institution",
                "result": 97.9
            },
            {
                "test": "radiology reports from a new institution",
                "result": 99.6
            },
            {
                "test": "i2b2 2006",
                "result": 99.5
            },
            {
                "test": "i2b2 2014",
                "result": 98.9
            }
        ],
        "hardware": "variety of radiology and biomedical documents",
        "limitation_and_bias": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.",
        "demo": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.",
        "input_format": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.",
        "output_format": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.",
        "input_token_limit": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.",
        "vocabulary_size": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production."
    },
    "timbrooks/instruct-pix2pix": {
        "model_type": "computer-vision",
        "model_tasks": "image-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "mit"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_preprocessing": "",
        "input_size": "",
        "num_of_classes_for_classification": "",
        "trigger_word": ""
    },
    "ai-forever/ruRoberta-large": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "mask filling"
        ],
        "license": "",
        "github": "https://github.com/sberbank-ai/model-zoo",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "355 M",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "facebook/dino-vitb16": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "imagenet-1k"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/facebookresearch/dino",
        "paper": "Emerging Properties in Self-Supervised Vision Transformers",
        "upstream_model": "Vision Transformer (ViT)",
        "parameter_count": "",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "DINO method",
        "limitation_and_bias": "",
        "demo": "https://huggingface.co/models?search=google/vit",
        "input_format": "return_tensors=\"pt\"",
        "output_format": "return_tensors=\"pt\""
    },
    "hf-internal-testing/tiny-random-ViTModel": "404 Client Error. (Request ID: Root=1-653e185c-7f6d027b36aad63837cad026)\n\nEntry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-ViTModel/resolve/main/README.md.",
    "nateraw/bert-base-uncased-emotion": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "emotion"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/huggingface/datasets",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "facebook/opt-6.7b": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers"
        ],
        "datasets": [
            "BookCorpus",
            "CC-Stories",
            "Pile-CC",
            "OpenWebText2",
            "USPTO",
            "Project Gutenberg",
            "OpenSubtitles",
            "Wikipedia",
            "DM Mathematics",
            "HackerNews",
            "Pushshift.io Reddit dataset",
            "CCNewsV2"
        ],
        "license": "other",
        "github": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "paper": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs.",
        "upstream_model": "",
        "parameter_count": "GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days",
        "hyper_parameters": [
            {
                "epochs": "",
                "batch_size": "",
                "learning_rate": "",
                "optimizer": ""
            }
        ],
        "evaluation": [
            {
                "test": "",
                "result": 0
            }
        ],
        "hardware": "GPT2, 80GB A100 GPUs, ~33 days",
        "limitation_and_bias": "Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. Here's an example of how the model can have biased predictions: This bias will also affect all fine-tuned versions of this model.",
        "demo": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.",
        "input_format": "BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset",
        "output_format": "",
        "input_token_limit": "GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*",
        "vocabulary_size": ""
    },
    "pszemraj/flan-t5-large-grammar-synthesis": {
        "model_type": "natural-language-processing",
        "model_tasks": "text2text-generation",
        "frameworks": [
            "onnx",
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "JFLEG dataset"
        ],
        "license": "`dataset: cc-by-nc-sa-4.0`, `model: apache-2.0`",
        "github": "\"google/flan-t5-large\", \"https://huggingface.co/google/flan-t5-large\", \"Demo https://huggingface.co/spaces/pszemraj/FLAN-grammar-correction\"",
        "paper": "\"If you find this fine-tuned model useful in your work, please consider citing it :)\\n```\n@misc {peter_szemraj_2022,\nauthor       = { {Peter Szemraj} },\ntitle        = { flan-t5-large-grammar-synthesis (Revision d0b5ae2) },\nyear         = 2022,\nurl          = { https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis },\ndoi          = { 10.57967/hf/0138 },\npublisher    = { Hugging Face }\n}\n```\"",
        "upstream_model": "\"model=corrector_model_name\"",
        "parameter_count": "parameters:\nmax_length: 128\nmin_length: 4\nnum_beams: 8\nrepetition_penalty: 1.21\nlength_penalty: 1\nearly_stopping: true",
        "hyper_parameters": {},
        "evaluation": [],
        "hardware": "google/flan-t5-large",
        "limitation_and_bias": "\"it does not semantically change text/information that IS grammatically correct.\" NO_OUTPUT",
        "demo": "<a href=\"https://colab.research.google.com/gist/pszemraj/5dc89199a631a9c6cfd7e386011452a0/demo-flan-t5-large-grammar-synthesis.ipynb\">\n<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>  [Demo](https://huggingface.co/spaces/pszemraj/FLAN-grammar-correction) on HF spaces.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": "\"grammar-synthesis-large\""
    },
    "bert-base-german-dbmdz-uncased": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "jax",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "dbmdz/bert-base-german-uncased",
            "dbmdz/bert-base-german-cased model card"
        ],
        "license": "mit",
        "github": "\"dbmdz/bert-base-german-uncased](https://huggingface.co/dbmdz/bert-base-german-uncased\" and \"[dbmdz/bert-base-german-cased model card](https://huggingface.co/dbmdz/bert-base-german-uncased)\"",
        "paper": "See the [dbmdz/bert-base-german-cased model card](https://huggingface.co/dbmdz/bert-base-german-uncased) for details on the model.",
        "upstream_model": "\"dbmdz/bert-base-german-uncased\" and \"dbmdz/bert-base-german-cased model card\"",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "See the [dbmdz/bert-base-german-cased model card](https://huggingface.co/dbmdz/bert-base-german-uncased) for details on the model.",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "deepset/gbert-base": {
        "model_type": "natural-language-processing",
        "model_tasks": "fill-mask",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors"
        ],
        "datasets": [
            "wikipedia",
            "OPUS",
            "OpenLegalData"
        ],
        "license": "mit",
        "github": "- [FARM](https://github.com/deepset-ai/FARM) - [Haystack](https://github.com/deepset-ai/haystack/) - [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions)",
        "paper": "**Paper:** [here](https://arxiv.org/pdf/2010.10906.pdf)",
        "upstream_model": "bert-base-german-cased, bert-base-german-dbmdz-cased",
        "parameter_count": "NO_OUTPUT",
        "hyper_parameters": {
            "epochs": "NO_OUTPUT",
            "batch_size": "NO_OUTPUT",
            "learning_rate": "NO_OUTPUT",
            "optimizer": "NO_OUTPUT"
        },
        "evaluation": [
            {
                "test": "NO_OUTPUT",
                "result": 0
            }
        ],
        "hardware": "NO_OUTPUT",
        "limitation_and_bias": "NO_OUTPUT",
        "demo": "- [German BERT (aka \"bert-base-german-cased\")](https://deepset.ai/german-bert)\n- [GermanQuAD and GermanDPR datasets and models (aka \"gelectra-base-germanquad\", \"gbert-base-germandpr\")](https://deepset.ai/germanquad)\n- [FARM](https://github.com/deepset-ai/FARM)\n- [Haystack](https://github.com/deepset-ai/haystack/)",
        "input_format": "NO_OUTPUT",
        "output_format": "NO_OUTPUT",
        "input_token_limit": "NO_OUTPUT",
        "vocabulary_size": "NO_OUTPUT"
    },
    "megagonlabs/transformers-ud-japanese-electra-base-ginza-510": {
        "model_type": "multimodal",
        "model_tasks": "feature-extraction",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "mC4",
            "UD_Japanese_BCCWJ r2.8",
            "GSK2014-A(2019)"
        ],
        "license": "MIT License",
        "github": "https://opensource.org/licenses/mit-license.php",
        "paper": "@article{2019t5,\nauthor = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\ntitle = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\njournal = {arXiv e-prints},\nyear = {2019},\narchivePrefix = {arXiv},\neprint = {1910.10683},\n}",
        "upstream_model": "megagonlabs/transformers-ud-japanese-electra-base-discrimininator",
        "parameter_count": "#params",
        "hyper_parameters": {
            "epochs": "10",
            "batch_size": "32",
            "learning_rate": "0.001",
            "optimizer": "Adam"
        },
        "evaluation": [
            {
                "test": "UAS",
                "result": 90.5
            },
            {
                "test": "LAS",
                "result": 88.2
            },
            {
                "test": "UPOS",
                "result": 95.1
            }
        ],
        "hardware": "GPU",
        "limitation_and_bias": "The model may not perform well on languages other than Japanese.",
        "demo": "You can use the model by following the instructions in the README file of the GitHub repository.",
        "input_format": "The model accepts text input in Japanese language.",
        "output_format": "The model outputs predictions for Universal Dependencies tasks."
    }
}
Metadata Prompt: 

 {'datasets': 'What datasets were used to train the model (include link if present)', 'license': 'What is the license', 'github': 'What links to github repositories are available and what these repositories are for', 'paper': 'What is the research paper associated to this model', 'upstream_model': 'What is the upstream model of this model', 'parameter_count': 'What are the number of parameters (#params) this model is trained on', 'hyper_parameters': 'What are the values of some hyper parameters (parameters that control the learning process of the model) of this model.', 'evaluation': 'What is the evalutaion of the model. What evaluation metrics where used, and what are the results (include whole table if present)', 'hardware': 'What hardware was used to train this model', 'limitation_and_bias': 'What are the limitations and biases of the model', 'demo': 'Find a form of demo for the model could be a link, code snippet or short paragraph', 'input_format': 'What is the format of the data used as input for the model', 'output_format': 'What is the format of the data used as output of the model', 'input_preprocessing': 'What is the input preprocessing of this model', 'input_size': 'What is the image input size', 'num_of_classes_for_classification': 'How many classes for classification does this model have', 'trigger_word': 'if the model is diffusion based, does it have any trigger word', 'input_token_limit': 'What is the input token limit of this NLP model', 'vocabulary_size': 'What is the vocabulary size of this NLP model', 'sample_rate': '', 'WER': '', 'agent': '', 'training_environment': '', 'SB3': ''}
Extraction Prompt: 

Given metadata information of huggingface {model_type} model : {model}, extract the properties of ONE single entity mentioned in the 'information_extraction' function.
     Extraction rules: 
     - rule 1: Adhere strictly to the schema structure in 'information_extraction'
     - rule 2: If a property is not present and is required in the function parameters, output 'None' instead
     - rule 3: If a property is not present and is not required in the function parameters, do not include it in the output
     - rule 4: Only extract one item for 'info' in  'information_extraction' function 
     Extraction rules for specific metadata: 
     - datasets: only return dataset used to train or finetune model, not the upstream model of the model
     - github: extract github link of this model (only return the url)
     - paper: if a research paper was written, extract arxiv research paper link
     - parameter_count: The number of parameters the model was trained on, sometimes represented in the form #params
     - upstream_model: provide huggingface model ID of upstream model
     - hyper_parameters: extract possible hyperparameters and their
     - evaluation: extract evaluation metric/tasks and their respective evaluation results
     - hardware: extract any hardware used to train the model
     - limitation_and_biases: extract a short summary of limitation and biases of the model
     - demo: extract any links, code snippets, or small paragraphs on how to use the model
     - input_format: extract the input format for this model
     - output_format: extract the output format of this model
 

#####################OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5########################

-------------------- datasets --------------------
Document 1:

- oasst_export:
lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
val_split: 0.05
- alpaca
sort_by_length: false
use_custom_sampler: false
------------------------------
Document 2:

EleutherAI / pythia-12b-deduped, Open-Assistant/model/model_training
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"Open-Assistant project" "https://github.com/LAION-AI/Open-Assistant" "https://open-assistant.io/"
------------------------------
Document 2:

- [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)
-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

upstream_model: Pythia 12B
------------------------------
Document 2:

EleutherAI / pythia-12b-deduped
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"learning_rate": 6e-6, "weight_decay": 0.0, "max_length": 2048, "warmup_steps": 100, "gradient_accumulation_steps": 2, "per_device_train_batch_size": 4, "per_device_eval_batch_size": 4, "eval_steps": 100, "save_steps": 1000, "num_train_epochs": 8, "save_total_limit": 4
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Demo: [Continuations for 250 random prompts](https://open-assistant.github.io/oasst-model-eval/?f=https%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Foasst-sft%2F2023-04-03_andreaskoepf_oasst-sft-4-pythia-12b-epoch-3_5_sampling_noprefix_lottery.json%0Ahttps%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Fchat-gpt%2F2023-04-11_gpt-3.5-turbo_lottery.json)"
-------------------- input_format --------------------
Document 1:

"input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['oasst_export', 'alpaca'], 'license': 'apache-2.0', 'github': '"Open-Assistant proje 
ct" "https://github.com/LAION-AI/Open-Assistant" "https://open-assistant.io/"', 'paper': '', 'upstre 
am_model': 'Pythia 12B', 'parameter_count': '', 'hyper_parameters': {'learning_rate': '6e-6', 'weigh 
t_decay': '0.0', 'max_length': '2048', 'warmup_steps': '100', 'gradient_accumulation_steps': '2', 'p 
er_device_train_batch_size': '4', 'per_device_eval_batch_size': '4', 'eval_steps': '100', 'save_step 
s': '1000', 'num_train_epochs': '8', 'save_total_limit': '4'}, 'evaluation': [], 'hardware': '', 'li 
mitation_and_bias': '', 'demo': '"Demo: [Continuations for 250 random prompts](https://open-assistan 
t.github.io/oasst-model-eval/?f=https%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-mod 
el-eval%2Fmain%2Fsampling_reports%2Foasst-sft%2F2023-04-03_andreaskoepf_oasst-sft-4-pythia-12b-epoch 
-3_5_sampling_noprefix_lottery.json%0Ahttps%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foas 
st-model-eval%2Fmain%2Fsampling_reports%2Fchat-gpt%2F2023-04-11_gpt-3.5-turbo_lottery.json)"', 'inpu 
t_format': '"input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz"', 'output_forma 
t': ''}, {'datasets': ['EleutherAI / pythia-12b-deduped'], 'license': '', 'github': '- [andreaskoepf 
/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)', 'paper': '', 'upstr 
eam_model': 'EleutherAI / pythia-12b-deduped', 'parameter_count': '', 'hyper_parameters': {}, 'evalu 
ation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_forma 
t': ''}]                                                                                             

#####################nomic-ai/gpt4all-j########################

-------------------- datasets --------------------
Document 1:

datasets: - nomic-ai/gpt4all-j-prompt-generations
------------------------------
Document 2:

- **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)  
We have released several versions of our finetuned GPT-J model using [different dataset versions](https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations)  
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

Apache-2
------------------------------
Document 3:

Apache-2 Licensed
-------------------- github --------------------
Document 1:

- **Repository:** [https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)
- **Base Model Repository:** [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
------------------------------
Document 2:

"nomic-ai/gpt4all-j-prompt-generations"
-------------------- paper --------------------
Document 1:

**Paper [optional]:** [GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf)
------------------------------
Document 2:

"We have released several versions of our finetuned GPT-J model using [different dataset versions](https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations)"
-------------------- upstream_model --------------------
Document 1:

"Base Model Repository: [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)"
------------------------------
Document 2:

[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)
-------------------- parameter_count --------------------
Document 1:

"Model Type: A finetuned GPT-J model on assistant style interaction data" and "Finetuned from model [optional]: [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)"
-------------------- hyper_parameters --------------------
Document 1:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).
-------------------- evaluation --------------------
Document 1:

| Model                     |  BoolQ   |   PIQA   | HellaSwag | WinoGrande |  ARC-e   |  ARC-c   |   OBQA   |   Avg.   |
|:--------------------------|:--------:|:--------:|:---------:|:----------:|:--------:|:--------:|:--------:|:--------:|
| GPT4All-J 6B v1.0         |   73.4   |   74.8   |   63.4    |    64.7    |   54.9   |   36.0   |   40.2   |   58.2   |
| GPT4All-J v1.1-breezy     |   74.0   |   75.1   |   63.2    |    63.6    |   55.4   |   34.9   |   38.4   |   57.8   |
| GPT4All-J v1.2-jazzy      |   74.8   |   74.9   |   63.6    |
------------------------------
Document 2:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data
- **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)  
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).
-------------------- hardware --------------------
Document 1:

"8 A100 80GB GPUs"
-------------------- limitation_and_bias --------------------
Document 1:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data
- **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)  
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).
-------------------- demo --------------------
Document 1:

- **Demo [optional]:** [https://gpt4all.io/](https://gpt4all.io/)
------------------------------
Document 2:

"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories."
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

"Model Type: A finetuned GPT-J model on assistant style interaction data" "Finetuned from model [optional]: [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

vocabulary_size

[{'datasets': ['nomic-ai/gpt4all-j-prompt-generations'], 'license': 'apache-2.0', 'github': 'https: 
//github.com/nomic-ai/gpt4all', 'paper': 'https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4 
All-J_Technical_Report_2.pdf', 'upstream_model': 'https://github.com/kingoflolz/mesh-transformer-jax 
', 'parameter_count': 'A finetuned GPT-J model on assistant style interaction data', 'hyper_paramete 
rs': [{'epochs': 'v1.0', 'batch_size': 'The original model trained on the v1.0 dataset', 'learning_r 
ate': 'v1.1-breezy', 'optimizer': "Trained on a filtered dataset where we also removed instances lik 
e I'm sorry, I can't answer... and AI language model"}], 'evaluation': [{'test': 'GPT4All-J 6B v1.0' 
, 'result': 58.2}], 'hardware': '8 A100 80GB GPUs', 'limitation_and_bias': 'An Apache-2 licensed cha 
tbot trained over a massive curated corpus of assistant interactions including word problems, multi- 
turn dialogue, code, poems, songs, and stories.', 'demo': 'https://gpt4all.io/', 'input_format': 'A  
finetuned GPT-J model on assistant style interaction data', 'output_format': 'Finetuned from model [ 
optional]: [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)', 'input_token_limit': 'vocabulary_si 
ze'}]                                                                                                

#####################chavinlo/gpt4-x-alpaca########################

-------------------- datasets --------------------
Document 1:

https://huggingface.co/chavinlo/alpaca-13b
-------------------- license --------------------
Document 1:

"NO LORA"
-------------------- github --------------------
Document 1:

"https://huggingface.co/chavinlo/alpaca-13b" "repos that automatically fix this" "NOTE THE UPPER AND LOWER CASE!!!!"
-------------------- paper --------------------
Document 1:

"Finetuned on GPT4's responses, for 3 epochs."
-------------------- upstream_model --------------------
Document 1:

"https://huggingface.co/chavinlo/alpaca-13b" and "NOTE THE UPPER AND LOWER CASE!!!!"
-------------------- parameter_count --------------------
Document 1:

"3 epochs" and "NOTE THE UPPER AND LOWER CASE!!!!"
-------------------- hyper_parameters --------------------
Document 1:

"Finetuned on GPT4's responses, for 3 epochs. NO LORA"
-------------------- evaluation --------------------
Document 1:

"As a base model we used: https://huggingface.co/chavinlo/alpaca-13b Finetuned on GPT4's responses, for 3 epochs. NO LORA"
-------------------- hardware --------------------
Document 1:

"Finetuned on GPT4's responses, for 3 epochs."
-------------------- limitation_and_bias --------------------
Document 1:

"Finetuned on GPT4's responses, for 3 epochs. NO LORA"
-------------------- demo --------------------
Document 1:

"https://huggingface.co/chavinlo/alpaca-13b"
-------------------- input_format --------------------
Document 1:

"Finetuned on GPT4's responses, for 3 epochs." NO_OUTPUT
------------------------------
Document 2:

input_format
-------------------- output_format --------------------
Document 1:

"Finetuned on GPT4's responses, for 3 epochs." "NO LORA" "Generally you just need to change anything that's under the name of "LLaMa" to "Llama" NOTE THE UPPER AND LOWER CASE!!!!"
------------------------------
Document 2:

output_format, compressed doc
-------------------- input_token_limit --------------------
Document 1:

"https://huggingface.co/chavinlo/alpaca-13b" "input_token_limit"
-------------------- vocabulary_size --------------------
Document 1:

"https://huggingface.co/chavinlo/alpaca-13b" "vocabulary_size"

[{'datasets': ['https://huggingface.co/chavinlo/alpaca-13b'], 'license': 'NO LORA', 'github': 'http 
s://huggingface.co/chavinlo/alpaca-13b', 'paper': "Finetuned on GPT4's responses, for 3 epochs.", 'u 
pstream_model': 'https://huggingface.co/chavinlo/alpaca-13b', 'parameter_count': '3 epochs', 'hyper_ 
parameters': {'epochs': '3 epochs', 'batch_size': None, 'learning_rate': None, 'optimizer': None}, ' 
evaluation': [{'test': "As a base model we used: https://huggingface.co/chavinlo/alpaca-13b Finetune 
d on GPT4's responses, for 3 epochs. NO LORA", 'result': None}], 'hardware': "Finetuned on GPT4's re 
sponses, for 3 epochs.", 'limitation_and_bias': "Finetuned on GPT4's responses, for 3 epochs. NO LOR 
A", 'demo': 'https://huggingface.co/chavinlo/alpaca-13b', 'input_format': "Finetuned on GPT4's respo 
nses, for 3 epochs.", 'output_format': "Finetuned on GPT4's responses, for 3 epochs. NO LORA", 'inpu 
t_token_limit': 'https://huggingface.co/chavinlo/alpaca-13b', 'vocabulary_size': 'https://huggingfac 
e.co/chavinlo/alpaca-13b'}]                                                                          

#####################KoboldAI/GPT-J-6B-Janeway########################

-------------------- datasets --------------------
Document 1:

"The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy genres. The dataset is based on the same dataset used by GPT-Neo-2.7B-Picard, with 20% more data in various genres. Some parts of the dataset have been prepended using the following text: `[Genre: <genre1>,<genre2>]`"
------------------------------
Document 2:

The Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile.
-------------------- license --------------------
Document 1:

"license: mit"
------------------------------
Document 2:

"howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}}"
-------------------- github --------------------
Document 1:

\url{https://github.com/kingoflolz/mesh-transformer-jax}
-------------------- paper --------------------
Document 1:

```bibtex
@misc{gpt-j,
author = {Wang, Ben and Komatsuzaki, Aran},
title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
year = 2021,
month = May
```
-------------------- upstream_model --------------------
Document 1:

upstream_model: ```bibtex
@misc{gpt-j,
author = {Wang, Ben and Komatsuzaki, Aran},
title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
year = 2021,
month = May
```
------------------------------
Document 2:

KoboldAI/GPT-J-6B-Janeway
-------------------- parameter_count --------------------
Document 1:

"6 Billion Parameter"
-------------------- hyper_parameters --------------------
Document 1:

"hyper_parameters"
------------------------------
Document 2:

"20% more data in various genres"
-------------------- evaluation --------------------
Document 1:

"The core functionality of GPT-J is taking a string of text and predicting the next token. ... GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. ... As with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning."
-------------------- hardware --------------------
Document 1:

Google, TPU Research Cloud, Cloud TPU VM
-------------------- limitation_and_bias --------------------
Document 1:

"When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most "accurate" text. Never depend upon GPT-J to produce factually accurate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile. As with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.
------------------------------
Document 2:

"The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy genres. The dataset is based on the same dataset used by GPT-Neo-2.7B-Picard, with 20% more data in various genres. Some parts of the dataset have been prepended using the following text: `[Genre: <genre1>,<genre2>]`"
-------------------- demo --------------------
Document 1:

\url{https://github.com/kingoflolz/mesh-transformer-jax}
------------------------------
Document 2:

```py from transformers import pipeline generator = pipeline('text-generation', model='KoboldAI/GPT-J-6B-Janeway') generator("Welcome Captain Janeway, I apologize for the delay.", do_sample=True, min_length=50)```
-------------------- input_format --------------------
Document 1:

`[Genre: <genre1>,<genre2>]` input_format: compressed doc
-------------------- output_format --------------------
Document 1:

`[Genre: <genre1>,<genre2>]` output_format compressed doc
------------------------------
Document 2:

"pipeline('text-generation', model='KoboldAI/GPT-J-6B-Janeway')"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"20% more data in various genres"

[{'datasets': ["The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy gen 
res. The dataset is based on the same dataset used by GPT-Neo-2.7B-Picard, with 20% more data in var 
ious genres. Some parts of the dataset have been prepended using the following text: '[Genre: <genre 
1>,<genre2>]'"], 'license': 'mit', 'github': 'https://github.com/kingoflolz/mesh-transformer-jax', ' 
paper': '```bibtex\n@misc{gpt-j,\nauthor = {Wang, Ben and Komatsuzaki, Aran},\ntitle = {{GPT-J-6B: A 
 6 Billion Parameter Autoregressive Language Model}},\nhowpublished = {\\url{https://github.com/king 
oflolz/mesh-transformer-jax}},\nyear = 2021,\nmonth = May\n```', 'upstream_model': '```bibtex\n@misc 
{gpt-j,\nauthor = {Wang, Ben and Komatsuzaki, Aran},\ntitle = {{GPT-J-6B: A 6 Billion Parameter Auto 
regressive Language Model}},\nhowpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-j 
ax}},\nyear = 2021,\nmonth = May\n```', 'parameter_count': '6 Billion Parameter', 'hyper_parameters' 
: 'hyper_parameters', 'evaluation': [{'test': 'The core functionality of GPT-J is taking a string of 
 text and predicting the next token. ... GPT-J was trained on the Pile, a dataset known to contain p 
rofanity, lewd, and otherwise abrasive language. ... As with all language models, it is hard to pred 
ict in advance how GPT-J will respond to particular prompts and offensive content may occur without  
warning.'}], 'hardware': 'Google, TPU Research Cloud, Cloud TPU VM', 'limitation_and_bias': 'When pr 
ompting GPT-J it is important to remember that the statistically most likely next token is often not 
 the token that produces the most "accurate" text. Never depend upon GPT-J to produce factually accu 
rate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwis 
e abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sect 
ions 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of th 
e biases in the Pile. As with all language models, it is hard to predict in advance how GPT-J will r 
espond to particular prompts and offensive content may occur without warning. We recommend having a  
human curate or filter the outputs before releasing them, both to censor undesirable content and to  
improve the quality of the results.', 'demo': 'https://github.com/kingoflolz/mesh-transformer-jax',  
'input_format': '`[Genre: <genre1>,<genre2>]` input_format: compressed doc', 'output_format': '`[Gen 
re: <genre1>,<genre2>]` output_format compressed doc', 'input_token_limit': None, 'vocabulary_size': 
 '20% more data in various genres'}]                                                                 

#####################KoboldAI/GPT-NeoX-20B-Erebus########################

-------------------- datasets --------------------
Document 1:

"The data can be divided in 6 different datasets: - Literotica (everything with 4.5/5 or higher) - Sexstories (everything with 90 or higher) - Dataset-G (private dataset of X-rated stories) - Doc's Lab (all stories) - Pike Dataset (novels with "adult" rating) - SoFurry (collection of various animals)"
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

`[Genre: <comma-separated list of genres>]`
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

```bibtex
@inproceedings{gpt-neox-20b,
title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
url={https://arxiv.org/abs/2204.06745},
year={2022}
}
```
------------------------------
Document 2:

"Ben Wang's Mesh Transformer JAX library"
-------------------- upstream_model --------------------
Document 1:

"Mesh Transformer JAX library"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"GPT-NeoX-20B-Erebus" "TPUv3-256 TPU pod" "Ben Wang's Mesh Transformer JAX library" "EleutherAI" "GPT-J-6B model"
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

TPUv3-256 TPU pod
-------------------- limitation_and_bias --------------------
Document 1:

"Warning: This model has a very strong NSFW bias!"
-------------------- demo --------------------
Document 1:

"Mesh Transformer JAX library: ```bibtex @misc{mesh-transformer-jax, author = {Wang, Ben}, title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}}, howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}}, year = 2021, month = May }```"
-------------------- input_format --------------------
Document 1:

input_format: `[Genre: <comma-separated list of genres>]`
-------------------- output_format --------------------
Document 1:

output_format: `[Genre: <comma-separated list of genres>]`
------------------------------
Document 2:

output_format
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Literotica', 'Sexstories', 'Dataset-G', "Doc's Lab", 'Pike Dataset', 'SoFurry'], 'l 
icense': 'apache-2.0', 'github': '', 'paper': '@inproceedings{gpt-neox-20b,\ntitle={{GPT-NeoX-20B}:  
An Open-Source Autoregressive Language Model},\nauthor={Black, Sid and Biderman, Stella and Hallahan 
, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and  
McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu  
and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},\nbooktitle={Proceedings o 
f the ACL Workshop on Challenges \\& Perspectives in Creating Large Language Models},\nurl={https:// 
arxiv.org/abs/2204.06745},\nyear={2022}\n}', 'upstream_model': 'Mesh Transformer JAX library', 'para 
meter_count': '', 'hyper_parameters': [{'epochs': 'GPT-NeoX-20B-Erebus', 'batch_size': 'TPUv3-256 TP 
U pod', 'learning_rate': "Ben Wang's Mesh Transformer JAX library", 'optimizer': 'EleutherAI', 'eval 
uation': [{'test': 'GPT-J-6B model', 'result': 0}]}], 'hardware': 'TPUv3-256 TPU pod', 'limitation_a 
nd_bias': 'Warning: This model has a very strong NSFW bias!', 'demo': 'Mesh Transformer JAX library: 
 ```bibtex @misc{mesh-transformer-jax, author = {Wang, Ben}, title = {{Mesh-Transformer-JAX: Model-P 
arallel Implementation of Transformer Language Model with JAX}}, howpublished = {\\url{https://githu 
b.com/kingoflolz/mesh-transformer-jax}}, year = 2021, month = May }```', 'input_format': '[Genre: <c 
omma-separated list of genres>]', 'output_format': '[Genre: <comma-separated list of genres>]', 'inp 
ut_token_limit': '', 'vocabulary_size': ''}, {'datasets': [], 'license': '', 'github': "Ben Wang's M 
esh Transformer JAX library", 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_param 
eters': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_forma 
t': 'output_format', 'input_token_limit': '', 'vocabulary_size': ''}]                                

#####################OpenAssistant/pythia-12b-sft-v8-7k-steps########################

-------------------- datasets --------------------
Document 1:

- oasst_export:
lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
input_file_path: 2023-05-06_OASST_labels.jsonl.gz
val_split: 0.05
- vicuna:
val_split: 0.05
max_val_set: 800
fraction: 0.4
- dolly15k:
val_split: 0.05
max_val_set: 300
- grade_school_math_instructions:
val_split: 0.05
- code_alpaca:
val_split: 0.05
max_val_set: 250
- red_pajama:
fraction: 0.05
max_val_set: 1000
- wizardlm_70k:
val_split: 0.05
max_val_set: 500
fraction: 0.4
- poem_instructions:
fraction: 0.5
val_split:
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

OpenAssistant/pythia-12b-pre-v8-12.5k-steps
-------------------- parameter_count --------------------
Document 1:

"num_train_epochs: 8" "save_total_limit: 3" "per_device_train_batch_size: 4" "per_device_eval_batch_size: 4" "eval_steps: 251" "save_steps: 500" "gradient_accumulation_steps: 2"
-------------------- hyper_parameters --------------------
Document 1:

pythia-12b-sft-8:
dtype: fp16
log_dir: "pythia_log_12b"
learning_rate: 6e-6
model_name: OpenAssistant/pythia-12b-pre-v8-12.5k-steps
output_dir: pythia_model_12b
weight_decay: 0.0
residual_dropout: 0.0
max_length: 2048
use_flash_attention: true
warmup_steps: 100
gradient_checkpointing: true
gradient_accumulation_steps: 2
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
eval_steps: 251
save_steps: 500
num_train_epochs: 8
save_total_limit: 4
num_train_epochs: 8
save_total_limit: 3
use_custom_sampler: true
sort_by_length: false
save_strategy: steps
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

pythia-12b-sft-8:
dtype: fp16
log_dir: "pythia_log_12b"
learning_rate: 6e-6
model_name: OpenAssistant/pythia-12b-pre-v8-12.5k-steps
output_dir: pythia_model_12b
weight_decay: 0.0
residual_dropout: 0.0
max_length: 2048
use_flash_attention: true
warmup_steps: 100
gradient_checkpointing: true
gradient_accumulation_steps: 2
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
eval_steps: 251
save_steps: 500
num_train_epochs: 8
save_total_limit: 4
num_train_epochs: 8
save_total_limit: 3
use_custom_sampler: true
sort_by_length: false
save_strategy: steps
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

input_file_path: 2023-05-06_OASST_labels.jsonl.gz
-------------------- output_format --------------------
Document 1:

output_format: fp16
-------------------- input_token_limit --------------------
Document 1:

max_length: 2048, per_device_train_batch_size: 4, per_device_eval_batch_size: 4
-------------------- vocabulary_size --------------------


[{'datasets': ['oasst_export', 'vicuna', 'dolly15k', 'grade_school_math_instructions', 'code_alpaca 
', 'red_pajama', 'wizardlm_70k', 'poem_instructions'], 'license': 'apache-2.0', 'github': '', 'paper 
': '', 'upstream_model': 'OpenAssistant/pythia-12b-pre-v8-12.5k-steps', 'parameter_count': '"num_tra 
in_epochs: 8" "save_total_limit: 3" "per_device_train_batch_size: 4" "per_device_eval_batch_size: 4" 
 "eval_steps: 251" "save_steps: 500" "gradient_accumulation_steps: 2"', 'hyper_parameters': {'dtype' 
: 'fp16', 'log_dir': 'pythia_log_12b', 'learning_rate': '6e-6', 'model_name': 'OpenAssistant/pythia- 
12b-pre-v8-12.5k-steps', 'output_dir': 'pythia_model_12b', 'weight_decay': '0.0', 'residual_dropout' 
: '0.0', 'max_length': '2048', 'use_flash_attention': 'true', 'warmup_steps': '100', 'gradient_check 
pointing': 'true', 'gradient_accumulation_steps': '2', 'per_device_train_batch_size': '4', 'per_devi 
ce_eval_batch_size': '4', 'eval_steps': '251', 'save_steps': '500', 'num_train_epochs': '8', 'save_t 
otal_limit': '4', 'use_custom_sampler': 'true', 'sort_by_length': 'false', 'save_strategy': 'steps'} 
, 'evaluation': [], 'hardware': {'dtype': 'fp16', 'log_dir': 'pythia_log_12b', 'learning_rate': '6e- 
6', 'model_name': 'OpenAssistant/pythia-12b-pre-v8-12.5k-steps', 'output_dir': 'pythia_model_12b', ' 
weight_decay': '0.0', 'residual_dropout': '0.0', 'max_length': '2048', 'use_flash_attention': 'true' 
, 'warmup_steps': '100', 'gradient_checkpointing': 'true', 'gradient_accumulation_steps': '2', 'per_ 
device_train_batch_size': '4', 'per_device_eval_batch_size': '4', 'eval_steps': '251', 'save_steps': 
 '500', 'num_train_epochs': '8', 'save_total_limit': '4', 'use_custom_sampler': 'true', 'sort_by_len 
gth': 'false', 'save_strategy': 'steps'}, 'limitation_and_bias': '', 'demo': '', 'input_format': {'i 
nput_file_path': '2023-05-06_OASST_labels.jsonl.gz'}, 'output_format': {'output_format': 'fp16'}, 'i 
nput_token_limit': {'max_length': '2048', 'per_device_train_batch_size': '4', 'per_device_eval_batch 
_size': '4'}, 'vocabulary_size': ''}]                                                                

#####################TheBloke/wizardLM-13B-1.0-fp16########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

"The resources, including code, data, and model weights, associated with this project are restricted for academic research purposes only and cannot be used for commercial purposes."
------------------------------
Document 2:

license: other
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

@misc{xu2023wizardlm, title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang}, year={2023}, eprint={2304.12244}, archivePrefix={arXiv}, primaryClass={cs.CL}
------------------------------
Document 2:

"We adopt the automatic evaluation framework based on GPT-4 proposed by FastChat to assess the performance of chatbot models." NO_OUTPUT
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"We released **13B** version of **WizardLM** trained with **250k** evolved instructions (from ShareGPT)." parameter_count: 250k
-------------------- hyper_parameters --------------------
Document 1:

"Hyperparameter | LLaMA-7B | LLaMA-13B|
|----------------|----------|----------|
| Batch size     | 64       | 384      |
| Learning rate  | 2e-5     | 2e-5     |
| Epochs         | 3        | 3        |
| Max length     | 2048     | 2048     |
| Warmup step    | 2        | 50       |
| LR scheduler   | cosine   | cosine   |"
-------------------- evaluation --------------------
Document 1:

"We adopt the automatic evaluation framework based on GPT-4 proposed by FastChat to assess the performance of chatbot models. As shown in the following figure, WizardLM-13B achieved better results than Vicuna-13b."
------------------------------
Document 2:

7. [Evaluation](#evaluation)
------------------------------
Document 3:

To evaluate Wizard, we conduct human evaluation on the inputs from our human instruct evaluation set [`WizardLM_testset.jsonl`](./data/WizardLM_testset.jsonl). We performed a blind pairwise comparison between Wizard and baselines. Specifically, we recruit 10 well-educated annotators to rank the models from 1 to 5 on relevance, knowledgeable, reasoning, calculation and accuracy. WizardLM achieved significantly better results than Alpaca and Vicuna-7b. In the high-difficulty section of our test set (difficulty level >= 8), WizardLM even outperforms ChatGPT, with a win rate 7.9% larger than Chatgpt (42.9% vs. 35.0%).
-------------------- hardware --------------------
Document 1:

"We released **13B** version of **WizardLM** trained with **250k** evolved instructions (from ShareGPT)."
-------------------- limitation_and_bias --------------------
Document 1:

"The content produced by any version of WizardLM is influenced by uncontrollable variables such as randomness, and therefore, the accuracy of the output cannot be guaranteed by this project."
-------------------- demo --------------------
Document 1:

[Demo Link](https://011fc8477ad734d7.gradio.app)  
[Demo Backup 1](https://1825e531c43a23c7.gradio.app)
------------------------------
Document 2:

[Case Show](https://github.com/nlpxucan/WizardLM/blob/main/src/case_show.md)
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

output_format compressed doc
-------------------- input_token_limit --------------------
Document 1:

"13B version of WizardLM trained with 250k evolved instructions" and "7B version of WizardLM trained with 70k evolved instructions"
-------------------- vocabulary_size --------------------
Document 1:

**33B**, **13B**, **250k**, **7B**, **70k**

[{'datasets': ['WizardLM'], 'license': 'academic research purposes only', 'github': 'https://github 
.com/nlpxucan/WizardLM', 'paper': 'https://arxiv.org/abs/2304.12244', 'upstream_model': None, 'param 
eter_count': '250k', 'hyper_parameters': {'epochs': '3', 'batch_size': '384', 'learning_rate': '2e-5 
', 'optimizer': None}, 'evaluation': [{'test': 'chatbot models', 'result': None}], 'hardware': None, 
 'limitation_and_bias': 'The content produced by any version of WizardLM is influenced by uncontroll 
able variables such as randomness, and therefore, the accuracy of the output cannot be guaranteed by 
 this project.', 'demo': '[Demo Link](https://011fc8477ad734d7.gradio.app)\n[Demo Backup 1](https:// 
1825e531c43a23c7.gradio.app)', 'input_format': None, 'output_format': 'compressed doc', 'input_token 
_limit': '13B version of WizardLM trained with 250k evolved instructions and 7B version of WizardLM  
trained with 70k evolved instructions', 'vocabulary_size': '33B, 13B, 250k, 7B, 70k'}]               

#####################mosaicml/mpt-30b-chat########################

-------------------- datasets --------------------
Document 1:

"Airoboros/GPT4-1.2", "Baize", "Camel", "GPTeacher", "Guanaco", "LongCoversations", "ShareGPT", "WizardLM"
------------------------------
Document 2:

- camel-ai/code
- ehartford/wizard_vicuna_70k_unfiltered
- anon8231489123/ShareGPT_Vicuna_unfiltered
- teknium1/GPTeacher/roleplay-instruct-v2-final
- teknium1/GPTeacher/codegen-isntruct
- timdettmers/openassistant-guanaco
- camel-ai/math
- project-baize/baize-chatbot/medical_chat_data
- project-baize/baize-chatbot/quora_chat_data
- project-baize/baize-chatbot/stackoverflow_chat_data
- camel-ai/biology
- camel-ai/chemistry
- camel-ai/ai_society
- jondurbin/airoboros-gpt4-1.2
- LongConversations
- camel-ai/physics
-------------------- license --------------------
Document 1:

_CC-By-NC-SA-4.0_
------------------------------
Document 2:

license: cc-by-nc-sa-4.0
-------------------- github --------------------
Document 1:

[Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)
------------------------------
Document 2:

camel-ai/code, ehartford/wizard_vicuna_70k_unfiltered, anon8231489123/ShareGPT_Vicuna_unfiltered, teknium1/GPTeacher/roleplay-instruct-v2-final, teknium1/GPTeacher/codegen-isntruct, timdettmers/openassistant-guanaco, camel-ai/math, project-baize/baize-chatbot/medical_chat_data, project-baize/baize-chatbot/quora_chat_data, project-baize/baize-chatbot/stackoverflow_chat_data, camel-ai/biology, camel-ai/chemistry, camel-ai/ai_society, jondurbin/airoboros-gpt4-1.2, LongConversations, camel-ai/physics
-------------------- paper --------------------
Document 1:

"Introducing MPT-30B: Raising the bar for open-source foundation models"
------------------------------
Document 2:

"LongConversations" is a GPT3.5/4-generated dataset, details of which will be released at a later date.
-------------------- upstream_model --------------------
Document 1:

standard decoder-only transformer, FlashAttention, Attention with Linear Biases
-------------------- parameter_count --------------------
Document 1:

"26.4M", "55.0M", "301M", "7.56M", "15.6M", "18.4M", "821M", "297M"
------------------------------
Document 2:

n_parameters | 29.95B |
------------------------------
Document 3:

parameter_count 64
-------------------- hyper_parameters --------------------
Document 1:

| Hyperparameter | Value |
|----------------|-------|
|n_parameters | 29.95B |
|n_layers | 48 |
| n_heads | 64 |
| d_model | 7168 |
| vocab size | 50432 |
| sequence length | 8192 |
------------------------------
Document 2:

"64 H100s", "MosaicML Platform", "sharded data parallelism", "FSDP", "AdamW optimizer"
------------------------------
Document 3:

"LongConversations" is a GPT3.5/4-generated dataset, details of which will be released at a later date.
-------------------- evaluation --------------------
Document 1:

"The model was trained on the following data mix:  
| Data Source | Number of Tokens in Source | Proportion |
|-------------|----------------------------|------------|
| Airoboros/GPT4-1.2 | 26.4M | 1.71% |
| Baize | 55.0M | 3.57% |
| Camel	| 301M | 19.54% |
| GPTeacher	| 7.56M | 0.49% |
| Guanaco | 15.6M | 1.02% |
| LongCoversations | 18.4M | 1.19% |
| ShareGPT | 821M | 53.24% |
| WizardLM | 297M | 19.23% |  
"LongConversations" is a GPT3.5/4-generated dataset, details of which will be released at a later date.
-------------------- hardware --------------------
Document 1:

64 H100s, [MosaicML Platform](https://www.mosaicml.com/platform), [FSDP](https://pytorch.org/docs/stable/fsdp.html)
-------------------- limitation_and_bias --------------------
Document 1:

"LongConversations is a GPT3.5/4-generated dataset, details of which will be released at a later date."
------------------------------
Document 2:

* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases
-------------------- demo --------------------
Document 1:

[sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b)
-------------------- input_format --------------------
Document 1:

"LongConversations" is a GPT3.5/4-generated dataset, input_format compressed doc
------------------------------
Document 2:

vocab size | 50432 | sequence length | 8192 | input_format: compressed doc
-------------------- output_format --------------------
Document 1:

"LongConversations" is a GPT3.5/4-generated dataset, details of which will be released at a later date.
-------------------- input_token_limit --------------------
Document 1:

"26.4M", "55.0M", "301M", "7.56M", "15.6M", "18.4M", "821M", "297M"
-------------------- vocabulary_size --------------------
Document 1:

"26.4M", "55.0M", "301M", "7.56M", "15.6M", "18.4M", "821M", "297M"
------------------------------
Document 2:

vocab size | 50432

[{'datasets': ['Airoboros/GPT4-1.2', 'Baize', 'Camel', 'GPTeacher', 'Guanaco', 'LongCoversations',  
'ShareGPT', 'WizardLM'], 'license': '_CC-By-NC-SA-4.0_', 'github': '[Codebase (mosaicml/llm-foundry  
repo)](https://github.com/mosaicml/llm-foundry/)', 'paper': '"Introducing MPT-30B: Raising the bar f 
or open-source foundation models"', 'upstream_model': 'standard decoder-only transformer, FlashAtten 
tion, Attention with Linear Biases', 'parameter_count': '"26.4M", "55.0M", "301M", "7.56M", "15.6M", 
 "18.4M", "821M", "297M"', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 
 'optimizer': ''}, 'evaluation': [], 'hardware': '64 H100s, [MosaicML Platform](https://www.mosaicml 
.com/platform), [FSDP](https://pytorch.org/docs/stable/fsdp.html)', 'limitation_and_bias': '"LongCon 
versations is a GPT3.5/4-generated dataset, details of which will be released at a later date."', 'd 
emo': '[sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm 
_campaign=mpt-7b)', 'input_format': '"LongConversations" is a GPT3.5/4-generated dataset, input_form 
at compressed doc', 'output_format': '"LongConversations" is a GPT3.5/4-generated dataset, details o 
f which will be released at a later date.', 'input_token_limit': '"26.4M", "55.0M", "301M", "7.56M", 
 "15.6M", "18.4M", "821M", "297M"', 'vocabulary_size': '"26.4M", "55.0M", "301M", "7.56M", "15.6M",  
"18.4M", "821M", "297M"'}, {'datasets': ['camel-ai/code', 'ehartford/wizard_vicuna_70k_unfiltered',  
'anon8231489123/ShareGPT_Vicuna_unfiltered', 'teknium1/GPTeacher/roleplay-instruct-v2-final', 'tekni 
um1/GPTeacher/codegen-isntruct', 'timdettmers/openassistant-guanaco', 'camel-ai/math', 'project-baiz 
e/baize-chatbot/medical_chat_data', 'project-baize/baize-chatbot/quora_chat_data', 'project-baize/ba 
ize-chatbot/stackoverflow_chat_data', 'camel-ai/biology', 'camel-ai/chemistry', 'camel-ai/ai_society 
', 'jondurbin/airoboros-gpt4-1.2', 'LongConversations', 'camel-ai/physics'], 'license': 'cc-by-nc-sa 
-4.0', 'github': 'camel-ai/code, ehartford/wizard_vicuna_70k_unfiltered, anon8231489123/ShareGPT_Vic 
una_unfiltered, teknium1/GPTeacher/roleplay-instruct-v2-final, teknium1/GPTeacher/codegen-isntruct,  
timdettmers/openassistant-guanaco, camel-ai/math, project-baize/baize-chatbot/medical_chat_data, pro 
ject-baize/baize-chatbot/quora_chat_data, project-baize/baize-chatbot/stackoverflow_chat_data, camel 
-ai/biology, camel-ai/chemistry, camel-ai/ai_society, jondurbin/airoboros-gpt4-1.2, LongConversation 
s, camel-ai/physics', 'paper': '"LongConversations" is a GPT3.5/4-generated dataset, details of whic 
h will be released at a later date.', 'upstream_model': '', 'parameter_count': 'n_parameters | 29.95 
B |', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'e 
valuation': [], 'hardware': '', 'limitation_and_bias': '"LongConversations is a GPT3.5/4-generated d 
ataset, details of which will be released at a later date."', 'demo': '', 'input_format': 'vocab siz 
e | 50432 | sequence length | 8192 | input_format: compressed doc', 'output_format': '"LongConversat 
ions" is a GPT3.5/4-generated dataset, details of which will be released at a later date.', 'input_t 
oken_limit': '"26.4M", "55.0M", "301M", "7.56M", "15.6M", "18.4M", "821M", "297M"', 'vocabulary_size 
': 'vocab size | 50432'}]                                                                            

#####################hf-internal-testing/tiny-random-bloom########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

```GPT-2```
-------------------- github --------------------
Document 1:

"language: - eng tags: - integration pipeline_tag: text-generation"
-------------------- paper --------------------
Document 1:

```GPT-2```-like model
-------------------- upstream_model --------------------
Document 1:

```GPT-2```
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

```GPT-2```
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

```GPT-2```
-------------------- limitation_and_bias --------------------
Document 1:

```GPT-2```-like model
-------------------- demo --------------------
Document 1:

```GPT-2```-like model
-------------------- input_format --------------------
Document 1:

```GPT-2```, input_format
-------------------- output_format --------------------
Document 1:

```GPT-2```, output_format
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

```GPT-2```

[{'datasets': ['GPT-2'], 'license': 'GPT-2', 'github': '"language: - eng tags: - integration pipeli 
ne_tag: text-generation"', 'paper': 'GPT-2-like model', 'upstream_model': 'GPT-2', 'parameter_count' 
: 'GPT-2', 'hyper_parameters': [], 'evaluation': [], 'hardware': 'GPT-2', 'limitation_and_bias': 'GP 
T-2-like model', 'demo': 'GPT-2-like model', 'input_format': 'GPT-2, input_format', 'output_format': 
 'GPT-2, output_format', 'input_token_limit': 'GPT-2', 'vocabulary_size': 'GPT-2'}]                  

#####################mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis########################

-------------------- datasets --------------------
Document 1:

datasets: - financial_phrasebank, metrics: - accuracy, model-index: - name: distilRoberta-financial-sentiment, results: - task: type: text-classification name: Text Classification dataset: name: financial_phrasebank type: financial_phrasebank args: sentences_allagree metrics: - type: accuracy value: 0.9823008849557522 name: Accuracy
------------------------------
Document 2:

Datasets 1.12.1
------------------------------
Document 3:

"distilroberta-base" and "financial_phrasebank dataset"
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

- generated_from_trainer
- financial
- stocks
- sentiment
- financial_phrasebank
- distilRoberta-financial-sentiment
------------------------------
Document 2:

upstream_model distilroberta-base
-------------------- parameter_count --------------------
Document 1:

parameter_count: 0
------------------------------
Document 2:

- type: text-classification
- metrics:
- type: accuracy
- value: 0.9823008849557522
- name: Accuracy
-------------------- hyper_parameters --------------------
Document 1:

learning_rate: 2e-05, train_batch_size: 8, eval_batch_size: 8, seed: 42, optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08, lr_scheduler_type: linear, num_epochs: 5
-------------------- evaluation --------------------
Document 1:

- license: apache-2.0
- tags:
- generated_from_trainer
- financial
- stocks
- sentiment
- datasets:
- financial_phrasebank
- metrics:
- accuracy
- widget:
- text: Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .
- model-index:
- name: distilRoberta-financial-sentiment
- results:
- task:
type: text-classification
name: Text Classification
dataset:
name: financial_phrasebank
type: financial_phrasebank
args: sentences_allagree
metrics:
- type: accuracy
value: 0.9823008849557522
name: Accuracy
------------------------------
Document 2:

"It achieves the following results on the evaluation set: - Loss: 0.1116 - Accuracy: 0.9823"
-------------------- hardware --------------------
Document 1:

"model-index: - name: distilRoberta-financial-sentiment"
-------------------- limitation_and_bias --------------------
Document 1:

- financial
- stocks
- sentiment
- financial_phrasebank
- accuracy
- Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .
- distilRoberta-financial-sentiment
- Text Classification
- financial_phrasebank
- sentences_allagree
- accuracy
- 0.9823008849557522
-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

- type: text-classification
- name: Text Classification
- dataset:
name: financial_phrasebank
type: financial_phrasebank
- args: sentences_allagree
- metrics:
- type: accuracy
value: 0.9823008849557522
name: Accuracy
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

- name: distilRoberta-financial-sentiment
- type: text-classification
- dataset:
name: financial_phrasebank
type: financial_phrasebank
args: sentences_allagree

[{'datasets': ['financial_phrasebank'], 'license': '', 'github': '', 'paper': '', 'upstream_model': 
 '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_an 
d_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabula 
ry_size': ''}]                                                                                       

#####################LiYuan/amazon-query-product-ranking########################

-------------------- datasets --------------------
Document 1:

[dataset](https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search/dataset_files)
------------------------------
Document 2:

- generated_from_trainer
- name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping

NO_OUTPUT
------------------------------
Document 3:

[Amazon shopping query dataset](https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

license: apache-2.0, tags: - generated_from_trainer, metrics: - accuracy, model-index: - name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping, results: []
-------------------- paper --------------------
Document 1:

Amazon shopping query dataset, [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased), [here](https://github.com/vanderbilt-data-science/sna)
------------------------------
Document 2:

"DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher." "We replaced its head with our shopping relevance category to fine-tune it on 571,223 rows of training set while validate it on 142,806 rows of dev set. Finally, we evaluated our model performance on a held-out test set: 79,337 rows."
------------------------------
Document 3:

"model-index: - name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping"
-------------------- upstream_model --------------------
Document 1:

distilbert-base-uncased, Amazon shopping query dataset
------------------------------
Document 2:

- name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping
- license: apache-2.0
- tags:
- generated_from_trainer
- metrics:
- accuracy
- results: []
upstream_model: NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

license: apache-2.0
tags:
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping
results: []

NO_OUTPUT
------------------------------
Document 2:

parameter_count: 5
-------------------- hyper_parameters --------------------
Document 1:

learning_rate: 2e-05, train_batch_size: 16, eval_batch_size: 16, seed: 42, optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08, lr_scheduler_type: linear, num_epochs: 2
------------------------------
Document 2:

"This model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on an [Amazon shopping query dataset](https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search). The code for the fine-tuning process can be found [here](https://github.com/vanderbilt-data-science/sna)."
------------------------------
Document 3:

- license: apache-2.0
- tags:
- generated_from_trainer
- metrics:
- accuracy
- model-index:
- name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping
- results: []
-------------------- evaluation --------------------
Document 1:

license: apache-2.0
tags:
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping
results: []
------------------------------
Document 2:

"This model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on an [Amazon shopping query dataset](https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search). The code for the fine-tuning process can be found [here](https://github.com/vanderbilt-data-science/sna). This model is uncased: it does not make a difference between english and English. It achieves the following results on the evaluation set: - Loss: 0.8244 - Accuracy: 0.6617"
------------------------------
Document 3:

"DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. We replaced its head with our shopping relevance category to fine-tune it on 571,223 rows of training set while validate it on 142,806 rows of dev set. Finally, we evaluated our model performance on a held-out test set: 79,337 rows."
-------------------- hardware --------------------
Document 1:

"BERT base model" and "571,223 rows of training set while validate it on 142,806 rows of dev set"
------------------------------
Document 2:

"model-index: - name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping"
-------------------- limitation_and_bias --------------------
Document 1:

"The limitations are this trained model is focusing on queries and products on Amazon."
------------------------------
Document 2:

license: apache-2.0, tags: - generated_from_trainer, metrics: - accuracy, model-index: - name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping, results: []
-------------------- demo --------------------
Document 1:

"You can use this model directly by downloading the trained weights and configurations like the below code snippet:  
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("LiYuan/amazon-query-product-ranking")

model = AutoModelForSequenceClassification.from_pretrained("LiYuan/amazon-query-product-ranking")
```
------------------------------
Document 2:

[distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased), [Amazon shopping query dataset](https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search), [here](https://github.com/vanderbilt-data-science/sna)
------------------------------
Document 3:

license: apache-2.0, tags: - generated_from_trainer, metrics: - accuracy, model-index: - name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping, results: []
-------------------- input_format --------------------
Document 1:

"raw texts only" "BERT base model" "571,223 rows of training set" "142,806 rows of dev set" "79,337 rows of test set"
------------------------------
Document 2:

input_format: compressed doc
-------------------- output_format --------------------
Document 1:

license: apache-2.0, tags: - generated_from_trainer, metrics: - accuracy, model-index: - name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping, results: []
-------------------- input_token_limit --------------------
Document 1:

license: apache-2.0, tags: - generated_from_trainer, metrics: - accuracy, model-index: - name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping, results: []
-------------------- vocabulary_size --------------------
Document 1:

license: apache-2.0, tags: - generated_from_trainer, metrics: - accuracy, model-index: - name: distilbert-base-uncased-finetuned-mnli-amazon-query-shopping, results: []

[{'datasets': ['https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search/data 
set_files'], 'license': 'apache-2.0', 'github': 'https://github.com/vanderbilt-data-science/sna', 'p 
aper': 'https://huggingface.co/distilbert-base-uncased', 'upstream_model': 'distilbert-base-uncased' 
, 'parameter_count': '5', 'hyper_parameters': {'epochs': '2', 'batch_size': '16', 'learning_rate': ' 
2e-05', 'optimizer': 'Adam with betas=(0.9,0.999) and epsilon=1e-08'}, 'evaluation': [{'test': 'accu 
racy', 'result': 0.6617}], 'hardware': 'BERT base model', 'limitation_and_bias': 'The limitations ar 
e this trained model is focusing on queries and products on Amazon.', 'demo': 'You can use this mode 
l directly by downloading the trained weights and configurations like the below code snippet:\n```py 
thon\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = Auto 
Tokenizer.from_pretrained("LiYuan/amazon-query-product-ranking")\n\nmodel = AutoModelForSequenceClas 
sification.from_pretrained("LiYuan/amazon-query-product-ranking")\n```', 'input_format': 'raw texts  
only', 'output_format': None, 'input_token_limit': None, 'vocabulary_size': None}]                   

#####################laion/mscoco_finetuned_CoCa-ViT-L-14-laion2B-s13B-b90k########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"license: mit" "pipeline_tag: image-to-text"
-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

#tags  
---
license: mit
pipeline_tag: image-to-text
---
-------------------- evaluation --------------------
Document 1:

license: mit
pipeline_tag: image-to-text

NO_OUTPUT
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"license: mit pipeline_tag: image-to-text"
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

license: mit, pipeline_tag: image-to-text

[{'datasets': ['dataset1', 'dataset2'], 'license': 'mit', 'github': 'https://github.com/model', 'pa 
per': 'https://arxiv.org/1234', 'upstream_model': 'huggingface/model', 'parameter_count': '100M', 'h 
yper_parameters': {'epochs': '10', 'batch_size': '32', 'learning_rate': '0.001', 'optimizer': 'adam' 
}, 'evaluation': [{'test': 'accuracy', 'result': 0.85}, {'test': 'f1-score', 'result': 0.78}], 'hard 
ware': 'GPU', 'limitation_and_bias': 'Limited to English language', 'demo': 'https://demo.com', 'inp 
ut_format': 'text', 'output_format': 'text'}]                                                        

#####################hf-internal-testing/tiny-stable-diffusion-pipe########################

-------------------- datasets --------------------
Document 1:

library_name: diffusers, from diffusers import DiffusionPipeline, DiffusionPipeline.from_pretrained("hf-internal-testing/tiny-stable-diffusion-pipe")
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

library_name: diffusers, DiffusionPipeline.from_pretrained("hf-internal-testing/tiny-stable-diffusion-pipe")
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

library_name: diffusers, DiffusionPipeline.from_pretrained("hf-internal-testing/tiny-stable-diffusion-pipe")
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

library_name: diffusers

from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("hf-internal-testing/tiny-stable-diffusion-pipe")
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['diffusers'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'param 
eter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '' 
, 'demo': '', 'input_format': '', 'output_format': ''}]                                              

#####################DeepFloyd/IF-II-L-v1.0########################


Metadata Prompt: 

 {'datasets': 'What datasets were used to train the model (include link if possible)', 'license': 'What is the license', 'github': 'What links to github repositories are available and what these links are for', 'paper': 'What is the research paper associated to this model', 'upstream_model': 'What is the upstream model of this model', 'parameter_count': 'What are the number of parameters (#params) this model is trained on', 'hyper_parameters': 'What are the values of some hyper parameters (parameters that control the learning process of the model) of this model.', 'evaluation': 'What is the evalutaion of the model. What evaluation metrics where used, and what are the results (include whole table if possible)', 'hardware': 'What hardware was used to train this model', 'limitation_and_bias': 'What are the limitations and biases of the model', 'demo': 'Find a form of demo for the model could be a link, code snippet or short paragraph', 'input_format': 'What is the format of the data used as input for the model', 'output_format': 'What is the format of the data used as output of the model', 'input_preprocessing': 'What is the input preprocessing of this model', 'input_size': 'What is the image input size', 'max_sequence_length': 'What is the max sequence length of this NLP model', 'vocabulary_size': 'What is the vocabulary size of this NLP model', 'sample_rate': 'What is the sample rate of this model', 'agent': 'What is the agent of this reinforcement learning model', 'training_environment': 'What is the training environment of this reinforcement learning model', 'SB3': 'Is SB3 used in this reinforcement learning model'}
Extraction Prompt: 

Given metadata information of huggingface {model_type} model : {model}, extract the properties of ONE single entity mentioned in the 'information_extraction' function.
     Extraction rules: 
     - rule 1: Adhere strictly to the schema structure in 'information_extraction'
     - rule 2: If a property is not present but is required in the function parameters, output  instead
     - rule 3: If a property is not present and is not required in the function parameters, do not include it in the output
     - rule 4: Only extract one item for 'info' in  'information_extraction' function 
     Extraction rules for specific metadata: 
     - datasets: only return dataset used to train or finetune model, not the upstream model of the model
     - github: extract github link of this model (only return the github link)
     - paper: if a research paper was written, extract arxiv research paper link (only return the url of the paper)
     - parameter_count: The number of parameters the model was trained on, sometimes represented in the form #params
     - upstream_model: provide huggingface model ID of upstream model
     - hyper_parameters: extract possible hyperparameters
     - evaluation: extract evaluation metric/tasks and their respective evaluation results
     - hardware: extract any hardware used to train the model
     - limitation_and_biases: extract a short summary of limitation and biases of the model
     - demo: extract any links, code snippets, or small paragraphs on how to use the model
     - input_format: extract the input format/requirement for this model
     - output_format: extract the output format of this model
 

#####################OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5########################

-------------------- datasets --------------------

Document 1:

reference-data:
datasets:
- oasst_export:
lang: 'bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk'
input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
val_split: 0.05
------------------------------
Document 2:

[https://open-assistant.io/](https://open-assistant.io/)
------------------------------
Document 3:

[EleutherAI / pythia-12b-deduped](https://huggingface.co/EleutherAI/pythia-12b-deduped)

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

License: Apache 2.0

-------------------- github --------------------

Document 1:

[Open-Assistant](https://github.com/LAION-AI/Open-Assistant)
------------------------------
Document 2:

Two special tokens are used to mark the beginning of user and assistant turns:
`<|prompter|>` and `<|assistant|>`. Each turn ends with a `

-------------------- paper --------------------

Document 1:

- **Code:** [Open-Assistant/model/model_training](https://github.com/LAION-AI/Open-Assistant/tree/main/model/model_training)
------------------------------
Document 2:

- wandb: https://wandb.ai/open-assistant/supervised-finetuning/runs/770a0t41
- base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)
- checkpoint: 4000 steps  
command: `deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000`  
data:
```
reference-data:
datasets:
- oasst_export:
lang: 'bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk'
input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
val_split: 0.05
- alpaca
sort_by_length: false
use_custom_sampler: false
```  
pythia:
```
reference-pythia-12b:
dtype: fp16
log_dir: 'pythia_log_12b'
learning_rate: 6e-6
model_name: EleutherAI/pythia-12b-deduped
output_dir: pythia_model_12b
weight_decay: 0.0
max_length: 2048
warmup_steps: 100
gradient_checkpointing: true
gradient_accumulation_steps: 2
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
eval_steps: 100
save_steps: 1000
num_train_epochs: 8
save_total_limit: 4
```

-------------------- upstream_model --------------------

Document 1:

Pythia 12B
------------------------------
Document 2:

[EleutherAI / pythia-12b-deduped](https://huggingface.co/EleutherAI/pythia-12b-deduped)

-------------------- parameter_count --------------------

Document 1:

reference-pythia-12b:
dtype: fp16
log_dir: 'pythia_log_12b'
learning_rate: 6e-6
model_name: EleutherAI/pythia-12b-deduped
output_dir: pythia_model_12b
weight_decay: 0.0
max_length: 2048
warmup_steps: 100
gradient_checkpointing: true
gradient_accumulation_steps: 2
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
eval_steps: 100
save_steps: 1000
num_train_epochs: 8
save_total_limit: 4
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

- learning_rate: 6e-6
- num_train_epochs: 8

-------------------- evaluation --------------------

Document 1:

- wandb: https://wandb.ai/open-assistant/supervised-finetuning/runs/770a0t41
- base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)
- checkpoint: 4000 steps  
command: `deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000`  
data:
```
reference-data:
datasets:
- oasst_export:
lang: 'bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk'
input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
val_split: 0.05
- alpaca
sort_by_length: false
use_custom_sampler: false
```  
pythia:
```
reference-pythia-12b:
dtype: fp16
log_dir: 'pythia_log_12b'
learning_rate: 6e-6
model_name: EleutherAI/pythia-12b-deduped
output_dir: pythia_model_12b
weight_decay: 0.0
max_length: 2048
warmup_steps: 100
gradient_checkpointing: true
gradient_accumulation_steps: 2
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
eval_steps: 100
save_steps: 1000
num_train_epochs: 8
save_total_limit: 4
```

-------------------- hardware --------------------

Document 1:

- wandb: https://wandb.ai/open-assistant/supervised-finetuning/runs/770a0t41
- base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)
- checkpoint: 4000 steps  
command: `deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000`

-------------------- limitation_and_bias --------------------

Document 1:

limitation_and_bias: It is based on a Pythia 12B that was fine-tuned on human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023.
------------------------------
Document 2:

- **Limitations and biases of the model:**

-------------------- demo --------------------

Document 1:

It is based on a Pythia 12B that was fine-tuned on human demonstrations
of assistant conversations collected through the
[https://open-assistant.io/](https://open-assistant.io/) human feedback web
app before March 25, 2023.
------------------------------
Document 2:

- **Demo:** [Continuations for 250 random prompts](https://open-assistant.github.io/oasst-model-eval/?f=https%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Foasst-sft%2F2023-04-03_andreaskoepf_oasst-sft-4-pythia-12b-epoch-3_5_sampling_noprefix_lottery.json%0Ahttps%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Fchat-gpt%2F2023-04-11_gpt-3.5-turbo_lottery.json)

-------------------- input_format --------------------

Document 1:

data:
```
reference-data:
datasets:
- oasst_export:
lang: 'bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk'
input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
val_split: 0.05
- alpaca
sort_by_length: false
use_custom_sampler: false
```
------------------------------
Document 2:

Two special tokens are used to mark the beginning of user and assistant turns:
`<|prompter|>` and `<|assistant|>`. Each turn ends with a `

-------------------- output_format --------------------

Document 1:

output_format: fp16
------------------------------
Document 2:

output_format
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length
------------------------------
Document 3:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['oasst_export'], 'license': 'apache-2.0', 'github': 'https://github.com/LAION-AI/Ope 
n-Assistant', 'paper': 'https://github.com/LAION-AI/Open-Assistant/tree/main/model/model_training',  
'upstream_model': 'Pythia 12B', 'parameter_count': "reference-pythia-12b:\ndtype: fp16\nlog_dir: 'py 
thia_log_12b'\nlearning_rate: 6e-6\nmodel_name: EleutherAI/pythia-12b-deduped\noutput_dir: pythia_mo 
del_12b\nweight_decay: 0.0\nmax_length: 2048\nwarmup_steps: 100\ngradient_checkpointing: true\ngradi 
ent_accumulation_steps: 2\nper_device_train_batch_size: 4\nper_device_eval_batch_size: 4\neval_steps 
: 100\nsave_steps: 1000\nnum_train_epochs: 8\nsave_total_limit: 4", 'hyper_parameters': {'learning_r 
ate': '6e-6', 'num_train_epochs': '8'}, 'evaluation': [{'test': 'wandb', 'result': 'https://wandb.ai 
/open-assistant/supervised-finetuning/runs/770a0t41'}], 'hardware': 'wandb: https://wandb.ai/open-as 
sistant/supervised-finetuning/runs/770a0t41\nbase model: [andreaskoepf/pythia-12b-pre-2000](https:// 
huggingface.co/andreaskoepf/pythia-12b-pre-2000)\ncheckpoint: 4000 steps\ncommand: `deepspeed traine 
r_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache  
--output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0 
.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000`', 'limitati 
on_and_bias': 'It is based on a Pythia 12B that was fine-tuned on human demonstrations of assistant  
conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human f 
eedback web app before March 25, 2023.', 'demo': 'It is based on a Pythia 12B that was fine-tuned on 
 human demonstrations\nof assistant conversations collected through the\n[https://open-assistant.io/ 
](https://open-assistant.io/) human feedback web\napp before March 25, 2023.', 'input_format': "data 
:\n```\nreference-data:\ndatasets:\n- oasst_export:\nlang: 'bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,p 
t,ro,ru,sl,sr,sv,uk'\ninput_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz\nval_sp 
lit: 0.05\n- alpaca\nsort_by_length: false\nuse_custom_sampler: false\n```", 'output_format': 'fp16' 
, 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': ''}]                              

#####################nomic-ai/gpt4all-j########################

-------------------- datasets --------------------

Document 1:

- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).

-------------------- license --------------------

Document 1:

Apache-2 licensed
------------------------------
Document 2:

license: apache-2.0
------------------------------
Document 3:

[GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf)

-------------------- github --------------------

Document 1:

- **Repository:** [https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)
- **Base Model Repository:** [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
------------------------------
Document 2:

nomic-ai/gpt4all-j-prompt-generations

-------------------- paper --------------------

Document 1:

[GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf)

-------------------- upstream_model --------------------

Document 1:

[https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
------------------------------
Document 2:

This model has been finetuned from [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)  
- **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)

-------------------- parameter_count --------------------

Document 1:

Trained on a DGX cluster with 8 A100 80GB GPUs for ~12 hours.
------------------------------
Document 2:

v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model

-------------------- hyper_parameters --------------------

Document 1:

Using Deepspeed + Accelerate, we use a global batch size of 256 with a learning rate of 2e-5.
------------------------------
Document 2:

- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model

-------------------- evaluation --------------------

Document 1:

| Model                     |  BoolQ   |   PIQA   | HellaSwag | WinoGrande |  ARC-e   |  ARC-c   |   OBQA   |   Avg.   |
|:--------------------------|:--------:|:--------:|:---------:|:----------:|:--------:|:--------:|:--------:|:--------:|
| GPT4All-J 6B v1.0         |   73.4   |   74.8   |   63.4    |    64.7    |   54.9   |   36.0   |   40.2   |   58.2   |
| GPT4All-J v1.1-breezy     |   74.0   |   75.1   |   63.2    |    63.6    |   55.4   |   34.9   |   38.4   |   57.8   |
| GPT4All-J v1.2-jazzy      |   74.8   |   74.9   |   63.6    |    63.8    |   56.6   |   35.3   |   41.0   |   58.6   |
| GPT4All-J v1.3-groovy     |   73.6   |   74.3   |   63.8    |    63.5    |   57.7   |   35.0   |   38.8   |   58.1   |
| GPT4All-J Lora 6B         |   68.6   |   75.8   |   66.2    |    63.5    |   56.4   |   35.7   |   40.2   |   58.1   |
| GPT4All LLaMa Lora 7B     |   73.1   |   77.6   |   72.1    |    67.8    |   51.1   |   40.4   |   40.2   |   60.3   |
| GPT4All 13B snoozy        | **83.3** |   79.2   |   75.0    |  **71.3**  |   60.9   |   44.2   |   43.4   | **65.3** |
| Dolly 6B                  |   68.8   |   77.3   |   67.6    |    63.9    |   62.9   |   38.7   |   41.2   |   60.1   |
| Dolly 12B                 |   56.7   |   75.4   |   71.0    |    62.2    |   64.6   |   38.5   |   40.4   |   58.4   |
| Alpaca 7B                 |   73.9   |   77.2   |   73.9    |    66.1    |   59.8   |   43.3   |   43.4   |   62.4   |
| Alpaca Lora 7B            |   74.3   | **79.3** |   74.0    |    68.8    |   56.6   |   43.9   |   42.6   |   62.8   |
| GPT-J 6.7B                |   65.4   |   76.2   |   66.2    |    64.1    |   62.2   |   36.6   |   38.2   |   58.4   |
| LLama 7B                  |   73.1   |   77.4   |   73.0    |    66.9    |   52.5   |   41.4   |   42.4   |   61.0   |
| LLama 13B                 |   68.5   |   79.1   |   76.2    |    70.1    |   60.0   | **44.6** |   42.2   |   63.0   |
| Pythia 6.7B               |   63.5   |   76.3   |   64.0    |    61.1    |   61.3   |   35.2   |   37.2   |   57.0   |
| Pythia 12B                |   67.7   |   76.6   |   67.3    |    63.8    |   63.9   |   34.8   |    38    |   58.9   |
| Fastchat T5               |   81.5   |   64.6   |   46.3    |    61.8    |   49.3   |   33.3   |   39.4   |   53.7   |
| Fastchat Vicuña 7B        |   76.6   |   77.2   |   70.7    |    67.3    |   53.5   |   41.2   |   40.8   |   61.0   |
| Fastchat Vicuña 13B       |   81.5   |   76.8   |   73.3    |    66.7    |   57.4   |   42.7   |   43.6   |   63.1   |
| StableVicuña RLHF         |   82.3   |   78.6   |   74.1    |    70.9    |   61.0   |   43.5   | **44.4** |   65.0   |
| StableLM Tuned            |   62.5   |   71.2   |   53.6    |    54.8    |   52.4   |   31.1   |   33.4   |   51.3   |
| StableLM Base             |   60.1   |   67.4   |   41.2    |    50.1    |   44.9   |   27.0   |   32.0   |   42.2   |
| Koala 13B                 |   76.5   |   77.9   |   72.6    |    68.8    |   54.3   |   41.0   |   42.8   |   62.0   |
| Open Assistant Pythia 12B |   67.9   |   78.0   |   68.1    |    65.0    |   64.2   |   40.4   |   43.2   |   61.0   |
| Mosaic mpt-7b             |   74.8   | **79.3** | **76.3**  |    68.6    | **70.0** |   42.2   |   42.6   |   64.8   |
| text-davinci-003          |   88.1   |   83.8   |   83.4    |    75.8    |   83.9   |   63.9   |   51.0   |   75.7   |

-------------------- hardware --------------------

Document 1:

Trained on a DGX cluster with 8 A100 80GB GPUs for ~12 hours.

-------------------- limitation_and_bias --------------------

Document 1:

- **Paper [optional]:** [GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf)
------------------------------
Document 2:

- **Limitations and biases of the model**

-------------------- demo --------------------

Document 1:

- **Demo [optional]:** [https://gpt4all.io/](https://gpt4all.io/)

-------------------- input_format --------------------

Document 1:

- **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)  
We have released several versions of our finetuned GPT-J model using [different dataset versions](https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations)  
- v1.0: The original model trained on the v1.0 dataset
- v1.1-breezy: Trained on afiltered dataset where we removed all instances of AI language model
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model
- v1.3-groovy: We added Dolly and ShareGPT to the v1.2 dataset and removed ~8% of the dataset in v1.2 that contained semantic duplicates using [Atlas](https://atlas.nomic.ai/).  
To download a model with a specific revision run  
```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained('nomic-ai/gpt4all-j', revision='v1.2-jazzy')
```
------------------------------
Document 2:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

- **Developed by:** [Nomic AI](https://home.nomic.ai)
- **Model Type:** A finetuned GPT-J model on assistant style interaction data
- **Language(s) (NLP):** English
- **Finetuned from model [optional]:** [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)

-------------------- max_sequence_length --------------------

Document 1:

- **Model Type:** A finetuned GPT-J model on assistant style interaction data
- v1.2-jazzy: Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model

-------------------- vocabulary_size --------------------

Document 1:

v1.2-jazzy
------------------------------
Document 2:

vocabulary_size


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Unterminated string starting at: line 58 column 23 (char 1666) 

#####################microsoft/biogpt########################

-------------------- datasets --------------------

Document 1:

https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf
------------------------------
Document 2:

BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature.

-------------------- license --------------------

Document 1:

license: mit
------------------------------
Document 2:

@article{10.1093/bib/bbac409,
author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
title = '{BioGPT: generative pre-trained transformer for biomedical text generation and mining}',
journal = {Briefings in Bioinformatics},
volume = {23},
number = {6},
year = {2022},
month = {09},
abstract = '{Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.}',
issn = {1477-4054},
doi = {10.1093/bib/bbac409},
url = {https://doi.org/10.1093/bib/bbac409},
note = {bbac409},
eprint = {https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf},
}
------------------------------
Document 3:

You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:  
```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the United States alone.'},
{'generated_text': 'COVID-19 is a pandemic, the incidence has been increased in a manner similar to that in other'},
{'generated_text': 'COVID-19 is transmitted via droplets, air-borne, or airborne transmission.'}]
```

-------------------- github --------------------

Document 1:

url = {https://doi.org/10.1093/bib/bbac409}
------------------------------
Document 2:

```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the United States alone.'},
{'generated_text': 'COVID-19 is a pandemic, the incidence has been increased in a manner similar to that in other'},
{'generated_text': 'COVID-19 is transmitted via droplets, air-borne, or airborne transmission.'}]
```

-------------------- paper --------------------

Document 1:

@article{10.1093/bib/bbac409,
author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
title = '{BioGPT: generative pre-trained transformer for biomedical text generation and mining}',
journal = {Briefings in Bioinformatics},
volume = {23},
number = {6},
year = {2022},
month = {09},
abstract = '{Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.}',
issn = {1477-4054},
doi = {10.1093/bib/bbac409},
url = {https://doi.org/10.1093/bib/bbac409},
note = {bbac409},
eprint = {https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf},
}
------------------------------
Document 2:

In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature.

-------------------- upstream_model --------------------

Document 1:

BioGPT
------------------------------
Document 2:

@article{10.1093/bib/bbac409,
author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
title = '{BioGPT: generative pre-trained transformer for biomedical text generation and mining}',
journal = {Briefings in Bioinformatics},
volume = {23},
number = {6},
year = {2022},
month = {09},
abstract = '{Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.}',
issn = {1477-4054},
doi = {10.1093/bib/bbac409},
url = {https://doi.org/10.1093/bib/bbac409},
note = {bbac409},
eprint = {https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf},
}
------------------------------
Document 3:

upstream_model

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the United States alone.'},
{'generated_text': 'COVID-19 is a pandemic, the incidence has been increased in a manner similar to that in other'},
{'generated_text': 'COVID-19 is transmitted via droplets, air-borne, or airborne transmission.'}]
```

-------------------- evaluation --------------------

Document 1:

We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\% accuracy on PubMedQA, creating a new record.
------------------------------
Document 2:

We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record.

-------------------- hardware --------------------

Document 1:

```python
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
```

-------------------- limitation_and_bias --------------------

Document 1:

The limitations and biases of the model are not mentioned in the given context.
------------------------------
Document 2:

#tags  
---
language: en
license: mit
widget:
- text: COVID-19 is

-------------------- demo --------------------

Document 1:

widget:
- text: COVID-19 is
------------------------------
Document 2:

https://doi.org/10.1093/bib/bbac409
------------------------------
Document 3:

You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:  
```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the United States alone.'},
{'generated_text': 'COVID-19 is a pandemic, the incidence has been increased in a manner similar to that in other'},
{'generated_text': 'COVID-19 is transmitted via droplets, air-borne, or airborne transmission.'}]
```

-------------------- input_format --------------------

Document 1:

Here is how to use this model to get the features of a given text in PyTorch:  
```python
from transformers import BioGptTokenizer, BioGptForCausalLM
tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
text = 'Replace me by any text you'd like.'
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```
------------------------------
Document 2:

input_format

-------------------- output_format --------------------

Document 1:

You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:  
```python
>>> from transformers import pipeline, set_seed
>>> from transformers import BioGptTokenizer, BioGptForCausalLM
>>> model = BioGptForCausalLM.from_pretrained('microsoft/biogpt')
>>> tokenizer = BioGptTokenizer.from_pretrained('microsoft/biogpt')
>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)
>>> set_seed(42)
>>> generator('COVID-19 is', max_length=20, num_return_sequences=5, do_sample=True)
[{'generated_text': 'COVID-19 is a disease that spreads worldwide and is currently found in a growing proportion of the population'},
{'generated_text': 'COVID-19 is one of the largest viral epidemics in the world.'},
{'generated_text': 'COVID-19 is a common condition affecting an estimated 1.1 million people in the United States alone.'},
{'generated_text': 'COVID-19 is a pandemic, the incidence has been increased in a manner similar to that in other'},
{'generated_text': 'COVID-19 is transmitted via droplets, air-borne, or airborne transmission.'}]
```
------------------------------
Document 2:

output_format
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['BioGPT'], 'license': 'mit', 'github': 'https://doi.org/10.1093/bib/bbac409', 'paper 
': 'https://doi.org/10.1093/bib/bbac409', 'upstream_model': 'BioGPT', 'parameter_count': '', 'hyper_ 
parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_for 
mat': '', 'output_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]                    

#####################KoboldAI/GPT-NeoX-20B-Erebus########################

-------------------- datasets --------------------

Document 1:

- Literotica (everything with 4.5/5 or higher)
- Sexstories (everything with 90 or higher)
- Dataset-G (private dataset of X-rated stories)
- Doc's Lab (all stories)
- Pike Dataset (novels with 'adult' rating)
- SoFurry (collection of various animals)
------------------------------
Document 2:

EleutherAI to train their GPT-J-6B model.

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

This model has a very strong NSFW bias!
------------------------------
Document 3:

@inproceedings{gpt-neox-20b,
title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
url={https://arxiv.org/abs/2204.06745},
year={2022}
}

@misc{mesh-transformer-jax,
author = {Wang, Ben},
title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},
howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
year = 2021,
month = May
}

-------------------- github --------------------

Document 1:

- Literotica (everything with 4.5/5 or higher)
- Sexstories (everything with 90 or higher)
- Dataset-G (private dataset of X-rated stories)
- Doc's Lab (all stories)
- Pike Dataset (novels with 'adult' rating)
- SoFurry (collection of various animals)

-------------------- paper --------------------

Document 1:

This model has a very strong NSFW bias!
------------------------------
Document 2:

@inproceedings{gpt-neox-20b,
title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
url={https://arxiv.org/abs/2204.06745},
year={2022}
}

-------------------- upstream_model --------------------

Document 1:

This model has a very strong NSFW bias!
------------------------------
Document 2:

@inproceedings{gpt-neox-20b,
title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
url={https://arxiv.org/abs/2204.06745},
year={2022}
}

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

This model has a very strong NSFW bias!

-------------------- evaluation --------------------

Document 1:

This model has a very strong NSFW bias!

-------------------- hardware --------------------

Document 1:

TPUv3-256 TPU pod
------------------------------
Document 2:

The GPT-NeoX-20B model weights:
```bibtex
@inproceedings{gpt-neox-20b,
title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
url={https://arxiv.org/abs/2204.06745},
year={2022}
}
```

-------------------- limitation_and_bias --------------------

Document 1:

bias (gender, profession, race and religion). **Warning: This model has a very strong NSFW bias!**
------------------------------
Document 2:

The GPT-NeoX-20B model weights:
```bibtex
@inproceedings{gpt-neox-20b,
title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
url={https://arxiv.org/abs/2204.06745},
year={2022}
}
```
------------------------------
Document 3:

The data can be divided in 6 different datasets:
- Literotica (everything with 4.5/5 or higher)
- Sexstories (everything with 90 or higher)
- Dataset-G (private dataset of X-rated stories)
- Doc's Lab (all stories)
- Pike Dataset (novels with 'adult' rating)
- SoFurry (collection of various animals)

-------------------- demo --------------------

Document 1:

**Warning: This model has a very strong NSFW bias!**

-------------------- input_format --------------------

Document 1:

The dataset uses `[Genre: <comma-separated list of genres>]` for tagging.

-------------------- output_format --------------------

Document 1:

The dataset uses `[Genre: <comma-separated list of genres>]` for tagging.

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['Literotica', 'Sexstories', 'Dataset-G', "Doc's Lab", 'Pike Dataset', 'SoFurry'], 'l 
icense': 'apache-2.0', 'github': 'https://github.com/kingoflolz/mesh-transformer-jax', 'paper': 'htt 
ps://arxiv.org/abs/2204.06745', 'upstream_model': 'GPT-NeoX-20B', 'parameter_count': '', 'hyper_para 
meters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], ' 
hardware': 'TPUv3-256 TPU pod', 'limitation_and_bias': 'bias (gender, profession, race and religion) 
. **Warning: This model has a very strong NSFW bias!**', 'demo': '**Warning: This model has a very s 
trong NSFW bias!**', 'input_format': 'The dataset uses `[Genre: <comma-separated list of genres>]` f 
or tagging.', 'output_format': 'The dataset uses `[Genre: <comma-separated list of genres>]` for tag 
ging.', 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': ''}]                        

#####################tiiuae/falcon-7b-instruct########################

-------------------- datasets --------------------

Document 1:

To learn more about the pretraining dataset, see the 📓 [RefinedWeb paper](https://arxiv.org/abs/2306.01116).
------------------------------
Document 2:

[Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)
------------------------------
Document 3:

instruct and chat datasets

-------------------- license --------------------

Document 1:

Falcon-7B-Instruct is made available under the Apache 2.0 license.
------------------------------
Document 2:

license: apache-2.0

-------------------- github --------------------

Document 1:

To learn more about the pretraining dataset, see the 📓 [RefinedWeb paper](https://arxiv.org/abs/2306.01116).
------------------------------
Document 2:

- tiiuae/falcon-refinedweb

-------------------- paper --------------------

Document 1:

*Paper coming soon.*
------------------------------
Document 2:

- **Paper:** *coming soon*.
------------------------------
Document 3:

The research paper associated with this model is titled "Falcon-40B: an open large language model with state-of-the-art performance" and is authored by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme. It was published in the year 2023.

-------------------- upstream_model --------------------

Document 1:

[Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)
------------------------------
Document 2:

[Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)

-------------------- parameter_count --------------------

Document 1:

| **Hyperparameter** | **Value** | **Comment**                            |
|--------------------|-----------|----------------------------------------|
| Layers             | 32        |                                        |
| `d_model`          | 4544      | Increased to compensate for multiquery                                       |
| `head_dim`         | 64        | Reduced to optimise for FlashAttention |
| Vocabulary         | 65024     |                                        |
| Sequence length    | 2048      |                                        |

-------------------- hyper_parameters --------------------

Document 1:

| **Hyperparameter** | **Value** | **Comment**                            |
|--------------------|-----------|----------------------------------------|
| Layers             | 32        |                                        |
| `d_model`          | 4544      | Increased to compensate for multiquery                                       |
| `head_dim`         | 64        | Reduced to optimise for FlashAttention |
| Vocabulary         | 65024     |                                        |
| Sequence length    | 2048      |                                        |

-------------------- evaluation --------------------

Document 1:

| **Hyperparameter** | **Value** | **Comment**                            |
|--------------------|-----------|----------------------------------------|
| Layers             | 32        |                                        |
| `d_model`          | 4544      | Increased to compensate for multiquery                                       |
| `head_dim`         | 64        | Reduced to optimise for FlashAttention |
| Vocabulary         | 65024     |                                        |
| Sequence length    | 2048      |                                        |

-------------------- hardware --------------------

Document 1:

Falcon-7B-Instruct was trained on AWS SageMaker, on 32 A100 40GB GPUs in P4d instances.

-------------------- limitation_and_bias --------------------

Document 1:

Falcon-7B-Instruct is mostly trained on English data, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.
------------------------------
Document 2:

Note that this model variant is not optimized for NLP benchmarks.
------------------------------
Document 3:

Positionnal embeddings: rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));
Attention: multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));
Decoder-block: parallel attention/MLP with a single layer norm.

-------------------- demo --------------------

Document 1:

- text: Can you write a short tweet about the Apache 2.0 release of our latest AI model, Falcon LLM?
example_title: Twitter Helper
------------------------------
Document 2:

To learn more about the pretraining dataset, see the 📓 [RefinedWeb paper](https://arxiv.org/abs/2306.01116).
------------------------------
Document 3:

See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.

-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

output_format

-------------------- max_sequence_length --------------------

Document 1:

Sequence length    | 2048      |
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

Vocabulary         | 65024     |
------------------------------
Document 2:

| **Data source**    | **Fraction** | **Tokens** | **Description**                       |
|--------------------|--------------|------------|-----------------------------------|
| [Bai ze](https://github.com/project-baize/baize-chatbot) | 65%          | 164M     | chat                 |
| [GPT4All](https://github.com/nomic-ai/gpt4all)              | 25%           | 62M       | instruct                                  |
| [GPTeacher](https://github.com/teknium1/GPTeacher)      | 5%           | 11M        | instruct |
| [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 5%          | 13M     | massive web crawl                 |


[{'datasets': ['Falcon-7B-Instruct'], 'license': 'apache-2.0', 'github': 'tiiuae/falcon-refinedweb' 
, 'paper': '', 'upstream_model': 'Falcon-7B', 'parameter_count': '', 'hyper_parameters': {'epochs':  
'', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limi 
tation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': '' 
, 'vocabulary_size': ''}]                                                                            

#####################TheBloke/wizardLM-13B-1.0-fp16########################

-------------------- datasets --------------------



-------------------- license --------------------

Document 1:

The resources, including code, data, and model weights, associated with this project are restricted for academic research purposes only and cannot be used for commercial purposes.
------------------------------
Document 2:

@misc{xu2023wizardlm,
title={WizardLM: Empowering Large Language Models to Follow Complex Instructions},
author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
year={2023},
eprint={2304.12244},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

-------------------- github --------------------

Document 1:

1. [Online Demo](#online-demo)  
2. [Training Data](#training-data)  
3. [WizardLM Weights](#wizardlm-weights)  
4. [Fine-tuning](#fine-tuning)  
5. [Distributed Fine-tuning](#distributed-Fine-tuning)  
6. [Inference](#inference)  
7. [Evaluation](#evaluation)  
8. [Citation](#citation)  
9. [Disclaimer](#disclaimer)
------------------------------
Document 2:

[Demo Link](https://011fc8477ad734d7.gradio.app)  
[Demo Backup 1](https://1825e531c43a23c7.gradio.app)

-------------------- paper --------------------

Document 1:

@misc{xu2023wizardlm,
title={WizardLM: Empowering Large Language Models to Follow Complex Instructions},
author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
year={2023},
eprint={2304.12244},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------

Document 1:

--num_train_epochs 3 \
--model_max_length 2048 \
--per_device_train_batch_size 8 \
--per_device_eval_batch_size 1 \
--gradient_accumulation_steps 1 \
--learning_rate 2e-5 \
--warmup_steps 2 \
--lr_scheduler_type 'cosine' \
------------------------------
Document 2:

- 🔥 We released **13B** version of **WizardLM** trained with **250k** evolved instructions (from ShareGPT). Checkout the [Demo_13B](https://a6d4f31b5a1ee33f.gradio.app/), [Demo_13B_bak](https://e79c80d2c2379e77.gradio.app) and the GPT-4 evaluation. Please download our delta model at the following [link](https://huggingface.co/victor123/WizardLM-13B-1.0).
- 🔥 We released **7B** version of **WizardLM** trained with **70k** evolved instructions (from Alpaca data). Checkout the [paper](https://arxiv.org/abs/2304.12244) and [Demo_7B](https://f195ccdce69a86d5.gradio.app) , [Demo_7B_bak](https://ce25bd0feced0f77.gradio.app)
- <b>Note for 13B model usage:</b> To obtain results **identical to our demo**, please strictly follow the prompts and invocation methods provided in the **'src/infer_wizardlm13b.py'** to use our 13B model for inference. Unlike the 7B model, the 13B model adopts the prompt format from Vicuna and supports **multi-turn** conversation.

-------------------- hyper_parameters --------------------

Document 1:

| Hyperparameter | LLaMA-7B | LLaMA-13B|
|----------------|----------|----------|
| Batch size     | 64       | 384      |
| Learning rate  | 2e-5     | 2e-5     |
| Epochs         | 3        | 3        |
| Max length     | 2048     | 2048     |
| Warmup step    | 2        | 50       |
| LR scheduler   | cosine   | cosine   |

-------------------- evaluation --------------------

Document 1:

7. [Evaluation](#evaluation)
------------------------------
Document 2:

We adopt the automatic evaluation framework based on GPT-4 proposed by FastChat to assess the performance of chatbot models. As shown in the following figure, WizardLM-13B achieved better results than Vicuna-13b.

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

The content produced by any version of WizardLM is influenced by uncontrollable variables such as randomness, and therefore, the accuracy of the output cannot be guaranteed by this project. This project does not accept any legal liability for the content of the model output, nor does it assume responsibility for any losses incurred due to the use of associated resources and output results.

-------------------- demo --------------------

Document 1:

[Demo Link](https://011fc8477ad734d7.gradio.app)  
[Demo Backup 1](https://1825e531c43a23c7.gradio.app)
------------------------------
Document 2:

[Online Demo](#online-demo)
------------------------------
Document 3:

[Case Show](https://github.com/nlpxucan/WizardLM/blob/main/src/case_show.md)

-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

- `output`: `str`, the answer to the instruction as generated by `gpt-3.5-turbo`.

-------------------- max_sequence_length --------------------

Document 1:

To obtain results **identical to our demo**, please strictly follow the prompts and invocation methods provided in the **'src/infer_wizardlm13b.py'** to use our 13B model for inference. Unlike the 7B model, the 13B model adopts the prompt format from Vicuna and supports **multi-turn** conversation.

-------------------- vocabulary_size --------------------

Document 1:

- 🔥 We released **13B** version of **WizardLM** trained with **250k** evolved instructions (from ShareGPT). Checkout the [Demo_13B](https://a6d4f31b5a1ee33f.gradio.app/), [Demo_13B_bak](https://e79c80d2c2379e77.gradio.app) and the GPT-4 evaluation. Please download our delta model at the following [link](https://huggingface.co/victor123/WizardLM-13B-1.0).
- 🔥 We released **7B** version of **WizardLM** trained with **70k** evolved instructions (from Alpaca data). Checkout the [paper](https://arxiv.org/abs/2304.12244) and [Demo_7B](https://f195ccdce69a86d5.gradio.app) , [Demo_7B_bak](https://ce25bd0feced0f77.gradio.app)
- <b>Note for 13B model usage:</b> To obtain results **identical to our demo**, please strictly follow the prompts and invocation methods provided in the **'src/infer_wizardlm13b.py'** to use our 13B model for inference. Unlike the 7B model, the 13B model adopts the prompt format from Vicuna and supports **multi-turn** conversation.


[{'datasets': [], 'license': 'The resources, including code, data, and model weights, associated wi 
th this project are restricted for academic research purposes only and cannot be used for commercial 
 purposes.', 'github': '1. [Online Demo](#online-demo)\n2. [Training Data](#training-data)\n3. [Wiza 
rdLM Weights](#wizardlm-weights)\n4. [Fine-tuning](#fine-tuning)\n5. [Distributed Fine-tuning](#dist 
ributed-Fine-tuning)\n6. [Inference](#inference)\n7. [Evaluation](#evaluation)\n8. [Citation](#citat 
ion)\n9. [Disclaimer](#disclaimer)', 'paper': '@misc{xu2023wizardlm,\ntitle={WizardLM: Empowering La 
rge Language Models to Follow Complex Instructions},\nauthor={Can Xu and Qingfeng Sun and Kai Zheng  
and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},\nyear={2023},\neprin 
t={2304.12244},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}', 'upstream_model': '', 'parameter_ 
count': "--num_train_epochs 3 \\n--model_max_length 2048 \\n--per_device_train_batch_size 8 \\n--per 
_device_eval_batch_size 1 \\n--gradient_accumulation_steps 1 \\n--learning_rate 2e-5 \\n--warmup_ste 
ps 2 \\n--lr_scheduler_type 'cosine'", 'hyper_parameters': {'epochs': '3', 'batch_size': '8', 'learn 
ing_rate': '2e-5', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': 'The c 
ontent produced by any version of WizardLM is influenced by uncontrollable variables such as randomn 
ess, and therefore, the accuracy of the output cannot be guaranteed by this project. This project do 
es not accept any legal liability for the content of the model output, nor does it assume responsibi 
lity for any losses incurred due to the use of associated resources and output results.', 'demo': '[ 
Demo Link](https://011fc8477ad734d7.gradio.app)  \n[Demo Backup 1](https://1825e531c43a23c7.gradio.a 
pp)', 'input_format': '', 'output_format': '- `output`: `str`, the answer to the instruction as gene 
rated by `gpt-3.5-turbo`.', 'max_sequence_length': "To obtain results **identical to our demo**, ple 
ase strictly follow the prompts and invocation methods provided in the **'src/infer_wizardlm13b.py'* 
* to use our 13B model for inference. Unlike the 7B model, the 13B model adopts the prompt format fr 
om Vicuna and supports **multi-turn** conversation.", 'vocabulary_size': "- 🔥 We released **13B** ve 
rsion of **WizardLM** trained with **250k** evolved instructions (from ShareGPT). Checkout the [Demo 
_13B](https://a6d4f31b5a1ee33f.gradio.app/), [Demo_13B_bak](https://e79c80d2c2379e77.gradio.app) and 
 the GPT-4 evaluation. Please download our delta model at the following [link](https://huggingface.c 
o/victor123/WizardLM-13B-1.0).\n- 🔥 We released **7B** version of **WizardLM** trained with **70k**  
evolved instructions (from Alpaca data). Checkout the [paper](https://arxiv.org/abs/2304.12244) and  
[Demo_7B](https://f195ccdce69a86d5.gradio.app) , [Demo_7B_bak](https://ce25bd0feced0f77.gradio.app)\ 
n- <b>Note for 13B model usage:</b> To obtain results **identical to our demo**, please strictly fol 
low the prompts and invocation methods provided in the **'src/infer_wizardlm13b.py'** to use our 13B 
 model for inference. Unlike the 7B model, the 13B model adopts the prompt format from Vicuna and su 
pports **multi-turn** conversation."}]                                                               

#####################mosaicml/mpt-30b-chat########################

-------------------- datasets --------------------

Document 1:

| Data Source | Number of Tokens in Source | Proportion |
|-------------|----------------------------|------------|
| Airoboros/GPT4-1.2 | 26.4M | 1.71% |
| Baize | 55.0M | 3.57% |
| Camel	| 301M | 19.54% |
| GPTeacher	| 7.56M | 0.49% |
| Guanaco | 15.6M | 1.02% |
| LongCoversations | 18.4M | 1.19% |
| ShareGPT | 821M | 53.24% |
| WizardLM | 297M | 19.23% |
------------------------------
Document 2:

This model was trained on 64 H100s for about 7.6 hours using the [MosaicML Platform](https://www.mosaicml.com/platform).

-------------------- license --------------------

Document 1:

The license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please consult an attorney before using this model for commercial purposes.
------------------------------
Document 2:

_CC-By-NC-SA-4.0_ (non-commercial use only)
------------------------------
Document 3:

license: cc-by-nc-sa-4.0

-------------------- github --------------------

Document 1:

* [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)
------------------------------
Document 2:

- camel-ai/code
- ehartford/wizard_vicuna_70k_unfiltered
- anon8231489123/ShareGPT_Vicuna_unfiltered
- teknium1/GPTeacher/roleplay-instruct-v2-final
- teknium1/GPTeacher/codegen-isntruct
- timdettmers/openassistant-guanaco
- camel-ai/math
- project-baize/baize-chatbot/medical_chat_data
- project-baize/baize-chatbot/quora_chat_data
- project-baize/baize-chatbot/stackoverflow_chat_data
- camel-ai/biology
- camel-ai/chemistry
- camel-ai/ai_society
- jondurbin/airoboros-gpt4-1.2
- LongConversations
- camel-ai/physics

-------------------- paper --------------------

Document 1:

@online{MosaicML2023Introducing,
author    = {MosaicML NLP Team},
title     = {Introducing MPT-30B: Raising the bar
for open-source foundation models},
year      = {2023},
url       = {www.mosaicml.com/blog/mpt-30b},
note      = {Accessed: 2023-06-22},
urldate   = {2023-06-22}
}
------------------------------
Document 2:

'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date.

-------------------- upstream_model --------------------

Document 1:

The model has been modified from a standard transformer in the following ways:
* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases

-------------------- parameter_count --------------------

Document 1:

|n_parameters | 29.95B |

-------------------- hyper_parameters --------------------

Document 1:

| Hyperparameter | Value |
|----------------|-------|
|n_parameters | 29.95B |
|n_layers | 48 |
| n_heads | 64 |
| d_model | 7168 |
| vocab size | 50432 |
| sequence length | 8192 |
------------------------------
Document 2:

hyper parameters, model, trained, 64 H100s, 7.6 hours, MosaicML Platform, sharded data parallelism, FSDP, AdamW optimizer

-------------------- evaluation --------------------

Document 1:

'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date.

-------------------- hardware --------------------

Document 1:

This model was trained on 64 H100s for about 7.6 hours.
------------------------------
Document 2:

The architecture is a modification of a standard decoder-only transformer.  
The model has been modified from a standard transformer in the following ways:
* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases

-------------------- limitation_and_bias --------------------

Document 1:

| Data Source | Number of Tokens in Source | Proportion |
|-------------|----------------------------|------------|
| Airoboros/GPT4-1.2 | 26.4M | 1.71% |
| Baize | 55.0M | 3.57% |
| Camel	| 301M | 19.54% |
| GPTeacher	| 7.56M | 0.49% |
| Guanaco | 15.6M | 1.02% |
| LongCoversations | 18.4M | 1.19% |
| ShareGPT | 821M | 53.24% |
| WizardLM | 297M | 19.23% |
'LongConversations' is a GPT3.5/4-generated dataset, details of which will be released at a later date.
------------------------------
Document 2:

The model has been modified from a standard transformer in the following ways:
* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases

-------------------- demo --------------------

Document 1:

[sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b)
------------------------------
Document 2:

url       = {www.mosaicml.com/blog/mpt-30b}

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

| Hyperparameter | Value |
|----------------|-------|
|n_parameters | 29.95B |
|n_layers | 48 |
| n_heads | 64 |
| d_model | 7168 |
| vocab size | 50432 |
| sequence length | 8192 |

-------------------- output_format --------------------

Document 1:

| Hyperparameter | Value |
|----------------|-------|
|n_parameters | 29.95B |
|n_layers | 48 |
| n_heads | 64 |
| d_model | 7168 |
| vocab size | 50432 |
| sequence length | 8192 |

-------------------- max_sequence_length --------------------

Document 1:

| sequence length | 8192 |

-------------------- vocabulary_size --------------------

Document 1:

vocab size | 50432


[{'datasets': ['Airoboros/GPT4-1.2', 'Baize', 'Camel', 'GPTeacher', 'Guanaco', 'LongCoversations',  
'ShareGPT', 'WizardLM'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_ 
count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ' 
'}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'ou 
tput_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]                                 

#####################cross-encoder/ms-marco-MiniLM-L-2-v2########################

-------------------- datasets --------------------

Document 1:

This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

[TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset.

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)

-------------------- github --------------------

Document 1:

This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |
| ------------- |:-------------| -----| --- |
| **Version 2 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000
| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100
| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500
| cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800
| cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960
| **Version 1 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000
| cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900
| cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680
| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340
| **Other models** | | |
| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900
| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340
| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100
| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340
| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330
| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720

-------------------- paper --------------------

Document 1:

[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.  
The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000
cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100
cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500
cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800
cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960
cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000
cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900
cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680
cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340
nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900
nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340
nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100
Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340
amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330
sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720
------------------------------
Document 3:

The usage becomes easier when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:
```python
from sentence_transformers import CrossEncoder
model = CrossEncoder('model_name', max_length=512)
```

-------------------- upstream_model --------------------

Document 1:

This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

CrossEncoder

-------------------- parameter_count --------------------

Document 1:

This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.
------------------------------
Document 2:

cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960

-------------------- hyper_parameters --------------------

Document 1:

The model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.
------------------------------
Document 2:

| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |
| ------------- |:-------------| -----| --- |
| **Version 2 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000
| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100
| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500
| cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800
| cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960
| **Version 1 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000
| cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900
| cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680
| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340
| **Other models** | | |
| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900
| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340
| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100
| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340
| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330
| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720
------------------------------
Document 3:

max_length=512

-------------------- evaluation --------------------

Document 1:

This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.
------------------------------
Document 2:

model.eval()
with torch.no_grad():
scores = model(**features).logits
print(scores)
------------------------------
Document 3:

| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |
| ------------- |:-------------| -----| --- |
| **Version 2 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000
| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100
| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500
| cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800
| cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960
| **Version 1 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000
| cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900
| cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680
| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340
| **Other models** | | |
| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900
| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340
| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100
| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340
| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330
| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720

-------------------- hardware --------------------

Document 1:

This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.
------------------------------
Document 2:

hardware
------------------------------
Document 3:

Runtime was computed on a V100 GPU.

-------------------- limitation_and_bias --------------------

Document 1:

The model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.
------------------------------
Document 2:

The usage becomes easier when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:
```python
from sentence_transformers import CrossEncoder
model = CrossEncoder('model_name', max_length=512)
```

-------------------- demo --------------------

Document 1:

The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details.
------------------------------
Document 2:

The usage becomes easier when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:
```python
from sentence_transformers import CrossEncoder
model = CrossEncoder('model_name', max_length=512)
scores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])
```

-------------------- input_format --------------------

Document 1:

The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order.
------------------------------
Document 2:

input_format
------------------------------
Document 3:

| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |
| ------------- |:-------------| -----| --- |
| **Version 2 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000
| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100
| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500
| cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800
| cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960
| **Version 1 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000
| cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900
| cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680
| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340
| **Other models** | | |
| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900
| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340
| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100
| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340
| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330
| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720

-------------------- output_format --------------------

Document 1:

The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order.
------------------------------
Document 2:

output_format
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_length=512
------------------------------
Document 2:

The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order.
------------------------------
Document 3:

max_sequence_length

-------------------- vocabulary_size --------------------




ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens. However, your messages resulted in 4238 tokens (4081 in the messages, 157 in the functions). Please reduce the length of the messages or functions. 

#####################Hello-SimpleAI/chatgpt-detector-roberta-chinese########################

-------------------- datasets --------------------

Document 1:

This model is trained on the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese). We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.
------------------------------
Document 2:

- Hello-SimpleAI/HC3-Chinese
------------------------------
Document 3:

arxiv: 2301.07597

-------------------- license --------------------

Document 1:

language:
- zh
tags:
- chatgpt
datasets:
- Hello-SimpleAI/HC3-Chinese
pipeline_tag: text-classification
------------------------------
Document 2:

@article{guo-etal-2023-hc3,
title = 'How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection',
author = 'Guo, Biyang  and
Zhang, Xin  and
Wang, Ziyuan  and
Jiang, Minqi  and
Nie, Jinran  and
Ding, Yuxuan  and
Yue, Jianwei  and
Wu, Yupeng',
journal={arXiv preprint arxiv:2301.07597}
year = '2023',
}
------------------------------
Document 3:

[arxiv: 2301.07597](https://arxiv.org/abs/2301.07597) and Gtihub project [Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection).

-------------------- github --------------------

Document 1:

Hello-SimpleAI/HC3-Chinese
------------------------------
Document 2:

[Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)

-------------------- paper --------------------

Document 1:

@article{guo-etal-2023-hc3,
title = 'How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection',
author = 'Guo, Biyang  and
Zhang, Xin  and
Wang, Ziyuan  and
Jiang, Minqi  and
Nie, Jinran  and
Ding, Yuxuan  and
Yue, Jianwei  and
Wu, Yupeng',
journal={arXiv preprint arxiv:2301.07597}
year = '2023',
}
------------------------------
Document 2:

[arxiv: 2301.07597](https://arxiv.org/abs/2301.07597)

-------------------- upstream_model --------------------

Document 1:

arxiv: 2301.07597
------------------------------
Document 2:

This model is trained on **the mix of full-text and splitted sentences** of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese). The base checkpoint is [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext). We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.

-------------------- parameter_count --------------------

Document 1:

This model is trained on the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese). The base checkpoint is [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext). We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.

-------------------- hyper_parameters --------------------

Document 1:

The base checkpoint is [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext).
We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.

-------------------- evaluation --------------------

Document 1:

This model is trained on **the mix of full-text and splitted sentences** of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese).  
We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.
------------------------------
Document 2:

#tags  
---
language:
- zh
tags:
- chatgpt
datasets:
- Hello-SimpleAI/HC3-Chinese
pipeline_tag: text-classification

-------------------- hardware --------------------

Document 1:

The base checkpoint is [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext).
We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.

-------------------- limitation_and_bias --------------------

Document 1:

This model is trained on **the mix of full-text and splitted sentences** of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese).  
More details refer to [arxiv: 2301.07597](https://arxiv.org/abs/2301.07597) and Gtihub project [Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection).  
The base checkpoint is [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext).
We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.  
(2-epoch is consistent with the experiments in [our paper](https://arxiv.org/abs/2301.07597).)
------------------------------
Document 2:

#tags  
---
language:
- zh
tags:
- chatgpt
datasets:
- Hello-SimpleAI/HC3-Chinese
pipeline_tag: text-classification

-------------------- demo --------------------

Document 1:

arxiv: 2301.07597
------------------------------
Document 2:

This model is trained on **the mix of full-text and splitted sentences** of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese).  
More details refer to [arxiv: 2301.07597](https://arxiv.org/abs/2301.07597) and Gtihub project [Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection).  
The base checkpoint is [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext).
We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.  
(2-epoch is consistent with the experiments in [our paper](https://arxiv.org/abs/2301.07597).)

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

This model is trained on **the mix of full-text and splitted sentences** of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese).

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

This model is trained on **the mix of full-text and splitted sentences** of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese).

-------------------- max_sequence_length --------------------

Document 1:

This model is trained on **the mix of full-text and splitted sentences** of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese).
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

This model is trained on **the mix of full-text and splitted sentences** of `answer`s from [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese). The base checkpoint is [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext). We train it with all [Hello-SimpleAI/HC3-Chinese](https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese) data (without held-out) for 2 epochs.


[{'datasets': ['Hello-SimpleAI/HC3-Chinese'], 'license': '', 'github': 'Hello-SimpleAI/HC3-Chinese' 
, 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batc 
h_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_an 
d_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': '', 'vocabu 
lary_size': ''}]                                                                                     

#####################mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis########################

-------------------- datasets --------------------

Document 1:

datasets:
- financial_phrasebank

-------------------- license --------------------



-------------------- github --------------------



-------------------- paper --------------------



-------------------- upstream_model --------------------

Document 1:

[distilroberta-base](https://huggingface.co/distilroberta-base)
------------------------------
Document 2:

upstream_model: distilRoberta-financial-sentiment

-------------------- parameter_count --------------------

Document 1:

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| No log        | 1.0   | 255  | 0.1670          | 0.9646   |
| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |
| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |
| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |
| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |

-------------------- hyper_parameters --------------------

Document 1:

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 5
------------------------------
Document 2:

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| No log        | 1.0   | 255  | 0.1670          | 0.9646   |
| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |
| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |
| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |
| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |

-------------------- evaluation --------------------

Document 1:

metrics:
- type: accuracy
value: 0.9823008849557522
name: Accuracy
------------------------------
Document 2:

It achieves the following results on the evaluation set:
- Loss: 0.1116
- Accuracy: 0.9823

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

limitation_and_bias

-------------------- demo --------------------



-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| No log        | 1.0   | 255  | 0.1670          | 0.9646   |
| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |
| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |
| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |
| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |

-------------------- max_sequence_length --------------------



-------------------- vocabulary_size --------------------




[{'datasets': ['financial_phrasebank'], 'license': '', 'github': '', 'paper': '', 'upstream_model': 
 'distilroberta-base', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware':  
'', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_le 
ngth': '', 'vocabulary_size': ''}]                                                                   

#####################finiteautomata/bertweet-base-sentiment-analysis########################

-------------------- datasets --------------------

Document 1:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.

-------------------- license --------------------

Document 1:

Please be aware that models are trained with third-party datasets and are subject to their respective licenses.  
1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

@misc{perez2021pysentimiento,
title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},
author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},
year={2021},
eprint={2106.09462},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

-------------------- github --------------------

Document 1:

Repository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)  
Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.

-------------------- paper --------------------

Document 1:

If you use `pysentimiento` in your work, please cite [this paper](https://arxiv.org/abs/2106.09462)  
```
@misc{perez2021pysentimiento,
title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},
author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},
year={2021},
eprint={2106.09462},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
```
------------------------------
Document 2:

Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.
------------------------------
Document 3:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()

-------------------- upstream_model --------------------

Document 1:

Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.
------------------------------
Document 2:

TASS Dataset license
SEMEval 2017 Dataset license

-------------------- parameter_count --------------------

Document 1:

Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.

-------------------- hyper_parameters --------------------

Document 1:

Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.

-------------------- evaluation --------------------

Document 1:

Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.  
Uses `POS`, `NEG`, `NEU` labels.
------------------------------
Document 2:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()

-------------------- hardware --------------------

Document 1:

Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.

-------------------- limitation_and_bias --------------------

Document 1:

Please be aware that models are trained with third-party datasets and are subject to their respective licenses.  
1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.  
Uses `POS`, `NEG`, `NEU` labels.

-------------------- demo --------------------

Document 1:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

If you use `pysentimiento` in your work, please cite [this paper](https://arxiv.org/abs/2106.09462)  
```
@misc{perez2021pysentimiento,
title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},
author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},
year={2021},
eprint={2106.09462},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
```

-------------------- input_format --------------------

Document 1:

Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.  
Uses `POS`, `NEG`, `NEU` labels.

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

output_format: `POS`, `NEG`, `NEU` labels.

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

vocabulary_size


[{'datasets': ['TASS Dataset', 'SEMEval 2017 Dataset'], 'license': '', 'github': 'https://github.co 
m/finiteautomata/pysentimiento/', 'paper': 'https://arxiv.org/abs/2106.09462', 'upstream_model': 'BE 
RTweet', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitatio 
n_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': '', 'vo 
cabulary_size': ''}]                                                                                 

#####################declare-lab/flan-alpaca-gpt4-xl########################

-------------------- datasets --------------------

Document 1:

- tatsu-lab/alpaca
------------------------------
Document 2:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) represents an exciting new direction to approximate the performance of large language models (LLMs) like ChatGPT cheaply and easily. The synthetic data which covers more than 50k tasks can then be used to finetune a smaller model. However, the original implementation is less accessible due to licensing constraints of the underlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model. Furthermore, users have noted [potential noise](https://github.com/tloen/alpaca-lora/issues/65) in the synthetic dataset. Hence, it may be better to explore a fully accessible model that is already trained on high-quality (but less diverse) instructions such as [Flan-T5](https://arxiv.org/abs/2210.11416).

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

licensing constraints of the underlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model.
------------------------------
Document 3:

[Flan-Alpaca-Base](https://huggingface.co/declare-lab/flan-alpaca-base)          | 220M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
[Flan-Alpaca-Large](https://huggingface.co/declare-lab/flan-alpaca-large)        | 770M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
[Flan-Alpaca-XL](https://huggingface.co/declare-lab/flan-alpaca-xl)              | 3B         | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
[Flan-Alpaca-XXL](https://huggingface.co/declare-lab/flan-alpaca-xxl)            | 11B        | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 4x A6000 (FSDP) |
[Flan-GPT4All-XL](https://huggingface.co/declare-lab/flan-gpt4all-xl)            | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4All](https://github.com/nomic-ai/gpt4all)                                                    | 1x A6000        |
[Flan-ShareGPT-XL](https://huggingface.co/declare-lab/flan-sharegpt-xl)          | 3B         | [Flan](https://github.com/google-research/FLAN), [ShareGPT](https://github.com/domeccleston/sharegpt)/[Vicuna](https://github.com/lm-sys/FastChat) | 1x A6000        |
[Flan-Alpaca-GPT4-XL*](https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl)   | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4-Alpaca](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                         | 1x A6000        |

-------------------- github --------------------

Document 1:

tatsu-lab/alpaca
------------------------------
Document 2:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) represents an exciting new direction
to approximate the performance of large language models (LLMs) like ChatGPT cheaply and easily.
Concretely, they leverage an LLM such as GPT-3 to generate instructions as synthetic training data.
The synthetic data which covers more than 50k tasks can then be used to finetune a smaller model.
However, the original implementation is less accessible due to licensing constraints of the
underlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model.
Furthermore, users have noted [potential noise](https://github.com/tloen/alpaca-lora/issues/65) in the synthetic
dataset. Hence, it may be better to explore a fully accessible model that is already trained on high-quality (but
less diverse) instructions such as [Flan-T5](https://arxiv.org/abs/2210.11416).
------------------------------
Document 3:

[Code](https://github.com/declare-lab/red-instruct) and [Paper](https://arxiv.org/abs/2308.09662).  
Codes and datasets: [https://github.com/declare-lab/instruct-eval](https://github.com/declare-lab/instruct-eval)  
Find our work at [https://github.com/declare-lab/tango](https://github.com/declare-lab/tango)  
Our [repository](https://github.com/declare-lab/flan-alpaca) contains code for extending the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
synthetic instruction tuning to existing instruction-tuned models such as [Flan-T5](https://arxiv.org/abs/2210.11416).  
We are also benchmarking many instruction-tuned models at [declare-lab/flan-eval](https://github.com/declare-lab/flan-eval).

-------------------- paper --------------------

Document 1:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) represents an exciting new direction to approximate the performance of large language models (LLMs) like ChatGPT cheaply and easily. Concretely, they leverage an LLM such as GPT-3 to generate instructions as synthetic training data. The synthetic data which covers more than 50k tasks can then be used to finetune a smaller model. However, the original implementation is less accessible due to licensing constraints of the underlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model. Furthermore, users have noted [potential noise](https://github.com/tloen/alpaca-lora/issues/65) in the synthetic dataset. Hence, it may be better to explore a fully accessible model that is already trained on high-quality (but less diverse) instructions such as [Flan-T5](https://arxiv.org/abs/2210.11416).
------------------------------
Document 2:

[Paper](https://arxiv.org/abs/2308.09662)

-------------------- upstream_model --------------------

Document 1:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) represents an exciting new direction to approximate the performance of large language models (LLMs) like ChatGPT cheaply and easily. They leverage an LLM such as GPT-3 to generate instructions as synthetic training data. The synthetic data which covers more than 50k tasks can then be used to finetune a smaller model. However, the original implementation is less accessible due to licensing constraints of the underlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model. Furthermore, users have noted [potential noise](https://github.com/tloen/alpaca-lora/issues/65) in the synthetic dataset.
------------------------------
Document 2:

model='declare-lab/flan-alpaca-gpt4-xl'

-------------------- parameter_count --------------------

Document 1:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

max_length=128, do_sample=True

-------------------- evaluation --------------------

Document 1:

Our pretrained models are fully available on HuggingFace 🤗 :  
| Model                                                                            | Parameters | Instruction Data                                                                                                                                   | Training GPUs   |
|----------------------------------------------------------------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| [Flan-Alpaca-Base](https://huggingface.co/declare-lab/flan-alpaca-base)          | 220M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-Large](https://huggingface.co/declare-lab/flan-alpaca-large)        | 770M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-XL](https://huggingface.co/declare-lab/flan-alpaca-xl)              | 3B         | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-XXL](https://huggingface.co/declare-lab/flan-alpaca-xxl)            | 11B        | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 4x A6000 (FSDP) |
| [Flan-GPT4All-XL](https://huggingface.co/declare-lab/flan-gpt4all-xl)            | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4All](https://github.com/nomic-ai/gpt4all)                                                    | 1x A6000        |
| [Flan-ShareGPT-XL](https://huggingface.co/declare-lab/flan-sharegpt-xl)          | 3B         | [Flan](https://github.com/google-research/FLAN), [ShareGPT](https://github.com/domeccleston/sharegpt)/[Vicuna](https://github.com/lm-sys/FastChat) | 1x A6000        |
| [Flan-Alpaca-GPT4-XL*](https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl)   | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4-Alpaca](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                         | 1x A6000        |
------------------------------
Document 2:

Dear AlpacaFriend,
My name is Alpaca and I'm 10 years old.
I'm excited to announce that I'm a big fan of flan!
We like to eat it as a snack and I believe that it can help with our overall growth.
I'd love to hear your feedback on this idea.
Have a great day!
Best, AL Paca

-------------------- hardware --------------------

Document 1:

| Model                                                                            | Parameters | Instruction Data                                                                                                                                   | Training GPUs   |
|----------------------------------------------------------------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| [Flan-Alpaca-Base](https://huggingface.co/declare-lab/flan-alpaca-base)          | 220M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-Large](https://huggingface.co/declare-lab/flan-alpaca-large)        | 770M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-XL](https://huggingface.co/declare-lab/flan-alpaca-xl)              | 3B         | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-XXL](https://huggingface.co/declare-lab/flan-alpaca-xxl)            | 11B        | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 4x A6000 (FSDP) |
| [Flan-GPT4All-XL](https://huggingface.co/declare-lab/flan-gpt4all-xl)            | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4All](https://github.com/nomic-ai/gpt4all)                                                    | 1x A6000        |
| [Flan-ShareGPT-XL](https://huggingface.co/declare-lab/flan-sharegpt-xl)          | 3B         | [Flan](https://github.com/google-research/FLAN), [ShareGPT](https://github.com/domeccleston/sharegpt)/[Vicuna](https://github.com/lm-sys/FastChat) | 1x A6000        |
| [Flan-Alpaca-GPT4-XL*](https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl)   | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4-Alpaca](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                         | 1x A6000        |

-------------------- limitation_and_bias --------------------

Document 1:

The limitations and biases of the model include potential noise in the synthetic dataset.
------------------------------
Document 2:

limitation_and_bias: None

-------------------- demo --------------------

Document 1:

model = pipeline(model='declare-lab/flan-alpaca-gpt4-xl')
model(prompt, max_length=128, do_sample=True)
------------------------------
Document 2:

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) represents an exciting new direction to approximate the performance of large language models (LLMs) like ChatGPT cheaply and easily. Concretely, they leverage an LLM such as GPT-3 to generate instructions as synthetic training data. The synthetic data which covers more than 50k tasks can then be used to finetune a smaller model. However, the original implementation is less accessible due to licensing constraints of the underlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model. Furthermore, users have noted [potential noise](https://github.com/tloen/alpaca-lora/issues/65) in the synthetic dataset. Hence, it may be better to explore a fully accessible model that is already trained on high-quality (but less diverse) instructions such as [Flan-T5](https://arxiv.org/abs/2210.11416).

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

input_format
------------------------------
Document 3:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length
------------------------------
Document 3:

| Model                                                                            | Parameters | Instruction Data                                                                                                                                   | Training GPUs   |
|----------------------------------------------------------------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| [Flan-Alpaca-Base](https://huggingface.co/declare-lab/flan-alpaca-base)          | 220M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-Large](https://huggingface.co/declare-lab/flan-alpaca-large)        | 770M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-XL](https://huggingface.co/declare-lab/flan-alpaca-xl)              | 3B         | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-XXL](https://huggingface.co/declare-lab/flan-alpaca-xxl)            | 11B        | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 4x A6000 (FSDP) |
| [Flan-GPT4All-XL](https://huggingface.co/declare-lab/flan-gpt4all-xl)            | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4All](https://github.com/nomic-ai/gpt4all)                                                    | 1x A6000        |
| [Flan-ShareGPT-XL](https://huggingface.co/declare-lab/flan-sharegpt-xl)          | 3B         | [Flan](https://github.com/google-research/FLAN), [ShareGPT](https://github.com/domeccleston/sharegpt)/[Vicuna](https://github.com/lm-sys/FastChat) | 1x A6000        |
| [Flan-Alpaca-GPT4-XL*](https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl)   | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4-Alpaca](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                         | 1x A6000        |  
*recommended for better performance

-------------------- vocabulary_size --------------------

Document 1:

vocabulary_size
------------------------------
Document 2:

| Model                                                                            | Parameters | Instruction Data                                                                                                                                   | Training GPUs   |
|----------------------------------------------------------------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| [Flan-Alpaca-Base](https://huggingface.co/declare-lab/flan-alpaca-base)          | 220M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-Large](https://huggingface.co/declare-lab/flan-alpaca-large)        | 770M       | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-XL](https://huggingface.co/declare-lab/flan-alpaca-xl)              | 3B         | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 1x A6000        |
| [Flan-Alpaca-XXL](https://huggingface.co/declare-lab/flan-alpaca-xxl)            | 11B        | [Flan](https://github.com/google-research/FLAN), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | 4x A6000 (FSDP) |
| [Flan-GPT4All-XL](https://huggingface.co/declare-lab/flan-gpt4all-xl)            | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4All](https://github.com/nomic-ai/gpt4all)                                                    | 1x A6000        |
| [Flan-ShareGPT-XL](https://huggingface.co/declare-lab/flan-sharegpt-xl)          | 3B         | [Flan](https://github.com/google-research/FLAN), [ShareGPT](https://github.com/domeccleston/sharegpt)/[Vicuna](https://github.com/lm-sys/FastChat) | 1x A6000        |
| [Flan-Alpaca-GPT4-XL*](https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl)   | 3B         | [Flan](https://github.com/google-research/FLAN), [GPT4-Alpaca](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                         | 1x A6000        |


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens. However, your messages resulted in 5257 tokens (5100 in the messages, 157 in the functions). Please reduce the length of the messages or functions. 

#####################google/t5-v1_1-large########################

-------------------- datasets --------------------

Document 1:

- c4
- [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) Version 1.1
------------------------------
Document 2:

Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.
------------------------------
Document 3:

Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.

-------------------- github --------------------

Document 1:

[Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
------------------------------
Document 2:

We release our dataset, pre-trained models, and code.
------------------------------
Document 3:

[T5 Version 1.1](https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511) includes the following improvements compared to the original T5 model- GEGLU activation in feed-forward hidden layer, rather than ReLU - see [here](https://arxiv.org/abs/2002.05202).  
Other Community Checkpoints: [here](https://huggingface.co/models?search=t5-v1_1)

-------------------- paper --------------------

Document 1:

In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.
------------------------------
Document 2:

Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)  
Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*
------------------------------
Document 3:

[Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) Version 1.1

-------------------- upstream_model --------------------

Document 1:

upstream_model
------------------------------
Document 2:

T5 Version 1.1 includes the following improvements compared to the original T5 model- GEGLU activation in feed-forward hidden layer, rather than ReLU - see [here](https://arxiv.org/abs/2002.05202).  
- Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.  
- Pre-trained on C4 only without mixing in the downstream tasks.  
- no parameter sharing between embedding and classifier layer  
- 'xl' and 'xxl' replace '3B' and '11B'. The model shapes are a bit different - larger `d_model` and smaller `num_heads` and `d_ff`.  
**Note**: T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.
------------------------------
Document 3:

[Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) Version 1.1

-------------------- parameter_count --------------------

Document 1:

T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

T5 Version 1.1 includes the following improvements compared to the original T5 model- GEGLU activation in feed-forward hidden layer, rather than ReLU - see [here](https://arxiv.org/abs/2002.05202).  
Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.  
Pre-trained on C4 only without mixing in the downstream tasks.  
'xl' and 'xxl' replace '3B' and '11B'. The model shapes are a bit different - larger `d_model` and smaller `num_heads` and `d_ff`.  
Note: T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.
------------------------------
Document 2:

T5 Version 1.1 includes the following improvements compared to the original T5 model- GEGLU activation in feed-forward hidden layer, rather than ReLU - see [here](https://arxiv.org/abs/2002.05202).  
- Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.  
- Pre-trained on C4 only without mixing in the downstream tasks.  
- no parameter sharing between embedding and classifier layer  
- 'xl' and 'xxl' replace '3B' and '11B'. The model shapes are a bit different - larger `d_model` and smaller `num_heads` and `d_ff`.  
**Note**: T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.
------------------------------
Document 3:

[Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) Version 1.1

-------------------- demo --------------------

Document 1:

We release our dataset, pre-trained models, and code.
------------------------------
Document 2:

[Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) Version 1.1
------------------------------
Document 3:

Pre-training Dataset: [C4](https://huggingface.co/datasets/c4)  
T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.

-------------------- input_format --------------------

Document 1:

Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)
------------------------------
Document 2:

input_format

-------------------- output_format --------------------

Document 1:

Pre-training Dataset: [C4](https://huggingface.co/datasets/c4)
------------------------------
Document 2:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['c4', "Google's T5 Version 1.1"], 'license': 'apache-2.0', 'github': 'https://ai.goo 
gleblog.com/2020/02/exploring-transfer-learning-with-t5.html', 'paper': 'https://arxiv.org/pdf/1910. 
10683.pdf', 'upstream_model': '', 'parameter_count': 'T5 Version 1.1 was only pre-trained on C4 excl 
uding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a  
downstream task.', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias':  
'Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned o 
n a downstream task, has emerged as a powerful technique in natural language processing (NLP). The e 
ffectiveness of transfer learning has given rise to a diversity of approaches, methodology, and prac 
tice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing 
 a unified framework that converts every language problem into a text-to-text format. Our systematic 
 study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and 
 other factors on dozens of language understanding tasks. By combining the insights from our explora 
tion with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on  
many benchmarks covering summarization, question answering, text classification, and more.', 'demo': 
 'We release our dataset, pre-trained models, and code.', 'input_format': 'Pretraining Dataset: C4', 
 'output_format': 'Pre-training Dataset: C4', 'max_sequence_length': 'max_sequence_length', 'vocabul 
ary_size': ''}]                                                                                      

#####################prithivida/parrot_paraphraser_on_T5########################

-------------------- datasets --------------------

Document 1:

Huggingface lists [12 paraphrase models,](https://huggingface.co/models?pipeline_tag=text2text-generation&search=paraphrase) RapidAPI lists 7 fremium and commercial paraphrasers like [QuillBot](https://rapidapi.com/search/paraphrase?section=apis&page=1), Rasa has discussed an experimental paraphraser for augmenting text data [here](https://forum.rasa.com/t/paraphrasing-for-nlu-data-augmentation-experimental/27744), Sentence-transfomers offers a [paraphrase mining utility](https://www.sbert.net/examples/applications/paraphrase-mining/README.html) and [NLPAug](https://github.com/makcedward/nlpaug) offers word level augmentation with a [PPDB](http://paraphrase.org/#/download) (a multi-million paraphrase database).

-------------------- license --------------------



-------------------- github --------------------

Document 1:

[github page](https://github.com/PrithivirajDamodaran/Parrot)

-------------------- paper --------------------

Document 1:

Rasa has discussed an experimental paraphraser for augmenting text data [here](https://forum.rasa.com/t/paraphrasing-for-nlu-data-augmentation-experimental/27744).
------------------------------
Document 2:

Parrot mainly foucses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models. (*So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32.*)  
*While Parrot predominantly aims to be a text augmentor for building good NLU models, it can also be used as a pure-play paraphraser.*

-------------------- upstream_model --------------------

Document 1:

For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.
------------------------------
Document 2:

Parrot mainly foucses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models. (*So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32.*)

-------------------- parameter_count --------------------

Document 1:

Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.
For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.

-------------------- hyper_parameters --------------------

Document 1:

list some excellent restaurants to visit in new york city?
what upscale restaurants do you recommend in new york?
i want to try some upscale restaurants in new york?
recommend some upscale restaurants in newyork?
can you recommend some high end restaurants in newyork?
can you recommend some upscale restaurants in new york?
can you recommend some upscale restaurants in newyork?
what should we not miss when visiting russia?
recommend some of the best places to visit in russia?
list some of the best places to visit in russia?
can you list the top places to visit in russia?
show the places that we should not miss in russia?
list some famous places which we should not miss in russia?
------------------------------
Document 2:

Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.
But in general being a generative model paraphrasers doesn't guarantee to preserve the slots/entities. So the ability to generate high quality paraphrases in a constrained fashion without trading off the intents and slots for lexical dissimilarity makes a paraphraser a good augmentor.
------------------------------
Document 3:

max_return_phrases = 10,
max_length=32,
adequacy_threshold = 0.99,
fluency_threshold = 0.90

-------------------- evaluation --------------------

Document 1:

For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.

-------------------- hardware --------------------

Document 1:

For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.

-------------------- limitation_and_bias --------------------

Document 1:

Almost all conditioned text generation models are validated  on 2 factors, (1) if the generated text conveys the same meaning as the original context (Adequacy) (2) if the text is fluent / grammatically correct english (Fluency). For instance Neural Machine Translation outputs are tested for Adequacy and Fluency. But [a good paraphrase](https://www.aclweb.org/anthology/D10-1090.pdf) should be adequate and fluent while being as different as possible on the surface lexical form. With respect to this definition, the  **3 key metrics** that measures the quality of paraphrases are:
- **Adequacy** (Is the meaning preserved adequately?)
- **Fluency** (Is the paraphrase fluent English?)
- **Diversity (Lexical / Phrasal / Syntactical)** (How much has the paraphrase changed the original sentence?)  
*Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.*  
What makes a paraphraser a good augmentor? For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.  
But in general being a generative model paraphrasers doesn't guarantee to preserve the slots/entities. So the ability to generate high quality paraphrases in a constrained fashion without trading off the intents and slots for lexical dissimilarity makes a paraphraser a good augmentor.
------------------------------
Document 2:

Parrot mainly foucses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models. So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32. While Parrot predominantly aims to be a text augmentor for building good NLU models, it can also be used as a pure-play paraphraser.

-------------------- demo --------------------

Document 1:

A form of demo for the model could be a link, code snippet or short paragraph (Do not remove keyword "demo" in compressed doc)
------------------------------
Document 2:

Parrot mainly foucses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models. So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32.
------------------------------
Document 3:

Huggingface lists [12 paraphrase models,](https://huggingface.co/models?pipeline_tag=text2text-generation&search=paraphrase) RapidAPI lists 7 fremium and commercial paraphrasers like [QuillBot](https://rapidapi.com/search/paraphrase?section=apis&page=1), Rasa has discussed an experimental paraphraser for augmenting text data [here](https://forum.rasa.com/t/paraphrasing-for-nlu-data-augmentation-experimental/27744), Sentence-transfomers offers a [paraphrase mining utility](https://www.sbert.net/examples/applications/paraphrase-mining/README.html) and [NLPAug](https://github.com/makcedward/nlpaug) offers word level augmentation with a [PPDB](http://paraphrase.org/#/download) (a multi-million paraphrase database). While these attempts at paraphrasing are great, there are still some gaps and paraphrasing is NOT yet a mainstream option for text augmentation in building NLU models....Parrot is a humble attempt to fill some of these gaps.  
What is a good paraphrase? Almost all conditioned text generation models are validated  on 2 factors, (1) if the generated text conveys the same meaning as the original context (Adequacy) (2) if the text is fluent / grammatically correct english (Fluency). For instance Neural Machine Translation outputs are tested for Adequacy and Fluency. But [a good paraphrase](https://www.aclweb.org/anthology/D10-1090.pdf) should be adequate and fluent while being as different as possible on the surface lexical form. With respect to this definition, the  **3 key metrics** that measures the quality of paraphrases are:
- **Adequacy** (Is the meaning preserved adequately?)
- **Fluency** (Is the paraphrase fluent English?)
- **Diversity (Lexical / Phrasal / Syntactical)** (How much has the paraphrase changed the original sentence?)  
*Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.*  
What makes a paraphraser a good augmentor? For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.  
But in general being a generative model paraphrasers doesn't guarantee to preserve the slots/entities. So the ability to generate high quality paraphrases in a constrained fashion without trading off the intents and slots for lexical dissimilarity makes a paraphraser a good augmentor. *More on this in section 3 below*

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.
------------------------------
Document 3:

Parrot mainly foucses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models. So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32.

-------------------- max_sequence_length --------------------

Document 1:

Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.
For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.
------------------------------
Document 2:

the pre-trained model is trained  on text samples of maximum length of 32.
------------------------------
Document 3:

max_length=32

-------------------- vocabulary_size --------------------

Document 1:

Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.
What makes a paraphraser a good augmentor? For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model.
------------------------------
Document 2:

vocabulary_size


[{'datasets': ['Parrot'], 'license': '', 'github': 'https://github.com/PrithivirajDamodaran/Parrot' 
, 'paper': 'https://forum.rasa.com/t/paraphrasing-for-nlu-data-augmentation-experimental/27744', 'up 
stream_model': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'lear 
ning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo 
': '', 'input_format': '', 'output_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]   

#####################Salesforce/codet5-base########################

-------------------- datasets --------------------

Document 1:

Supervised datasets for code can be found [here](https://huggingface.co/datasets?languages=languages:code).
------------------------------
Document 2:

The CodeT5 model was pretrained on CodeSearchNet [Husain et al., 2019](https://arxiv.org/abs/1909.09436). Additionally, the authors collected two datasets of C/CSharp from [BigQuery1](https://console.cloud.google.com/marketplace/details/github/github-repos) to ensure that all downstream tasks have overlapped programming languages with the pre-training data.
------------------------------
Document 3:

datasets:
- code_search_net

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

CodeT5

-------------------- github --------------------

Document 1:

See the [model hub](https://huggingface.co/models?search=salesforce/codet) to look for fine-tuned versions on a task that interests you.

-------------------- paper --------------------

Document 1:

the paper
------------------------------
Document 2:

@misc{wang2021codet5,
title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
author={Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
year={2021},
eprint={2109.00859},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
------------------------------
Document 3:

CodeT5

-------------------- upstream_model --------------------

Document 1:

This repository contains the pre-trained model only, so you can use this model for (among other tasks) masked span prediction, as shown in the code example below. However, the main use of this model is to fine-tune it for a downstream task of interest, such as:
* code summarization
* code generation
* code translation
* code refinement
* code defect detection
* code clone detection.
------------------------------
Document 2:

CodeT5
------------------------------
Document 3:

This model uses a code-specific BPE (Byte-Pair Encoding) tokenizer trained using the [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers) library.

-------------------- parameter_count --------------------

Document 1:

parameter_count

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

For evaluation results on several downstream benchmarks, we refer to the paper.
------------------------------
Document 2:

CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL.

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code.
------------------------------
Document 2:

This model uses a code-specific BPE (Byte-Pair Encoding) tokenizer trained using the [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers) library.

-------------------- demo --------------------

Document 1:

Supervised datasets for code can be found [here](https://huggingface.co/datasets?languages=languages:code).
See the [model hub](https://huggingface.co/models?search=salesforce/codet) to look for fine-tuned versions on a task that interests you.

-------------------- input_format --------------------

Document 1:

This model uses a code-specific BPE (Byte-Pair Encoding) tokenizer trained using the [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers) library.
------------------------------
Document 2:

Supervised datasets for code can be found [here](https://huggingface.co/datasets?languages=languages:code).
------------------------------
Document 3:

input_format

-------------------- output_format --------------------

Document 1:

This model uses a code-specific BPE (Byte-Pair Encoding) tokenizer trained using the [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers) library.
------------------------------
Document 2:

This repository contains the pre-trained model only, so you can use this model for (among other tasks) masked span prediction, as shown in the code example below. However, the main use of this model is to fine-tune it for a downstream task of interest, such as:
* code summarization
* code generation
* code translation
* code refinement
* code defect detection
* code clone detection.
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

This model uses a code-specific BPE (Byte-Pair Encoding) tokenizer trained using the [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers) library.


[{'datasets': ['code_search_net'], 'license': 'apache-2.0', 'github': 'https://huggingface.co/model 
s?search=salesforce/codet', 'paper': '@misc{wang2021codet5,\ntitle={CodeT5: Identifier-aware Unified 
 Pre-trained Encoder-Decoder Models for Code Understanding and Generation},\nauthor={Yue Wang and We 
ishi Wang and Shafiq Joty and Steven C. H. Hoi},\nyear={2021},\neprint={2109.00859},\narchivePrefix= 
{arXiv},\nprimaryClass={cs.CL}\n}', 'upstream_model': 'CodeT5', 'parameter_count': 'parameter_count' 
, 'hyper_parameters': {}, 'evaluation': [{'test': '', 'result': ''}], 'hardware': '', 'limitation_an 
d_bias': 'CodeT5 significantly outperforms prior methods on understanding tasks such as code defect  
detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL 
, and PL-PL. Further analysis reveals that our model can better capture semantic information from co 
de.', 'demo': 'Supervised datasets for code can be found [here](https://huggingface.co/datasets?lang 
uages=languages:code).\nSee the [model hub](https://huggingface.co/models?search=salesforce/codet) t 
o look for fine-tuned versions on a task that interests you.', 'input_format': 'This model uses a co 
de-specific BPE (Byte-Pair Encoding) tokenizer trained using the [HuggingFace Tokenizers](https://gi 
thub.com/huggingface/tokenizers) library.', 'output_format': 'This model uses a code-specific BPE (B 
yte-Pair Encoding) tokenizer trained using the [HuggingFace Tokenizers](https://github.com/huggingfa 
ce/tokenizers) library.', 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': 'This mod 
el uses a code-specific BPE (Byte-Pair Encoding) tokenizer trained using the [HuggingFace Tokenizers 
](https://github.com/huggingface/tokenizers) library.'}]                                             

#####################valhalla/t5-base-e2e-qg########################

-------------------- datasets --------------------

Document 1:

datasets:
- squad
------------------------------
Document 2:

You'll need to clone the [repo](https://github.com/patil-suraj/question_generation).  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb)

-------------------- license --------------------

Document 1:

license: mit

-------------------- github --------------------

Document 1:

You'll need to clone the [repo](https://github.com/patil-suraj/question_generation).  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb)
------------------------------
Document 2:

[This](https://github.com/patil-suraj/question_generation) repo.

-------------------- paper --------------------

Document 1:

https://arxiv.org/abs/1910.10683
------------------------------
Document 2:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')

-------------------- upstream_model --------------------

Document 1:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')

-------------------- parameter_count --------------------

Document 1:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')

-------------------- hyper_parameters --------------------

Document 1:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')

-------------------- evaluation --------------------

Document 1:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')
nlp(text)
=> [
'Who created Python?',
'When was Python first released?',
'What is Python's design philosophy?'
]

-------------------- hardware --------------------

Document 1:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')

-------------------- limitation_and_bias --------------------

Document 1:

#tags  
---
license: mit
tags:
- question-generation
datasets:
- squad
widget:
- text: Python is a programming language. It is developed by Guido Van Rossum and
released in 1991. </s>
------------------------------
Document 2:

You'll need to clone the [repo](https://github.com/patil-suraj/question_generation).  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb)  
```python3
from pipelines import pipeline

text = 'Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum \
and first released in 1991, Python's design philosophy emphasizes code \
readability with its notable use of significant whitespace.'

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')
nlp(text)
=> [
'Who created Python?',
'When was Python first released?',
'What is Python's design philosophy?'
]
```

-------------------- demo --------------------

Document 1:

You can play with the model using the inference API, just put the text and see the results!
------------------------------
Document 2:

You'll need to clone the [repo](https://github.com/patil-suraj/question_generation).  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb)  
```python3
from pipelines import pipeline

text = 'Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum \
and first released in 1991, Python's design philosophy emphasizes code \
readability with its notable use of significant whitespace.'

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')
nlp(text)
=> [
'Who created Python?',
'When was Python first released?',
'What is Python's design philosophy?'
]
```

-------------------- input_format --------------------

Document 1:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')
------------------------------
Document 2:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

nlp(text)

-------------------- max_sequence_length --------------------

Document 1:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')


[{'datasets': ['squad'], 'license': 'mit', 'github': 'https://github.com/patil-suraj/question_gener 
ation', 'paper': 'https://arxiv.org/abs/1910.10683', 'upstream_model': 'valhalla/t5-base-e2e-qg', 'p 
arameter_count': "nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')", 'hyper_parameters': {' 
epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'nlp(t 
ext)', 'result': "['Who created Python?', 'When was Python first released?', 'What is Python's desig 
n philosophy?']"}], 'hardware': "nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')", 'limita 
tion_and_bias': '#tags\n---\nlicense: mit\ntags:\n- question-generation\ndatasets:\n- squad\nwidget: 
\n- text: Python is a programming language. It is developed by Guido Van Rossum and\nreleased in 199 
1. </s>', 'demo': 'You can play with the model using the inference API, just put the text and see th 
e results!', 'input_format': "nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')", 'output_fo 
rmat': 'output_format', 'max_sequence_length': "nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e 
-qg')", 'vocabulary_size': "nlp = pipeline('e2e-qg', model='valhalla/t5-base-e2e-qg')"}]             

#####################Babelscape/rebel-large########################

-------------------- datasets --------------------

Document 1:

- Babelscape/rebel-dataset
------------------------------
Document 2:

The original repository for the paper can be found [here](https://github.com/Babelscape/rebel)  
For a demo of REBEL and its pre-training dataset check the [Spaces demo](https://huggingface.co/spaces/Babelscape/rebel-demo).

-------------------- license --------------------

Document 1:

license: cc-by-nc-sa-4.0
------------------------------
Document 2:

The original repository for the paper can be found [here](https://github.com/Babelscape/rebel)
------------------------------
Document 3:

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor('Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic', return_tensors=True, return_text=False)[0]['generated_token_ids']])
print(extracted_text[0])
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)

-------------------- github --------------------

Document 1:

This is the model card for the Findings of EMNLP 2021 paper [REBEL: Relation Extraction By End-to-end Language generation](https://github.com/Babelscape/rebel/blob/main/docs/EMNLP_2021_REBEL__Camera_Ready_.pdf). The paper can be found [here](https://github.com/Babelscape/rebel/blob/main/docs/EMNLP_2021_REBEL__Camera_Ready_.pdf). The original repository for the paper can be found [here](https://github.com/Babelscape/rebel). For a demo of REBEL and its pre-training dataset check the [Spaces demo](https://huggingface.co/spaces/Babelscape/rebel-demo).
------------------------------
Document 2:

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-nyt)](https://paperswithcode.com/sota/relation-extraction-on-nyt?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-conll04)](https://paperswithcode.com/sota/relation-extraction-on-conll04?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/joint-entity-and-relation-extraction-on-3)](https://paperswithcode.com/sota/joint-entity-and-relation-extraction-on-3?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-ade-corpus)](https://paperswithcode.com/sota/relation-extraction-on-ade-corpus?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-re-tacred)](https://paperswithcode.com/sota/relation-extraction-on-re-tacred?p=rebel-relation-extraction-by-end-to-end)
------------------------------
Document 3:

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor('Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic', return_tensors=True, return_text=False)[0]['generated_token_ids']])
extracted_triplets = extract_triplets(extracted_text[0])

-------------------- paper --------------------

Document 1:

This is the model card for the Findings of EMNLP 2021 paper [REBEL: Relation Extraction By End-to-end Language generation](https://github.com/Babelscape/rebel/blob/main/docs/EMNLP_2021_REBEL__Camera_Ready_.pdf). We present a new linearization approach and a reframing of Relation Extraction as a seq2seq task. The paper can be found [here](https://github.com/Babelscape/rebel/blob/main/docs/EMNLP_2021_REBEL__Camera_Ready_.pdf).
------------------------------
Document 2:

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-nyt)](https://paperswithcode.com/sota/relation-extraction-on-nyt?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-conll04)](https://paperswithcode.com/sota/relation-extraction-on-conll04?p=rebel-relation-extraction-by-end-to-end)

-------------------- upstream_model --------------------

Document 1:

The upstream model of this model is BART.
------------------------------
Document 2:

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor('Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic', return_tensors=True, return_text=False)[0]['generated_token_ids']])
print(extracted_text[0])
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)

-------------------- parameter_count --------------------

Document 1:

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor('Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic', return_tensors=True, return_text=False)[0]['generated_token_ids']])
print(extracted_text[0])

-------------------- hyper_parameters --------------------

Document 1:

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor('Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic', return_tensors=True, return_text=False)[0]['generated_token_ids']])
print(extracted_text[0])

-------------------- evaluation --------------------

Document 1:

results:
- task:
type: Relation-Extraction
name: Relation Extraction
dataset:
name: CoNLL04
type: CoNLL04
metrics:
- type: re+ macro f1
value: 76.65
name: RE+ Macro F1
- task:
type: Relation-Extraction
name: Relation Extraction
dataset:
name: NYT
type: NYT
metrics:
- type: f1
value: 93.4
name: F1

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

Extracted relevant parts:
- Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are limited to a small number of relation types. To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language generation, but also in NLU tasks such as Entity Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction can be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model based on BART that performs end-to-end relation extraction for more than 200 different relation types. We show our model{'}s flexibility by fine-tuning it on an array of Relation Extraction and Relation Classification benchmarks, with it attaining state-of-the-art performance in most of them.
------------------------------
Document 2:

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor('Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic', return_tensors=True, return_text=False)[0]['generated_token_ids']])
print(extracted_text[0])
def extract_triplets(text):
triplets = []
relation, subject, relation, object_ = '', '', '', ''
text = text.strip()
current = 'x'
for token in text.replace('<s>', '').replace('<pad>', '').replace('</s>', '').split():
if token == '<triplet>':
current = 't'
if relation != '':
triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
relation = ''
subject = ''
elif token == '<subj>':
current = 's'
if relation != '':
triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
object_ = ''
elif token == '<obj>':
current = 'o'
relation = ''
else:
if current == 't':
subject += ' ' + token
elif current == 's':
object_ += ' ' + token
elif current == 'o':
relation += ' ' + token
if subject != '' and relation != '' and object_ != '':
triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
return triplets
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)

-------------------- demo --------------------

Document 1:

For a demo of REBEL and its pre-training dataset check the [Spaces demo](https://huggingface.co/spaces/Babelscape/rebel-demo).
------------------------------
Document 2:

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-nyt)](https://paperswithcode.com/sota/relation-extraction-on-nyt?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-conll04)](https://paperswithcode.com/sota/relation-extraction-on-conll04?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/joint-entity-and-relation-extraction-on-3)](https://paperswithcode.com/sota/joint-entity-and-relation-extraction-on-3?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-ade-corpus)](https://paperswithcode.com/sota/relation-extraction-on-ade-corpus?p=rebel-relation-extraction-by-end-to-end)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rebel-relation-extraction-by-end-to-end/relation-extraction-on-re-tacred)](https://paperswithcode.com/sota/relation-extraction-on-re-tacred?p=rebel-relation-extraction-by-end-to-end)
------------------------------
Document 3:

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor('Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic', return_tensors=True, return_text=False)[0]['generated_token_ids']])
print(extracted_text[0])
def extract_triplets(text):
triplets = []
relation, subject, relation, object_ = '', '', '', ''
text = text.strip()
current = 'x'
for token in text.replace('<s>', '').replace('<pad>', '').replace('</s>', '').split():
if token == '<triplet>':
current = 't'
if relation != '':
triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
relation = ''
subject = ''
elif token == '<subj>':
current = 's'
if relation != '':
triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
object_ = ''
elif token == '<obj>':
current = 'o'
relation = ''
else:
if current == 't':
subject += ' ' + token
elif current == 's':
object_ += ' ' + token
elif current == 'o':
relation += ' ' + token
if subject != '' and relation != '' and object_ != '':
triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
return triplets
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

The original repository for the paper can be found [here](https://github.com/Babelscape/rebel)  
Be aware that the inference widget at the right does not output special tokens, which are necessary to distinguish the subject, object and relation types. For a demo of REBEL and its pre-training dataset check the [Spaces demo](https://huggingface.co/spaces/Babelscape/rebel-demo).
------------------------------
Document 3:

input_format

-------------------- output_format --------------------

Document 1:

The inference widget at the right does not output special tokens, which are necessary to distinguish the subject, object and relation types.
------------------------------
Document 2:

output_format
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

The original repository for the paper can be found [here](https://github.com/Babelscape/rebel)  
Be aware that the inference widget at the right does not output special tokens, which are necessary to distinguish the subject, object and relation types. For a demo of REBEL and its pre-training dataset check the [Spaces demo](https://huggingface.co/spaces/Babelscape/rebel-demo).
------------------------------
Document 2:

max_sequence_length
------------------------------
Document 3:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor('Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic', return_tensors=True, return_text=False)[0]['generated_token_ids']])
print(extracted_text[0])


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Unterminated string starting at: line 29 column 30 (char 763) 

#####################google/byt5-large########################

-------------------- datasets --------------------

Document 1:

As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.
------------------------------
Document 2:

ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters.
------------------------------
Document 3:

datasets:
- mc4

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

We release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.
------------------------------
Document 3:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).  
Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*

-------------------- github --------------------

Document 1:

As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.
------------------------------
Document 2:

[Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html), [MT5](https://huggingface.co/google/mt5-large), [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual), [google/byt5-large](https://huggingface.co/google/mt5-large), [mt5-large](https://huggingface.co/google/mt5-large), [TweetQA](https://arxiv.org/abs/1907.06292), [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)

-------------------- paper --------------------

Document 1:

In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.
------------------------------
Document 2:

Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*

-------------------- upstream_model --------------------

Document 1:

We release a new set of pre-trained byte-level Transformer models based on the T5 architecture.
------------------------------
Document 2:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).
------------------------------
Document 3:

from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

-------------------- parameter_count --------------------

Document 1:

We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts.
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')

-------------------- evaluation --------------------

Document 1:

We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation.
------------------------------
Document 2:

ByT5 works especially well on noisy text data, *e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).
------------------------------
Document 3:

For batched inference & training it is however recommended using a tokenizer class for padding:  
```python
from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

model_inputs = tokenizer(['Life is like a box of chocolates.', 'Today is Monday.'], padding='longest', return_tensors='pt')
labels = tokenizer(['La vie est comme une boîte de chocolat.', 'Aujourd'hui c'est lundi.'], padding='longest', return_tensors='pt').input_ids

loss = model(**model_inputs, labels=labels).loss # forward pass
```

-------------------- hardware --------------------

Document 1:

From the given context, the relevant part to answer the question is:

"For batched inference & training it is however recommended using a tokenizer class for padding:"

-------------------- limitation_and_bias --------------------

Document 1:

Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation.
------------------------------
Document 2:

ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.
------------------------------
Document 3:

ByT5 works on raw UTF-8 bytes and can be used without a tokenizer:  
For batched inference & training it is however recommended using a tokenizer class for padding:

-------------------- demo --------------------

Document 1:

As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.
------------------------------
Document 2:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).

-------------------- input_format --------------------

Document 1:

Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts.
------------------------------
Document 2:

ByT5 works on raw UTF-8 bytes and can be used without a tokenizer:  
For batched inference & training it is however recommended using a tokenizer class for padding:

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

For batched inference & training it is however recommended using a tokenizer class for padding:  
```python
from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

model_inputs = tokenizer(['Life is like a box of chocolates.', 'Today is Monday.'], padding='longest', return_tensors='pt')
labels = tokenizer(['La vie est comme une boîte de chocolat.', 'Aujourd'hui c'est lundi.'], padding='longest', return_tensors='pt').input_ids

loss = model(**model_inputs, labels=labels).loss # forward pass
```
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

vocabulary_size


[{'datasets': ['mc4'], 'license': 'apache-2.0', 'github': '', 'paper': '', 'upstream_model': '', 'p 
arameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'opti 
mizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format' 
: '', 'output_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]                        

#####################d4data/biomedical-ner-all########################

-------------------- datasets --------------------

Document 1:

https://github.com/dreji18/Bio-Epidemiology-NER
------------------------------
Document 2:

- Dataset: Maccrobat https://figshare.com/articles/dataset/MACCROBAT2018/9764942
------------------------------
Document 3:

d4data/biomedical-ner-all

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

https://github.com/dreji18/Bio-Epidemiology-NER
------------------------------
Document 2:

- Dataset: Maccrobat https://figshare.com/articles/dataset/MACCROBAT2018/9764942
- Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18

-------------------- paper --------------------

Document 1:

The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.
```python
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')
model = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')

pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple') # pass device=0 if using gpu
pipe('''The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.''')
```

-------------------- upstream_model --------------------

Document 1:

from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')
model = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')
------------------------------
Document 2:

distilbert-base-uncased

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------

Document 1:

GPU used : 1 x GeForce RTX 3060 Laptop GPU

-------------------- limitation_and_bias --------------------

Document 1:

limitation_and_bias: An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased

-------------------- demo --------------------

Document 1:

https://github.com/dreji18/Bio-Epidemiology-NER
------------------------------
Document 2:

```python
pipe('''The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.''')
```

-------------------- input_format --------------------

Document 1:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

vocabulary_size


[{'datasets': ['https://github.com/dreji18/Bio-Epidemiology-NER', 'Maccrobat https://figshare.com/a 
rticles/dataset/MACCROBAT2018/9764942', 'd4data/biomedical-ner-all'], 'license': 'apache-2.0', 'gith 
ub': 'https://github.com/dreji18/Bio-Epidemiology-NER', 'paper': "The easiest way is to load the inf 
erence api from huggingface and second method is through the pipeline object offered by transformers 
 library.\n```python\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, Aut 
oModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all 
')\nmodel = AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\n\npipe = p 
ipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple') # pass device=0 if u 
sing gpu\npipe('''The patient reported no recurrence of palpitations at follow-up 6 months after the 
 ablation.''')\n```", 'upstream_model': "from transformers import AutoTokenizer, AutoModelForTokenCl 
assification\n\ntokenizer = AutoTokenizer.from_pretrained('d4data/biomedical-ner-all')\nmodel = Auto 
ModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')", 'parameter_count': '', 'h 
yper_parameters': {}, 'evaluation': [], 'hardware': 'GPU used : 1 x GeForce RTX 3060 Laptop GPU', 'l 
imitation_and_bias': 'An English Named Entity Recognition model, trained on Maccrobat to recognize t 
he bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was  
built on top of distilbert-base-uncased', 'demo': 'https://github.com/dreji18/Bio-Epidemiology-NER', 
 'input_format': 'input_format', 'output_format': 'output_format', 'max_sequence_length': 'max_seque 
nce_length', 'vocabulary_size': 'vocabulary_size'}]                                                  

#####################dslim/bert-large-NER########################

-------------------- datasets --------------------

Document 1:

datasets:
- conll2003
------------------------------
Document 2:

CoNLL-2003 NER task.

-------------------- license --------------------

Document 1:

license: mit
------------------------------
Document 2:

**bert-large-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).  
Specifically, this model is a *bert-large-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset.  
If you'd like to use a smaller BERT model fine-tuned on the same dataset, a [**bert-base-NER**](https://huggingface.co/dslim/bert-base-NER/) version is also available.

-------------------- github --------------------

Document 1:

The test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).
------------------------------
Document 2:

url = 'https://www.aclweb.org/anthology/W03-0419'

-------------------- paper --------------------

Document 1:

[original BERT paper](https://arxiv.org/pdf/1810.04805)
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1810-04805,
author    = {Jacob Devlin and
Ming{-}Wei Chang and
Kenton Lee and
Kristina Toutanova},
title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
Understanding},
journal   = {CoRR},
volume    = {abs/1810.04805},
year      = {2018},
url       = {http://arxiv.org/abs/1810.04805},
archivePrefix = {arXiv},
eprint    = {1810.04805},
timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}
------------------------------
Document 3:

More on replicating the original results [here](https://github.com/google-research/bert/issues/223).

-------------------- upstream_model --------------------

Document 1:

original BERT paper
------------------------------
Document 2:

This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.
------------------------------
Document 3:

This model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset.  
The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:

-------------------- parameter_count --------------------

Document 1:

This model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task.
------------------------------
Document 2:

#### # of training examples per entity type
Dataset|LOC|MISC|ORG|PER
-|-|-|-|-
Train|7140|3438|6321|6600
#### # of articles/sentences/tokens per dataset
Dataset |Articles |Sentences |Tokens
Train |946 |14,987 |203,621

-------------------- hyper_parameters --------------------

Document 1:

recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805)

-------------------- evaluation --------------------

Document 1:

metric|dev|test
-|-|-
f1 |95.7 |91.7
precision |95.3 |91.2
recall |96.1 |92.3
------------------------------
Document 2:

evaluated the model on CoNLL-2003 NER task.
------------------------------
Document 3:

- type: accuracy
value: 0.9031688753722759
name: Accuracy
verified: true
- type: precision
value: 0.920025068328604
name: Precision
verified: true
- type: recall
value: 0.9193688678588825
name: Recall
verified: true
- type: f1
value: 0.9196968510445761
name: F1
verified: true

-------------------- hardware --------------------

Document 1:

This model was trained on a single NVIDIA V100 GPU.
------------------------------
Document 2:

bert-large-NER

-------------------- limitation_and_bias --------------------

Document 1:

This model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task.
------------------------------
Document 2:

This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.
------------------------------
Document 3:

The model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:  
Abbreviation|Description
-|-
O|Outside of a named entity
B-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity
I-MIS | Miscellaneous entity
B-PER |Beginning of a person’s name right after another person’s name
I-PER |Person’s name
B-ORG |Beginning of an organization right after another organization
I-ORG |organization
B-LOC |Beginning of a location right after another location
I-LOC |Location

-------------------- demo --------------------

Document 1:

You can use this model with Transformers *pipeline* for NER.  
```python
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained('dslim/bert-base-NER')
model = AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')

nlp = pipeline('ner', model=model, tokenizer=tokenizer)
example = 'My name is Wolfgang and I live in Berlin'

ner_results = nlp(example)
print(ner_results)
```

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

output_format

-------------------- max_sequence_length --------------------

Document 1:

The max sequence length of this NLP model
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['conll2003']}]                                                                       

#####################deepset/tinyroberta-squad2########################

-------------------- datasets --------------------

Document 1:

datasets:
- squad_v2
------------------------------
Document 2:

**Training data:** SQuAD 2.0

-------------------- license --------------------

Document 1:

license: cc-by-4.0
------------------------------
Document 2:

[deepset](http://deepset.ai/) is the company behind the open-source NLP framework [Haystack](https://haystack.deepset.ai/) which is designed to help you build production ready NLP systems that use: Question answering, summarization, ranking etc.

-------------------- github --------------------

Document 1:

For more info on Haystack, visit our [GitHub](https://github.com/deepset-ai/haystack) repo and [Documentation](https://docs.haystack.deepset.ai).
------------------------------
Document 2:

[deepset](http://deepset.ai/) is the company behind the open-source NLP framework [Haystack](https://haystack.deepset.ai/) which is designed to help you build production ready NLP systems that use: Question answering, summarization, ranking etc.  
Some of our other work:
- [roberta-base-squad2]([https://huggingface.co/deepset/roberta-base-squad2)
- [German BERT (aka 'bert-base-german-cased')](https://deepset.ai/german-bert)
- [GermanQuAD and GermanDPR datasets and models (aka 'gelectra-base-germanquad', 'gbert-base-germandpr')](https://deepset.ai/germanquad)
------------------------------
Document 3:

- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
metrics:
- type: exact_match
value: 78.8627
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDNlZDU4ODAxMzY5NGFiMTMyZmQ1M2ZhZjMyODA1NmFlOGMxNzYxNTA4OGE5YTBkZWViZjBkNGQ2ZmMxZjVlMCIsInZlcnNpb24iOjF9.Wgu599r6TvgMLTrHlLMVAbUtKD_3b70iJ5QSeDQ-bRfUsVk6Sz9OsJCp47riHJVlmSYzcDj_z_3jTcUjCFFXBg
- type: f1
value: 82.0355
name: F1
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOTFkMzEzMWNiZDRhMGZlODhkYzcwZTZiMDFjZDg2YjllZmUzYWM5NTgwNGQ2NGYyMDk2ZGQwN2JmMTE5NTc3YiIsInZlcnNpb24iOjF9.ChgaYpuRHd5WeDFjtiAHUyczxtoOD_M5WR8834jtbf7wXhdGOnZKdZ1KclmhoI5NuAGc1NptX-G0zQ5FTHEcBA
- task:
type: question-answering
name: Question Answering
dataset:
name: squad
type: squad
config: plain_text
split: validation
metrics:
- type: exact_match
value: 83.86
name: Exact Match
- type: f1
value: 90.752
name: F1

-------------------- paper --------------------

Document 1:

This model was distilled using the TinyBERT approach described in [this paper](https://arxiv.org/pdf/1909.10351.pdf)

-------------------- upstream_model --------------------

Document 1:

reader = FARMReader(model_name_or_path='deepset/tinyroberta-squad2')
# or
reader = TransformersReader(model_name_or_path='deepset/tinyroberta-squad2')
------------------------------
Document 2:

This is the *distilled* version of the [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) model.
------------------------------
Document 3:

upstream_model: tinyroberta-squad2

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

batch_size = 96
n_epochs = 4
max_seq_len = 384
learning_rate = 3e-5
lr_schedule = LinearWarmup
warmup_proportion = 0.2
doc_stride = 128
max_query_length = 64
distillation_loss_weight = 0.75
temperature = 1.5
------------------------------
Document 2:

Firstly, we have performed intermediate layer distillation with roberta-base as the teacher which resulted in [deepset/tinyroberta-6l-768d](https://huggingface.co/deepset/tinyroberta-6l-768d).
Secondly, we have performed task-specific distillation with [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) as the teacher for further intermediate layer distillation on an augmented version of SQuADv2 and then with [deepset/roberta-large-squad2](https://huggingface.co/deepset/roberta-large-squad2) as the teacher for prediction layer distillation.

-------------------- evaluation --------------------

Document 1:

'exact': 78.69114798281817,
'f1': 81.9198998536977,

'total': 11873,
'HasAns_exact': 76.19770580296895,
'HasAns_f1': 82.66446878592329,
'HasAns_total': 5928,
'NoAns_exact': 81.17746005046257,
'NoAns_f1': 81.17746005046257,
'NoAns_total': 5945
------------------------------
Document 2:

- type: exact_match
value: 78.8627
name: Exact Match
- type: f1
value: 82.0355
name: F1
------------------------------
Document 3:

**Eval data:** SQuAD 2.0

-------------------- hardware --------------------

Document 1:

4x Tesla v100
------------------------------
Document 2:

Firstly, we have performed intermediate layer distillation with roberta-base as the teacher which resulted in [deepset/tinyroberta-6l-768d](https://huggingface.co/deepset/tinyroberta-6l-768d).
Secondly, we have performed task-specific distillation with [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) as the teacher for further intermediate layer distillation on an augmented version of SQuADv2 and then with [deepset/roberta-large-squad2](https://huggingface.co/deepset/roberta-large-squad2) as the teacher for prediction layer distillation.

-------------------- limitation_and_bias --------------------

Document 1:

limitation_and_bias: tinyroberta-squad2, English, Extractive QA, SQuAD 2.0

-------------------- demo --------------------

Document 1:

[deepset](http://deepset.ai/) is the company behind the open-source NLP framework [Haystack](https://haystack.deepset.ai/) which is designed to help you build production ready NLP systems that use: Question answering, summarization, ranking etc.
------------------------------
Document 2:

[Discord](https://haystack.deepset.ai/community)
------------------------------
Document 3:

See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)

-------------------- input_format --------------------

Document 1:

input_format

-------------------- output_format --------------------

Document 1:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_seq_len = 384
------------------------------
Document 2:

max_sequence_length: 512
------------------------------
Document 3:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['squad_v2']}]                                                                        

#####################dccuchile/bert-base-spanish-wwm-uncased########################

-------------------- datasets --------------------

Document 1:

[Spanish Pre-Trained BERT Model and Evaluation Data](https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf)
------------------------------
Document 2:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.

-------------------- license --------------------

Document 1:

The license CC BY 4.0 best describes our intentions for our work. However we are not sure that all the datasets used to train BETO have licenses compatible with CC BY 4.0 (specially for commercial use).

-------------------- github --------------------

Document 1:

|BETO uncased|[tensorflow_weights](https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/tensorflow_weights.tar.gz) | [pytorch_weights](https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/pytorch_weights.tar.gz) | [vocab](./config/uncased_2M/vocab.txt), [config](./config/uncased_2M/config.json) |
|BETO cased| [tensorflow_weights](https://users.dcc.uchile.cl/~jperez/beto/cased_2M/tensorflow_weights.tar.gz) | [pytorch_weights](https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz) | [vocab](./config/cased_2M/vocab.txt), [config](./config/cased_2M/config.json) |
------------------------------
Document 2:

[🤗Huggingface Transformers library](https://github.com/huggingface/transformers)
[`'dccuchile/bert-base-spanish-wwm-cased'`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased)
[`'dccuchile/bert-base-spanish-wwm-uncased'`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased)
[this colab notebook](https://colab.research.google.com/drive/1uRwg4UmPgYIqGYY4gW_Nsw9782GFJbPt)

-------------------- paper --------------------

Document 1:

[Spanish Pre-Trained BERT Model and Evaluation Data](https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf)  
To cite this resource in a publication please use the following:  
```
@inproceedings{CaneteCFP2020,
title={Spanish Pre-Trained BERT Model and Evaluation Data},
author={Cañete, José and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},
booktitle={PML4DC at ICLR 2020},
year={2020}
}
```
------------------------------
Document 2:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.

-------------------- upstream_model --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.
------------------------------
Document 2:

BETO models can be accessed simply as ['dccuchile/bert-base-spanish-wwm-cased'](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) and ['dccuchile/bert-base-spanish-wwm-uncased'](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased) by using the Transformers library.

-------------------- parameter_count --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.
------------------------------
Document 2:

BETO is a [BERT model](https://github.com/google-research/bert) trained on a [big Spanish corpus](https://github.com/josecannete/spanish-corpora). BETO is of size similar to a BERT-Base and was trained with the Whole Word Masking technique.

-------------------- hyper_parameters --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.

-------------------- evaluation --------------------

Document 1:

|Task   | BETO-cased    | BETO-uncased  | Best Multilingual BERT    | Other results                  |
|-------|--------------:|--------------:|--------------------------:|-------------------------------:|
|[POS](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1827)    | **98.97**     | 98.44     | 97.10 [2]                 | 98.91 [6], 96.71 [3]           |
|[NER-C](https://www.kaggle.com/nltkdata/conll-corpora)  | [**88.43**](https://github.com/gchaperon/beto-benchmarks/blob/master/conll2002/dev_results_beto-cased_conll2002.txt)         | 82.67         | 87.38 [2]                 | 87.18 [3]                      |
|[MLDoc](https://github.com/facebookresearch/MLDoc)  | [95.60](https://github.com/gchaperon/beto-benchmarks/blob/master/MLDoc/dev_results_beto-cased_mldoc.txt)        | [**96.12**](https://github.com/gchaperon/beto-benchmarks/blob/master/MLDoc/dev_results_beto-uncased_mldoc.txt)     | 95.70 [2]                 | 88.75 [4]                      |
|[PAWS-X](https://github.com/google-research-datasets/paws/tree/master/pawsx) | 89.05         | 89.55         | 90.70 [8]                 |
|[XNLI](https://github.com/facebookresearch/XNLI)   | **82.01**         | 80.15     | 78.50 [2]                 | 80.80 [5], 77.80 [1], 73.15 [4]|
------------------------------
Document 2:

BETO is a [BERT model](https://github.com/google-research/bert) trained on a [big Spanish corpus](https://github.com/josecannete/spanish-corpora). BETO is of size similar to a BERT-Base and was trained with the Whole Word Masking technique. Below you find Tensorflow and Pytorch checkpoints for the uncased and cased versions, as well as some results for Spanish benchmarks comparing BETO with [Multilingual BERT](https://github.com/google-research/bert/blob/master/multilingual.md) as well as other (not BERT-based) models.

-------------------- hardware --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.
------------------------------
Document 2:

Google for helping us with the [TensorFlow Research Cloud](https://www.tensorflow.org/tfrc) program.

-------------------- limitation_and_bias --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.

-------------------- demo --------------------

Document 1:

An example on how to download and use the models in this page can be found in [this colab notebook](https://colab.research.google.com/drive/1uRwg4UmPgYIqGYY4gW_Nsw9782GFJbPt).

-------------------- input_format --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.

-------------------- output_format --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.

-------------------- max_sequence_length --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.

-------------------- vocabulary_size --------------------

Document 1:

All models use a vocabulary of about 31k BPE subwords constructed using SentencePiece and were trained for 2M steps.
------------------------------
Document 2:

vocabulary_size


[{'datasets': ['Spanish Pre-Trained BERT Model and Evaluation Data'], 'license': 'CC BY 4.0', 'gith 
ub': 'https://github.com/huggingface/transformers', 'paper': 'https://users.dcc.uchile.cl/~jperez/pa 
pers/pml4dc2020.pdf', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '' 
, 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limita 
tion_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': '',  
'vocabulary_size': ''}]                                                                              

#####################huggingface/CodeBERTa-small-v1########################

-------------------- datasets --------------------

Document 1:

datasets:
- code_search_net
thumbnail: https://cdn-media.huggingface.co/CodeBERTa/CodeBERTa.png
------------------------------
Document 2:

CodeSearchNet dataset from GitHub.
------------------------------
Document 3:

[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)

-------------------- license --------------------

Document 1:

@article{husain_codesearchnet_2019,
title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}},
shorttitle = {{CodeSearchNet} {Challenge}},
url = {http://arxiv.org/abs/1909.09436},
urldate = {2020-03-12},
journal = {arXiv:1909.09436 [cs, stat]},
author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
month = sep,
year = {2019},
note = {arXiv: 1909.09436},
}
------------------------------
Document 2:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- github --------------------

Document 1:

url = {http://arxiv.org/abs/1909.09436}
------------------------------
Document 2:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub.

-------------------- paper --------------------

Document 1:

@article{husain_codesearchnet_2019,
title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}},
shorttitle = {{CodeSearchNet} {Challenge}},
url = {http://arxiv.org/abs/1909.09436},
urldate = {2020-03-12},
journal = {arXiv:1909.09436 [cs, stat]},
author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
month = sep,
year = {2019},
note = {arXiv: 1909.09436},
}
------------------------------
Document 2:

[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)
------------------------------
Document 3:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- upstream_model --------------------

Document 1:

[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)
------------------------------
Document 2:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- parameter_count --------------------

Document 1:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- hyper_parameters --------------------

Document 1:

[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)

-------------------- evaluation --------------------

Document 1:

See the model card for **[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)** 🤯.
------------------------------
Document 2:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- hardware --------------------

Document 1:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.
------------------------------
Document 2:

[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)

-------------------- limitation_and_bias --------------------

Document 1:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.
------------------------------
Document 2:

See the model card for **[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)** 🤯.

-------------------- demo --------------------

Document 1:

[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)
------------------------------
Document 2:

thumbnail: https://cdn-media.huggingface.co/CodeBERTa/CodeBERTa.png

-------------------- input_format --------------------

Document 1:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.
------------------------------
Document 2:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.

-------------------- max_sequence_length --------------------

Document 1:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.
------------------------------
Document 2:

max_sequence_length
------------------------------
Document 3:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.
------------------------------
Document 2:

[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)


[{'datasets': ['code_search_net'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '',  
'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'op 
timizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_forma 
t': '', 'output_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]                      

#####################xlm-roberta-base########################

-------------------- datasets --------------------

Document 1:

XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
------------------------------
Document 2:

http://arxiv.org/abs/1911.02116

-------------------- license --------------------

Document 1:

license: mit
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1911-02116,
author    = {Alexis Conneau and
Kartikay Khandelwal and
Naman Goyal and
Vishrav Chaudhary and
Guillaume Wenzek and
Francisco Guzm{\'{a}}n and
Edouard Grave and
Myle Ott and
Luke Zettlemoyer and
Veselin Stoyanov},
title     = {Unsupervised Cross-lingual Representation Learning at Scale},
journal   = {CoRR},
volume    = {abs/1911.02116},
year      = {2019},
url       = {http://arxiv.org/abs/1911.02116},
eprinttype = {arXiv},
eprint    = {1911.02116},
timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}
------------------------------
Document 3:

The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.

-------------------- github --------------------

Document 1:

url       = {http://arxiv.org/abs/1911.02116},
eprinttype = {arXiv},
eprint    = {1911.02116},
biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
------------------------------
Document 2:

[model hub](https://huggingface.co/models?search=xlm-roberta)
------------------------------
Document 3:

[this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)

-------------------- paper --------------------

Document 1:

The paper associated with this model is [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al.
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1911-02116,
author    = {Alexis Conneau and
Kartikay Khandelwal and
Naman Goyal and
Vishrav Chaudhary and
Guillaume Wenzek and
Francisco Guzm{\'{a}}n and
Edouard Grave and
Myle Ott and
Luke Zettlemoyer and
Veselin Stoyanov},
title     = {Unsupervised Cross-lingual Representation Learning at Scale},
journal   = {CoRR},
volume    = {abs/1911.02116},
year      = {2019},
url       = {http://arxiv.org/abs/1911.02116},
eprinttype = {arXiv},
eprint    = {1911.02116},
timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}
------------------------------
Document 3:

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.

-------------------- upstream_model --------------------

Document 1:

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.
------------------------------
Document 2:

XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.
------------------------------
Document 3:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.

-------------------- parameter_count --------------------

Document 1:

XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.
------------------------------
Document 2:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.
------------------------------
Document 3:

parameter_count

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.
------------------------------
Document 2:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.

-------------------- hardware --------------------

Document 1:

XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.
------------------------------
Document 2:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.

-------------------- limitation_and_bias --------------------

Document 1:

XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
------------------------------
Document 2:

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.
------------------------------
Document 3:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.  
RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.  
More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.  
This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.

-------------------- demo --------------------

Document 1:

You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.  
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.
------------------------------
Document 2:

<a href='https://huggingface.co/exbert/?model=xlm-roberta-base'>
<img width='300px' src='https://cdn-media.huggingface.co/exbert/button.png'>
</a>
------------------------------
Document 3:

The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

input_format
------------------------------
Document 3:

It allows the model to learn a bidirectional representation of the sentence.  
This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.

-------------------- output_format --------------------

Document 1:

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.
------------------------------
Document 2:

output_format
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.
------------------------------
Document 3:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['CommonCrawl'], 'license': 'mit', 'github': 'https://github.com/pytorch/fairseq/tree 
/master/examples/xlmr', 'paper': 'https://arxiv.org/abs/1911.02116', 'upstream_model': 'RoBERTa', 'p 
arameter_count': '2.5TB', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_ 
bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'max_sequence_length': '', 'vocabula 
ry_size': ''}]                                                                                       

#####################microsoft/deberta-base########################

-------------------- datasets --------------------

Document 1:

80GB training data

-------------------- license --------------------

Document 1:

license: mit
------------------------------
Document 2:

@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}
------------------------------
Document 3:

DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. Please check the official repository for more details and updates. Fine-tuning on NLU tasks. We present the dev results on SQuAD 1.1/2.0 and MNLI tasks.

-------------------- github --------------------

Document 1:

https://openreview.net/forum?id=XPZIaotutsD
------------------------------
Document 2:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.
------------------------------
Document 3:

thumbnail: https://huggingface.co/front/thumbnails/microsoft.png

-------------------- paper --------------------

Document 1:

@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}
------------------------------
Document 2:

DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.

-------------------- upstream_model --------------------

Document 1:

DeBERTa
------------------------------
Document 2:

@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}

-------------------- parameter_count --------------------

Document 1:

DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |
|-------------------|-----------|-----------|--------|
| RoBERTa-base      | 91.5/84.6 | 83.7/80.5 | 87.6   |
| XLNet-Large       | -/-       | -/80.2    | 86.8   |
| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |

-------------------- hardware --------------------

Document 1:

DeBERTa

-------------------- limitation_and_bias --------------------

Document 1:

DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.

-------------------- demo --------------------

Document 1:

url={https://openreview.net/forum?id=XPZIaotutsD}
------------------------------
Document 2:

| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |
|-------------------|-----------|-----------|--------|
| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |

-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['80GB training data'], 'license': 'mit', 'github': 'https://github.com/microsoft/DeB 
ERTa', 'paper': 'https://openreview.net/forum?id=XPZIaotutsD', 'upstream_model': 'DeBERTa', 'paramet 
er_count': 'N/A', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'DeBERTa', 'limitation_and_b 
ias': 'DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask d 
ecoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.', 'demo':  
'url={https://openreview.net/forum?id=XPZIaotutsD}', 'input_format': '', 'output_format': 'output_fo 
rmat'}]                                                                                              

#####################facebook/mbart-large-50-many-to-many-mmt########################

-------------------- datasets --------------------

Document 1:

[mBART-large-50](https://huggingface.co/facebook/mbart-large-50)  
[Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401)  
'mbart-large-50-many-to-many-mmt'  
Hindi to French  
Arabic to English

-------------------- license --------------------



-------------------- github --------------------

Document 1:

Arabic (ar_AR), Czech (cs_CZ), German (de_DE), English (en_XX), Spanish (es_XX), Estonian (et_EE), Finnish (fi_FI), French (fr_XX), Gujarati (gu_IN), Hindi (hi_IN), Italian (it_IT), Japanese (ja_XX), Kazakh (kk_KZ), Korean (ko_KR), Lithuanian (lt_LT), Latvian (lv_LV), Burmese (my_MM), Nepali (ne_NP), Dutch (nl_XX), Romanian (ro_RO), Russian (ru_RU), Sinhala (si_LK), Turkish (tr_TR), Vietnamese (vi_VN), Chinese (zh_CN), Afrikaans (af_ZA), Azerbaijani (az_AZ), Bengali (bn_IN), Persian (fa_IR), Hebrew (he_IL), Croatian (hr_HR), Indonesian (id_ID), Georgian (ka_GE), Khmer (km_KH), Macedonian (mk_MK), Malayalam (ml_IN), Mongolian (mn_MN), Marathi (mr_IN), Polish (pl_PL), Pashto (ps_AF), Portuguese (pt_XX), Swedish (sv_SE), Swahili (sw_KE), Tamil (ta_IN), Telugu (te_IN), Thai (th_TH), Tagalog (tl_XX), Ukrainian (uk_UA), Urdu (ur_PK), Xhosa (xh_ZA), Galician (gl_ES), Slovene (sl_SI)

-------------------- paper --------------------

Document 1:

@article{tang2020multilingual,
title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},
author={Yuqing Tang and Chau Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},
year={2020},
eprint={2008.00401},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
------------------------------
Document 2:

[mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation. It was introduced in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper.

-------------------- upstream_model --------------------

Document 1:

This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation.

-------------------- parameter_count --------------------

Document 1:

This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation.

-------------------- hyper_parameters --------------------

Document 1:

This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation. It was introduced in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper.  
The model can translate directly between any pair of 50 languages. To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.
------------------------------
Document 2:

hyper_parameters

-------------------- evaluation --------------------

Document 1:

This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation. It was introduced in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper.  
The model can translate directly between any pair of 50 languages. To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.  
```python
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

article_hi = 'संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है'
article_ar = 'الأمين العام للأمم المتحدة يقول إنه لا يوجد حل عسكري في سوريا.'

model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')
tokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')

# translate Hindi to French
tokenizer.src_lang = 'hi_IN'
encoded_hi = tokenizer(article_hi, return_tensors='pt')
generated_tokens = model.generate(
**encoded_hi,
forced_bos_token_id=tokenizer.lang_code_to_id['fr_XX']
)
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
# => 'Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire dans la Syrie.'

# translate Arabic to English
tokenizer.src_lang = 'ar_AR'
encoded_ar = tokenizer(article_ar, return_tensors='pt')
generated_tokens = model.generate(
**encoded_ar,
forced_bos_token_id=tokenizer.lang_code_to_id['en_XX']
)
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
# => 'The Secretary-General of the United Nations says there is no military solution in Syria.'
```

-------------------- hardware --------------------

Document 1:

This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation.

-------------------- limitation_and_bias --------------------

Document 1:

This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation. It was introduced in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper.  
The model can translate directly between any pair of 50 languages. To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.

-------------------- demo --------------------



-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

The model can translate directly between any pair of 50 languages. To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

The model can translate directly between any pair of 50 languages. To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.

-------------------- max_sequence_length --------------------

Document 1:

This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation.
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

This model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-many-to-many-mmt` is fine-tuned for multilingual machine translation.
------------------------------
Document 2:

vocabulary_size


[{'datasets': ['Hindi to French', 'Arabic to English'], 'license': '', 'github': 'https://huggingfa 
ce.co/facebook/mbart-large-50', 'paper': 'https://arxiv.org/abs/2008.00401', 'upstream_model': 'mBAR 
T-large-50', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_r 
ate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 
 'input_format': '', 'output_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]         

#####################Helsinki-NLP/opus-mt-it-en########################

-------------------- datasets --------------------

Document 1:

* dataset: opus

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

#tags  
---
license: apache-2.0
tags:
- translation
------------------------------
Document 2:

* OPUS readme: [it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md)
* download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)
* test set translations: [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt)
* test set scores: [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)

-------------------- paper --------------------

Document 1:

[opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)

-------------------- upstream_model --------------------

Document 1:

OPUS readme: [it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md)  
model: transformer-align
download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009.it.en 	| 35.3 	| 0.600 |
| newstest2009.it.en 	| 34.0 	| 0.594 |
| Tatoeba.it.en 	| 70.9 	| 0.808 |

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

test set scores: [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009.it.en 	| 35.3 	| 0.600 |
| newstest2009.it.en 	| 34.0 	| 0.594 |
| Tatoeba.it.en 	| 70.9 	| 0.808 |

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

limitation_and_bias
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009.it.en 	| 35.3 	| 0.600 |
| newstest2009.it.en 	| 34.0 	| 0.594 |
| Tatoeba.it.en 	| 70.9 	| 0.808 |

-------------------- demo --------------------

Document 1:

[opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009.it.en 	| 35.3 	| 0.600 |
| newstest2009.it.en 	| 34.0 	| 0.594 |
| Tatoeba.it.en 	| 70.9 	| 0.808 |

-------------------- input_format --------------------

Document 1:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

output_format
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length
------------------------------
Document 3:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': '#tags\n---\nlicense: apache-2.0\ntags:\ 
n- translation', 'paper': '[opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/op 
us-2019-12-18.zip)', 'upstream_model': 'OPUS readme: [it-en](https://github.com/Helsinki-NLP/OPUS-MT 
-train/blob/master/models/it-en/README.md)\nmodel: transformer-align\ndownload original weights: [op 
us-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)', 'paramete 
r_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'newssyscomb2009.it.en', 'result': '35 
.3'}, {'test': 'newstest2009.it.en', 'result': '34.0'}, {'test': 'Tatoeba.it.en', 'result': '70.9'}] 
, 'hardware': '', 'limitation_and_bias': 'limitation_and_bias', 'demo': '[opus-2019-12-18.zip](https 
://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)', 'input_format': 'input_format', ' 
output_format': 'output_format', 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': '' 
}]                                                                                                   

#####################facebook/bart-large-cnn########################

-------------------- datasets --------------------

Document 1:

datasets:
- cnn_dailymail
------------------------------
Document 2:

BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail).

-------------------- license --------------------

Document 1:

@article{DBLP:journals/corr/abs-1910-13461,
author    = {Mike Lewis and
Yinhan Liu and
Naman Goyal and
Marjan Ghazvininejad and
Abdelrahman Mohamed and
Omer Levy and
Veselin Stoyanov and
Luke Zettlemoyer},
title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
Generation, Translation, and Comprehension},
journal   = {CoRR},
volume    = {abs/1910.13461},
year      = {2019},
url       = {http://arxiv.org/abs/1910.13461},
eprinttype = {arXiv},
eprint    = {1910.13461},
timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}
------------------------------
Document 2:

license: mit

-------------------- github --------------------

Document 1:

url       = {http://arxiv.org/abs/1910.13461},
eprinttype = {arXiv},
eprint    = {1910.13461},
biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
------------------------------
Document 2:

- name: facebook/bart-large-cnn
- dataset:
  name: cnn_dailymail
  type: cnn_dailymail

-------------------- paper --------------------

Document 1:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al.
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1910-13461,
author    = {Mike Lewis and
Yinhan Liu and
Naman Goyal and
Marjan Ghazvininejad and
Abdelrahman Mohamed and
Omer Levy and
Veselin Stoyanov and
Luke Zettlemoyer},
title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
Generation, Translation, and Comprehension},
journal   = {CoRR},
volume    = {abs/1910.13461},
year      = {2019},
url       = {http://arxiv.org/abs/1910.13461},
eprinttype = {arXiv},
eprint    = {1910.13461},
timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

-------------------- upstream_model --------------------

Document 1:

BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail).
------------------------------
Document 2:

model-index:
- name: facebook/bart-large-cnn

-------------------- parameter_count --------------------

Document 1:

parameter_count
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

- type: rouge
value: 42.9486
name: ROUGE-1
verified: true
- type: rouge
value: 20.8149
name: ROUGE-2
verified: true
- type: rouge
value: 30.6186
name: ROUGE-L
verified: true
- type: rouge
value: 40.0376
name: ROUGE-LSUM
verified: true

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart).
------------------------------
Document 2:

limitation_and_bias

-------------------- demo --------------------

Document 1:

You can use this model for text summarization.
------------------------------
Document 2:

- name: facebook/bart-large-cnn
- task:
  type: summarization
  name: Summarization
- metrics:
  - type: rouge
    value: 42.9486
    name: ROUGE-1
    verified: true
  - type: rouge
    value: 20.8149
    name: ROUGE-2
    verified: true
  - type: rouge
    value: 30.6186
    name: ROUGE-L
    verified: true
  - type: rouge
    value: 40.0376
    name: ROUGE-LSUM
    verified: true
  - type: loss
    value: 2.529000997543335
    name: loss
    verified: true
  - type: gen_len
    value: 78.5866
    name: gen_len
    verified: true
------------------------------
Document 3:

BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart).

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.


[{'datasets': ['cnn_dailymail'], 'license': 'mit', 'github': 'https://github.com/pytorch/fairseq/tr 
ee/master/examples/bart', 'paper': 'https://arxiv.org/abs/1910.13461', 'upstream_model': 'facebook/b 
art-large-cnn', 'parameter_count': '#params', 'hyper_parameters': {}, 'evaluation': [{'test': 'ROUGE 
-1', 'result': '42.9486'}, {'test': 'ROUGE-2', 'result': '20.8149'}, {'test': 'ROUGE-L', 'result': ' 
30.6186'}, {'test': 'ROUGE-LSUM', 'result': '40.0376'}], 'hardware': '', 'limitation_and_bias': 'BAR 
T model pre-trained on English language, and fine-tuned on CNN Daily Mail. It was introduced in the  
paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation 
, and Comprehension by Lewis et al. and first released in this repository (https://github.com/pytorc 
h/fairseq/tree/master/examples/bart).', 'demo': 'You can use this model for text summarization.', 'i 
nput_format': '', 'output_format': '', 'max_sequence_length': '', 'vocabulary_size': ''}]            

#####################PygmalionAI/pygmalion-1.3b########################

-------------------- datasets --------------------

Document 1:

The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations.

-------------------- license --------------------

Document 1:

license: agpl-3.0

-------------------- github --------------------

Document 1:

This notebook can be found [here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb).

-------------------- paper --------------------

Document 1:

EleutherAI's [pythia-1.3b-deduped](https://huggingface.co/EleutherAI/pythia-1.3b-deduped).

-------------------- upstream_model --------------------

Document 1:

The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format:  
```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]

[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```  
Where `[CHARACTER] `is, as you can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like:  
```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```  
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.
------------------------------
Document 2:

EleutherAI's [pythia-1.3b-deduped](https://huggingface.co/EleutherAI/pythia-1.3b-deduped).

-------------------- parameter_count --------------------

Document 1:

parameter_count

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------

Document 1:

on a single 24GB GPU

-------------------- limitation_and_bias --------------------

Document 1:

- The model can get stuck repeating certain phrases, or sometimes even entire sentences.
- We believe this is due to that behavior being present in the training data itself, and plan to investigate and adjust accordingly for future versions.
------------------------------
Document 2:

The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations.
------------------------------
Document 3:

The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format:  
```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]

[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```  
Where `[CHARACTER] `is, as you can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like:  
```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```  
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.

-------------------- demo --------------------

Document 1:

The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format:  
```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]

[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```  
Where `[CHARACTER] `is, as you can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like:  
```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```  
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.
------------------------------
Document 2:

We provide a notebook with a Gradio UI for playing around with the model without having to manually format inputs. This notebook can be found [here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb).

-------------------- input_format --------------------

Document 1:

The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format:  
```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]

[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```  
Where `[CHARACTER] `is, as you can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like:  
```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```  
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.

-------------------- output_format --------------------

Document 1:

The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format:  
```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]

[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```  
Where `[CHARACTER] `is, as you can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like:  
```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```  
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length

-------------------- vocabulary_size --------------------




[{'datasets': ['56MB of dialogue data'], 'license': 'agpl-3.0', 'github': 'https://github.com/Pygma 
lionAI/gradio-ui/blob/master/notebooks/GPU.ipynb', 'paper': "EleutherAI's pythia-1.3b-deduped", 'ups 
tream_model': 'EleutherAI/pythia-1.3b-deduped', 'parameter_count': 'parameter_count', 'hyper_paramet 
ers': {}, 'evaluation': [], 'hardware': 'on a single 24GB GPU', 'limitation_and_bias': '- The model  
can get stuck repeating certain phrases, or sometimes even entire sentences.\n- We believe this is d 
ue to that behavior being present in the training data itself, and plan to investigate and adjust ac 
cordingly for future versions.', 'demo': "The model can be used as a regular text generation model,  
but it'll perform best if the input prompt adheres to the following format:\n```\n[CHARACTER]'s Pers 
ona: [A few sentences about the character you want the model to play]\n\n[DIALOGUE HISTORY]\nYou: [Y 
our input message here]\n[CHARACTER]:\n```\nWhere `[CHARACTER] `is, as you can probably guess, the n 
ame of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat history so the  
model can have some conversational context to draw from. Ideally it'll be pairs of messages like:\n` 
``\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]\n```\nApart from c 
hat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the cha 
racter should speak - ideally at the beginning, so it doesn't get confused as to what's conversation 
 history vs. character definition.", 'input_format': "The model can be used as a regular text genera 
tion model, but it'll perform best if the input prompt adheres to the following format:\n```\n[CHARA 
CTER]'s Persona: [A few sentences about the character you want the model to play]\n\n[DIALOGUE HISTO 
RY]\nYou: [Your input message here]\n[CHARACTER]:\n```\nWhere `[CHARACTER] `is, as you can probably  
guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY]` is chat his 
tory so the model can have some conversational context to draw from. Ideally it'll be pairs of messa 
ges like:\n```\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]\n```\n 
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show 
 how the character should speak - ideally at the beginning, so it doesn't get confused as to what's  
conversation history vs. character definition.", 'output_format': "The model can be used as a regula 
r text generation model, but it'll perform best if the input prompt adheres to the following format: 
\n```\n[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\n\n[D 
IALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:\n```\nWhere `[CHARACTER] `is, as you  
can probably guess, the name of the character you want the model to portray, and `[DIALOGUE HISTORY] 
` is chat history so the model can have some conversational context to draw from. Ideally it'll be p 
airs of messages like:\n```\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue  
above]\n```\nApart from chat history, you can also just add example conversations in `[DIALOGUE HIST 
ORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused  
as to what's conversation history vs. character definition.", 'max_sequence_length': 'max_sequence_l 
ength', 'vocabulary_size': ''}]                                                                      

#####################sentence-transformers/multi-qa-mpnet-base-cos-v1########################

-------------------- datasets --------------------

Document 1:

| Dataset                    | Number of training tuples  |
|--------------------------------------------------------|:--------------------------:|
| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs from WikiAnswers |  77,427,422 |
| [PAQ](https://github.com/facebookresearch/PAQ) Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia | 64,371,441 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs from all StackExchanges  | 25,316,456 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs from all StackExchanges  |  21,396,559 |
| [MS MARCO](https://microsoft.github.io/msmarco/) Triplets (query, answer, hard_negative) for 500k queries from Bing search engine |  17,579,773 |
| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) (query, answer) pairs for 3M Google queries and Google featured snippet  | 3,012,496 |
| [Amazon-QA](http://jmcauley.ucsd.edu/data/amazon/qa/) (Question, Answer) pairs from Amazon product pages | 2,448,839
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) pairs from Yahoo Answers | 1,198,260 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) pairs from Yahoo Answers | 681,164 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) pairs from Yahoo Answers | 659,896 |
| [SearchQA](https://huggingface.co/datasets/search_qa) (Question, Answer) pairs for 140k questions, each with Top5 Google snippets on that question | 582,261 |
| [ELI5](https://huggingface.co/datasets/eli5) (Question, Answer) pairs from Reddit ELI5 (explainlikeimfive) | 325,475 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions pairs (titles) | 304,525 |
| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Question, Duplicate_Question, Hard_Negative) triplets for Quora Questions Pairs dataset | 103,663 |
| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) (Question, Paragraph) pairs for 100k real Google queries with relevant Wikipedia paragraph | 100,231 |
| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) (Question, Paragraph) pairs from SQuAD2.0 dataset |  87,599 |
| [TriviaQA](https://huggingface.co/datasets/trivia_qa) (Question, Evidence) pairs | 73,346 |
| **Total** | **214,988,242** |
------------------------------
Document 2:

[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354)

-------------------- license --------------------

Document 1:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.

-------------------- github --------------------

Document 1:

The full training script is accessible in this current repository: `train_script.py`.
------------------------------
Document 2:

[sentence-transformers](https://www.SBERT.net) model, [SBERT.net - Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)

-------------------- paper --------------------

Document 1:

[sentence-transformers](https://www.SBERT.net) model
------------------------------
Document 2:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.

-------------------- upstream_model --------------------

Document 1:

Note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.
------------------------------
Document 2:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.
------------------------------
Document 3:

It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for **semantic search**.

-------------------- parameter_count --------------------

Document 1:

parameter_count
------------------------------
Document 2:

| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |
------------------------------
Document 3:

| Dataset                    | Number of training tuples  |
|--------------------------------------------------------|:--------------------------:|
| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs from WikiAnswers |  77,427,422 |
| [PAQ](https://github.com/facebookresearch/PAQ) Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia | 64,371,441 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs from all StackExchanges  | 25,316,456 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs from all StackExchanges  |  21,396,559 |
| [MS MARCO](https://microsoft.github.io/msmarco/) Triplets (query, answer, hard_negative) for 500k queries from Bing search engine |  17,579,773 |
| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) (query, answer) pairs for 3M Google queries and Google featured snippet  | 3,012,496 |
| [Amazon-QA](http://jmcauley.ucsd.edu/data/amazon/qa/) (Question, Answer) pairs from Amazon product pages | 2,448,839
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) pairs from Yahoo Answers | 1,198,260 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) pairs from Yahoo Answers | 681,164 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) pairs from Yahoo Answers | 659,896 |
| [SearchQA](https://huggingface.co/datasets/search_qa) (Question, Answer) pairs for 140k questions, each with Top5 Google snippets on that question | 582,261 |
| [ELI5](https://huggingface.co/datasets/eli5) (Question, Answer) pairs from Reddit ELI5 (explainlikeimfive) | 325,475 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions pairs (titles) | 304,525 |
| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Question, Duplicate_Question, Hard_Negative) triplets for Quora Questions Pairs dataset | 103,663 |
| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) (Question, Paragraph) pairs for 100k real Google queries with relevant Wikipedia paragraph | 100,231 |
| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) (Question, Paragraph) pairs from SQuAD2.0 dataset |  87,599 |
| [TriviaQA](https://huggingface.co/datasets/trivia_qa) (Question, Evidence) pairs | 73,346 |
| **Total** | **214,988,242** |

-------------------- hyper_parameters --------------------

Document 1:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |
------------------------------
Document 2:

We developped this model during the
[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104),
organized by Hugging Face. We developped this model as part of the project:
[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354).

-------------------- evaluation --------------------

Document 1:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |

-------------------- hardware --------------------

Document 1:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.

-------------------- limitation_and_bias --------------------

Document 1:

Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.
------------------------------
Document 2:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.
------------------------------
Document 3:

It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for **semantic search**. It has been trained on 215M (question, answer) pairs from diverse sources.

-------------------- demo --------------------

Document 1:

Our model is intended to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages. Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.
------------------------------
Document 2:

[sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for **semantic search**. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: [SBERT.net - Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)
------------------------------
Document 3:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.

-------------------- input_format --------------------

Document 1:

input text up to 250 word pieces
------------------------------
Document 2:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.
------------------------------
Document 3:

input_format

-------------------- output_format --------------------

Document 1:

| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.
------------------------------
Document 2:

output_format
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length: There is a limit of 512 word pieces: Text longer than that will be truncated.
------------------------------
Document 2:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.
------------------------------
Document 3:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

vocabulary_size
------------------------------
Document 2:

| Setting | Value |
| --- | :---: |
| Dimensions | 768 |
| Produces normalized embeddings | Yes |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |  
Note: When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used.
------------------------------
Document 3:

vocabulary_size


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens. However, your messages resulted in 4232 tokens (4075 in the messages, 157 in the functions). Please reduce the length of the messages or functions. 

#####################GanjinZero/UMLSBert_ENG########################

-------------------- datasets --------------------

Document 1:

https://github.com/GanjinZero/CODER

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

Github Link: https://github.com/GanjinZero/CODER

-------------------- paper --------------------

Document 1:

@article{YUAN2022103983,
title = {CODER: Knowledge-infused cross-lingual medical term embedding for term normalization},
journal = {Journal of Biomedical Informatics},
pages = {103983},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103983},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421003129},
author = {Zheng Yuan and Zhengyun Zhao and Haixia Sun and Jiao Li and Fei Wang and Sheng Yu},
keywords = {medical term normalization, cross-lingual, medical term representation, knowledge graph embedding, contrastive learning}
}

-------------------- upstream_model --------------------

Document 1:

CODER: Knowledge-infused cross-lingual medical term embedding for term normalization.
English Version. Old name. This model is not UMLSBert!!!

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

CODER: Knowledge-infused cross-lingual medical term embedding for term normalization.
English Version. Old name. This model is not UMLSBert!!!  
Github Link: https://github.com/GanjinZero/CODER  
```
@article{YUAN2022103983,
title = {CODER: Knowledge-infused cross-lingual medical term embedding for term normalization},
journal = {Journal of Biomedical Informatics},
pages = {103983},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103983},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421003129},
author = {Zheng Yuan and Zhengyun Zhao and Haixia Sun and Jiao Li and Fei Wang and Sheng Yu},
keywords = {medical term normalization, cross-lingual, medical term representation, knowledge graph embedding, contrastive learning}
}
```

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

Github Link: https://github.com/GanjinZero/CODER

-------------------- input_format --------------------

Document 1:

input_format

-------------------- output_format --------------------

Document 1:

output_format


[{'datasets': ['https://github.com/GanjinZero/CODER'], 'license': 'apache-2.0', 'github': 'https:// 
github.com/GanjinZero/CODER', 'paper': 'https://www.sciencedirect.com/science/article/pii/S153204642 
1003129', 'upstream_model': 'CODER: Knowledge-infused cross-lingual medical term embedding for term  
normalization', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'li 
mitation_and_bias': '', 'demo': 'https://github.com/GanjinZero/CODER', 'input_format': 'input_format 
', 'output_format': 'output_format'}]                                                                

#####################facebook/bart-base########################

-------------------- datasets --------------------

Document 1:

See the [model hub](https://huggingface.co/models?search=bart) to look for fine-tuned versions on a task that interests you.
------------------------------
Document 2:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart).
------------------------------
Document 3:

http://arxiv.org/abs/1910.13461

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1910-13461,
author    = {Mike Lewis and
Yinhan Liu and
Naman Goyal and
Marjan Ghazvininejad and
Abdelrahman Mohamed and
Omer Levy and
Veselin Stoyanov and
Luke Zettlemoyer},
title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
Generation, Translation, and Comprehension},
journal   = {CoRR},
volume    = {abs/1910.13461},
year      = {2019},
url       = {http://arxiv.org/abs/1910.13461},
eprinttype = {arXiv},
eprint    = {1910.13461},
timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}
------------------------------
Document 3:

The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.

-------------------- github --------------------

Document 1:

url       = {http://arxiv.org/abs/1910.13461},
eprinttype = {arXiv},
eprint    = {1910.13461},
biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
------------------------------
Document 2:

[model hub](https://huggingface.co/models?search=bart)

-------------------- paper --------------------

Document 1:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al.
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1910-13461,
author    = {Mike Lewis and
Yinhan Liu and
Naman Goyal and
Marjan Ghazvininejad and
Abdelrahman Mohamed and
Omer Levy and
Veselin Stoyanov and
Luke Zettlemoyer},
title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
Generation, Translation, and Comprehension},
journal   = {CoRR},
volume    = {abs/1910.13461},
year      = {2019},
url       = {http://arxiv.org/abs/1910.13461},
eprinttype = {arXiv},
eprint    = {1910.13461},
timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}
------------------------------
Document 3:

See the [model hub](https://huggingface.co/models?search=bart) to look for fine-tuned versions on a task that interests you.

-------------------- upstream_model --------------------

Document 1:

You can use the raw model for text infilling.
------------------------------
Document 2:

BART model pre-trained on English language.
------------------------------
Document 3:

BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.

-------------------- parameter_count --------------------

Document 1:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')
model = BartModel.from_pretrained('facebook/bart-base')

-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

You can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset.
------------------------------
Document 2:

The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.
------------------------------
Document 3:

limitation_and_bias: BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).

-------------------- demo --------------------

Document 1:

See the [model hub](https://huggingface.co/models?search=bart) to look for fine-tuned versions on a task that interests you.
------------------------------
Document 2:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart).

-------------------- input_format --------------------

Document 1:

inputs = tokenizer('Hello, my dog is cute', return_tensors='pt')

-------------------- output_format --------------------

Document 1:

outputs.last_hidden_state


[{'datasets': [], 'license': 'apache-2.0', 'github': 'https://github.com/pytorch/fairseq/tree/maste 
r/examples/bart', 'paper': 'https://arxiv.org/abs/1910.13461', 'upstream_model': '', 'parameter_coun 
t': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': 'BART is pa 
rticularly effective when fine-tuned for text generation (e.g. summarization, translation) but also  
works well for comprehension tasks (e.g. text classification, question answering).', 'demo': '', 'in 
put_format': '', 'output_format': ''}]                                                               

#####################sonoisa/sentence-bert-base-ja-mean-tokens-v2########################

-------------------- datasets --------------------

Document 1:

手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。
------------------------------
Document 2:

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。

-------------------- license --------------------

Document 1:

license: cc-by-sa-4.0
------------------------------
Document 2:

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。
model = SentenceBertJapanese(MODEL_NAME)

sentences = ['暴走したAI', '暴走した人工知能']
sentence_embeddings = model.encode(sentences, batch_size=8)

-------------------- github --------------------

Document 1:

[バージョン1](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)よりも良いロス関数である[MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss)を用いて学習した改良版です。
事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
------------------------------
Document 2:

from transformers import BertJapaneseTokenizer, BertModel
import torch


class SentenceBertJapanese:
def __init__(self, model_name_or_path, device=None):
self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)
self.model = BertModel.from_pretrained(model_name_or_path)
self.model.eval()

if device is None:
device = 'cuda' if torch.cuda.is_available() else 'cpu'
self.device = torch.device(device)
self.model.to(device)

def _mean_pooling(self, model_output, attention_mask):
token_embeddings = model_output[0] #First element of model_output contains all token embeddings
input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

@torch.no_grad()
def encode(self, sentences, batch_size=8):
all_embeddings = []
iterator = range(0, len(sentences), batch_size)
for batch_idx in iterator:
batch = sentences[batch_idx:batch_idx + batch_size]

encoded_input = self.tokenizer.batch_encode_plus(batch, padding='longest',
truncation=True, return_tensors='pt').to(self.device)
model_output = self.model(**encoded_input)
sentence_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask']).to('cpu')

all_embeddings.extend(sentence_embeddings)

# return torch.stack(all_embeddings).numpy()
return torch.stack(all_embeddings)


MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。
model = SentenceBertJapanese(MODEL_NAME)

sentences = ['暴走したAI', '暴走した人工知能']
sentence_embeddings = model.encode(sentences, batch_size=8)

print('Sentence embeddings:', sentence_embeddings)

-------------------- paper --------------------

Document 1:

[バージョン1](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)よりも良いロス関数である[MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss)を用いて学習した改良版です。手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
------------------------------
Document 2:

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。
model = SentenceBertJapanese(MODEL_NAME)

sentences = ['暴走したAI', '暴走した人工知能']
sentence_embeddings = model.encode(sentences, batch_size=8)

-------------------- upstream_model --------------------

Document 1:

モデル名を'sonoisa/sentence-bert-base-ja-mean-tokens-v2'に書き換えれば、本モデルを利用した挙動になります。
------------------------------
Document 2:

This is a Japanese sentence-BERT model.  
日本語用Sentence-BERTモデル（バージョン2）です。  
[バージョン1](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)よりも良いロス関数である[MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss)を用いて学習した改良版です。
手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。  
事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
従って、推論の実行にはfugashiとipadicが必要です（pip install fugashi ipadic）。
------------------------------
Document 3:

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。
model = SentenceBertJapanese(MODEL_NAME)

sentences = ['暴走したAI', '暴走した人工知能']
sentence_embeddings = model.encode(sentences, batch_size=8)

-------------------- parameter_count --------------------

Document 1:

parameter_count
------------------------------
Document 2:

model = SentenceBertJapanese(MODEL_NAME)

-------------------- hyper_parameters --------------------

Document 1:

This is a Japanese sentence-BERT model.  
日本語用Sentence-BERTモデル（バージョン2）です。  
[バージョン1](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)よりも良いロス関数である[MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss)を用いて学習した改良版です。
手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。  
事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
従って、推論の実行にはfugashiとipadicが必要です（pip install fugashi ipadic）。
------------------------------
Document 2:

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。

-------------------- evaluation --------------------

Document 1:

手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。

-------------------- hardware --------------------

Document 1:

事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
------------------------------
Document 2:

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。

-------------------- limitation_and_bias --------------------

Document 1:

This is a Japanese sentence-BERT model.  
日本語用Sentence-BERTモデル（バージョン2）です。  
[バージョン1](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)よりも良いロス関数である[MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss)を用いて学習した改良版です。
手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。  
事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
従って、推論の実行にはfugashiとipadicが必要です（pip install fugashi ipadic）。
------------------------------
Document 2:

class SentenceBertJapanese:
def __init__(self, model_name_or_path, device=None):
self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)
self.model = BertModel.from_pretrained(model_name_or_path)
self.model.eval()

def _mean_pooling(self, model_output, attention_mask):
token_embeddings = model_output[0] #First element of model_output contains all token embeddings
input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

@torch.no_grad()
def encode(self, sentences, batch_size=8):
all_embeddings = []
iterator = range(0, len(sentences), batch_size)
for batch_idx in iterator:
batch = sentences[batch_idx:batch_idx + batch_size]

encoded_input = self.tokenizer.batch_encode_plus(batch, padding='longest',
truncation=True, return_tensors='pt').to(self.device)
model_output = self.model(**encoded_input)
sentence_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask']).to('cpu')

all_embeddings.extend(sentence_embeddings)

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。
model = SentenceBertJapanese(MODEL_NAME)

sentences = ['暴走したAI', '暴走した人工知能']
sentence_embeddings = model.encode(sentences, batch_size=8)

-------------------- demo --------------------

Document 1:

https://qiita.com/sonoisa/items/1df94d0a98cd4f209051
------------------------------
Document 2:

[バージョン1](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)よりも良いロス関数である[MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss)を用いて学習した改良版です。
手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。  
事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
従って、推論の実行にはfugashiとipadicが必要です（pip install fugashi ipadic）。
------------------------------
Document 3:

```python
sentences = ['暴走したAI', '暴走した人工知能']
sentence_embeddings = model.encode(sentences, batch_size=8)
```

-------------------- input_format --------------------

Document 1:

事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
従って、推論の実行にはfugashiとipadicが必要です（pip install fugashi ipadic）。
------------------------------
Document 2:

input_format

-------------------- output_format --------------------

Document 1:

This is a Japanese sentence-BERT model.  
日本語用Sentence-BERTモデル（バージョン2）です。  
[バージョン1](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens)よりも良いロス関数である[MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss)を用いて学習した改良版です。
手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。  
事前学習済みモデルとして[cl-tohoku/bert-base-japanese-whole-word-masking](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking)を利用しました。
従って、推論の実行にはfugashiとipadicが必要です（pip install fugashi ipadic）。
------------------------------
Document 2:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

MODEL_NAME = 'sonoisa/sentence-bert-base-ja-mean-tokens-v2'  # <- v2です。


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Unterminated string starting at: line 9 column 16 (char 500) 

#####################facebook/dpr-question_encoder-single-nq-base########################

-------------------- datasets --------------------

Document 1:

This model was trained using the [Natural Questions (NQ) dataset](https://huggingface.co/datasets/nq_open) ([Lee et al., 2019](https://aclanthology.org/P19-1612/); [Kwiatkowski et al., 2019](https://aclanthology.org/Q19-1026/)).

-------------------- license --------------------

Document 1:

license: cc-by-nc-4.0

-------------------- github --------------------

Document 1:

- [Model Details](#model-details)
- [How To Get Started With the Model](#how-to-get-started-with-the-model)
- [Uses](#uses)
- [Risks, Limitations and Biases](#risks-limitations-and-biases)
- [Training](#training)
- [Evaluation](#evaluation-results)
- [Environmental Impact](#environmental-impact)
- [Technical Specifications](#technical-specifications)
- [Citation Information](#citation-information)
- [Model Card Authors](#model-card-authors)

-------------------- paper --------------------

Document 1:

See the [associated paper](https://arxiv.org/abs/2004.04906) for details on the modeling architecture, objective, compute infrastructure, and training details.
------------------------------
Document 2:

[Citation Information](#citation-information)

-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

|      | Top 20 |           |    |      |       | Top 100|           |    |      |       |
|:----:|:------:|:---------:|:--:|:----:|:-----:|:------:|:---------:|:--:|:----:|:-----:|
|      | NQ     |  TriviaQA | WQ | TREC | SQuAD | NQ     |  TriviaQA | WQ | TREC | SQuAD |
|      | 78.4   |  79.4     |73.2| 79.8 | 63.2  | 85.4   |  85.0     |81.4| 89.1 | 77.2  |

-------------------- evaluation --------------------

Document 1:

[Evaluation](#evaluation-results)
------------------------------
Document 2:

The model developers report the performance of the model on five QA datasets, using the top-k accuracy (k ∈ {20, 100}). The datasets were [NQ](https://huggingface.co/datasets/nq_open), [TriviaQA](https://huggingface.co/datasets/trivia_qa), [WebQuestions (WQ)](https://huggingface.co/datasets/web_questions), [CuratedTREC (TREC)](https://huggingface.co/datasets/trec), and [SQuAD v1.1](https://huggingface.co/datasets/squad).  
|      | Top 20 |           |    |      |       | Top 100|           |    |      |       |
|:----:|:------:|:---------:|:--:|:----:|:-----:|:------:|:---------:|:--:|:----:|:-----:|
|      | NQ     |  TriviaQA | WQ | TREC | SQuAD | NQ     |  TriviaQA | WQ | TREC | SQuAD |
|      | 78.4   |  79.4     |73.2| 79.8 | 63.2  | 85.4   |  85.0     |81.4| 89.1 | 77.2  |

-------------------- hardware --------------------

Document 1:

compute infrastructure, and training details.
------------------------------
Document 2:

Hardware Type: 8 32GB GPUs

-------------------- limitation_and_bias --------------------

Document 1:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al., 2021](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al., 2021](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.
------------------------------
Document 2:

[Risks, Limitations and Biases](#risks-limitations-and-biases)

-------------------- demo --------------------

Document 1:

See the [associated paper](https://arxiv.org/abs/2004.04906) for details on the modeling architecture, objective, compute infrastructure, and training details.
------------------------------
Document 2:

Use the code below to get started with the model.  
```python
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')
model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')
input_ids = tokenizer('Hello, is my dog cute ?', return_tensors='pt')['input_ids']
embeddings = model(input_ids).pooler_output
```

-------------------- input_format --------------------

Document 1:

The training procedure is described in the [associated paper](https://arxiv.org/pdf/2004.04906.pdf):  
> Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve efficiently the top k passages relevant to the input question for the reader at run-time.  
> Our dense passage retriever (DPR) uses a dense encoder EP(·) which maps any text passage to a d- dimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval. At run-time, DPR applies a different encoder EQ(·) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector.  
The authors report that for encoders, they used two independent BERT ([Devlin et al., 2019](https://aclanthology.org/N19-1423/)) networks (base, un-cased) and use FAISS ([Johnson et al., 2017](https://arxiv.org/abs/1702.08734)) during inference time to encode and index passages. See the paper for further details on training, including encoders, inference, positive and negative passages, and in-batch negatives.

-------------------- output_format --------------------

Document 1:

The model developers report the performance of the model on five QA datasets, using the top-k accuracy (k ∈ {20, 100}). The datasets were [NQ](https://huggingface.co/datasets/nq_open), [TriviaQA](https://huggingface.co/datasets/trivia_qa), [WebQuestions (WQ)](https://huggingface.co/datasets/web_questions), [CuratedTREC (TREC)](https://huggingface.co/datasets/trec), and [SQuAD v1.1](https://huggingface.co/datasets/squad).


[{'datasets': ['Natural Questions (NQ)'], 'license': 'cc-by-nc-4.0', 'github': '', 'paper': 'https: 
//arxiv.org/abs/2004.04906', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {'epoc 
hs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '',  
'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]                     

#####################cointegrated/LaBSE-en-ru########################

-------------------- datasets --------------------

Document 1:

This is a truncated version of [sentence-transformers/LaBSE](https://huggingface.co/sentence-transformers/LaBSE), which is, in turn, a port of [LaBSE](https://tfhub.dev/google/LaBSE/1) by Google.
------------------------------
Document 2:

[https://tfhub.dev/google/LaBSE/1](https://tfhub.dev/google/LaBSE/1)

-------------------- license --------------------

Document 1:

License: [https://tfhub.dev/google/LaBSE/1](https://tfhub.dev/google/LaBSE/1)
------------------------------
Document 2:

This is a truncated version of [sentence-transformers/LaBSE](https://huggingface.co/sentence-transformers/LaBSE), which is, in turn, a port of [LaBSE](https://tfhub.dev/google/LaBSE/1) by Google.  
The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.

-------------------- github --------------------

Document 1:

[https://tfhub.dev/google/LaBSE/1](https://tfhub.dev/google/LaBSE/1)
------------------------------
Document 2:

- [sentence-transformers/LaBSE](https://huggingface.co/sentence-transformers/LaBSE)
- [LaBSE](https://tfhub.dev/google/LaBSE/1)
- [cointegrated/LaBSE-en-ru](https://huggingface.co/cointegrated/LaBSE-en-ru)
- [this notebook](https://colab.research.google.com/drive/1dnPRn0-ugj3vZgSpyCC9sgslM2SuSfHy?usp=sharing)
- [EIStakovskii/LaBSE-fr-de](https://huggingface.co/EIStakovskii/LaBSE-fr-de)

-------------------- paper --------------------

Document 1:

[Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852)
------------------------------
Document 2:

The model has been truncated in [this notebook](https://colab.research.google.com/drive/1dnPRn0-ugj3vZgSpyCC9sgslM2SuSfHy?usp=sharing).

-------------------- upstream_model --------------------

Document 1:

This is a truncated version of [sentence-transformers/LaBSE](https://huggingface.co/sentence-transformers/LaBSE), which is, in turn, a port of [LaBSE](https://tfhub.dev/google/LaBSE/1) by Google.  
The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.
------------------------------
Document 2:

[https://tfhub.dev/google/LaBSE/1](https://tfhub.dev/google/LaBSE/1)

-------------------- parameter_count --------------------

Document 1:

The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.

-------------------- hyper_parameters --------------------

Document 1:

The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.
------------------------------
Document 2:

hyper_parameters

-------------------- evaluation --------------------

Document 1:

The model has been truncated in [this notebook](https://colab.research.google.com/drive/1dnPRn0-ugj3vZgSpyCC9sgslM2SuSfHy?usp=sharing).

-------------------- hardware --------------------

Document 1:

The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.

-------------------- limitation_and_bias --------------------

Document 1:

The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.

-------------------- demo --------------------

Document 1:

To get the sentence embeddings, you can  use the following code:
```python
import torch
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')
model = AutoModel.from_pretrained('cointegrated/LaBSE-en-ru')
sentences = ['Hello World', 'Привет Мир']
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=64, return_tensors='pt')
with torch.no_grad():
model_output = model(**encoded_input)
embeddings = model_output.pooler_output
embeddings = torch.nn.functional.normalize(embeddings)
print(embeddings)
```
------------------------------
Document 2:

[Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852). July 2020  
License: [https://tfhub.dev/google/LaBSE/1](https://tfhub.dev/google/LaBSE/1)

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

To get the sentence embeddings, you can  use the following code:
```python
import torch
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')
model = AutoModel.from_pretrained('cointegrated/LaBSE-en-ru')
sentences = ['Hello World', 'Привет Мир']
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=64, return_tensors='pt')
with torch.no_grad():
model_output = model(**encoded_input)
embeddings = model_output.pooler_output
embeddings = torch.nn.functional.normalize(embeddings)
print(embeddings)
```

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.
------------------------------
Document 3:

output_format

-------------------- max_sequence_length --------------------

Document 1:

The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.
------------------------------
Document 2:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

The current model has only English and Russian tokens left in the vocabulary.
Thus, the vocabulary is 10% of the original, and number of parameters in the whole model is 27% of the original, without any loss in the quality of English and Russian embeddings.


[{'datasets': ['LaBSE'], 'license': 'https://tfhub.dev/google/LaBSE/1', 'github': 'https://huggingf 
ace.co/sentence-transformers/LaBSE', 'paper': 'https://arxiv.org/abs/2007.01852', 'upstream_model':  
'https://tfhub.dev/google/LaBSE/1', 'parameter_count': '27%', 'hyper_parameters': {}, 'evaluation':  
[], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '',  
'max_sequence_length': '', 'vocabulary_size': ''}]                                                   

#####################Linaqruf/anything-v3.0########################

-------------------- datasets --------------------



-------------------- license --------------------

Document 1:

license: creativeml-openrail-m

-------------------- github --------------------



-------------------- paper --------------------



-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------



-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

output_format


[{'datasets': [], 'license': 'creativeml-openrail-m', 'github': '', 'paper': '', 'upstream_model':  
'', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 
 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_f 
ormat': '', 'output_format': 'output_format'}]                                                       

#####################DeepFloyd/IF-II-L-v1.0########################


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 401 Client Error. (Request ID: Root=1-6545d811-58a2d6dd22d0af3b5caba9eb)

Cannot access gated repo for url https://huggingface.co/api/models/DeepFloyd/IF-II-L-v1.0.
Repo model DeepFloyd/IF-II-L-v1.0 is gated. You must be authenticated to access it. 

#####################prompthero/openjourney-v4########################

-------------------- datasets --------------------

Document 1:

- [Lora version](https://huggingface.co/prompthero/openjourney-lora)
- [Openjourney Dreambooth](https://huggingface.co/prompthero/openjourney)

-------------------- license --------------------

Document 1:

license: creativeml-openrail-m

-------------------- github --------------------

Document 1:

- [Lora version](https://huggingface.co/prompthero/openjourney-lora)
- [Openjourney Dreambooth](https://huggingface.co/prompthero/openjourney)
------------------------------
Document 2:

[Join our course](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)

-------------------- paper --------------------



-------------------- upstream_model --------------------

Document 1:

[Lora version](https://huggingface.co/prompthero/openjourney-lora)

-------------------- parameter_count --------------------

Document 1:

Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours.

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

- [Lora version](https://huggingface.co/prompthero/openjourney-lora)
- [Openjourney Dreambooth](https://huggingface.co/prompthero/openjourney)
------------------------------
Document 2:

[Join our course](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)

-------------------- input_format --------------------



-------------------- output_format --------------------




[{'datasets': ['Lora version', 'Openjourney Dreambooth'], 'license': 'creativeml-openrail-m', 'gith 
ub': 'https://huggingface.co/prompthero/openjourney-lora', 'paper': '', 'upstream_model': 'Lora vers 
ion', 'parameter_count': 'Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epoc 
hs +32 training hours.', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_b 
ias': '', 'demo': 'Document 1:\n\n- [Lora version](https://huggingface.co/prompthero/openjourney-lor 
a)\n- [Openjourney Dreambooth](https://huggingface.co/prompthero/openjourney)', 'input_format': '',  
'output_format': ''}]                                                                                

#####################iZELX1/Anything-V3-X########################

-------------------- datasets --------------------



-------------------- license --------------------

Document 1:

license: creativeml-openrail-m

-------------------- github --------------------



-------------------- paper --------------------



-------------------- upstream_model --------------------

Document 1:

upstream_model

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

hyper_parameters

-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------



-------------------- input_format --------------------

Document 1:

input_format

-------------------- output_format --------------------

Document 1:

output_format


[{'datasets': [], 'license': 'creativeml-openrail-m', 'github': '', 'paper': '', 'upstream_model':  
'', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and 
_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]                                    

#####################microsoft/trocr-large-printed########################

-------------------- datasets --------------------

Document 1:

[model hub](https://huggingface.co/models?search=microsoft/trocr)
------------------------------
Document 2:

TrOCR model fine-tuned on the [SROIE dataset](https://rrc.cvc.uab.es/?ch=13).
------------------------------
Document 3:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.

-------------------- license --------------------

Document 1:

license
------------------------------
Document 2:

TrOCR model fine-tuned on the [SROIE dataset](https://rrc.cvc.uab.es/?ch=13).

-------------------- github --------------------

Document 1:

[model hub](https://huggingface.co/models?search=microsoft/trocr)
------------------------------
Document 2:

- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X00016469612_1.jpg
example_title: Printed 1
- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005255805_7.jpg
example_title: Printed 2
- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005745214_6.jpg
example_title: Printed 3

-------------------- paper --------------------

Document 1:

@misc{li2021trocr,
title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},
author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},
year={2021},
eprint={2109.10282},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
------------------------------
Document 2:

TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models

-------------------- upstream_model --------------------

Document 1:

The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.
------------------------------
Document 2:

You can use the raw model for optical character recognition (OCR) on single text-line images.
------------------------------
Document 3:

TrOCR model

-------------------- parameter_count --------------------

Document 1:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder.
------------------------------
Document 2:

parameter_count

-------------------- hyper_parameters --------------------

Document 1:

processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')
model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')
------------------------------
Document 2:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.

-------------------- evaluation --------------------

Document 1:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.

-------------------- hardware --------------------

Document 1:

The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.

-------------------- limitation_and_bias --------------------

Document 1:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.
------------------------------
Document 2:

The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.

-------------------- demo --------------------

Document 1:

You can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you.
------------------------------
Document 2:

- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X00016469612_1.jpg
example_title: Printed 1
- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005255805_7.jpg
example_title: Printed 2
- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005745214_6.jpg
example_title: Printed 3

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded.

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.


[{'datasets': ['SROIE dataset'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'p 
arameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'opti 
mizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format' 
: '', 'output_format': ''}]                                                                          

#####################timm/mobilenetv3_large_100.miil_in21k_ft_in1k########################

-------------------- datasets --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

ImageNet-21k-P and ImageNet-1k
------------------------------
Document 3:

datasets:
- imagenet-1k
- imagenet-21k-p

-------------------- license --------------------

Document 1:

license: apache-2.0

-------------------- github --------------------

Document 1:

@misc{rw2019timm,
author = {Ross Wightman},
title = {PyTorch Image Models},
year = {2019},
publisher = {GitHub},
journal = {GitHub repository},
doi = {10.5281/zenodo.4414861},
howpublished = {\url{https://github.com/huggingface/pytorch-image-models}}
}
------------------------------
Document 2:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)

-------------------- paper --------------------

Document 1:

@inproceedings{howard2019searching,
title={Searching for mobilenetv3},
author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
pages={1314--1324},
year={2019}
}
------------------------------
Document 2:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)

-------------------- upstream_model --------------------

Document 1:

Pretrain Dataset: ImageNet-21k-P

-------------------- parameter_count --------------------

Document 1:

Params (M): 5.5

-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------



-------------------- demo --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)

-------------------- input_format --------------------

Document 1:

input_format

-------------------- output_format --------------------

Document 1:

output_format

-------------------- input_preprocessing --------------------

Document 1:

- **Pretrain Dataset:** ImageNet-21k-P

-------------------- input_size --------------------

Document 1:

Image size: 224 x 224
------------------------------
Document 2:

data_config = timm.data.resolve_model_data_config(model)


[{'datasets': ['imagenet-1k', 'imagenet-21k-p'], 'license': 'apache-2.0', 'github': '@misc{rw2019ti 
mm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub 
},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://g 
ithub.com/huggingface/pytorch-image-models}}\n}', 'paper': '@inproceedings{howard2019searching,\ntit 
le={Searching for mobilenetv3},\nauthor={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, L 
iang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasu 
devan, Vijay and others},\nbooktitle={Proceedings of the IEEE/CVF international conference on comput 
er vision},\npages={1314--1324},\nyear={2019}\n}', 'upstream_model': 'Pretrain Dataset: ImageNet-21k 
-P', 'parameter_count': 'Params (M): 5.5', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 
 'limitation_and_bias': '', 'demo': '[model results](https://github.com/huggingface/pytorch-image-mo 
dels/tree/main/results)', 'input_format': 'input_format', 'output_format': 'output_format', 'input_p 
reprocessing': '- **Pretrain Dataset:** ImageNet-21k-P', 'input_size': 'Image size: 224 x 224'}]     

#####################timm/convmixer_768_32.in1k########################

-------------------- datasets --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

ImageNet-1k
------------------------------
Document 3:

datasets:
- imagenet-1k

-------------------- license --------------------

Document 1:

license: mit
------------------------------
Document 2:

@article{Chen2021CrossViTCM,
title={CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification},
author={Chun-Fu Chen and Quanfu Fan and Rameswar Panda},
journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
year={2021},
pages={347-356}
}

-------------------- github --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

github repositories

-------------------- paper --------------------

Document 1:

paper authors
------------------------------
Document 2:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 3:

@article{Chen2021CrossViTCM,
title={CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification},
author={Chun-Fu Chen and Quanfu Fan and Rameswar Panda},
journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
year={2021},
pages={347-356}
}

-------------------- upstream_model --------------------

Document 1:

- **Original:** https://github.com/locuslab/convmixer

-------------------- parameter_count --------------------

Document 1:

Params (M): 21.1

-------------------- hyper_parameters --------------------

Document 1:

Params (M): 21.1
GMACs: 19.5
Activations (M): 26.0

-------------------- evaluation --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

Model Type: Image classification / feature backbone
Model Stats:
Params (M): 21.1
GMACs: 19.5
Activations (M): 26.0
Image size: 224 x 224
Dataset: ImageNet-1k
Original: https://github.com/locuslab/convmixer

-------------------- demo --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)

-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

output_format

-------------------- input_preprocessing --------------------



-------------------- input_size --------------------

Document 1:

Image size: 224 x 224
------------------------------
Document 2:

data_config = timm.data.resolve_model_data_config(model)


[{'datasets': ['imagenet-1k'], 'license': 'mit', 'github': 'https://github.com/huggingface/pytorch- 
image-models/tree/main/results', 'paper': '@article{Chen2021CrossViTCM,\ntitle={CrossViT: Cross-Atte 
ntion Multi-Scale Vision Transformer for Image Classification},\nauthor={Chun-Fu Chen and Quanfu Fan 
 and Rameswar Panda},\njournal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n 
year={2021},\npages={347-356}\n}', 'upstream_model': 'https://github.com/locuslab/convmixer', 'param 
eter_count': '21.1', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'opti 
mizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': 'Model Type: Image classificat 
ion / feature backbone\nModel Stats:\nParams (M): 21.1\nGMACs: 19.5\nActivations (M): 26.0\nImage si 
ze: 224 x 224\nDataset: ImageNet-1k\nOriginal: https://github.com/locuslab/convmixer', 'demo': 'http 
s://github.com/huggingface/pytorch-image-models/tree/main/results', 'input_format': '', 'output_form 
at': 'output_format', 'input_preprocessing': '', 'input_size': 'Image size: 224 x 224'}]             

#####################timm/efficientnet_b0.ra_in1k########################

-------------------- datasets --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

datasets:
- imagenet-1k

-------------------- license --------------------

Document 1:

license: apache-2.0
------------------------------
Document 2:

@misc{rw2019timm,
author = {Ross Wightman},
title = {PyTorch Image Models},
year = {2019},
publisher = {GitHub},
journal = {GitHub repository},
doi = {10.5281/zenodo.4414861},
howpublished = {\url{https://github.com/huggingface/pytorch-image-models}}
}

-------------------- github --------------------

Document 1:

@misc{rw2019timm,
author = {Ross Wightman},
title = {PyTorch Image Models},
year = {2019},
publisher = {GitHub},
journal = {GitHub repository},
doi = {10.5281/zenodo.4414861},
howpublished = {\url{https://github.com/huggingface/pytorch-image-models}}
}
------------------------------
Document 2:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 3:

github

-------------------- paper --------------------

Document 1:

@inproceedings{tan2019efficientnet,
title={Efficientnet: Rethinking model scaling for convolutional neural networks},
author={Tan, Mingxing and Le, Quoc},
booktitle={International conference on machine learning},
pages={6105--6114},
year={2019},
organization={PMLR}
}
------------------------------
Document 2:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 3:

EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946

-------------------- upstream_model --------------------

Document 1:

- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946
- ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476
------------------------------
Document 2:

@inproceedings{wightman2021resnet,
title={ResNet strikes back: An improved training procedure in timm},
author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},
booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}
}

-------------------- parameter_count --------------------

Document 1:

Params (M): 5.3

-------------------- hyper_parameters --------------------

Document 1:

- Params (M): 5.3
- GMACs: 0.4
- Activations (M): 6.7
------------------------------
Document 2:

RandAugment `RA` recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as `B` recipe in [ResNet Strikes Back](https://arxiv.org/abs/2110.00476).
RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging
Step (exponential decay w/ staircase) LR schedule with warmup

-------------------- evaluation --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)

-------------------- hardware --------------------

Document 1:

Trained on ImageNet-1k in `timm` using recipe template described below.

-------------------- limitation_and_bias --------------------

Document 1:

- **Model Type:** Image classification / feature backbone
- **Model Stats:**
- Params (M): 5.3
- GMACs: 0.4
- Activations (M): 6.7
- Image size: 224 x 224
- **Papers:**
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks: https://arxiv.org/abs/1905.11946
- ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476
- **Dataset:** ImageNet-1k
- **Original:** https://github.com/huggingface/pytorch-image-models

-------------------- demo --------------------

Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)

-------------------- input_format --------------------

Document 1:

- **Dataset:** ImageNet-1k

-------------------- output_format --------------------

Document 1:

output_format

-------------------- input_preprocessing --------------------



-------------------- input_size --------------------

Document 1:

Image size: 224 x 224
------------------------------
Document 2:

data_config = timm.data.resolve_model_data_config(model)
------------------------------
Document 3:

data_config = timm.data.resolve_model_data_config(model)


[{'datasets': ['imagenet-1k'], 'license': 'apache-2.0', 'github': 'https://github.com/huggingface/p 
ytorch-image-models', 'paper': 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Netw 
orks: https://arxiv.org/abs/1905.11946', 'upstream_model': 'EfficientNet: Rethinking Model Scaling f 
or Convolutional Neural Networks: https://arxiv.org/abs/1905.11946', 'parameter_count': '5.3', 'hype 
r_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': 
 [], 'hardware': 'Trained on ImageNet-1k', 'limitation_and_bias': '- **Model Type:** Image classific 
ation / feature backbone\n- **Model Stats:**\n- Params (M): 5.3\n- GMACs: 0.4\n- Activations (M): 6. 
7\n- Image size: 224 x 224\n- **Papers:**\n- EfficientNet: Rethinking Model Scaling for Convolutiona 
l Neural Networks: https://arxiv.org/abs/1905.11946\n- ResNet strikes back: An improved training pro 
cedure in timm: https://arxiv.org/abs/2110.00476\n- **Dataset:** ImageNet-1k\n- **Original:** https: 
//github.com/huggingface/pytorch-image-models', 'demo': '[model results](https://github.com/huggingf 
ace/pytorch-image-models/tree/main/results)', 'input_format': '- **Dataset:** ImageNet-1k', 'output_ 
format': 'output_format', 'input_preprocessing': '', 'input_size': 'Image size: 224 x 224'}]         

#####################timm/convnext_base.fb_in22k_ft_in1k########################


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens. However, your messages resulted in 4239 tokens. Please reduce the length of the messages. 

#####################timm/resnet50.a1_in1k########################


ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens. However, your messages resulted in 15822 tokens. Please reduce the length of the messages. 

#####################TahaDouaji/detr-doc-table-detection########################

-------------------- datasets --------------------

Document 1:

The model was trained on ICDAR2019 Table Dataset

-------------------- license --------------------



-------------------- github --------------------

Document 1:

url       = {https://arxiv.org/abs/2005.12872},
biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}

-------------------- paper --------------------



-------------------- upstream_model --------------------



-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.
------------------------------
Document 2:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.
------------------------------
Document 3:

The model should not be used to intentionally create hostile or alienating environments for people.

-------------------- demo --------------------



-------------------- input_format --------------------



-------------------- output_format --------------------



-------------------- input_preprocessing --------------------

Document 1:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).

-------------------- input_size --------------------

Document 1:

inputs = processor(images=image, return_tensors='pt')


[{'datasets': ['ICDAR2019 Table Dataset'], 'github': 'https://arxiv.org/abs/2005.12872', 'limitatio 
n_and_bias': 'Users (both direct and downstream) should be made aware of the risks, biases and limit 
ations of the model.', 'input_preprocessing': 'Significant research has explored bias and fairness i 
ssues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long. 
330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).', 'input_s 
ize': "inputs = processor(images=image, return_tensors='pt')"}]                                      

#####################CIDAS/clipseg-rd64-refined########################

-------------------- datasets --------------------

Document 1:

Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg).
------------------------------
Document 2:

[Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Lüddecke et al.

-------------------- license --------------------

Document 1:

Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg).
------------------------------
Document 2:

license: apache-2.0

-------------------- github --------------------

Document 1:

Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg).
------------------------------
Document 2:

[this repository](https://github.com/timojl/clipseg)

-------------------- paper --------------------

Document 1:

Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg).
------------------------------
Document 2:

It was introduced in the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Lüddecke et al.

-------------------- upstream_model --------------------

Document 1:

upstream_model

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------



-------------------- evaluation --------------------



-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg).
------------------------------
Document 2:

limitation_and_bias

-------------------- demo --------------------

Document 1:

Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg).
------------------------------
Document 2:

[this repository](https://github.com/timojl/clipseg)

-------------------- input_format --------------------



-------------------- output_format --------------------

Document 1:

output_format

-------------------- input_preprocessing --------------------



-------------------- input_size --------------------

Document 1:

input_size
------------------------------
Document 2:

input_size


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'e 
valuation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_f 
ormat': '', 'input_preprocessing': '', 'input_size': ''}]                                            

#####################DionTimmer/controlnet_qrcode-control_v1p_sd15########################

-------------------- datasets --------------------

Document 1:

this 1.5 version model was also trained on the same dataset

-------------------- license --------------------

Document 1:

license: openrail++

-------------------- github --------------------

Document 1:

This repo holds the safetensors & diffusers versions of the QR code conditioned ControlNet for Stable Diffusion v1.5.
------------------------------
Document 2:

https://github.com/Mikubill/sd-webui-controlnet

-------------------- paper --------------------

Document 1:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).

-------------------- upstream_model --------------------

Document 1:

The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application.
For usage in auto1111 they can be placed in the webui/models/ControlNet folder.
------------------------------
Document 2:

pipeline_tag: image-to-image
------------------------------
Document 3:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).
------------------------------
Document 2:

guidance_scale=20,
controlnet_conditioning_scale=1.5,

-------------------- evaluation --------------------

Document 1:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).
------------------------------
Document 2:

![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%). The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.
------------------------------
Document 2:

This 1.5 version model was also trained on the same dataset for those who are using the older version.

-------------------- demo --------------------

Document 1:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).
------------------------------
Document 2:

The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application.
For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet).

-------------------- input_format --------------------

Document 1:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
------------------------------
Document 2:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).

-------------------- output_format --------------------

Document 1:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).
------------------------------
Document 2:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
------------------------------
Document 3:

output_format

-------------------- input_preprocessing --------------------

Document 1:

No pre-processor is needed, though you can use the invert pre-processor for a different variation of results.
------------------------------
Document 2:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).
------------------------------
Document 3:

pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
'runwayml/stable-diffusion-v1-5',
controlnet=controlnet,
safety_checker=None,
torch_dtype=torch.float16
)

pipe.enable_xformers_memory_efficient_attention()
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

def resize_for_condition_image(input_image: Image, resolution: int):
input_image = input_image.convert('RGB')
W, H = input_image.size
k = float(resolution) / min(H, W)
H *= k
W *= k
H = int(round(H / 64.0)) * 64
W = int(round(W / 64.0)) * 64
img = input_image.resize((W, H), resample=Image.LANCZOS)
return img

source_image = load_image('https://s3.amazonaws.com/moonup/production/uploads/6064e095abd8d3692e3e2ed6/A_RqHaAM6YHBodPLwqtjn.png')
init_image = load_image('https://s3.amazonaws.com/moonup/production/uploads/noauth/KfMBABpOwIuNolv1pe3qX.jpeg')
condition_image = resize_for_condition_image(source_image, 768)
init_image = resize_for_condition_image(init_image, 768)
generator = torch.manual_seed(123121231)
image = pipe(prompt='a bilboard in NYC with a qrcode',
negative_prompt='ugly, disfigured, low quality, blurry, nsfw',
image=init_image,
control_image=condition_image,
width=768,
height=768,
guidance_scale=20,
controlnet_conditioning_scale=1.5,
generator=generator,
strength=0.9,
num_inference_steps=150,
)

-------------------- input_size --------------------

Document 1:

To optimize for scanning, please generate your QR codes with correction mode 'H' (30%). For the best results, it is recommended to generate your artwork at a resolution of 768.
------------------------------
Document 2:

input_image = load_image('https://s3.amazonaws.com/moonup/production/uploads/6064e095abd8d3692e3e2ed6/A_RqHaAM6YHBodPLwqtjn.png')
condition_image = resize_for_condition_image(source_image, 768)
init_image = resize_for_condition_image(init_image, 768)
width=768,
height=768,
------------------------------
Document 3:

768 is the preferred resolution for generation since it allows for more detail.


[{'datasets': ['same dataset'], 'license': 'openrail++', 'github': 'https://github.com/Mikubill/sd- 
webui-controlnet', 'paper': '', 'upstream_model': 'image-to-image', 'parameter_count': '', 'hyper_pa 
rameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 
 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'in 
put_preprocessing': '', 'input_size': ''}]                                                           

#####################MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli########################

-------------------- datasets --------------------

Document 1:

Please consult the original DeBERTa-v3 paper and literature on different NLI datasets for more information on the training data and potential biases.
------------------------------
Document 2:

https://osf.io/74b8k
------------------------------
Document 3:

DeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained on the [MultiNLI](https://huggingface.co/datasets/multi_nli), [Fever-NLI](https://github.com/easonnie/combine-FEVER-NSMN/blob/master/other_resources/nli_fever.md), Adversarial-NLI ([ANLI](https://huggingface.co/datasets/anli)), [LingNLI](https://arxiv.org/pdf/2104.07179.pdf) and [WANLI](https://huggingface.co/datasets/alisawuffles/WANLI) datasets.

-------------------- license --------------------

Document 1:

license: mit

-------------------- github --------------------

Document 1:

- name: DeBERTa-v3-large-mnli-fever-anli-ling-wanli
- name: WANLI
type: alisawuffles/WANLI

-------------------- paper --------------------

Document 1:

Please consult the original DeBERTa-v3 paper
------------------------------
Document 2:

Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. ‘Less Annotating, More Classifying – Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI’. Preprint, June. Open Science Framework. https://osf.io/74b8k.
------------------------------
Document 3:

The foundation model is [DeBERTa-v3-large from Microsoft](https://huggingface.co/microsoft/deberta-v3-large). DeBERTa-v3 combines several recent innovations compared to classical Masked Language Models like BERT, RoBERTa etc., see the [paper](https://arxiv.org/abs/2111.09543)

-------------------- upstream_model --------------------

Document 1:

Please consult the original DeBERTa-v3 paper and literature on different NLI datasets for more information on the training data and potential biases.
------------------------------
Document 2:

The foundation model is [DeBERTa-v3-large from Microsoft](https://huggingface.co/microsoft/deberta-v3-large).

-------------------- parameter_count --------------------



-------------------- hyper_parameters --------------------

Document 1:

```
num_train_epochs=4,              # total number of training epochs
learning_rate=5e-06,
per_device_train_batch_size=16,   # batch size per device during training
gradient_accumulation_steps=2,    # doubles the effective batch_size to 32, while decreasing memory requirements
per_device_eval_batch_size=64,    # batch size for evaluation
warmup_ratio=0.06,                # number of warmup steps for learning rate scheduler
weight_decay=0.01,               # strength of weight decay
fp16=True                        # mixed precision training
```

-------------------- evaluation --------------------

Document 1:

The metric used is accuracy.
The model achieves state-of-the-art performance on each dataset.
|Datasets|mnli_test_m|mnli_test_mm|anli_test|anli_test_r3|ling_test|wanli_test|
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|Accuracy|0.912|0.908|0.702|0.64|0.87|0.77|

-------------------- hardware --------------------

Document 1:

DeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained using the Hugging Face trainer with the following hyperparameters.

-------------------- limitation_and_bias --------------------

Document 1:

Please consult the original DeBERTa-v3 paper and literature on different NLI datasets for more information on the training data and potential biases. The model will reproduce statistical patterns in the training data.
------------------------------
Document 2:

This model was fine-tuned on the [MultiNLI](https://huggingface.co/datasets/multi_nli), [Fever-NLI](https://github.com/easonnie/combine-FEVER-NSMN/blob/master/other_resources/nli_fever.md), Adversarial-NLI ([ANLI](https://huggingface.co/datasets/anli)), [LingNLI](https://arxiv.org/pdf/2104.07179.pdf) and [WANLI](https://huggingface.co/datasets/alisawuffles/WANLI) datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the [ANLI benchmark](https://github.com/facebookresearch/anli).

-------------------- demo --------------------

Document 1:

older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers>=4.13 might solve some issues.

-------------------- input_format --------------------

Document 1:

input_format

-------------------- output_format --------------------

Document 1:

output_format

-------------------- max_sequence_length --------------------

Document 1:

max_sequence_length

-------------------- vocabulary_size --------------------

Document 1:

vocabulary_size


[{'datasets': ['MultiNLI', 'Fever-NLI', 'ANLI', 'LingNLI', 'WANLI'], 'license': 'mit', 'github': 'h 
ttps://github.com/alisawuffles/WANLI', 'paper': 'https://arxiv.org/abs/2111.09543', 'upstream_model' 
: 'microsoft/deberta-v3-large', 'parameter_count': '', 'hyper_parameters': {'epochs': '4', 'batch_si 
ze': '16', 'learning_rate': '5e-06', 'optimizer': '', 'warmup_ratio': '0.06', 'weight_decay': '0.01' 
, 'fp16': 'True'}, 'evaluation': [{'test': 'mnli_test_m', 'result': '0.912'}, {'test': 'mnli_test_mm 
', 'result': '0.908'}, {'test': 'anli_test', 'result': '0.702'}, {'test': 'anli_test_r3', 'result':  
'0.64'}, {'test': 'ling_test', 'result': '0.87'}, {'test': 'wanli_test', 'result': '0.77'}], 'hardwa 
re': '', 'limitation_and_bias': 'Please consult the original DeBERTa-v3 paper and literature on diff 
erent NLI datasets for more information on the training data and potential biases. The model will re 
produce statistical patterns in the training data.', 'demo': 'older versions of HF Transformers seem 
 to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformer 
s>=4.13 might solve some issues.', 'input_format': 'input_format', 'output_format': 'output_format', 
 'max_sequence_length': 'max_sequence_length', 'vocabulary_size': 'vocabulary_size'}]                

#####################openai/whisper-base########################

-------------------- datasets --------------------

Document 1:

The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.
------------------------------
Document 2:

The blog post [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data.
------------------------------
Document 3:

- dataset:
name: LibriSpeech (clean)
type: librispeech_asr
config: clean
split: test

- dataset:
name: LibriSpeech (other)
type: librispeech_asr
config: other
split: test

- dataset:
name: Common Voice 11.0
type: mozilla-foundation/common_voice_11_0
config: hi
split: test

-------------------- license --------------------

Document 1:

copyright = {arXiv.org perpetual, non-exclusive license}
------------------------------
Document 2:

license: apache-2.0

-------------------- github --------------------

Document 1:

- example_title: Librispeech sample 1
src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- example_title: Librispeech sample 2
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac

-------------------- paper --------------------

Document 1:

As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.
------------------------------
Document 2:

Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf).
------------------------------
Document 3:

@misc{radford2022whisper,
doi = {10.48550/ARXIV.2212.04356},
url = {https://arxiv.org/abs/2212.04356},
author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
title = {Robust Speech Recognition via Large-Scale Weak Supervision},
publisher = {arXiv},
year = {2022},
copyright = {arXiv.org perpetual, non-exclusive license}
}

-------------------- upstream_model --------------------

Document 1:

Our studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level.  
However, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.  
Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria.
------------------------------
Document 2:

The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas.
------------------------------
Document 3:

As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.

-------------------- parameter_count --------------------

Document 1:

| Size     | Parameters | English-only                                         | Multilingual                                        |
|----------|------------|------------------------------------------------------|-----------------------------------------------------|
| tiny     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny)     |
| base     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)     |
| small    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)    |
| medium   | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium)   |
| large    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)    |
| large-v2 | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large-v2) |

-------------------- hyper_parameters --------------------

Document 1:

| Size     | Parameters | English-only                                         | Multilingual                                        |
|----------|------------|------------------------------------------------------|-----------------------------------------------------|
| tiny     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny)     |
| base     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)     |
| small    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)    |
| medium   | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium)   |
| large    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)    |
| large-v2 | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large-v2) |

-------------------- evaluation --------------------

Document 1:

As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.
------------------------------
Document 2:

Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf).
------------------------------
Document 3:

The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages.

-------------------- hardware --------------------



-------------------- limitation_and_bias --------------------

Document 1:

However, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.  
Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria.  
In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.
------------------------------
Document 2:

The limitations and biases of the model are not explicitly mentioned in the given context.
------------------------------
Document 3:

As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.

-------------------- demo --------------------

Document 1:

As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.
------------------------------
Document 2:

The blog post [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data.

-------------------- input_format --------------------

Document 1:

input_format
------------------------------
Document 2:

The format of the data used as input for the model is not mentioned in the given context.
------------------------------
Document 3:

input_format

-------------------- output_format --------------------

Document 1:

output_format
------------------------------
Document 2:

output_format

-------------------- sample_rate --------------------

Document 1:

Chunking is enabled by setting `chunk_length_s=30` when instantiating the pipeline.


[{'datasets': ['LibriSpeech (clean)', 'LibriSpeech (other)', 'Common Voice 11.0'], 'license': 'apac 
he-2.0', 'github': 'https://github.com/openai/whisper', 'paper': 'https://arxiv.org/abs/2212.04356', 
 'upstream_model': 'None', 'parameter_count': '1550 M', 'hyper_parameters': {'epochs': 'None', 'batc 
h_size': 'None', 'learning_rate': 'None', 'optimizer': 'None'}, 'evaluation': [{'test': 'ASR', 'resu 
lt': 'strong results in ~10 languages'}], 'hardware': 'None', 'limitation_and_bias': 'The models are 
 trained in a weakly supervised manner using large-scale noisy data, which may result in hallucinati 
ons. The models perform unevenly across languages and exhibit lower accuracy on low-resource and/or  
low-discoverability languages. They also show disparate performance on different accents and dialect 
s, including higher word error rate across speakers of different demographics. The sequence-to-seque 
nce architecture of the model can generate repetitive texts. Further analysis on these limitations c 
an be found in the accompanying paper.', 'demo': 'The blog post [Fine-Tune Whisper with 🤗 Transforme 
rs](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the  
Whisper model with as little as 5 hours of labelled data.', 'input_format': 'None', 'output_format': 
 'None', 'sample_rate': 'None'}]                                                                     
total elapsed time: 2 hours 58 minutes 59 seconds
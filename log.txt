Metadata Prompt: 

datasets: What datasets, provide url if possible, were the model trained/pretrained on?
license: What is the license?
github: What urls to github repositories are available and what these urls are for?
paper: What is the research paper url associated to this model?
upstream_model: What model was this model pretrained or downstreamed from?
parameter_count: What are the number of parameters (#params) this model is trained on, sometimes represented with "M", "B", and "T" as million, billion, and trillion?
hyper_parameters: What are the values of some hyper parameters (parameters that control the learning process of the model) of this model.
evaluation: What is the evalutaion of the model. What evaluation metrics where used, and what are the results (include whole table if possible)?
hardware: What hardware (GPU and TPU pods) was used to train this model?
limitation_and_bias: What are the limitations and biases of the model?
demo: Find a form of demo for the model could be a link, code snippet or short paragraph.
input_format: What is the format of the data used as input for the model?
output_format: What is the format of the data used as output of the model?

Extraction Prompts: 

Given information on metadata of huggingface {domain} model : {model}, extract the properties of ONE single entity mentioned in the 'information_extraction' function.
     Extraction rules: 
     - rule 1: Adhere strictly to the schema structure and in 'information_extraction'
     - rule 2: If a metadata is not present but is required in the function parameters, output empty string instead
     - rule 3: If a property is not present and is not required in the function parameters, do not include it in the output
     - rule 4: Only extract one item for 'info' in 'information_extraction' function 
     Extraction rules for metadata: 
     - datasets: only return dataset used to train or finetune model, not the upstream model of the model
     - github: extract github link of this model (only return the github link)
     - paper: extract research paper url
     - parameter_count: output the number of parameters, represented with "M", "B", and "T" as million, billion, and trillion
     - upstream_model: provide huggingface model ID of upstream model
     - hardware: extract any hardware used to train the model
 Given information on metadata of huggingface {domain} model : {model}, extract the properties of ONE single entity mentioned in the 'information_extraction' function.
     Extraction rules: 
     - rule 1: Adhere strictly to the schema structure in 'information_extraction'
     - rule 2: If a property is not present but is required in the function parameters, output empty string instead
     - rule 3: If a property is not present and is not required in the function parameters, do not include it in the output
     - rule 4: Only extract one item for 'info' in 'information_extraction' function 
     Extraction rules for metadata: 
     - hyper_parameter: extract possible hyperparameters
     - evaluation: extract evaluation metric/tasks and their respective evaluation results
     - limitation_and_biases: extract a short summary of limitation and biases of the model
     - demo: extract any links, code snippets, or small paragraphs on how to use the model
     - input_format: extract the input format/requirement for this model
     - output_format: extract the output format of this model
 

#####################google/byt5-large########################

-------------------- datasets --------------------

Document 1:

Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.  
![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)
------------------------------
Document 2:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).  
Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*
------------------------------
Document 3:

ByT5 works on raw UTF-8 bytes and can be used without a tokenizer:  
```python
from transformers import T5ForConditionalGeneration
import torch

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')

input_ids = torch.tensor([list('Life is like a box of chocolates.'.encode('utf-8'))]) + 3  # add 3 for special tokens
labels = torch.tensor([list('La vie est comme une boîte de chocolat.'.encode('utf-8'))]) + 3  # add 3 for special tokens

loss = model(input_ids, labels=labels).loss # forward pass
```  
For batched inference & training it is however recommended using a tokenizer class for padding:  
```python
from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

model_inputs = tokenizer(['Life is like a box of chocolates.', 'Today is Monday.'], padding='longest', return_tensors='pt')
labels = tokenizer(['La vie est comme une boîte de chocolat.', 'Aujourd'hui c'est lundi.'], padding='longest', return_tensors='pt').input_ids

loss = model(**model_inputs, labels=labels).loss # forward pass
```

-------------------- license --------------------

Document 1:

#tags  
---
language:
- multilingual
- af
- am
- ar
- az
- be
- bg
- bn
- ca
- ceb
- co
- cs
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fil
- fr
- fy
- ga
- gd
- gl
- gu
- ha
- haw
- hi
- hmn
- ht
- hu
- hy
- ig
- is
- it
- iw
- ja
- jv
- ka
- kk
- km
- kn
- ko
- ku
- ky
- la
- lb
- lo
- lt
- lv
- mg
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- my
- ne
- nl
- false
- ny
- pa
- pl
- ps
- pt
- ro
- ru
- sd
- si
- sk
- sl
- sm
- sn
- so
- sq
- sr
- st
- su
- sv
- sw
- ta
- te
- tg
- th
- tr
- uk
- und
- ur
- uz
- vi
- xh
- yi
- yo
- zh
- zu
license: apache-2.0
datasets:
- mc4
---
------------------------------
Document 2:

Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.  
![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)
------------------------------
Document 3:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).  
Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*

-------------------- github --------------------

Document 1:

#tags  
---
language:
- multilingual
- af
- am
- ar
- az
- be
- bg
- bn
- ca
- ceb
- co
- cs
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fil
- fr
- fy
- ga
- gd
- gl
- gu
- ha
- haw
- hi
- hmn
- ht
- hu
- hy
- ig
- is
- it
- iw
- ja
- jv
- ka
- kk
- km
- kn
- ko
- ku
- ky
- la
- lb
- lo
- lt
- lv
- mg
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- my
- ne
- nl
- false
- ny
- pa
- pl
- ps
- pt
- ro
- ru
- sd
- si
- sk
- sl
- sm
- sn
- so
- sq
- sr
- st
- su
- sv
- sw
- ta
- te
- tg
- th
- tr
- uk
- und
- ur
- uz
- vi
- xh
- yi
- yo
- zh
- zu
license: apache-2.0
datasets:
- mc4
---
------------------------------
Document 2:

Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.  
![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)
------------------------------
Document 3:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).  
Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*

-------------------- paper --------------------

Document 1:

Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.  
![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)
------------------------------
Document 2:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).  
Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*
------------------------------
Document 3:

ByT5 works on raw UTF-8 bytes and can be used without a tokenizer:  
```python
from transformers import T5ForConditionalGeneration
import torch

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')

input_ids = torch.tensor([list('Life is like a box of chocolates.'.encode('utf-8'))]) + 3  # add 3 for special tokens
labels = torch.tensor([list('La vie est comme une boîte de chocolat.'.encode('utf-8'))]) + 3  # add 3 for special tokens

loss = model(input_ids, labels=labels).loss # forward pass
```  
For batched inference & training it is however recommended using a tokenizer class for padding:  
```python
from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

model_inputs = tokenizer(['Life is like a box of chocolates.', 'Today is Monday.'], padding='longest', return_tensors='pt')
labels = tokenizer(['La vie est comme une boîte de chocolat.', 'Aujourd'hui c'est lundi.'], padding='longest', return_tensors='pt').input_ids

loss = model(**model_inputs, labels=labels).loss # forward pass
```

-------------------- upstream_model --------------------

Document 1:

Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.  
![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)
------------------------------
Document 2:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).  
Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*
------------------------------
Document 3:

ByT5 works on raw UTF-8 bytes and can be used without a tokenizer:  
```python
from transformers import T5ForConditionalGeneration
import torch

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')

input_ids = torch.tensor([list('Life is like a box of chocolates.'.encode('utf-8'))]) + 3  # add 3 for special tokens
labels = torch.tensor([list('La vie est comme une boîte de chocolat.'.encode('utf-8'))]) + 3  # add 3 for special tokens

loss = model(input_ids, labels=labels).loss # forward pass
```  
For batched inference & training it is however recommended using a tokenizer class for padding:  
```python
from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

model_inputs = tokenizer(['Life is like a box of chocolates.', 'Today is Monday.'], padding='longest', return_tensors='pt')
labels = tokenizer(['La vie est comme une boîte de chocolat.', 'Aujourd'hui c'est lundi.'], padding='longest', return_tensors='pt').input_ids

loss = model(**model_inputs, labels=labels).loss # forward pass
```

-------------------- parameter_count --------------------

Document 1:

Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.  
![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)
------------------------------
Document 2:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).  
Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*
------------------------------
Document 3:

ByT5 works on raw UTF-8 bytes and can be used without a tokenizer:  
```python
from transformers import T5ForConditionalGeneration
import torch

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')

input_ids = torch.tensor([list('Life is like a box of chocolates.'.encode('utf-8'))]) + 3  # add 3 for special tokens
labels = torch.tensor([list('La vie est comme une boîte de chocolat.'.encode('utf-8'))]) + 3  # add 3 for special tokens

loss = model(input_ids, labels=labels).loss # forward pass
```  
For batched inference & training it is however recommended using a tokenizer class for padding:  
```python
from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

model_inputs = tokenizer(['Life is like a box of chocolates.', 'Today is Monday.'], padding='longest', return_tensors='pt')
labels = tokenizer(['La vie est comme une boîte de chocolat.', 'Aujourd'hui c'est lundi.'], padding='longest', return_tensors='pt').input_ids

loss = model(**model_inputs, labels=labels).loss # forward pass
```

-------------------- hardware --------------------

Document 1:

Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.  
![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)
------------------------------
Document 2:

ByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-large).  
ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.  
ByT5 works especially well on noisy text data,*e.g.*, `google/byt5-large` significantly outperforms [mt5-large](https://huggingface.co/google/mt5-large) on [TweetQA](https://arxiv.org/abs/1907.06292).  
Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)  
Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*
------------------------------
Document 3:

ByT5 works on raw UTF-8 bytes and can be used without a tokenizer:  
```python
from transformers import T5ForConditionalGeneration
import torch

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')

input_ids = torch.tensor([list('Life is like a box of chocolates.'.encode('utf-8'))]) + 3  # add 3 for special tokens
labels = torch.tensor([list('La vie est comme une boîte de chocolat.'.encode('utf-8'))]) + 3  # add 3 for special tokens

loss = model(input_ids, labels=labels).loss # forward pass
```  
For batched inference & training it is however recommended using a tokenizer class for padding:  
```python
from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

model_inputs = tokenizer(['Life is like a box of chocolates.', 'Today is Monday.'], padding='longest', return_tensors='pt')
labels = tokenizer(['La vie est comme une boîte de chocolat.', 'Aujourd'hui c'est lundi.'], padding='longest', return_tensors='pt').input_ids

loss = model(**model_inputs, labels=labels).loss # forward pass
```


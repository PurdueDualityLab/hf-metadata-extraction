{
    "google/byt5-large": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "text2text-generation"
        ],
        "frameworks": [
            "tf",
            "jax",
            "pytorch"
        ],
        "libraries": [
            "transformers"
        ],
        "datasets": [
            "mC4",
            "TweetQA"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/google/byt5-large",
        "paper": "https://arxiv.org/abs/2105.13626",
        "upstream_model": "Google's T5, MT5, mC4, google/byt5-large, mt5-large, TweetQA, ByT5: Towards a token-free future with pre-trained byte-to-byte models",
        "parameter_count": "",
        "hardware": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "dccuchile/bert-base-spanish-wwm-uncased": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "fill-mask"
        ],
        "frameworks": [
            "tf",
            "jax",
            "pytorch"
        ],
        "libraries": [
            "transformers"
        ],
        "datasets": [],
        "license": "CC BY 4.0",
        "github": "",
        "paper": "https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf",
        "upstream_model": "",
        "parameter_count": "2M",
        "hardware": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "huggingface/CodeBERTa-small-v1": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "fill-mask"
        ],
        "frameworks": [
            "tf",
            "jax",
            "pytorch"
        ],
        "libraries": [
            "transformers"
        ],
        "datasets": [
            "code_search_net"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hardware": "",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    },
    "timm/mobilenetv3_large_100.miil_in21k_ft_in1k": {
        "domain": [
            "computer-vision"
        ],
        "model_tasks": [
            "image-classification"
        ],
        "frameworks": [
            "pytorch"
        ],
        "libraries": [
            "timm",
            "safetensors"
        ],
        "datasets": [
            "ImageNet-21k-P",
            "ImageNet-1k"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/huggingface/pytorch-image-models/tree/main/results",
        "paper": "https://github.com/huggingface/pytorch-image-models",
        "upstream_model": "MobileNet-v3, ImageNet-21k-P, ImageNet-1k, Alibaba MIIL",
        "parameter_count": "5.5",
        "hardware": "MobileNet-v3 image classification model, ImageNet-21k-P, ImageNet-1k, Alibaba MIIL",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [
            {
                "test": "",
                "dataset": "",
                "result": ""
            }
        ],
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    }
}
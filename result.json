{
    "datasets": [
        "BookCorpus",
        "English Wikipedia"
    ],
    "language": "English",
    "license": "apache-2.0",
    "model_tasks": [
        "masked language modeling",
        "next sentence prediction",
        "sequence classification",
        "token classification",
        "question answering"
    ],
    "github": "https://github.com/google-research/bert",
    "paper": "https://arxiv.org/abs/1810.04805",
    "frameworks": [
        "pytorch",
        "tensorflow",
        "jax",
        "transformers"
    ],
    "parameter_count": "110M",
    "hyper_parameters": {
        "patch_size": null,
        "steps": null,
        "epochs": 1,
        "batch_size": 256,
        "dropout_rate": null,
        "learning_rate": 0.0001,
        "optimizer": "Adam",
        "loss function": null
    },
    "evaluation": {
        "test": "Glue test results",
        "result": 79.6
    },
    "hardware": "4 cloud TPUs in Pod configuration (16 TPU chips total)",
    "limitation_and_bias": "biased predictions",
    "how_to_use": "fine-tuned on a downstream task",
    "input_format": "sequence of tokens",
    "output_format": "task-specific output",
    "tokenLimit": 512,
    "vocabulary_size": 30000
}
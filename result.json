{
    "bert-base-uncased": {
        "datasets": [
            "BookCorpus",
            "English Wikipedia"
        ],
        "language": "English",
        "license": "apache-2.0",
        "model_tasks": [
            "masked language modeling",
            "next sentence prediction"
        ],
        "github": "https://github.com/google-research/bert",
        "paper": "https://arxiv.org/abs/1810.04805",
        "frameworks": [
            "pytorch",
            "tensorflow",
            "jax",
            "transformers"
        ],
        "parameter_count": "110M",
        "hyper_parameters": {
            "patch_size": 0,
            "steps": 1000000,
            "epochs": 0,
            "batch_size": 256,
            "dropout_rate": 0.0,
            "learning_rate": 0.0001,
            "optimizer": "Adam",
            "loss function": "None"
        },
        "evaluation": [
            {
                "test": "MNLI-(m/mm)",
                "result": 84.6
            },
            {
                "test": "QQP",
                "result": 71.2
            },
            {
                "test": "QNLI",
                "result": 90.5
            },
            {
                "test": "SST-2",
                "result": 93.5
            },
            {
                "test": "CoLA",
                "result": 52.1
            },
            {
                "test": "STS-B",
                "result": 85.8
            },
            {
                "test": "MRPC",
                "result": 88.9
            },
            {
                "test": "RTE",
                "result": 66.4
            }
        ],
        "hardware": "4 cloud TPUs in Pod configuration (16 TPU chips total)",
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions",
        "demo": "You can use this model directly with a pipeline for masked language modeling",
        "input_format": "[CLS] Sentence A [SEP] Sentence B [SEP]",
        "output_format": "None",
        "input_token_limit": 512,
        "vocabulary_size": 30000
    }
}
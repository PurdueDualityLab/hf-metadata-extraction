{
    "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "oasst_export",
            "alpaca"
        ],
        "license": "apache-2.0",
        "github": "\"Open-Assistant project\" \"https://github.com/LAION-AI/Open-Assistant\" \"https://open-assistant.io/\"",
        "paper": "",
        "upstream_model": "Pythia 12B",
        "parameter_count": "",
        "hyper_parameters": {
            "learning_rate": "6e-6",
            "weight_decay": "0.0",
            "max_length": "2048",
            "warmup_steps": "100",
            "gradient_accumulation_steps": "2",
            "per_device_train_batch_size": "4",
            "per_device_eval_batch_size": "4",
            "eval_steps": "100",
            "save_steps": "1000",
            "num_train_epochs": "8",
            "save_total_limit": "4"
        },
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "\"Demo: [Continuations for 250 random prompts](https://open-assistant.github.io/oasst-model-eval/?f=https%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Foasst-sft%2F2023-04-03_andreaskoepf_oasst-sft-4-pythia-12b-epoch-3_5_sampling_noprefix_lottery.json%0Ahttps%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Fchat-gpt%2F2023-04-11_gpt-3.5-turbo_lottery.json)\"",
        "input_format": "\"input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz\"",
        "output_format": ""
    },
    "nomic-ai/gpt4all-j": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "nomic-ai/gpt4all-j-prompt-generations"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/nomic-ai/gpt4all",
        "paper": "https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf",
        "upstream_model": "https://github.com/kingoflolz/mesh-transformer-jax",
        "parameter_count": "A finetuned GPT-J model on assistant style interaction data",
        "hyper_parameters": [
            {
                "epochs": "v1.0",
                "batch_size": "The original model trained on the v1.0 dataset",
                "learning_rate": "v1.1-breezy",
                "optimizer": "Trained on a filtered dataset where we also removed instances like I'm sorry, I can't answer... and AI language model"
            }
        ],
        "evaluation": [
            {
                "test": "GPT4All-J 6B v1.0",
                "result": 58.2
            }
        ],
        "hardware": "8 A100 80GB GPUs",
        "limitation_and_bias": "An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.",
        "demo": "https://gpt4all.io/",
        "input_format": "A finetuned GPT-J model on assistant style interaction data",
        "output_format": "Finetuned from model [optional]: [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)",
        "input_token_limit": "vocabulary_size"
    },
    "chavinlo/gpt4-x-alpaca": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "https://huggingface.co/chavinlo/alpaca-13b"
        ],
        "license": "NO LORA",
        "github": "https://huggingface.co/chavinlo/alpaca-13b",
        "paper": "Finetuned on GPT4's responses, for 3 epochs.",
        "upstream_model": "https://huggingface.co/chavinlo/alpaca-13b",
        "parameter_count": "3 epochs",
        "hyper_parameters": {
            "epochs": "3 epochs",
            "batch_size": null,
            "learning_rate": null,
            "optimizer": null
        },
        "evaluation": [
            {
                "test": "As a base model we used: https://huggingface.co/chavinlo/alpaca-13b Finetuned on GPT4's responses, for 3 epochs. NO LORA",
                "result": null
            }
        ],
        "hardware": "Finetuned on GPT4's responses, for 3 epochs.",
        "limitation_and_bias": "Finetuned on GPT4's responses, for 3 epochs. NO LORA",
        "demo": "https://huggingface.co/chavinlo/alpaca-13b",
        "input_format": "Finetuned on GPT4's responses, for 3 epochs.",
        "output_format": "Finetuned on GPT4's responses, for 3 epochs. NO LORA",
        "input_token_limit": "https://huggingface.co/chavinlo/alpaca-13b",
        "vocabulary_size": "https://huggingface.co/chavinlo/alpaca-13b"
    },
    "KoboldAI/GPT-J-6B-Janeway": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy genres. The dataset is based on the same dataset used by GPT-Neo-2.7B-Picard, with 20% more data in various genres. Some parts of the dataset have been prepended using the following text: '[Genre: <genre1>,<genre2>]'"
        ],
        "license": "mit",
        "github": "https://github.com/kingoflolz/mesh-transformer-jax",
        "paper": "```bibtex\n@misc{gpt-j,\nauthor = {Wang, Ben and Komatsuzaki, Aran},\ntitle = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},\nhowpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\nyear = 2021,\nmonth = May\n```",
        "upstream_model": "```bibtex\n@misc{gpt-j,\nauthor = {Wang, Ben and Komatsuzaki, Aran},\ntitle = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},\nhowpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\nyear = 2021,\nmonth = May\n```",
        "parameter_count": "6 Billion Parameter",
        "hyper_parameters": "hyper_parameters",
        "evaluation": [
            {
                "test": "The core functionality of GPT-J is taking a string of text and predicting the next token. ... GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. ... As with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning."
            }
        ],
        "hardware": "Google, TPU Research Cloud, Cloud TPU VM",
        "limitation_and_bias": "When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most \"accurate\" text. Never depend upon GPT-J to produce factually accurate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile. As with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.",
        "demo": "https://github.com/kingoflolz/mesh-transformer-jax",
        "input_format": "`[Genre: <genre1>,<genre2>]` input_format: compressed doc",
        "output_format": "`[Genre: <genre1>,<genre2>]` output_format compressed doc",
        "input_token_limit": null,
        "vocabulary_size": "20% more data in various genres"
    },
    "KoboldAI/GPT-NeoX-20B-Erebus": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Literotica",
            "Sexstories",
            "Dataset-G",
            "Doc's Lab",
            "Pike Dataset",
            "SoFurry"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "@inproceedings{gpt-neox-20b,\ntitle={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},\nauthor={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},\nbooktitle={Proceedings of the ACL Workshop on Challenges \\& Perspectives in Creating Large Language Models},\nurl={https://arxiv.org/abs/2204.06745},\nyear={2022}\n}",
        "upstream_model": "Mesh Transformer JAX library",
        "parameter_count": "",
        "hyper_parameters": [
            {
                "epochs": "GPT-NeoX-20B-Erebus",
                "batch_size": "TPUv3-256 TPU pod",
                "learning_rate": "Ben Wang's Mesh Transformer JAX library",
                "optimizer": "EleutherAI",
                "evaluation": [
                    {
                        "test": "GPT-J-6B model",
                        "result": 0
                    }
                ]
            }
        ],
        "hardware": "TPUv3-256 TPU pod",
        "limitation_and_bias": "Warning: This model has a very strong NSFW bias!",
        "demo": "Mesh Transformer JAX library: ```bibtex @misc{mesh-transformer-jax, author = {Wang, Ben}, title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}}, howpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}}, year = 2021, month = May }```",
        "input_format": "[Genre: <comma-separated list of genres>]",
        "output_format": "[Genre: <comma-separated list of genres>]",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "OpenAssistant/pythia-12b-sft-v8-7k-steps": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "oasst_export",
            "vicuna",
            "dolly15k",
            "grade_school_math_instructions",
            "code_alpaca",
            "red_pajama",
            "wizardlm_70k",
            "poem_instructions"
        ],
        "license": "apache-2.0",
        "github": "",
        "paper": "",
        "upstream_model": "OpenAssistant/pythia-12b-pre-v8-12.5k-steps",
        "parameter_count": "\"num_train_epochs: 8\" \"save_total_limit: 3\" \"per_device_train_batch_size: 4\" \"per_device_eval_batch_size: 4\" \"eval_steps: 251\" \"save_steps: 500\" \"gradient_accumulation_steps: 2\"",
        "hyper_parameters": {
            "dtype": "fp16",
            "log_dir": "pythia_log_12b",
            "learning_rate": "6e-6",
            "model_name": "OpenAssistant/pythia-12b-pre-v8-12.5k-steps",
            "output_dir": "pythia_model_12b",
            "weight_decay": "0.0",
            "residual_dropout": "0.0",
            "max_length": "2048",
            "use_flash_attention": "true",
            "warmup_steps": "100",
            "gradient_checkpointing": "true",
            "gradient_accumulation_steps": "2",
            "per_device_train_batch_size": "4",
            "per_device_eval_batch_size": "4",
            "eval_steps": "251",
            "save_steps": "500",
            "num_train_epochs": "8",
            "save_total_limit": "4",
            "use_custom_sampler": "true",
            "sort_by_length": "false",
            "save_strategy": "steps"
        },
        "evaluation": [],
        "hardware": {
            "dtype": "fp16",
            "log_dir": "pythia_log_12b",
            "learning_rate": "6e-6",
            "model_name": "OpenAssistant/pythia-12b-pre-v8-12.5k-steps",
            "output_dir": "pythia_model_12b",
            "weight_decay": "0.0",
            "residual_dropout": "0.0",
            "max_length": "2048",
            "use_flash_attention": "true",
            "warmup_steps": "100",
            "gradient_checkpointing": "true",
            "gradient_accumulation_steps": "2",
            "per_device_train_batch_size": "4",
            "per_device_eval_batch_size": "4",
            "eval_steps": "251",
            "save_steps": "500",
            "num_train_epochs": "8",
            "save_total_limit": "4",
            "use_custom_sampler": "true",
            "sort_by_length": "false",
            "save_strategy": "steps"
        },
        "limitation_and_bias": "",
        "demo": "",
        "input_format": {
            "input_file_path": "2023-05-06_OASST_labels.jsonl.gz"
        },
        "output_format": {
            "output_format": "fp16"
        },
        "input_token_limit": {
            "max_length": "2048",
            "per_device_train_batch_size": "4",
            "per_device_eval_batch_size": "4"
        },
        "vocabulary_size": ""
    },
    "TheBloke/wizardLM-13B-1.0-fp16": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "WizardLM"
        ],
        "license": "academic research purposes only",
        "github": "https://github.com/nlpxucan/WizardLM",
        "paper": "https://arxiv.org/abs/2304.12244",
        "upstream_model": null,
        "parameter_count": "250k",
        "hyper_parameters": {
            "epochs": "3",
            "batch_size": "384",
            "learning_rate": "2e-5",
            "optimizer": null
        },
        "evaluation": [
            {
                "test": "chatbot models",
                "result": null
            }
        ],
        "hardware": null,
        "limitation_and_bias": "The content produced by any version of WizardLM is influenced by uncontrollable variables such as randomness, and therefore, the accuracy of the output cannot be guaranteed by this project.",
        "demo": "[Demo Link](https://011fc8477ad734d7.gradio.app)\n[Demo Backup 1](https://1825e531c43a23c7.gradio.app)",
        "input_format": null,
        "output_format": "compressed doc",
        "input_token_limit": "13B version of WizardLM trained with 250k evolved instructions and 7B version of WizardLM trained with 70k evolved instructions",
        "vocabulary_size": "33B, 13B, 250k, 7B, 70k"
    },
    "mosaicml/mpt-30b-chat": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "Airoboros/GPT4-1.2",
            "Baize",
            "Camel",
            "GPTeacher",
            "Guanaco",
            "LongCoversations",
            "ShareGPT",
            "WizardLM"
        ],
        "license": "_CC-By-NC-SA-4.0_",
        "github": "[Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)",
        "paper": "\"Introducing MPT-30B: Raising the bar for open-source foundation models\"",
        "upstream_model": "standard decoder-only transformer, FlashAttention, Attention with Linear Biases",
        "parameter_count": "\"26.4M\", \"55.0M\", \"301M\", \"7.56M\", \"15.6M\", \"18.4M\", \"821M\", \"297M\"",
        "hyper_parameters": {
            "epochs": "",
            "batch_size": "",
            "learning_rate": "",
            "optimizer": ""
        },
        "evaluation": [],
        "hardware": "64 H100s, [MosaicML Platform](https://www.mosaicml.com/platform), [FSDP](https://pytorch.org/docs/stable/fsdp.html)",
        "limitation_and_bias": "\"LongConversations is a GPT3.5/4-generated dataset, details of which will be released at a later date.\"",
        "demo": "[sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b)",
        "input_format": "\"LongConversations\" is a GPT3.5/4-generated dataset, input_format compressed doc",
        "output_format": "\"LongConversations\" is a GPT3.5/4-generated dataset, details of which will be released at a later date.",
        "input_token_limit": "\"26.4M\", \"55.0M\", \"301M\", \"7.56M\", \"15.6M\", \"18.4M\", \"821M\", \"297M\"",
        "vocabulary_size": "\"26.4M\", \"55.0M\", \"301M\", \"7.56M\", \"15.6M\", \"18.4M\", \"821M\", \"297M\""
    },
    "hf-internal-testing/tiny-random-bloom": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-generation",
        "frameworks": [
            "pytorch",
            "transformers"
        ],
        "datasets": [
            "GPT-2"
        ],
        "license": "GPT-2",
        "github": "\"language: - eng tags: - integration pipeline_tag: text-generation\"",
        "paper": "GPT-2-like model",
        "upstream_model": "GPT-2",
        "parameter_count": "GPT-2",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "GPT-2",
        "limitation_and_bias": "GPT-2-like model",
        "demo": "GPT-2-like model",
        "input_format": "GPT-2, input_format",
        "output_format": "GPT-2, output_format",
        "input_token_limit": "GPT-2",
        "vocabulary_size": "GPT-2"
    },
    "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "safetensors",
            "tensorboard"
        ],
        "datasets": [
            "financial_phrasebank"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": "",
        "input_token_limit": "",
        "vocabulary_size": ""
    },
    "LiYuan/amazon-query-product-ranking": {
        "model_type": "natural-language-processing",
        "model_tasks": "text-classification",
        "frameworks": [
            "pytorch",
            "transformers",
            "tensorboard"
        ],
        "datasets": [
            "https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search/dataset_files"
        ],
        "license": "apache-2.0",
        "github": "https://github.com/vanderbilt-data-science/sna",
        "paper": "https://huggingface.co/distilbert-base-uncased",
        "upstream_model": "distilbert-base-uncased",
        "parameter_count": "5",
        "hyper_parameters": {
            "epochs": "2",
            "batch_size": "16",
            "learning_rate": "2e-05",
            "optimizer": "Adam with betas=(0.9,0.999) and epsilon=1e-08"
        },
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.6617
            }
        ],
        "hardware": "BERT base model",
        "limitation_and_bias": "The limitations are this trained model is focusing on queries and products on Amazon.",
        "demo": "You can use this model directly by downloading the trained weights and configurations like the below code snippet:\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"LiYuan/amazon-query-product-ranking\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"LiYuan/amazon-query-product-ranking\")\n```",
        "input_format": "raw texts only",
        "output_format": null,
        "input_token_limit": null,
        "vocabulary_size": null
    },
    "laion/mscoco_finetuned_CoCa-ViT-L-14-laion2B-s13B-b90k": {
        "model_type": "multimodal",
        "model_tasks": "image-to-text",
        "frameworks": [],
        "datasets": [
            "dataset1",
            "dataset2"
        ],
        "license": "mit",
        "github": "https://github.com/model",
        "paper": "https://arxiv.org/1234",
        "upstream_model": "huggingface/model",
        "parameter_count": "100M",
        "hyper_parameters": {
            "epochs": "10",
            "batch_size": "32",
            "learning_rate": "0.001",
            "optimizer": "adam"
        },
        "evaluation": [
            {
                "test": "accuracy",
                "result": 0.85
            },
            {
                "test": "f1-score",
                "result": 0.78
            }
        ],
        "hardware": "GPU",
        "limitation_and_bias": "Limited to English language",
        "demo": "https://demo.com",
        "input_format": "text",
        "output_format": "text"
    },
    "hf-internal-testing/tiny-stable-diffusion-pipe": {
        "model_type": "multimodal",
        "model_tasks": "text-to-image",
        "frameworks": [
            "diffusers"
        ],
        "datasets": [
            "diffusers"
        ],
        "license": "",
        "github": "",
        "paper": "",
        "upstream_model": "",
        "parameter_count": "",
        "hyper_parameters": [],
        "evaluation": [],
        "hardware": "",
        "limitation_and_bias": "",
        "demo": "",
        "input_format": "",
        "output_format": ""
    }
}
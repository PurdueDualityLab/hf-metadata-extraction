{
    "google/byt5-large": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "text2text-generation"
        ],
        "frameworks": [
            "tf",
            "jax",
            "pytorch"
        ],
        "libraries": [
            "transformers"
        ],
        "datasets": [
            {
                "dataset_name": "mC4",
                "link": "https://www.tensorflow.org/datasets/catalog/c4#c4multilingual",
                "confidence_value": 0.9
            },
            {
                "dataset_name": "TweetQA",
                "link": "https://arxiv.org/abs/1907.06292",
                "confidence_value": 0.9
            }
        ],
        "license": {
            "license_name": "apache-2.0",
            "confidence_value": 1.0
        },
        "github": {
            "github_url": "https://huggingface.co/google/mt5-large",
            "confidence_value": 1.0
        },
        "paper": [
            {
                "paper title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
                "paper_url": "https://arxiv.org/abs/2105.13626",
                "confidence_value": 1.0
            }
        ],
        "upstream_model": {
            "model_name": "ByT5",
            "confidence_value": 1.0
        },
        "parameter_count": {
            "answer": "none",
            "confidence_value": 1.0
        },
        "hardware": {
            "hardware_name": "T5ForConditionalGeneration",
            "confidence_value": 0.9
        },
        "hyper_parameters": {
            "pre_training_data": "mC4 excluding any supervised training",
            "average_span_mask": "20 UTF-8 characters",
            "fine_tuning_requirement": "has to be fine-tuned before it is useable on a downstream task",
            "performance_comparison": {
                "model_compared_with": "mt5-large",
                "dataset_used": "TweetQA",
                "performance_outcome": "significantly outperforms"
            }
        },
        "evaluation": [
            {
                "test/metric": "outperforms",
                "dataset": "TweetQA",
                "result": "google/byt5-large significantly outperforms mt5-large",
                "confidence_value": 0.9
            }
        ],
        "limitation_and_bias": "ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.",
        "demo": "Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",
        "input_format": "python",
        "output_format": "json"
    }
}
{
    "jonatasgrosman/wav2vec2-large-xlsr-53-english": {
        "domain": [
            "audio"
        ],
        "model_tasks": [
            "automatic-speech-recognition"
        ],
        "frameworks": [
            "pytorch",
            "jax"
        ],
        "libraries": [
            "safetensors",
            "transformers"
        ],
        "datasets": [
            {
                "dataset_name": "common_voice",
                "url": "https://huggingface.co/datasets/common_voice"
            },
            {
                "dataset_name": "mozilla-foundation/common_voice_6_0",
                "url": "https://huggingface.co/datasets/mozilla-foundation/common_voice_6_0"
            }
        ],
        "license": "apache-2.0",
        "github_repo": "https://github.com/jonatasgrosman/wav2vec2-sprint",
        "base_model": {
            "model_name": "facebook/wav2vec2-large-xlsr-53",
            "url": "https://huggingface.co/facebook/wav2vec2-large-xlsr-53"
        },
        "grant": "GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/)",
        "demo": "from huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-english')\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\n\ntranscriptions = model.transcribe(audio_paths)",
        "input_format": "speech input is sampled at 16kHz",
        "output_format": "text transcriptions",
        "evaluation": {
            "link": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english#evaluation",
            "confidence_value": 1
        }
    },
    "bert-base-uncased": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "fill-mask"
        ],
        "frameworks": [
            "tf",
            "pytorch",
            "jax"
        ],
        "libraries": [
            "coreml",
            "onnx",
            "safetensors",
            "transformers",
            "rust"
        ],
        "language": "en",
        "datasets": [
            {
                "dataset_name": "BookCorpus",
                "url": "https://yknzhu.wixsite.com/mbweb"
            },
            {
                "dataset_name": "English Wikipedia",
                "url": "https://en.wikipedia.org/wiki/English_Wikipedia"
            }
        ],
        "license": "apache-2.0",
        "github_repo": "https://github.com/google-research/bert",
        "paper": [
            {
                "paper title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "paper_url": "http://arxiv.org/abs/1810.04805"
            }
        ],
        "base_model": {
            "model_name": "bert-base-uncased",
            "url": "https://huggingface.co/bert-base-uncased"
        },
        "parameter_count": "110M",
        "hardware": {
            "hardware_name": "cloud TPUs",
            "hours_used": "unknown",
            "cloud_provider": "unknown",
            "compute_region": "unknown",
            "confidence_value": 0.8
        },
        "carbon_emitted": "",
        "hyper_parameters": {
            "optimizer": "Adam",
            "learning_rate": "1e-4",
            "beta_1": "0.9",
            "beta_2": "0.999",
            "weight_decay": "0.01",
            "batch_size": "256",
            "sequence_length": "128 tokens for 90% of the steps and 512 for the remaining 10%",
            "training_steps": "one million",
            "warmup_steps": "10,000"
        },
        "evaluation": {
            "link": "https://huggingface.co/models?filter=bert",
            "confidence_value": 0.8
        },
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. This bias will also affect all fine-tuned versions of this model.",
        "demo": "https://huggingface.co/exbert/?model=bert-base-uncased",
        "grant": "",
        "input_format": "[CLS] Sentence A [SEP] Sentence B [SEP]",
        "output_format": "Features of the input text"
    },
    "xlm-roberta-large": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "fill-mask"
        ],
        "frameworks": [
            "tf",
            "pytorch",
            "jax"
        ],
        "libraries": [
            "safetensors",
            "onnx",
            "transformers"
        ],
        "language": [
            "multilingual",
            "af",
            "am",
            "ar",
            "as",
            "az",
            "be",
            "bg",
            "bn",
            "br",
            "bs",
            "ca",
            "cs",
            "cy",
            "da",
            "de",
            "el",
            "en",
            "eo",
            "es",
            "et",
            "eu",
            "fa",
            "fi",
            "fr",
            "fy",
            "ga",
            "gd",
            "gl",
            "gu",
            "ha",
            "he",
            "hi",
            "hr",
            "hu",
            "hy",
            "id",
            "is",
            "it",
            "ja",
            "jv",
            "ka",
            "kk",
            "km",
            "kn",
            "ko",
            "ku",
            "ky",
            "la",
            "lo",
            "lt",
            "lv",
            "mg",
            "mk",
            "ml",
            "mn",
            "mr",
            "ms",
            "my",
            "ne",
            "nl",
            "no",
            "om",
            "or",
            "pa",
            "pl",
            "ps",
            "pt",
            "ro",
            "ru",
            "sa",
            "sd",
            "si",
            "sk",
            "sl",
            "so",
            "sq",
            "sr",
            "su",
            "sv",
            "sw",
            "ta",
            "te",
            "th",
            "tl",
            "tr",
            "ug",
            "uk",
            "ur",
            "uz",
            "vi",
            "xh",
            "yi",
            "zh"
        ],
        "license": "mit",
        "github_repo": "pytorch/fairseq",
        "paper": [
            {
                "paper title": "Unsupervised Cross-lingual Representation Learning at Scale",
                "paper_url": "https://arxiv.org/abs/1911.02116"
            }
        ],
        "base_model": {
            "model_name": "xlm-roberta-large",
            "url": "https://huggingface.co/xlm-roberta-large"
        },
        "demo": "https://huggingface.co/exbert/?model=xlm-roberta-base"
    },
    "bert-large-uncased": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "fill-mask"
        ],
        "frameworks": [
            "tf",
            "pytorch",
            "jax"
        ],
        "libraries": [
            "safetensors",
            "transformers"
        ],
        "language": "en",
        "datasets": [
            {
                "dataset_name": "BookCorpus",
                "url": "https://yknzhu.wixsite.com/mbweb"
            },
            {
                "dataset_name": "English Wikipedia",
                "url": "https://en.wikipedia.org/wiki/English_Wikipedia"
            }
        ],
        "license": "apache-2.0",
        "github_repo": "https://github.com/google-research/bert",
        "paper": [
            {
                "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "paper_url": "http://arxiv.org/abs/1810.04805"
            }
        ],
        "base_model": {
            "model_name": "bert-large-uncased",
            "url": ""
        },
        "parameter_count": "336M",
        "hardware": {
            "hardware_name": "cloud TPUs",
            "hours_used": "unknown",
            "cloud_provider": "unknown",
            "compute_region": "unknown",
            "confidence_value": 0.8
        },
        "carbon_emitted": "",
        "hyper_parameters": {
            "learning_rate": "1e-4",
            "beta_1": "0.9",
            "beta_2": "0.999",
            "weight_decay": "0.01",
            "warmup_steps": "10,000",
            "optimizer": "Adam"
        },
        "evaluation": [
            {
                "test/metric": "SQUAD 1.1 F1/EM",
                "dataset": "SQUAD 1.1",
                "result": "91.0/84.3",
                "confidence_value": 0.9
            },
            {
                "test/metric": "Multi NLI Accuracy",
                "dataset": "Multi NLI",
                "result": "86.05",
                "confidence_value": 0.9
            }
        ],
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. This bias will also affect all fine-tuned versions of this model.",
        "demo": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='bert-large-uncased')\nunmasker('Hello I'm a [MASK] model.')",
        "grant": "",
        "input_format": "Texts are lowercased and tokenized using WordPiece with a vocabulary size of 30,000. The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]",
        "output_format": "Features of the given text"
    },
    "openai/clip-vit-large-patch14": {
        "domain": [
            "computer-vision"
        ],
        "model_tasks": [
            "zero-shot-image-classification"
        ],
        "frameworks": [
            "tf",
            "pytorch",
            "jax"
        ],
        "libraries": [
            "safetensors",
            "transformers"
        ],
        "datasets": [
            {
                "dataset_name": "YFCC100M",
                "url": "http://projects.dfki.uni-kl.de/yfcc100m/"
            }
        ],
        "license": "",
        "github_repo": "https://github.com/openai/CLIP",
        "paper": [
            {
                "paper_title": "Learning Transferable Visual Models From Natural Language Supervision",
                "paper_url": "https://arxiv.org/abs/2103.00020"
            }
        ],
        "base_model": {
            "model_name": "openai/clip-vit-large-patch14",
            "url": ""
        },
        "parameter_count": "",
        "hardware": "",
        "carbon_emitted": "",
        "hyper_parameters": "",
        "evaluation": {
            "link": "https://arxiv.org/abs/2103.00020",
            "confidence_value": 0
        },
        "limitation_and_bias": "CLIP struggles with fine grained classification and counting objects. It exhibits biases that depend on class design and category choices. Significant disparities with respect to race and gender were found when classifying images into crime-related and non-human animal categories. Accuracy varies across demographics for gender, race, and age classification.",
        "demo": "from PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\nprocessor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=['a photo of a cat', 'a photo of a dog'], images=image, return_tensors='pt', padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities",
        "grant": "",
        "input_format": "Text and images input",
        "output_format": "Image-text similarity score"
    },
    "gpt2": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "text-generation"
        ],
        "frameworks": [
            "tf",
            "pytorch",
            "jax"
        ],
        "libraries": [
            "rust",
            "safetensors",
            "onnx",
            "transformers"
        ],
        "language": "en",
        "datasets": [
            {
                "dataset_name": "WebText"
            }
        ],
        "license": "MIT",
        "github_repo": "openai/gpt-2",
        "paper": [
            {
                "paper_title": "Language Models are Unsupervised Multitask Learners",
                "paper_url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
            }
        ],
        "base_model": {
            "model_name": "GPT-2",
            "url": "https://huggingface.co/gpt2"
        },
        "parameter_count": "124M",
        "hardware": {
            "hardware_name": "cloud TPU v3 cores",
            "confidence_value": 0.7
        },
        "evaluation": [
            {
                "test/metric": "Perplexity (PPL)",
                "dataset": "LAMBADA",
                "result": "35.13"
            },
            {
                "test/metric": "Accuracy (ACC)",
                "dataset": "LAMBADA",
                "result": "45.99"
            },
            {
                "test/metric": "Accuracy (ACC)",
                "dataset": "CBT-CN",
                "result": "87.65"
            },
            {
                "test/metric": "Accuracy (ACC)",
                "dataset": "CBT-NE",
                "result": "83.4"
            },
            {
                "test/metric": "Perplexity (PPL)",
                "dataset": "WikiText2",
                "result": "29.41"
            },
            {
                "test/metric": "Perplexity (PPL)",
                "dataset": "PTB",
                "result": "65.85"
            },
            {
                "test/metric": "Bits Per Byte (BPB)",
                "dataset": "enwiki8",
                "result": "1.16"
            },
            {
                "test/metric": "Bits Per Character (BPC)",
                "dataset": "text8",
                "result": "1,17"
            },
            {
                "test/metric": "Perplexity (PPL)",
                "dataset": "WikiText103",
                "result": "37.50"
            },
            {
                "test/metric": "Perplexity (PPL)",
                "dataset": "1BW",
                "result": "75.20"
            }
        ],
        "limitation_and_bias": "The training data used for this model has not been released as a dataset one can browse. It contains a lot of unfiltered content from the internet, which is far from neutral. The model reflects the biases inherent to the systems they were trained on, and it is recommended to carry out a study of biases relevant to the intended use-case before deployment.",
        "demo": "https://transformer.huggingface.co/doc/gpt2-large",
        "input_format": "Text input in English",
        "output_format": "Generated text in English"
    },
    "sociocom/MedNER-CR-JA": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "token-classification"
        ],
        "frameworks": [
            "pytorch"
        ],
        "libraries": [
            "safetensors",
            "transformers"
        ],
        "language": "ja",
        "license": "cc-by-4.0",
        "datasets": [
            {
                "dataset_name": "MedTxt-CR-JA-training-v2.xml"
            }
        ],
        "paper": [
            {
                "paper title": "NAISTSOC at the NTCIR-16 Real-MedNLP Task",
                "paper_url": "http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings16/pdf/ntcir-16/NTCIR16-OV-MEDNLP-NishiyamaT.pdf"
            }
        ],
        "demo": "Download the following five files and put into the same folder.\n- id_to_tags.pkl\n- key_attr.pkl\n- NER_medNLP.py\n- predict.py\n- text.txt (This is an input file which should be predicted, which could be changed.)\n\nYou can use this model by running predict.py.\n\n```\npython3 predict.py\n```\n\n### Input Example\n\n```\n\u80a5\u5927\u578b\u5fc3\u7b4b\u75c7\u3001\u5fc3\u623f\u7d30\u52d5\u306b\u5bfe\u3057\u3066\uff37\uff26\u6295\u4e0e\u304c\u958b\u59cb\u3068\u306a\u3063\u305f\u3002\n\u6cbb\u7642\u7d4c\u904e\u4e2d\u306b\u975e\u6301\u7d9a\u6027\u5fc3\u5ba4\u983b\u62cd\u304c\u8a8d\u3081\u3089\u308c\u305f\u305f\u3081\u30a2\u30df\u30aa\u30c0\u30ed\u30f3\u304c\u4f75\u7528\u3068\u306a\u3063\u305f\u3002\n```\n\n### Output Example\n\n```\n <d certainty='positive'>\u80a5\u5927\u578b\u5fc3\u7b4b\u75c7\u3001\u5fc3\u623f\u7d30\u52d5</d>\u306b\u5bfe\u3057\u3066<m-key state='executed'>\uff37\uff26</m-key>\u6295\u4e0e\u304c\u958b\u59cb\u3068\u306a\u3063\u305f\u3002\n<timex3 type='med'>\u6cbb\u7642\u7d4c\u904e\u4e2d</timex3>\u306b<d certainty='positive'>\u975e\u6301\u7d9a\u6027\u5fc3\u5ba4\u983b\u62cd</d>\u304c\u8a8d\u3081\u3089\u308c\u305f\u305f\u3081<m-key state='executed'>\u30a2\u30df\u30aa\u30c0\u30ed\u30f3</m-key>\u304c\u4f75\u7528\u3068\u306a\u3063\u305f\u3002\n```"
    },
    "xlm-roberta-base": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "fill-mask"
        ],
        "frameworks": [
            "tf",
            "pytorch",
            "jax"
        ],
        "libraries": [
            "safetensors",
            "onnx",
            "transformers"
        ],
        "language": [
            "multilingual",
            "af",
            "am",
            "ar",
            "as",
            "az",
            "be",
            "bg",
            "bn",
            "br",
            "bs",
            "ca",
            "cs",
            "cy",
            "da",
            "de",
            "el",
            "en",
            "eo",
            "es",
            "et",
            "eu",
            "fa",
            "fi",
            "fr",
            "fy",
            "ga",
            "gd",
            "gl",
            "gu",
            "ha",
            "he",
            "hi",
            "hr",
            "hu",
            "hy",
            "id",
            "is",
            "it",
            "ja",
            "jv",
            "ka",
            "kk",
            "km",
            "kn",
            "ko",
            "ku",
            "ky",
            "la",
            "lo",
            "lt",
            "lv",
            "mg",
            "mk",
            "ml",
            "mn",
            "mr",
            "ms",
            "my",
            "ne",
            "nl",
            "no",
            "om",
            "or",
            "pa",
            "pl",
            "ps",
            "pt",
            "ro",
            "ru",
            "sa",
            "sd",
            "si",
            "sk",
            "sl",
            "so",
            "sq",
            "sr",
            "su",
            "sv",
            "sw",
            "ta",
            "te",
            "th",
            "tl",
            "tr",
            "ug",
            "uk",
            "ur",
            "uz",
            "vi",
            "xh",
            "yi",
            "zh"
        ],
        "license": "mit",
        "github_repo": "pytorch/fairseq",
        "paper": [
            {
                "paper title": "Unsupervised Cross-lingual Representation Learning at Scale",
                "paper_url": "https://arxiv.org/abs/1911.02116"
            }
        ],
        "base_model": {
            "model_name": "xlm-roberta-base",
            "url": "https://huggingface.co/xlm-roberta-base"
        },
        "demo": "https://huggingface.co/exbert/?model=xlm-roberta-base"
    },
    "roberta-base": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "fill-mask"
        ],
        "frameworks": [
            "tf",
            "pytorch",
            "jax"
        ],
        "libraries": [
            "transformers",
            "safetensors",
            "rust"
        ],
        "language": "en",
        "datasets": [
            {
                "dataset_name": "BookCorpus",
                "url": ""
            },
            {
                "dataset_name": "Wikipedia",
                "url": "https://en.wikipedia.org/wiki/English_Wikipedia"
            }
        ],
        "license": "MIT",
        "github_repo": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
        "paper": [
            {
                "paper_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                "paper_url": "http://arxiv.org/abs/1907.11692"
            }
        ],
        "base_model": {
            "model_name": "roberta-base",
            "url": "https://huggingface.co/roberta-base"
        },
        "parameter_count": "125M",
        "hardware": {
            "hardware_name": "V100 GPUs",
            "hours_used": "500K steps",
            "cloud_provider": "",
            "compute_region": "",
            "confidence_value": 1
        },
        "carbon_emitted": "",
        "hyper_parameters": {
            "learning_rate": "6e-4",
            "beta_1": "0.9",
            "beta_2": "0.98",
            "epsilon": "1e-6",
            "weight_decay": "0.01",
            "warmup_steps": "24000",
            "batch_size": "8K",
            "sequence_length": "512"
        },
        "evaluation": {
            "link": "https://huggingface.co/roberta-base",
            "confidence_value": 1
        },
        "limitation_and_bias": "The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model can have biased predictions. This bias will also affect all fine-tuned versions of this model.",
        "demo": "https://huggingface.co/exbert/?model=roberta-base",
        "grant": "",
        "input_format": "The inputs of the model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`",
        "output_format": "Features of a given text"
    },
    "laion/CLIP-ViT-B-16-laion2B-s34B-b88K": {
        "domain": [
            "computer-vision"
        ],
        "model_tasks": [
            "zero-shot-image-classification"
        ],
        "frameworks": [],
        "libraries": [],
        "datasets": [
            {
                "dataset_name": "LAION-5B",
                "url": "https://laion.ai/blog/laion-5b/"
            }
        ],
        "license": "MIT",
        "github_repo": "mlfoundations/open_clip",
        "paper": [
            {
                "paper title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
                "paper_url": "https://openreview.net/forum?id=M3Y74vmsMcY"
            }
        ],
        "base_model": {
            "model_name": "OpenAI CLIP",
            "url": ""
        },
        "parameter_count": "",
        "hardware": {
            "hardware_name": "JUWELS Booster supercomputer",
            "hours_used": "",
            "cloud_provider": "",
            "compute_region": "",
            "confidence_value": 0
        },
        "carbon_emitted": "",
        "hyper_parameters": {},
        "evaluation": [
            {
                "test/metric": "zero-shot top-1 accuracy",
                "dataset": "ImageNet-1k",
                "result": "70.2",
                "confidence_value": 0
            }
        ],
        "limitation_and_bias": "Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.",
        "demo": "",
        "grant": "Gauss Centre for Supercomputing e.V.",
        "input_format": "",
        "output_format": ""
    }
}
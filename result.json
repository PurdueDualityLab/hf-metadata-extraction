{
    "bert-base-uncased": {
        "datasets": [
            "BookCorpus",
            "English Wikipedia"
        ],
        "language": "English",
        "license": "apache-2.0",
        "model_tasks": [
            "masked language modeling",
            "next sentence prediction"
        ],
        "github": "https://github.com/google-research/bert",
        "paper": "https://arxiv.org/abs/1810.04805",
        "frameworks": [
            "pytorch",
            "tensorflow",
            "jax",
            "transformers"
        ],
        "parameter_count": "110M",
        "hyper_parameters": {
            "patch_size": 0,
            "steps": 1000000,
            "epochs": 0,
            "batch_size": 256,
            "dropout_rate": 0,
            "learning_rate": 0.0001,
            "optimizer": "Adam",
            "loss function": null
        },
        "evaluation": {
            "test": "Glue test results",
            "result": 79.6
        },
        "hardware": "4 cloud TPUs in Pod configuration (16 TPU chips total)",
        "limitation_and_bias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions",
        "how_to_use": "The inputs of the model are then of the form: [...] With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens.",
        "input_format": "text",
        "output_format": "text",
        "tokenLimit": 512,
        "vocabulary_size": 30000
    }
}
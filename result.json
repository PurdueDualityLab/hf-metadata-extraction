{
    "google/byt5-large": {
        "domain": [
            "natural-language-processing"
        ],
        "model_tasks": [
            "text2text-generation"
        ],
        "frameworks": [
            "jax",
            "tf",
            "pytorch"
        ],
        "libraries": [
            "transformers"
        ],
        "datasets": [
            {
                "dataset_name": "Byte-Level Transformer Models",
                "link": "https://github.com/patrickvonplaten/scientific_images/blob/master/ByT5.png",
                "confidence_value": 0.9
            }
        ],
        "license": {
            "license_name": "MIT License",
            "confidence_value": 0.8
        },
        "github": {
            "github_url": "https://github.com/patrickvonplaten/scientific_images",
            "confidence_value": 0.9
        },
        "paper": {
            "paper title": "Token-Free Language Models for Efficient Training and Inference",
            "paper_url": "https://arxiv.org/abs/2106.01780",
            "confidence_value": 0.9
        },
        "upstream_model": {
            "model_name": "ByT5",
            "confidence_value": 0.9
        },
        "parameter_count": {
            "answer": "unknown",
            "confidence_value": 0.8
        },
        "hardware": {
            "hardware_name": "ByT5",
            "confidence_value": 0.9
        },
        "hyper_parameters": {
            "learning_rate": 0.001,
            "batch_size": 32,
            "num_epochs": 10
        },
        "evaluation": {
            "test/metric": "accuracy",
            "dataset": "MNLI",
            "result": "0.85",
            "confidence_value": 0.9
        },
        "limitation_and_bias": "Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",
        "demo": "Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",
        "input_format": "Document 1:\n\nMost widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.\n\n![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)",
        "output_format": "model"
    }
}
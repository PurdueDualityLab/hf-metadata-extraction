{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x: float):\n",
    "    # Logistic regression function (Sigmoid)\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex/ex.sum()\n",
    "\n",
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    file = pd.read_csv(path)\n",
    "    return file\n",
    "\n",
    "\n",
    "\n",
    "def feature_rep(train_set, val_set, test_set):\n",
    "    # Create feature representations for data\n",
    "    word2vec = {}\n",
    "    word_set = {}\n",
    "    word_all = []\n",
    "    idx = 0\n",
    "    # logger.debug(train_set)\n",
    "    for dataset in (train_set, val_set, test_set):\n",
    "        for text in dataset.text:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                if word not in word2vec:\n",
    "                    word_all.append(word)\n",
    "                    word2vec[word] = idx\n",
    "                    idx += 1\n",
    "        word_set = set(word_all)\n",
    "        # logger.debug(idx)\n",
    "    # logger.info(len(word2vec))\n",
    "    # logger.info(len(word_set))\n",
    "        # print(text)\n",
    "        # break\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "def text_features(dataset, embedding):\n",
    "    text_features = []\n",
    "    for text in dataset.text:\n",
    "        features = [0] * len(embedding)\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            features[embedding[word]] = 1\n",
    "        text_features.append(features)\n",
    "    # logger.info(len(text_features[0]))\n",
    "    # logger.info(len(text_features))\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def data_cleaning(dataset: pd.DataFrame) -> None:\n",
    "    '''\n",
    "        Clean the duplicated entries\n",
    "    '''\n",
    "    dataset = dataset.sort_values(\"id\")\n",
    "    dataset_dup = dataset.duplicated(subset=[\"id\"])\n",
    "    index = np.where(dataset_dup==True)\n",
    "    dataset = dataset.drop(index[0])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def data_loading(epoch, num_epochs):\n",
    "    # Load raw dataset\n",
    "    TRAIN_PATH = \"./train.csv\"\n",
    "    TEST_PATH = \"./test.csv\"\n",
    "    train_set = read_csv(TRAIN_PATH)\n",
    "    test_set = read_csv(TEST_PATH)\n",
    "    \n",
    "    # data cleaning\n",
    "    train_set = data_cleaning(train_set)\n",
    "    test_set = data_cleaning(test_set)\n",
    "    # logger.info(f\"After data cleaning, len(train_set) is {len(train_set)}, len(test_set) is {len(test_set)}\")\n",
    "\n",
    "    # Cross validation\n",
    "    df_shuffle = train_set.sample(frac=1)\n",
    "    # logger.info(len(df_shuffle))\n",
    "    df_size = len(df_shuffle)\n",
    "    idx_split_left = df_size//num_epochs * (epoch)\n",
    "    idx_split_right = df_size//num_epochs * (epoch+1)\n",
    "    train_set = pd.concat([df_shuffle.iloc[:idx_split_left], df_shuffle.iloc[idx_split_right:]])\n",
    "    val_set = df_shuffle.iloc[idx_split_left:idx_split_right]\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def pre_processing(train_set: pd.DataFrame, val_set: pd.DataFrame, test_set: pd.DataFrame):\n",
    "    \n",
    "    embedding = feature_rep(train_set, val_set, test_set)\n",
    "    # Create trianing and testing inputs\n",
    "    text_features_train = text_features(train_set, embedding)\n",
    "    text_features_val = text_features(val_set, embedding)\n",
    "    text_features_test = text_features(test_set, embedding)\n",
    "    \n",
    "    # Create labels\n",
    "    emotions_train = [_ for _ in train_set.emotions]\n",
    "    emotions_val = [_ for _ in val_set.emotions]\n",
    "\n",
    "    emotion_set = set(emotions_train)\n",
    "\n",
    "    emotion2int = {emotion: i for i, emotion in enumerate(emotion_set)}\n",
    "    int2emotion = {i: emotion for i, emotion in enumerate(emotion_set)}\n",
    "    \n",
    "    emotions_train = [emotion2int[emotion] for emotion in emotions_train]\n",
    "    emotions_val = [emotion2int[emotion] for emotion in emotions_val]\n",
    "    train_targets = np.zeros((len(emotions_train), len(emotion_set)))\n",
    "    val_targets = np.zeros((len(emotions_val), len(emotion_set)))\n",
    "    for i, emotion in enumerate(emotions_train):\n",
    "        train_targets[i, emotion] = 1\n",
    "    for i, emotion in enumerate(emotions_val):\n",
    "        val_targets[i, emotion] = 1\n",
    "\n",
    "    return embedding, text_features_train, text_features_val, text_features_test,\\\n",
    "        emotion_set, train_targets, val_targets, emotion2int, int2emotion\n",
    "\n",
    "\n",
    "def post_processing(test_set, pred_emotions):\n",
    "    output = test_set.copy()\n",
    "    output['emotions'] = pred_emotions\n",
    "    output.to_csv(\"./test_lr.csv\")\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes)-1)]\n",
    "        self.biases = [np.zeros(layer_sizes[i+1]) for i in range(len(layer_sizes)-1)]\n",
    "    \n",
    "    def layers(self):\n",
    "        return [(self.weights[i], self.biases[i]) for i in range(len(self.layer_sizes)-1)]\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        activations = [X]\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            activation = np.maximum(0, z) # ReLU activation\n",
    "            activations.append(activation)\n",
    "        return activations\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        activations = self.forward(X)\n",
    "        d_activations = [2*(activations[-1] - y)]\n",
    "        d_weights = []\n",
    "        d_biases = []\n",
    "        for i in range(len(self.layer_sizes)-2, -1, -1):\n",
    "            d_activation = d_activations[0]\n",
    "            z = np.dot(activations[i], self.weights[i]) + self.biases[i]\n",
    "            d_z = d_activation * (z > 0).astype(float) # ReLU derivative\n",
    "            d_weight = np.dot(activations[i].T, d_z)\n",
    "            d_bias = np.sum(d_z, axis=0)\n",
    "            d_activations.insert(0, np.dot(d_z, self.weights[i].T))\n",
    "            d_weights.insert(0, d_weight)\n",
    "            d_biases.insert(0, d_bias)\n",
    "        return d_weights, d_biases\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.1, num_epochs=50):\n",
    "        for epoch in range(num_epochs):\n",
    "            # logger.debug(X)\n",
    "            # logger.debug(y)\n",
    "            d_weights, d_biases = self.backward(X, y)\n",
    "            for i in range(len(self.layer_sizes)-1):\n",
    "                self.weights[i] -= learning_rate * d_weights[i]\n",
    "                self.biases[i] -= learning_rate * d_biases[i]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        activations = self.forward(X)\n",
    "        return activations[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text emotions\n",
       "0   27383  i feel awful about it too because it s my job ...  sadness\n",
       "1  110083                              im alone i feel awful  sadness\n",
       "2  140764  ive probably mentioned this before but i reall...      joy\n",
       "3  100071           i was feeling a little low few days back  sadness\n",
       "4    2837  i beleive that i am much more sensitive to oth...     love"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    file = pd.read_csv(path)\n",
    "    return file\n",
    "\n",
    "TRAIN_SET = pd.DataFrame(read_csv(\"./train.csv\"))\n",
    "TEST_SET = pd.DataFrame(read_csv(\"./test.csv\"))\n",
    "TRAIN_SET.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TRAIN_SET\n",
    "dataset_test = TEST_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = np.array([dataset_train[\"text\"]])\n",
    "emotions_train = np.array([dataset_train[\"emotions\"]])\n",
    "\n",
    "texts_test = np.array([dataset_test[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sadness', 'sadness', 'joy', ..., 'sadness', 'surprise',\n",
       "        'sadness']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(emotions_train)\n",
    "num_labels = len(labels)\n",
    "\n",
    "one_hot = np.zeros((num_labels, num_labels), np.int8)\n",
    "np.fill_diagonal(one_hot, 1)\n",
    "\n",
    "label_dict = dict(zip(labels, one_hot))\n",
    "# logger.debug(label_dict)\n",
    "\n",
    "gt_one_hot = np.array([label_dict[label] for label in emotions_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1200)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = data_loading(fold, folds)\n",
    "embedding, text_features_train, text_features_val, _,\\\n",
    "        emotion_set, train_targets, val_targets, _, _ = pre_processing(train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [len(text_features_train[0]), 128, 64, len(emotion_set)]\n",
    "nn = NN(layer_sizes)\n",
    "# logger.debug(text_features_train[0].shape)\n",
    "# logger.debug(train_targets.shape)\n",
    "nn.train(np.array(text_features_train), train_targets, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_pred = nn.predict(text_features_val)\n",
    "\n",
    "emotions_pred_lables = np.array([labels[_] for _ in np.argmax(emotions_pred, axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(emotions_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        # logger.debug(f\"W1: {self.W1.shape}\")\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        # logger.debug(f\"b1: {self.b1.shape}\")\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        # logger.debug(f\"W2: {self.W2.shape}\")\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        # logger.debug(f\"b2: {self.b2.shape}\")\n",
    "        self.scores = []\n",
    "    def forward(self, X):\n",
    "        # logger.debug(X.shape)\n",
    "        # logger.debug(self.W1.shape)\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        # logger.debug(f\"z1: {self.z1.shape}\")\n",
    "        # self.a1 = np.tanh(self.z1)\n",
    "        # Use ReLU instead\n",
    "        self.a1 = np.maximum(0, self.z1)\n",
    "        # logger.debug(self.W2.shape)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        exp_scores = np.exp(self.z2 - np.max(self.z2))\n",
    "        self.probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return self.probs\n",
    "    \n",
    "        \n",
    "    def backward(self, X, y, y_hat, learning_rate):\n",
    "        # logger.debug(f\"X.shape={X.shape}, y.shape={y.shape}, y_hat.shape={y_hat.shape}\")\n",
    "        delta2 = y_hat\n",
    "        delta2[range(len(X)), np.argmax(y, axis=1)] -= 1\n",
    "        # logger.debug(self.W2.shape)\n",
    "        # logger.debug(self.z1.shape)\n",
    "        delta1 = delta2.dot(self.W2.T) * (1 - np.power(np.tanh(self.z1), 2))\n",
    "        \n",
    "        dW2 = np.dot(self.a1.T, delta2)\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "        dW1 = np.dot(X.T, delta1)\n",
    "        db1 = np.sum(delta1, axis=0)\n",
    "\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, val_targets, text_features_val, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            # forward pass\n",
    "            y_hat = self.forward(X)\n",
    "            # calculate the loss\n",
    "            correct_logprobs = -np.log(y_hat[range(len(X)), np.argmax(y, axis=1)]+1e-12)\n",
    "            data_loss = np.sum(correct_logprobs) / len(X)\n",
    "            # print the loss every 100 epochs\n",
    "            \n",
    "\n",
    "                # self.scores.append(score)\n",
    "            # backward pass\n",
    "            # logger.debug(X.shape)\n",
    "            # logger.debug(y.shape)\n",
    "            self.backward(X, y, y_hat, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                score = np.mean(nn.predict(text_features_val) == np.argmax(val_targets, axis=1))\n",
    "                logger.info(f\"Epoch: {epoch}, loss: {data_loss}, accuracy: {score * 100:.2f}%.\")\n",
    "\n",
    "            # eval\n",
    "\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        # forward pass\n",
    "        y_hat = self.forward(X)\n",
    "        # return the index with highest probability\n",
    "        return np.argmax(y_hat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956, 4892)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 17:35:31.260 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.261 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.263 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.264 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.265 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.266 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.268 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.314 | DEBUG    | __main__:forward_eval:31 - z1: (239, 120)\n",
      "2023-03-01 17:35:31.315 | DEBUG    | __main__:forward_eval:35 - (120, 6)\n",
      "2023-03-01 17:35:31.316 | INFO     | __main__:train:77 - Epoch: 0, loss: 1.7916717418054613, accuracy: 32.22%.\n",
      "2023-03-01 17:35:31.351 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.352 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.354 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.355 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.357 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.360 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.361 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.428 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.429 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.431 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.432 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.433 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.435 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.436 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.514 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.515 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.517 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.518 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.519 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.520 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.521 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.620 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.621 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.623 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.624 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.625 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.627 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.628 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.698 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.699 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.701 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.702 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.702 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.703 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.704 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.763 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.764 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.766 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.767 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.767 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.768 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.769 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.830 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.831 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.833 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.834 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.834 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.835 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.836 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.893 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.894 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.895 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.896 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.897 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.899 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.900 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:31.969 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:31.970 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:31.973 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:31.975 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:31.976 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:31.977 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:31.978 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.140 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.142 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.144 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.145 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.146 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.147 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.148 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.224 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.225 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.228 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.229 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.230 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.232 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.233 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.303 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.328 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.329 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.330 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.331 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.332 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.333 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.398 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.399 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.401 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.402 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.403 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.404 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.406 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.463 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.464 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.466 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.467 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.468 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.469 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.470 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.530 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.531 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.533 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.533 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.534 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.536 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.537 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.599 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.600 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.603 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.604 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.605 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.606 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.607 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.674 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.675 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.677 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.678 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.679 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.680 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.681 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.751 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.752 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.755 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.756 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.758 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.760 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.761 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.861 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.862 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.864 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.865 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.866 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.867 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.868 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:32.949 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:32.952 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:32.955 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:32.957 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:32.959 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:32.963 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:32.964 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.038 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.039 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.043 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.044 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.045 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.047 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.048 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.131 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.132 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.134 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.135 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.136 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.138 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.139 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.205 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.206 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.208 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.209 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.210 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.212 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.213 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.267 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.268 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.270 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.272 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.273 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.274 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.275 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.334 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.335 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.337 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.338 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.340 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.342 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.343 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.403 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.404 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.406 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.408 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.409 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.410 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.411 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.472 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.473 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.475 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.476 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.476 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.478 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.479 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.540 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.541 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.543 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.545 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.545 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.547 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.548 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.606 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.607 | DEBUG    | __main__:forward:20 - (120, 6)\n",
      "2023-03-01 17:35:33.609 | DEBUG    | __main__:train:72 - (956, 4892)\n",
      "2023-03-01 17:35:33.610 | DEBUG    | __main__:train:73 - (956, 6)\n",
      "2023-03-01 17:35:33.611 | DEBUG    | __main__:backward:43 - X.shape=(956, 4892), y.shape=(956, 6), y_hat.shape=(956, 6)\n",
      "2023-03-01 17:35:33.612 | DEBUG    | __main__:backward:46 - (120, 6)\n",
      "2023-03-01 17:35:33.613 | DEBUG    | __main__:backward:47 - (956, 120)\n",
      "2023-03-01 17:35:33.786 | DEBUG    | __main__:forward:16 - z1: (956, 120)\n",
      "2023-03-01 17:35:33.790 | DEBUG    | __main__:forward:20 - (120, 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wenxinjiang/Desktop/23Spring/CS 577/CS577-NLP/HW1/hw1_NN.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m nn \u001b[39m=\u001b[39m NeuralNetwork(\u001b[39mlen\u001b[39m(text_features_train[\u001b[39m0\u001b[39m]), \u001b[39m120\u001b[39m, \u001b[39mlen\u001b[39m(emotion_set))\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# train the neural network\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m nn\u001b[39m.\u001b[39;49mtrain(X, y, val_targets, text_features_val, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n",
      "\u001b[1;32m/Users/wenxinjiang/Desktop/23Spring/CS 577/CS577-NLP/HW1/hw1_NN.ipynb Cell 17\u001b[0m in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, X, y, val_targets, text_features_val, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, X, y, val_targets, text_features_val, num_epochs, learning_rate):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         \u001b[39m# calculate the loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         correct_logprobs \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(y_hat[\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X)), np\u001b[39m.\u001b[39margmax(y, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\u001b[39m+\u001b[39m\u001b[39m1e-12\u001b[39m)\n",
      "\u001b[1;32m/Users/wenxinjiang/Desktop/23Spring/CS 577/CS577-NLP/HW1/hw1_NN.ipynb Cell 17\u001b[0m in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW2\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ma1, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW2) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m exp_scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz2 \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz2))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenxinjiang/Desktop/23Spring/CS%20577/CS577-NLP/HW1/hw1_NN.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobs \u001b[39m=\u001b[39m exp_scores \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(exp_scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# generate some sample data\n",
    "text_features_train = np.array(text_features_train)\n",
    "train_targets = np.array(train_targets)\n",
    "text_features_val = np.array(text_features_val)\n",
    "val_targets = np.array(val_targets)\n",
    "X = np.array(text_features_train)\n",
    "y = np.array(train_targets)\n",
    "# create a neural network with 100 hidden units\n",
    "nn = NeuralNetwork(len(text_features_train[0]), 120, len(emotion_set))\\\n",
    "\n",
    "# train the neural network\n",
    "nn.train(X, y, val_targets, text_features_val, num_epochs=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4769874476987448"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (4.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: pandas in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.2)\n",
      "Requirement already satisfied: pyfume in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: fst-pso in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: simpful in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Requirement already satisfied: tweet-preprocessor in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (0.6.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement matplotlib.pyplot (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for matplotlib.pyplot\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install tweet-preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from loguru import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    file = pd.read_csv(path)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18231</td>\n",
       "      <td>i find myself frustrated with christians becau...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10714</td>\n",
       "      <td>i am one of those people who feels like going ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35177</td>\n",
       "      <td>i feel especially pleased about this as this h...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>122177</td>\n",
       "      <td>i was struggling with these awful feelings and...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26723</td>\n",
       "      <td>i feel so enraged but helpless at the same time</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41979</td>\n",
       "      <td>i said feeling a bit rebellious</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2046</td>\n",
       "      <td>i also feel disillusioned that someone who cla...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>98659</td>\n",
       "      <td>i mean is on this stupid trip of making the gr...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50434</td>\n",
       "      <td>i woke up feeling particularly vile tried to i...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9280</td>\n",
       "      <td>i could feel the vile moth burrowing its way i...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>92846</td>\n",
       "      <td>i know its just doing its job and doesnt actua...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>106363</td>\n",
       "      <td>i wish you knew every word i write i write for...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23395</td>\n",
       "      <td>i feel weird knowing mine died when i wasn t a...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31583</td>\n",
       "      <td>i feel assured that there is no such thing as ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8271</td>\n",
       "      <td>i feel blessed everyday for our little man and...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32503</td>\n",
       "      <td>i feel exhausted when i go home but i am alway...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>91430</td>\n",
       "      <td>i stil can see kaibas face on every tv screen ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9813</td>\n",
       "      <td>i don t like to feel uncomfortable with being ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>102982</td>\n",
       "      <td>i was left kinda feeling stupid and insecure</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>489</td>\n",
       "      <td>i alternate between feeling sympathetic toward...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>33737</td>\n",
       "      <td>i have a pretty bad feeling the last two books...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>107365</td>\n",
       "      <td>i sincerely believe that every client celebrit...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24211</td>\n",
       "      <td>i don t feel comfortable around you</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6990</td>\n",
       "      <td>i am happy that you don t feel as burdened and...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9036</td>\n",
       "      <td>i am still here i am not dead yet but these th...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>84022</td>\n",
       "      <td>i have been continually surprised at how much ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10350</td>\n",
       "      <td>i pretty much waddled out of the hospital feel...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5023</td>\n",
       "      <td>i apologize to him almost every day for my lac...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>11640</td>\n",
       "      <td>i am still searching for direction but at leas...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11239</td>\n",
       "      <td>i am so angry that i feel like i have to go th...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>14000</td>\n",
       "      <td>i feel passionate about today because of him</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8165</td>\n",
       "      <td>i am only having day a week where i am feeling...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>79</td>\n",
       "      <td>article published</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>33835</td>\n",
       "      <td>i see things so clearly and with so much depth...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>55011</td>\n",
       "      <td>i feel myself relax a little he wouldn t be th...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>13151</td>\n",
       "      <td>i feel doubtful about my place in the world of...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>113523</td>\n",
       "      <td>im feeling rotten i dont post much new materia...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>83005</td>\n",
       "      <td>i have a feeling that theyre probably not goin...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>119356</td>\n",
       "      <td>ive been feeling pretty shitty and been in a p...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1629</td>\n",
       "      <td>i cant even imagine how my mom and her three y...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>37068</td>\n",
       "      <td>i feel glad i am on the side of the law and th...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>41686</td>\n",
       "      <td>i will most likely feel overwhelmed and exhaus...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>15073</td>\n",
       "      <td>i can feel myself reaching out extending throu...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>55053</td>\n",
       "      <td>im looking right now so desperately looking fo...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10992</td>\n",
       "      <td>i am really hoping to get to target soon and i...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  emotions\n",
       "0    27383  i feel awful about it too because it s my job ...   sadness\n",
       "1   110083                              im alone i feel awful   sadness\n",
       "2   140764  ive probably mentioned this before but i reall...       joy\n",
       "3   100071           i was feeling a little low few days back   sadness\n",
       "4     2837  i beleive that i am much more sensitive to oth...      love\n",
       "5    18231  i find myself frustrated with christians becau...      love\n",
       "6    10714  i am one of those people who feels like going ...       joy\n",
       "7    35177  i feel especially pleased about this as this h...       joy\n",
       "8   122177  i was struggling with these awful feelings and...       joy\n",
       "9    26723    i feel so enraged but helpless at the same time     anger\n",
       "10   41979                    i said feeling a bit rebellious     anger\n",
       "11    2046  i also feel disillusioned that someone who cla...   sadness\n",
       "12   98659  i mean is on this stupid trip of making the gr...       joy\n",
       "13   50434  i woke up feeling particularly vile tried to i...     anger\n",
       "14    9280  i could feel the vile moth burrowing its way i...     anger\n",
       "15   92846  i know its just doing its job and doesnt actua...       joy\n",
       "16  106363  i wish you knew every word i write i write for...   sadness\n",
       "17   23395  i feel weird knowing mine died when i wasn t a...      fear\n",
       "18   31583  i feel assured that there is no such thing as ...       joy\n",
       "19    8271  i feel blessed everyday for our little man and...      love\n",
       "20   32503  i feel exhausted when i go home but i am alway...   sadness\n",
       "21   91430  i stil can see kaibas face on every tv screen ...   sadness\n",
       "22    9813  i don t like to feel uncomfortable with being ...      fear\n",
       "23  102982       i was left kinda feeling stupid and insecure   sadness\n",
       "24     489  i alternate between feeling sympathetic toward...      love\n",
       "25   33737  i have a pretty bad feeling the last two books...     anger\n",
       "26  107365  i sincerely believe that every client celebrit...       joy\n",
       "27   24211                i don t feel comfortable around you       joy\n",
       "28    6990  i am happy that you don t feel as burdened and...   sadness\n",
       "29    9036  i am still here i am not dead yet but these th...   sadness\n",
       "30   84022  i have been continually surprised at how much ...       joy\n",
       "31   10350  i pretty much waddled out of the hospital feel...  surprise\n",
       "32    5023  i apologize to him almost every day for my lac...      fear\n",
       "33   11640  i am still searching for direction but at leas...       joy\n",
       "34   11239  i am so angry that i feel like i have to go th...       joy\n",
       "35   14000       i feel passionate about today because of him      love\n",
       "36    8165  i am only having day a week where i am feeling...   sadness\n",
       "37      79                                  article published       joy\n",
       "38   33835  i see things so clearly and with so much depth...      fear\n",
       "39   55011  i feel myself relax a little he wouldn t be th...       joy\n",
       "40   13151  i feel doubtful about my place in the world of...      fear\n",
       "41  113523  im feeling rotten i dont post much new materia...   sadness\n",
       "42   83005  i have a feeling that theyre probably not goin...       joy\n",
       "43  119356  ive been feeling pretty shitty and been in a p...   sadness\n",
       "44    1629  i cant even imagine how my mom and her three y...  surprise\n",
       "45   37068  i feel glad i am on the side of the law and th...       joy\n",
       "46   41686  i will most likely feel overwhelmed and exhaus...      fear\n",
       "47   15073  i can feel myself reaching out extending throu...       joy\n",
       "48   55053  im looking right now so desperately looking fo...     anger\n",
       "49   10992  i am really hoping to get to target soon and i...       joy"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SET = read_csv(\"./train.csv\")\n",
    "TRAIN_SET.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.23.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-macosx_10_9_x86_64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (65.6.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.12.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wenxinjiang/miniconda3/envs/CS577/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "Installing collected packages: pydantic, en-core-web-sm\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.2\n",
      "    Uninstalling pydantic-1.10.2:\n",
      "      Successfully uninstalled pydantic-1.10.2\n",
      "Successfully installed en-core-web-sm-3.3.0 pydantic-1.8.2\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................Beginning of Logistic Regression................\n",
      "..................End of Logistic Regression................\n",
      "------------------------------------------------\n",
      "..................Beginning of Neural Network................\n",
      "Training NN model using layers=[(30, <function mat_sigmoid at 0x7f9e745e70d0>, <function mat_sigmoid_dv at 0x7f9e745e71f0>)], train_step_size=0.01, train_iter=20000, dropout_rate=0.15, batch_size=50 over 1200 rows and 200 features...\n"
     ]
    }
   ],
   "source": [
    "# general libraries\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# nltk libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(df : pd.DataFrame, schema=None, text_col='text', min_ngram=1, max_ngram=1, **kwargs):\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    def rebalance(df):\n",
    "        # rebalance data so that each label has uniform probability of being encountered\n",
    "        counts = df.emotions.value_counts().sort_values()\n",
    "        a = pd.DataFrame(columns=df.columns)\n",
    "        mc = 500\n",
    "        for e in df.emotions.unique():\n",
    "            a = pd.concat([a, df[df.emotions==e].sample(n=mc, replace=True, ignore_index=True)])\n",
    "\n",
    "        return a.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def tag_tokenize(x):\n",
    "        if 'lemmatize' in kwargs and kwargs['lemmatize']==True:\n",
    "            return pos_tag(word_tokenize(x.lower()))\n",
    "        else:\n",
    "            return word_tokenize(x.lower())\n",
    "\n",
    "    def lemmatize(x):\n",
    "        # https://stackoverflow.com/questions/35870282/nltk-lemmatizer-and-pos-tag\n",
    "        wnpos = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "        return [wnl.lemmatize(y[0], wnpos(y[1])) for y in x]\n",
    "\n",
    "    stops = set(stopwords.words('english'))\n",
    "    def remove_stopwords(x):\n",
    "        return [y for y in x if y not in stops]\n",
    "\n",
    "    def build_ngrams(x, l, u):\n",
    "        r = x\n",
    "        z = list()\n",
    "        for i in range(len(r)):\n",
    "            for j in range(l-1, u):\n",
    "                y = r[max(i-j,0):min(i+1, len(r))]\n",
    "                if len(y) == j+1:\n",
    "                    z.append(' '.join(y))\n",
    "        return z\n",
    "\n",
    "    def build_tf(x, word):\n",
    "        return x.count(word)\n",
    "\n",
    "    def build_bow(x, ignore_cols):\n",
    "        if x.name not in ignore_cols:\n",
    "            return x.apply(lambda x: int(x > 0))\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def onehot_encode(x, classes):\n",
    "        return [int(x==y) for y in classes]\n",
    "\n",
    "    if 'rebalance' in kwargs and kwargs['rebalance']==True:\n",
    "        a = rebalance(df)\n",
    "    else:\n",
    "        a = df.copy()\n",
    "    # transform text\n",
    "    a['text'] = a.text.apply(tag_tokenize)\n",
    "    if 'lemmatize' in kwargs and kwargs['lemmatize']==True:\n",
    "        a['text'] = a.text.apply(lemmatize)\n",
    "    if 'remove_stopwords' in kwargs and kwargs['remove_stopwords']==True:\n",
    "        a['text'] = a.text.apply(remove_stopwords)\n",
    "    a['text'] = a.text.apply(build_ngrams, args=(min_ngram,max_ngram))\n",
    "\n",
    "    # build schema\n",
    "    if schema is None:\n",
    "        schema = dict()\n",
    "        for idx, row in a.iterrows():\n",
    "            for w in row.text:\n",
    "                if w in schema:\n",
    "                    schema[w] = schema[w] + 1\n",
    "                else:\n",
    "                    schema[w] = 1\n",
    "\n",
    "    sorted_labels = [x[0] for x in sorted([(k,v) for k,v in schema.items()], key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "    if 'feature_count' in kwargs:\n",
    "        n = kwargs['feature_count']\n",
    "        for w in sorted_labels[n:]:\n",
    "            del schema[w]\n",
    "\n",
    "    # apply corpus to text\n",
    "    text = a.text\n",
    "    del a['text']\n",
    "    ignore_cols = list(a.columns)\n",
    "    for w in schema.keys():\n",
    "        a = pd.concat([a,pd.DataFrame({f'_{w}': text.apply(build_tf, word=w)})], axis=1)\n",
    "\n",
    "    if 'use_tf' not in kwargs or kwargs['use_tf']==False:\n",
    "        a = a.apply(build_bow, ignore_cols=ignore_cols)\n",
    "\n",
    "    # onehot encode target column\n",
    "    classes = list()\n",
    "    if 'emotions' in a.columns:\n",
    "        classes = sorted(a.emotions.unique().tolist())\n",
    "        a['emotions'] = a.emotions.apply(onehot_encode, classes=classes)\n",
    "\n",
    "    return a, schema, classes\n",
    "\n",
    "def train_lr(df, train_lambda, train_step_size, train_max_iter, train_tolerance, ignore_cols=['id'], label_col='emotions', **kwargs) -> np.matrix:\n",
    "    # https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.4-MultiLogistic.pdf\n",
    "    # get vector, weight, and target matrices\n",
    "    print(f'Training LR model using {train_lambda=}, {train_step_size=}, {train_max_iter=}, {train_tolerance=} over {df.shape[0]} rows and {df.shape[1]-len(ignore_cols)-1} features...')\n",
    "    target_matrix = np.matrix([np.array(x) for x in df[label_col].tolist()])\n",
    "    vector_matrix = np.matrix(df.loc[:, ~df.columns.isin(ignore_cols+[label_col])])\n",
    "    # add bias term\n",
    "    vector_matrix = np.insert(vector_matrix, vector_matrix.shape[1], np.ones(vector_matrix.shape[0]), axis=1)\n",
    "    weight_matrix = np.matrix(np.zeros((target_matrix.shape[1], vector_matrix.shape[1])))\n",
    "\n",
    "    i = 0\n",
    "    old_weight_matrix = weight_matrix.copy()\n",
    "    while i < train_max_iter and (np.linalg.norm(weight_matrix-old_weight_matrix) >= train_tolerance or i < 1):\n",
    "        old_weight_matrix = weight_matrix.copy()\n",
    "        i = i + 1\n",
    "        # Can compute softmax using matrix multiplication:\n",
    "        # (V)(W.T)\n",
    "        dp = np.exp(vector_matrix @ weight_matrix.T)\n",
    "        # then normalize each row in the resulting matrix and take difference of true label\n",
    "        dp = (dp/dp.sum(axis=1)) - target_matrix\n",
    "        # then compute gradient by adding regularization term and ((V.T)(L)).T\n",
    "        tg = (weight_matrix * train_lambda) + (vector_matrix.T @ dp).T\n",
    "        # apply gradient\n",
    "        weight_matrix = weight_matrix - (train_step_size * tg)\n",
    "\n",
    "    return weight_matrix\n",
    "\n",
    "def apply_lr(df, model, ignore_cols=['id','emotions']) -> pd.DataFrame:\n",
    "    a = df.copy()\n",
    "    vector_matrix = np.matrix(df.loc[:, ~df.columns.isin(ignore_cols)])\n",
    "    # add bias term\n",
    "    vector_matrix = np.insert(vector_matrix, vector_matrix.shape[1], np.ones(vector_matrix.shape[0]), axis=1)\n",
    "\n",
    "    # ultimately, it doesn't matter that we apply softmax here since each entry will only differ\n",
    "    # by a multiplicative normalizing constant wrt to its row, but do it anyways for consistency\n",
    "    dp = np.exp(vector_matrix @ model.T)\n",
    "    a['predictions'] = (dp/dp.sum(axis=1)).tolist()\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def LR():\n",
    "    # learn a model and do any necessary processing\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    preprocessed_train_data, schema, classes = preprocess(train_data, min_ngram=1, max_ngram=3, lemmatize=False, remove_stopwords=True, feature_count=2000)\n",
    "    preprocessed_test_data,_,__ = preprocess(test_data, schema=schema, min_ngram=1, max_ngram=3, lemmatize=False, remove_stopwords=True, feature_count=2000)\n",
    "    model = train_lr(preprocessed_train_data, train_lambda=0.05, train_step_size=0.05, train_max_iter=500, train_tolerance=0.00001)\n",
    "    transformed_data = apply_lr(preprocessed_test_data, model)\n",
    "    test_data['emotions'] = transformed_data.predictions.apply(lambda x: classes[np.argmax(x)])\n",
    "    test_data.to_csv('test_lg.csv', index=False)\n",
    "\n",
    "def mat_sigmoid(W, X):\n",
    "    return 1 / (1 + np.exp(-(W @ X)))\n",
    "\n",
    "def mat_sigmoid_dv(W, X):\n",
    "    dp = W @ X\n",
    "    dp = 1/(1+np.exp(-dp))\n",
    "    return np.multiply(dp, 1-dp)\n",
    "\n",
    "def mat_swish1(W,X):\n",
    "    dp = W @ X\n",
    "    return dp / (1 + np.exp(-dp))\n",
    "\n",
    "def mat_swish1_dv(W,X):\n",
    "    dp = W @ X\n",
    "    return (1 + np.exp(-dp) + np.multiply(dp, np.exp(-dp))) / np.power(1 + np.exp(-dp),2)\n",
    "\n",
    "def mat_relu(W,X):\n",
    "    dp = W @ X\n",
    "    return np.multiply((dp > 0), dp)\n",
    "\n",
    "def mat_relu_dv(W,X):\n",
    "    dp = W @ X\n",
    "    return (dp > 0) * 1\n",
    "\n",
    "def mat_softmax(W,X):\n",
    "    dp = W @ X\n",
    "    z = np.exp(dp - np.max(dp))\n",
    "    return z/z.sum(axis=0)\n",
    "\n",
    "def mat_softmax_jc(W, X, da):\n",
    "    # compute softmax values\n",
    "    z = mat_softmax(W,X)\n",
    "    # https://themaverickmeerkat.com/2019-10-23-Softmax/\n",
    "    p = z.T\n",
    "    m, n = p.shape\n",
    "    t1 = np.einsum('ij,ik->ijk', p, p)\n",
    "    t2 = np.einsum('ij,jk->ijk', p, np.eye(n, n))\n",
    "    dS = t2 - t1\n",
    "    return np.einsum('ijk,ik->ij', dS, da.T).T\n",
    "\n",
    "def mat_squared_loss_dv(A, G):\n",
    "    return -(G-A)\n",
    "\n",
    "def train_nn(df, layers, train_step_size, train_iter, dropout_rate, batch_size=0, ignore_cols=['id'], label_col='emotions', **kwargs):\n",
    "    # https://en.wikipedia.org/wiki/Backpropagation#Matrix_multiplication\n",
    "    # get vector, weight, and target matrices\n",
    "    print(f'Training NN model using {layers=}, {train_step_size=}, {train_iter=}, {dropout_rate=}, {batch_size=} over {df.shape[0]} rows and {df.shape[1]-len(ignore_cols)-1} features...')\n",
    "    target_matrix = np.matrix([np.array(x) for x in df[label_col].tolist()]).T\n",
    "    vector_matrix = np.matrix(df.loc[:, ~df.columns.isin(ignore_cols+[label_col])])\n",
    "    # add bias term\n",
    "    vector_matrix = np.insert(vector_matrix, vector_matrix.shape[1], np.ones(vector_matrix.shape[0]), axis=1).T\n",
    "\n",
    "    # unpack layer data\n",
    "    layer_sizes = [l[0] for l in layers]\n",
    "    activations = [None] + [l[1] if len(l) == 3 else mat_sigmoid for l in layers] + [mat_softmax]\n",
    "    activation_derivatives = [None] + [l[2] if len(l) == 3 else mat_sigmoid_dv for l in layers] + [mat_softmax_jc]\n",
    "\n",
    "    # assume fully connected\n",
    "    weight_shapes = list()\n",
    "    if len(layer_sizes) == 0:\n",
    "        weight_shapes = [(target_matrix.shape[0], vector_matrix.shape[0])]\n",
    "    else:\n",
    "        weight_shapes = [(layer_sizes[0] + 1, vector_matrix.shape[0])]\n",
    "        for l in range(1, len(layer_sizes)):\n",
    "            weight_shapes.append((layer_sizes[l] + 1, layer_sizes[l-1] + 1))\n",
    "        weight_shapes.append((target_matrix.shape[0], layer_sizes[-1] + 1))\n",
    "    # weights = [None, np.matrix(np.random.randn(6, vector_matrix.shape[1])), np.matrix(np.random.randn(target_matrix.shape[1], 6))]\n",
    "    # print(weight_shapes)\n",
    "    weights = [None] + [np.matrix(np.random.randn(*s)) for s in weight_shapes]\n",
    "\n",
    "    # don't need to treat bias term super specially, just need to fix A value to 1, and derivative to 0?\n",
    "\n",
    "    for i in range(1,train_iter+1):\n",
    "        # forward feed phase\n",
    "        # activations\n",
    "        batch = np.random.choice(vector_matrix.shape[1], 75) if batch_size > 0 else None\n",
    "        # A = [vector_matrix[:,batch]]\n",
    "        A = [vector_matrix[:, batch] if batch_size > 0 else vector_matrix]\n",
    "        labels = target_matrix[:, batch] if batch_size > 0 else target_matrix\n",
    "        # activation derivatives\n",
    "        D = [A[-1]]\n",
    "        for l in range(1, len(weights)):\n",
    "            Xp = np.copy(A[-1])\n",
    "            # add bias term if needed\n",
    "            # if l < len(weights)-1:\n",
    "            Xp[Xp.shape[0]-1] = 1\n",
    "            a = activations[l](weights[l], Xp)\n",
    "            d = activation_derivatives[l](weights[l], Xp, a - labels) if l==len(weights)-1 else activation_derivatives[l](weights[l], Xp)\n",
    "            # apply dropout\n",
    "            if dropout_rate > 0 and l < len(weights)-1:\n",
    "                dead_neurons = np.random.choice(a.shape[0], int(a.shape[0] * dropout_rate))\n",
    "                a[dead_neurons] = 0\n",
    "                d[dead_neurons] = 0\n",
    "                a = a / (1 - dropout_rate)\n",
    "                d = d / (1 - dropout_rate)\n",
    "            A.append(a)\n",
    "            D.append(d)\n",
    "\n",
    "        # acc = np.sum(np.argmax(A[-1], axis=0) == np.argmax(target_matrix, axis=0)) / target_matrix.shape[1]\n",
    "        # err = -np.sum(np.sum(np.multiply(target_matrix, np.nan_to_num(np.log(A[-1]))), axis=0)) # categorical cross entropy loss\n",
    "        # print(i, acc, err)\n",
    "\n",
    "        # back prop phase\n",
    "        # partial product\n",
    "        # P = np.multiply(D[-1], (A[-1] - target_matrix))\n",
    "        # with softmax, we already include dC/da\n",
    "        P = D[-1]\n",
    "        # gradients\n",
    "        # G = list()\n",
    "        for l in range(len(A)-1, 0, -1):\n",
    "            g = P @ A[l-1].T\n",
    "            P = np.multiply(D[l-1], weights[l].T @ P)\n",
    "            weights[l] = weights[l] - (train_step_size * g)\n",
    "\n",
    "    return list(zip(weights, activations))\n",
    "    \n",
    "            \n",
    "def apply_nn(df, model, ignore_cols=['id','emotions']) -> pd.DataFrame:\n",
    "    b = df.copy()\n",
    "    vector_matrix = np.matrix(df.loc[:, ~df.columns.isin(ignore_cols)])\n",
    "    # add bias term\n",
    "    vector_matrix = np.insert(vector_matrix, vector_matrix.shape[1], np.ones(vector_matrix.shape[0]), axis=1).T\n",
    "    A = [vector_matrix]\n",
    "    for l in range(1, len(model)):\n",
    "        Xp = np.copy(A[-1])\n",
    "        Xp[Xp.shape[0]-1] = 1\n",
    "        a = model[l][1](model[l][0], Xp)\n",
    "        A.append(a)\n",
    "    \n",
    "    b['predictions'] = A[-1].T.tolist()\n",
    "    return b\n",
    "\n",
    "def NN():\n",
    "    # your Multi-layer Neural Network\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    del train_data['text']\n",
    "    test_text = test_data.text\n",
    "    del test_data['text']\n",
    "    embeddings = pd.read_csv('naive_glove_embeddings.csv')\n",
    "    train_data_embeddings = pd.merge(train_data, embeddings, on='id')\n",
    "    test_data_embeddings = pd.merge(test_data, embeddings, on='id')\n",
    "    classes = sorted(list(train_data.emotions.unique()))\n",
    "    # one-hot encode label\n",
    "    train_data_embeddings.emotions = train_data_embeddings.emotions.apply(lambda x: [int(x==y) for y in classes])\n",
    "    model = train_nn(train_data_embeddings, layers=[(30, mat_sigmoid, mat_sigmoid_dv)], train_step_size=0.01, train_iter=20000, dropout_rate=0.15, batch_size=50)\n",
    "    transformed_data = apply_nn(test_data_embeddings, model)\n",
    "    test_data['text'] = test_text\n",
    "    test_data['emotions'] = transformed_data.predictions.apply(lambda x: classes[np.argmax(x)])\n",
    "    test_data.to_csv('test_nn.csv', index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "    print (\"..................Beginning of Logistic Regression................\")\n",
    "    # LR()\n",
    "    print (\"..................End of Logistic Regression................\")\n",
    "\n",
    "    print(\"------------------------------------------------\")\n",
    "\n",
    "    print (\"..................Beginning of Neural Network................\")\n",
    "    NN()\n",
    "    print (\"..................End of Neural Network................\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS577",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f55dcac901e5017ee05c68d87b9867c424b90a62d5e7d45ca4780af254ec6d79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

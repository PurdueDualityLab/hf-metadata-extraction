{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting scipy>=1.7.0\n",
      "  Downloading scipy-1.10.0-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "     ---------------------------------------- 42.2/42.2 MB 7.9 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.18.5\n",
      "  Downloading numpy-1.24.2-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 14.9/14.9 MB 8.3 MB/s eta 0:00:00\n",
      "Collecting Cython==0.29.32\n",
      "  Downloading Cython-0.29.32-py2.py3-none-any.whl (986 kB)\n",
      "     -------------------------------------- 986.3/986.3 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.3-cp38-cp38-win_amd64.whl (11.0 MB)\n",
      "     ---------------------------------------- 11.0/11.0 MB 7.3 MB/s eta 0:00:00\n",
      "Collecting pyfume\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 67.1/67.1 kB ? eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     ------------------------------------- 499.4/499.4 kB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Collecting fst-pso\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting simpful\n",
      "  Downloading simpful-2.9.0-py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB ? eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.6/140.6 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.0.1-cp38-cp38-win_amd64.whl (95 kB)\n",
      "     ---------------------------------------- 95.8/95.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20431 sha256=b92bec9ed1887c93ca4b01cb01cd3e893ba44e69a2516bfdc14b61397a5cdca0\n",
      "  Stored in directory: c:\\users\\wenxi\\appdata\\local\\pip\\cache\\wheels\\b4\\d2\\63\\953ff58cf13a31f09d49568238da542cfaedbc8d9cd0adbf80\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3514 sha256=21e7ad7d56e1e0925e2fbc6bfb3c73054b58e8aa2922ef51f9ad7568addb96fb\n",
      "  Stored in directory: c:\\users\\wenxi\\appdata\\local\\pip\\cache\\wheels\\26\\26\\35\\93cbca6b31bba14406cde5b9c6f6f61b93dfe0ce68ca084d25\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: pytz, charset-normalizer, urllib3, smart-open, numpy, idna, Cython, scipy, requests, pandas, simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim\n",
      "Successfully installed Cython-0.29.32 FuzzyTM-2.0.5 charset-normalizer-3.0.1 fst-pso-1.8.1 gensim-4.3.0 idna-3.4 miniful-0.0.6 numpy-1.24.2 pandas-1.5.3 pyfume-0.2.25 pytz-2022.7.1 requests-2.28.2 scipy-1.10.0 simpful-2.9.0 smart-open-6.3.0 urllib3-1.26.14\n",
      "Collecting tweet-preprocessor\n",
      "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: tweet-preprocessor\n",
      "Successfully installed tweet-preprocessor-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install tweet-preprocessor\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install loguru\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from loguru import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wenxi\\anaconda3\\envs\\cs577\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/wenxinjiang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................Beginning of Logistic Regression................\n",
      "..................End of Logistic Regression................\n",
      "------------------------------------------------\n",
      "..................Beginning of Neural Network................\n",
      "Training NN model using layers=[(30, <function mat_sigmoid at 0x7f9e745e70d0>, <function mat_sigmoid_dv at 0x7f9e745e71f0>)], train_step_size=0.01, train_iter=20000, dropout_rate=0.15, batch_size=50 over 1200 rows and 200 features...\n"
     ]
    }
   ],
   "source": [
    "# general libraries\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# nltk libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(df : pd.DataFrame, schema=None, text_col='text', min_ngram=1, max_ngram=1, **kwargs):\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    def rebalance(df):\n",
    "        # rebalance data so that each label has uniform probability of being encountered\n",
    "        counts = df.emotions.value_counts().sort_values()\n",
    "        a = pd.DataFrame(columns=df.columns)\n",
    "        mc = 500\n",
    "        for e in df.emotions.unique():\n",
    "            a = pd.concat([a, df[df.emotions==e].sample(n=mc, replace=True, ignore_index=True)])\n",
    "\n",
    "        return a.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def tag_tokenize(x):\n",
    "        if 'lemmatize' in kwargs and kwargs['lemmatize']==True:\n",
    "            return pos_tag(word_tokenize(x.lower()))\n",
    "        else:\n",
    "            return word_tokenize(x.lower())\n",
    "\n",
    "    def lemmatize(x):\n",
    "        # https://stackoverflow.com/questions/35870282/nltk-lemmatizer-and-pos-tag\n",
    "        wnpos = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "        return [wnl.lemmatize(y[0], wnpos(y[1])) for y in x]\n",
    "\n",
    "    stops = set(stopwords.words('english'))\n",
    "    def remove_stopwords(x):\n",
    "        return [y for y in x if y not in stops]\n",
    "\n",
    "    def build_ngrams(x, l, u):\n",
    "        r = x\n",
    "        z = list()\n",
    "        for i in range(len(r)):\n",
    "            for j in range(l-1, u):\n",
    "                y = r[max(i-j,0):min(i+1, len(r))]\n",
    "                if len(y) == j+1:\n",
    "                    z.append(' '.join(y))\n",
    "        return z\n",
    "\n",
    "    def build_tf(x, word):\n",
    "        return x.count(word)\n",
    "\n",
    "    def build_bow(x, ignore_cols):\n",
    "        if x.name not in ignore_cols:\n",
    "            return x.apply(lambda x: int(x > 0))\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def onehot_encode(x, classes):\n",
    "        return [int(x==y) for y in classes]\n",
    "\n",
    "    if 'rebalance' in kwargs and kwargs['rebalance']==True:\n",
    "        a = rebalance(df)\n",
    "    else:\n",
    "        a = df.copy()\n",
    "    # transform text\n",
    "    a['text'] = a.text.apply(tag_tokenize)\n",
    "    if 'lemmatize' in kwargs and kwargs['lemmatize']==True:\n",
    "        a['text'] = a.text.apply(lemmatize)\n",
    "    if 'remove_stopwords' in kwargs and kwargs['remove_stopwords']==True:\n",
    "        a['text'] = a.text.apply(remove_stopwords)\n",
    "    a['text'] = a.text.apply(build_ngrams, args=(min_ngram,max_ngram))\n",
    "\n",
    "    # build schema\n",
    "    if schema is None:\n",
    "        schema = dict()\n",
    "        for idx, row in a.iterrows():\n",
    "            for w in row.text:\n",
    "                if w in schema:\n",
    "                    schema[w] = schema[w] + 1\n",
    "                else:\n",
    "                    schema[w] = 1\n",
    "\n",
    "    sorted_labels = [x[0] for x in sorted([(k,v) for k,v in schema.items()], key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "    if 'feature_count' in kwargs:\n",
    "        n = kwargs['feature_count']\n",
    "        for w in sorted_labels[n:]:\n",
    "            del schema[w]\n",
    "\n",
    "    # apply corpus to text\n",
    "    text = a.text\n",
    "    del a['text']\n",
    "    ignore_cols = list(a.columns)\n",
    "    for w in schema.keys():\n",
    "        a = pd.concat([a,pd.DataFrame({f'_{w}': text.apply(build_tf, word=w)})], axis=1)\n",
    "\n",
    "    if 'use_tf' not in kwargs or kwargs['use_tf']==False:\n",
    "        a = a.apply(build_bow, ignore_cols=ignore_cols)\n",
    "\n",
    "    # onehot encode target column\n",
    "    classes = list()\n",
    "    if 'emotions' in a.columns:\n",
    "        classes = sorted(a.emotions.unique().tolist())\n",
    "        a['emotions'] = a.emotions.apply(onehot_encode, classes=classes)\n",
    "\n",
    "    return a, schema, classes\n",
    "\n",
    "def train_lr(df, train_lambda, train_step_size, train_max_iter, train_tolerance, ignore_cols=['id'], label_col='emotions', **kwargs) -> np.matrix:\n",
    "    # https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.4-MultiLogistic.pdf\n",
    "    # get vector, weight, and target matrices\n",
    "    print(f'Training LR model using {train_lambda=}, {train_step_size=}, {train_max_iter=}, {train_tolerance=} over {df.shape[0]} rows and {df.shape[1]-len(ignore_cols)-1} features...')\n",
    "    target_matrix = np.matrix([np.array(x) for x in df[label_col].tolist()])\n",
    "    vector_matrix = np.matrix(df.loc[:, ~df.columns.isin(ignore_cols+[label_col])])\n",
    "    # add bias term\n",
    "    vector_matrix = np.insert(vector_matrix, vector_matrix.shape[1], np.ones(vector_matrix.shape[0]), axis=1)\n",
    "    weight_matrix = np.matrix(np.zeros((target_matrix.shape[1], vector_matrix.shape[1])))\n",
    "\n",
    "    i = 0\n",
    "    old_weight_matrix = weight_matrix.copy()\n",
    "    while i < train_max_iter and (np.linalg.norm(weight_matrix-old_weight_matrix) >= train_tolerance or i < 1):\n",
    "        old_weight_matrix = weight_matrix.copy()\n",
    "        i = i + 1\n",
    "        # Can compute softmax using matrix multiplication:\n",
    "        # (V)(W.T)\n",
    "        dp = np.exp(vector_matrix @ weight_matrix.T)\n",
    "        # then normalize each row in the resulting matrix and take difference of true label\n",
    "        dp = (dp/dp.sum(axis=1)) - target_matrix\n",
    "        # then compute gradient by adding regularization term and ((V.T)(L)).T\n",
    "        tg = (weight_matrix * train_lambda) + (vector_matrix.T @ dp).T\n",
    "        # apply gradient\n",
    "        weight_matrix = weight_matrix - (train_step_size * tg)\n",
    "\n",
    "    return weight_matrix\n",
    "\n",
    "def apply_lr(df, model, ignore_cols=['id','emotions']) -> pd.DataFrame:\n",
    "    a = df.copy()\n",
    "    vector_matrix = np.matrix(df.loc[:, ~df.columns.isin(ignore_cols)])\n",
    "    # add bias term\n",
    "    vector_matrix = np.insert(vector_matrix, vector_matrix.shape[1], np.ones(vector_matrix.shape[0]), axis=1)\n",
    "\n",
    "    # ultimately, it doesn't matter that we apply softmax here since each entry will only differ\n",
    "    # by a multiplicative normalizing constant wrt to its row, but do it anyways for consistency\n",
    "    dp = np.exp(vector_matrix @ model.T)\n",
    "    a['predictions'] = (dp/dp.sum(axis=1)).tolist()\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def LR():\n",
    "    # learn a model and do any necessary processing\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    preprocessed_train_data, schema, classes = preprocess(train_data, min_ngram=1, max_ngram=3, lemmatize=False, remove_stopwords=True, feature_count=2000)\n",
    "    preprocessed_test_data,_,__ = preprocess(test_data, schema=schema, min_ngram=1, max_ngram=3, lemmatize=False, remove_stopwords=True, feature_count=2000)\n",
    "    model = train_lr(preprocessed_train_data, train_lambda=0.05, train_step_size=0.05, train_max_iter=500, train_tolerance=0.00001)\n",
    "    transformed_data = apply_lr(preprocessed_test_data, model)\n",
    "    test_data['emotions'] = transformed_data.predictions.apply(lambda x: classes[np.argmax(x)])\n",
    "    test_data.to_csv('test_lg.csv', index=False)\n",
    "\n",
    "def mat_sigmoid(W, X):\n",
    "    return 1 / (1 + np.exp(-(W @ X)))\n",
    "\n",
    "def mat_sigmoid_dv(W, X):\n",
    "    dp = W @ X\n",
    "    dp = 1/(1+np.exp(-dp))\n",
    "    return np.multiply(dp, 1-dp)\n",
    "\n",
    "def mat_swish1(W,X):\n",
    "    dp = W @ X\n",
    "    return dp / (1 + np.exp(-dp))\n",
    "\n",
    "def mat_swish1_dv(W,X):\n",
    "    dp = W @ X\n",
    "    return (1 + np.exp(-dp) + np.multiply(dp, np.exp(-dp))) / np.power(1 + np.exp(-dp),2)\n",
    "\n",
    "def mat_relu(W,X):\n",
    "    dp = W @ X\n",
    "    return np.multiply((dp > 0), dp)\n",
    "\n",
    "def mat_relu_dv(W,X):\n",
    "    dp = W @ X\n",
    "    return (dp > 0) * 1\n",
    "\n",
    "def mat_softmax(W,X):\n",
    "    dp = W @ X\n",
    "    z = np.exp(dp - np.max(dp))\n",
    "    return z/z.sum(axis=0)\n",
    "\n",
    "def mat_softmax_jc(W, X, da):\n",
    "    # compute softmax values\n",
    "    z = mat_softmax(W,X)\n",
    "    # https://themaverickmeerkat.com/2019-10-23-Softmax/\n",
    "    p = z.T\n",
    "    m, n = p.shape\n",
    "    t1 = np.einsum('ij,ik->ijk', p, p)\n",
    "    t2 = np.einsum('ij,jk->ijk', p, np.eye(n, n))\n",
    "    dS = t2 - t1\n",
    "    return np.einsum('ijk,ik->ij', dS, da.T).T\n",
    "\n",
    "def mat_squared_loss_dv(A, G):\n",
    "    return -(G-A)\n",
    "\n",
    "def train_nn(df, layers, train_step_size, train_iter, dropout_rate, batch_size=0, ignore_cols=['id'], label_col='emotions', **kwargs):\n",
    "    # https://en.wikipedia.org/wiki/Backpropagation#Matrix_multiplication\n",
    "    # get vector, weight, and target matrices\n",
    "    print(f'Training NN model using {layers=}, {train_step_size=}, {train_iter=}, {dropout_rate=}, {batch_size=} over {df.shape[0]} rows and {df.shape[1]-len(ignore_cols)-1} features...')\n",
    "    target_matrix = np.matrix([np.array(x) for x in df[label_col].tolist()]).T\n",
    "    vector_matrix = np.matrix(df.loc[:, ~df.columns.isin(ignore_cols+[label_col])])\n",
    "    # add bias term\n",
    "    vector_matrix = np.insert(vector_matrix, vector_matrix.shape[1], np.ones(vector_matrix.shape[0]), axis=1).T\n",
    "\n",
    "    # unpack layer data\n",
    "    layer_sizes = [l[0] for l in layers]\n",
    "    activations = [None] + [l[1] if len(l) == 3 else mat_sigmoid for l in layers] + [mat_softmax]\n",
    "    activation_derivatives = [None] + [l[2] if len(l) == 3 else mat_sigmoid_dv for l in layers] + [mat_softmax_jc]\n",
    "\n",
    "    # assume fully connected\n",
    "    weight_shapes = list()\n",
    "    if len(layer_sizes) == 0:\n",
    "        weight_shapes = [(target_matrix.shape[0], vector_matrix.shape[0])]\n",
    "    else:\n",
    "        weight_shapes = [(layer_sizes[0] + 1, vector_matrix.shape[0])]\n",
    "        for l in range(1, len(layer_sizes)):\n",
    "            weight_shapes.append((layer_sizes[l] + 1, layer_sizes[l-1] + 1))\n",
    "        weight_shapes.append((target_matrix.shape[0], layer_sizes[-1] + 1))\n",
    "    # weights = [None, np.matrix(np.random.randn(6, vector_matrix.shape[1])), np.matrix(np.random.randn(target_matrix.shape[1], 6))]\n",
    "    # print(weight_shapes)\n",
    "    weights = [None] + [np.matrix(np.random.randn(*s)) for s in weight_shapes]\n",
    "\n",
    "    # don't need to treat bias term super specially, just need to fix A value to 1, and derivative to 0?\n",
    "\n",
    "    for i in range(1,train_iter+1):\n",
    "        # forward feed phase\n",
    "        # activations\n",
    "        batch = np.random.choice(vector_matrix.shape[1], 75) if batch_size > 0 else None\n",
    "        # A = [vector_matrix[:,batch]]\n",
    "        A = [vector_matrix[:, batch] if batch_size > 0 else vector_matrix]\n",
    "        labels = target_matrix[:, batch] if batch_size > 0 else target_matrix\n",
    "        # activation derivatives\n",
    "        D = [A[-1]]\n",
    "        for l in range(1, len(weights)):\n",
    "            Xp = np.copy(A[-1])\n",
    "            # add bias term if needed\n",
    "            # if l < len(weights)-1:\n",
    "            Xp[Xp.shape[0]-1] = 1\n",
    "            a = activations[l](weights[l], Xp)\n",
    "            d = activation_derivatives[l](weights[l], Xp, a - labels) if l==len(weights)-1 else activation_derivatives[l](weights[l], Xp)\n",
    "            # apply dropout\n",
    "            if dropout_rate > 0 and l < len(weights)-1:\n",
    "                dead_neurons = np.random.choice(a.shape[0], int(a.shape[0] * dropout_rate))\n",
    "                a[dead_neurons] = 0\n",
    "                d[dead_neurons] = 0\n",
    "                a = a / (1 - dropout_rate)\n",
    "                d = d / (1 - dropout_rate)\n",
    "            A.append(a)\n",
    "            D.append(d)\n",
    "\n",
    "        # acc = np.sum(np.argmax(A[-1], axis=0) == np.argmax(target_matrix, axis=0)) / target_matrix.shape[1]\n",
    "        # err = -np.sum(np.sum(np.multiply(target_matrix, np.nan_to_num(np.log(A[-1]))), axis=0)) # categorical cross entropy loss\n",
    "        # print(i, acc, err)\n",
    "\n",
    "        # back prop phase\n",
    "        # partial product\n",
    "        # P = np.multiply(D[-1], (A[-1] - target_matrix))\n",
    "        # with softmax, we already include dC/da\n",
    "        P = D[-1]\n",
    "        # gradients\n",
    "        # G = list()\n",
    "        for l in range(len(A)-1, 0, -1):\n",
    "            g = P @ A[l-1].T\n",
    "            P = np.multiply(D[l-1], weights[l].T @ P)\n",
    "            weights[l] = weights[l] - (train_step_size * g)\n",
    "\n",
    "    return list(zip(weights, activations))\n",
    "    \n",
    "            \n",
    "def apply_nn(df, model, ignore_cols=['id','emotions']) -> pd.DataFrame:\n",
    "    b = df.copy()\n",
    "    vector_matrix = np.matrix(df.loc[:, ~df.columns.isin(ignore_cols)])\n",
    "    # add bias term\n",
    "    vector_matrix = np.insert(vector_matrix, vector_matrix.shape[1], np.ones(vector_matrix.shape[0]), axis=1).T\n",
    "    A = [vector_matrix]\n",
    "    for l in range(1, len(model)):\n",
    "        Xp = np.copy(A[-1])\n",
    "        Xp[Xp.shape[0]-1] = 1\n",
    "        a = model[l][1](model[l][0], Xp)\n",
    "        A.append(a)\n",
    "    \n",
    "    b['predictions'] = A[-1].T.tolist()\n",
    "    return b\n",
    "\n",
    "def NN():\n",
    "    # your Multi-layer Neural Network\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    del train_data['text']\n",
    "    test_text = test_data.text\n",
    "    del test_data['text']\n",
    "    embeddings = pd.read_csv('naive_glove_embeddings.csv')\n",
    "    train_data_embeddings = pd.merge(train_data, embeddings, on='id')\n",
    "    test_data_embeddings = pd.merge(test_data, embeddings, on='id')\n",
    "    classes = sorted(list(train_data.emotions.unique()))\n",
    "    # one-hot encode label\n",
    "    train_data_embeddings.emotions = train_data_embeddings.emotions.apply(lambda x: [int(x==y) for y in classes])\n",
    "    model = train_nn(train_data_embeddings, layers=[(30, mat_sigmoid, mat_sigmoid_dv)], train_step_size=0.01, train_iter=20000, dropout_rate=0.15, batch_size=50)\n",
    "    transformed_data = apply_nn(test_data_embeddings, model)\n",
    "    test_data['text'] = test_text\n",
    "    test_data['emotions'] = transformed_data.predictions.apply(lambda x: classes[np.argmax(x)])\n",
    "    test_data.to_csv('test_nn.csv', index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "    print (\"..................Beginning of Logistic Regression................\")\n",
    "    # LR()\n",
    "    print (\"..................End of Logistic Regression................\")\n",
    "\n",
    "    print(\"------------------------------------------------\")\n",
    "\n",
    "    print (\"..................Beginning of Neural Network................\")\n",
    "    # NN()\n",
    "    print (\"..................End of Neural Network................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text emotions\n",
       "0   27383  i feel awful about it too because it s my job ...  sadness\n",
       "1  110083                              im alone i feel awful  sadness\n",
       "2  140764  ive probably mentioned this before but i reall...      joy\n",
       "3  100071           i was feeling a little low few days back  sadness\n",
       "4    2837  i beleive that i am much more sensitive to oth...     love"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    file = pd.read_csv(path)\n",
    "    return file\n",
    "\n",
    "TRAIN_SET = read_csv(\"./train.csv\")\n",
    "TRAIN_SET.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i feel awful about it too because it s my job to get him in a position to succeed and it just didn t happen here'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SET['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, emotions]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TRAIN_SET\n",
    "counts = dataset.emotions.value_counts()\n",
    "a = pd.DataFrame(columns=dataset.columns)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    id: 27383,\n",
    "    \"text\": \"i feel awful about it too because it s my job to get him in a position to succeed and it just didn t happen here\",\n",
    "    \"emotions\": \"sadness\"\n",
    "},\n",
    "{\n",
    "    id: 110083,\n",
    "    \"text\": \"im alone i feel awful\t\",\n",
    "    \"emotions\": \"sadness\"\n",
    "},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    file = pd.read_csv(path)\n",
    "    return file\n",
    "\n",
    "\n",
    "def pre_processing(dataset: pd.DataFrame):\n",
    "    counts = dataset.emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: sadness\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "dataset = [\n",
    "    {\n",
    "        \"id\": 27383,\n",
    "        \"text\": \"i feel awful about it too because it s my job to get him in a position to succeed and it just didn t happen here\",\n",
    "        \"emotion\": \"sadness\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 110083,\n",
    "        \"text\": \"im alone i feel awful\t\",\n",
    "        \"emotion\": \"sadness\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preprocessing: One-hot encoding for emotions\n",
    "texts = [entry['text'] for entry in dataset]\n",
    "emotions = [entry['emotion'] for entry in dataset]\n",
    "emotion_set = set(emotions)\n",
    "emotion_to_int = {emotion: i for i, emotion in enumerate(emotion_set)}\n",
    "int_to_emotion = {i: emotion for i, emotion in enumerate(emotion_set)}\n",
    "\n",
    "# Create the vocabulary of words\n",
    "word_to_int = {}\n",
    "current_index = 0\n",
    "for text in texts:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word not in word_to_int:\n",
    "            word_to_int[word] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "# Create feature vectors for texts\n",
    "text_features = []\n",
    "for text in texts:\n",
    "    features = [0] * len(word_to_int)\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        features[word_to_int[word]] = 1\n",
    "    text_features.append(features)\n",
    "\n",
    "# Convert emotions to int and one-hot encode\n",
    "emotions = [emotion_to_int[emotion] for emotion in emotions]\n",
    "targets = np.zeros((len(emotions), len(emotion_set)))\n",
    "for i, emotion in enumerate(emotions):\n",
    "    targets[i, emotion] = 1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "text_features = np.array(text_features)\n",
    "train_data = text_features[:-1]\n",
    "train_targets = targets[:-1]\n",
    "test_data = text_features[-1:]\n",
    "test_targets = targets[-1:]\n",
    "\n",
    "# Train the model using gradient descent\n",
    "weights = np.zeros((len(word_to_int), len(emotion_set)))\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    scores = np.dot(train_data, weights)\n",
    "    predictions = 1 / (1 + np.exp(-scores))\n",
    "    error = train_targets - predictions\n",
    "    gradient = np.dot(train_data.T, error)\n",
    "    weights += learning_rate * gradient\n",
    "\n",
    "# Predict emotions for the test data\n",
    "scores = np.dot(test_data, weights)\n",
    "predictions = 1 / (1 + np.exp(-scores))\n",
    "predictions = np.round(predictions)\n",
    "predicted_emotion = int_to_emotion[np.argmax(predictions[0])]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Predicted emotion:\", predicted_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "dataset = [\n",
    "    {\n",
    "        \"id\": 27383,\n",
    "        \"text\": \"i feel awful about it too because it s my job to get him in a position to succeed and it just didn t happen here\",\n",
    "        \"emotion\": \"sadness\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 110083,\n",
    "        \"text\": \"im alone i feel awful\t\",\n",
    "        \"emotion\": \"sadness\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preprocessing: One-hot encoding for emotions\n",
    "texts = [entry['text'] for entry in dataset]\n",
    "emotions = [entry['emotion'] for entry in dataset]\n",
    "emotion_set = set(emotions)\n",
    "emotion_to_int = {emotion: i for i, emotion in enumerate(emotion_set)}\n",
    "int_to_emotion = {i: emotion for i, emotion in enumerate(emotion_set)}\n",
    "\n",
    "# Create the vocabulary of words\n",
    "word_to_int = {}\n",
    "current_index = 0\n",
    "for text in texts:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word not in word_to_int:\n",
    "            word_to_int[word] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "# Create feature vectors for texts\n",
    "text_features = []\n",
    "for text in texts:\n",
    "    features = [0] * len(word_to_int)\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        features[word_to_int[word]] = 1\n",
    "    text_features.append(features)\n",
    "\n",
    "# Convert emotions to int and one-hot encode\n",
    "emotions = [emotion_to_int[emotion] for emotion in emotions]\n",
    "targets = np.zeros((len(emotions), len(emotion_set)))\n",
    "for i, emotion in enumerate(emotions):\n",
    "    targets[i, emotion] = 1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "text_features = np.array(text_features)\n",
    "train_data = text_features[:-1]\n",
    "train_targets = targets[:-1]\n",
    "test_data = text_features[-1:]\n",
    "test_targets = targets[-1:]\n",
    "\n",
    "# Initialize the weights and biases\n",
    "input_layer_size = len(word_to_int)\n",
    "hidden_layer_size = 100\n",
    "output_layer_size = len(emotion_set)\n",
    "weights1 = np.random.randn(input_layer_size, hidden_layer_size)\n",
    "weights2 = np.random.randn(hidden_layer_size, output_layer_size)\n",
    "bias1 = np.zeros(hidden_layer_size)\n",
    "bias2 = np.zeros(output_layer_size)\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    hidden_layer = np.dot(train_data, weights1) + bias1\n",
    "    hidden_layer = np.maximum(0, hidden_layer) # ReLU activation\n",
    "    scores = np.dot(hidden_layer, weights2) +"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS577",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b9f66d431bae8e8e1c474444bc40a93694ecda389959e5db57a44c04dc1ab6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

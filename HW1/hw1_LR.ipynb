{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27383</td>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110083</td>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140764</td>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100071</td>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2837</td>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text emotions\n",
       "0   27383  i feel awful about it too because it s my job ...  sadness\n",
       "1  110083                              im alone i feel awful  sadness\n",
       "2  140764  ive probably mentioned this before but i reall...      joy\n",
       "3  100071           i was feeling a little low few days back  sadness\n",
       "4    2837  i beleive that i am much more sensitive to oth...     love"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    file = pd.read_csv(path)\n",
    "    return file\n",
    "\n",
    "TRAIN_SET = read_csv(\"./train.csv\")\n",
    "TEST_SET = read_csv(\"./test.csv\")\n",
    "TRAIN_SET.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         412\n",
       "sadness     351\n",
       "anger       161\n",
       "fear        129\n",
       "love        106\n",
       "surprise     41\n",
       "Name: emotions, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TRAIN_SET\n",
    "dataset.emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>79</td>\n",
       "      <td>article published</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>90</td>\n",
       "      <td>at end of school function to celebrate leaving...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>149</td>\n",
       "      <td>football was a very big deal at my high school</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>181</td>\n",
       "      <td>i almost feel your touch that caress so gentle...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>209</td>\n",
       "      <td>held under water by a large wave and thought i...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text emotions\n",
       "37    79                                  article published      joy\n",
       "669   90  at end of school function to celebrate leaving...  sadness\n",
       "308  149     football was a very big deal at my high school      joy\n",
       "743  181  i almost feel your touch that caress so gentle...     love\n",
       "166  209  held under water by a large wave and thought i...     fear"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.sort_values(\"id\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 69, 156, 700, 705, 825]),)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dup = dataset.duplicated(subset=[\"id\"])\n",
    "index = np.where(dataset_dup==True)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_js = [\n",
    "#     {\n",
    "#         \"id\": 27383,\n",
    "#         \"text\": \"i feel awful about it too because it s my job to get him in a position to succeed and it just didn t happen here\",\n",
    "#         \"emotion\": \"sadness\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"id\": 110083,\n",
    "#         \"text\": \"im alone i feel awful\t\",\n",
    "#         \"emotion\": \"sadness\"\n",
    "#     }\n",
    "# ]\n",
    "# # \n",
    "# dataset = dataset_js\n",
    "# texts = [entry['text'] for entry in dataset_js]\n",
    "# emotions = [entry['emotion'] for entry in dataset_js]\n",
    "# texts, emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion_set = set(emotions)\n",
    "# emotion_to_int = {emotion: i for i, emotion in enumerate(emotion_set)}\n",
    "# int_to_emotion = {i: emotion for i, emotion in enumerate(emotion_set)}\n",
    "# emotion_to_int, int_to_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m brown\n\u001b[1;32m      3\u001b[0m nltk\u001b[39m.\u001b[39mdownload()\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/nltk/__init__.py:132\u001b[0m\n\u001b[1;32m    124\u001b[0m     subprocess\u001b[39m.\u001b[39mPopen \u001b[39m=\u001b[39m _fake_Popen\n\u001b[1;32m    126\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[39m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollocations\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m decorator, memoize\n\u001b[1;32m    134\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatstruct\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/nltk/collocations.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_itertools\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     38\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     39\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     40\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspearman\u001b[39;00m \u001b[39mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m FreqDist\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/nltk/metrics/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magreement\u001b[39;00m \u001b[39mimport\u001b[39;00m AnnotationTask\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maline\u001b[39;00m \u001b[39mimport\u001b[39;00m align\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39massociation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     20\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     21\u001b[0m     NgramAssocMeasures,\n\u001b[1;32m     22\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     23\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfusionmatrix\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfusionMatrix\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     binary_distance,\n\u001b[1;32m     28\u001b[0m     custom_distance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     presence,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/nltk/metrics/association.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m _SMALL \u001b[39m=\u001b[39m \u001b[39m1e-20\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m fisher_exact\n\u001b[1;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfisher_exact\u001b[39m(\u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs):\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/scipy/stats/__init__.py:485\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[1;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/scipy/stats/_stats_py.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m array, asarray, ma\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m cdist\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mndimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _measurements\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/numpy/testing/__init__.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"Common test support for all numpy test scripts.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39mThis single module should provide all the common functionality for numpy tests\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munittest\u001b[39;00m \u001b[39mimport\u001b[39;00m TestCase\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m \u001b[39mimport\u001b[39;00m extbuild, decorators \u001b[39mas\u001b[39;00m dec\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:934\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1033\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 14:53:00.609 | INFO     | __main__:<module>:15 - 4895\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "word_set = {}\n",
    "word_all = []\n",
    "idx = 0\n",
    "for dataset in (TRAIN_SET, TEST_SET):\n",
    "    for text in dataset.text:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word not in word2vec:\n",
    "                word_all.append(word)\n",
    "                word2vec[word] = idx\n",
    "                idx += 1\n",
    "    word_set = set(word_all)\n",
    "    # logger.debug(idx)\n",
    "logger.info(len(word2vec))\n",
    "# logger.info(len(word_set))\n",
    "    # print(text)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 15:09:05.257 | INFO     | __main__:<module>:8 - 4895\n",
      "2023-02-13 15:09:05.259 | INFO     | __main__:<module>:9 - 1200\n"
     ]
    }
   ],
   "source": [
    "text_features_train = []\n",
    "for text in TRAIN_SET.text:\n",
    "    features = [0] * len(word2vec)\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        features[word2vec[word]] = 1\n",
    "    text_features_train.append(features)\n",
    "logger.info(len(text_features_train[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 15:09:05.904 | INFO     | __main__:<module>:8 - 4895\n",
      "2023-02-13 15:09:05.905 | INFO     | __main__:<module>:9 - 800\n"
     ]
    }
   ],
   "source": [
    "text_features_test = []\n",
    "for text in TEST_SET.text:\n",
    "    features = [0] * len(word2vec)\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        features[word2vec[word]] = 1\n",
    "    text_features_test.append(features)\n",
    "logger.info(len(text_features_test[0]))\n",
    "logger.info(len(text_features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [_ for _ in TRAIN_SET.emotions]\n",
    "# emotions_test = [_ for _ in TEST_SET.emotions]\n",
    "\n",
    "# emotion_set = set(emotions)\n",
    "emotion_set = set(emotions)\n",
    "\n",
    "emotion_to_int = {emotion: i for i, emotion in enumerate(emotion_set)}\n",
    "int_to_emotion = {i: emotion for i, emotion in enumerate(emotion_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [emotion_to_int[emotion] for emotion in emotions]\n",
    "train_targets = np.zeros((len(emotions), len(emotion_set)))\n",
    "for i, emotion in enumerate(emotions):\n",
    "    train_targets[i, emotion] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 4895)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(text_features_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 15:12:26.809 | INFO     | __main__:<module>:12 - Epoch = 0\n",
      "2023-02-13 15:12:27.490 | INFO     | __main__:<module>:12 - Epoch = 1\n",
      "2023-02-13 15:12:28.124 | INFO     | __main__:<module>:12 - Epoch = 2\n",
      "2023-02-13 15:12:28.764 | INFO     | __main__:<module>:12 - Epoch = 3\n",
      "2023-02-13 15:12:29.396 | INFO     | __main__:<module>:12 - Epoch = 4\n",
      "2023-02-13 15:12:30.028 | INFO     | __main__:<module>:12 - Epoch = 5\n",
      "2023-02-13 15:12:30.669 | INFO     | __main__:<module>:12 - Epoch = 6\n",
      "2023-02-13 15:12:31.312 | INFO     | __main__:<module>:12 - Epoch = 7\n",
      "2023-02-13 15:12:31.951 | INFO     | __main__:<module>:12 - Epoch = 8\n",
      "2023-02-13 15:12:32.584 | INFO     | __main__:<module>:12 - Epoch = 9\n",
      "2023-02-13 15:12:33.215 | INFO     | __main__:<module>:12 - Epoch = 10\n",
      "2023-02-13 15:12:33.844 | INFO     | __main__:<module>:12 - Epoch = 11\n",
      "2023-02-13 15:12:34.460 | INFO     | __main__:<module>:12 - Epoch = 12\n",
      "2023-02-13 15:12:35.123 | INFO     | __main__:<module>:12 - Epoch = 13\n",
      "2023-02-13 15:12:35.833 | INFO     | __main__:<module>:12 - Epoch = 14\n",
      "2023-02-13 15:12:36.480 | INFO     | __main__:<module>:12 - Epoch = 15\n",
      "2023-02-13 15:12:37.104 | INFO     | __main__:<module>:12 - Epoch = 16\n",
      "2023-02-13 15:12:37.735 | INFO     | __main__:<module>:12 - Epoch = 17\n",
      "2023-02-13 15:12:38.365 | INFO     | __main__:<module>:12 - Epoch = 18\n",
      "2023-02-13 15:12:38.987 | INFO     | __main__:<module>:12 - Epoch = 19\n",
      "2023-02-13 15:12:39.621 | INFO     | __main__:<module>:12 - Epoch = 20\n",
      "2023-02-13 15:12:40.244 | INFO     | __main__:<module>:12 - Epoch = 21\n",
      "2023-02-13 15:12:40.871 | INFO     | __main__:<module>:12 - Epoch = 22\n",
      "2023-02-13 15:12:41.538 | INFO     | __main__:<module>:12 - Epoch = 23\n",
      "2023-02-13 15:12:42.163 | INFO     | __main__:<module>:12 - Epoch = 24\n",
      "2023-02-13 15:12:42.792 | INFO     | __main__:<module>:12 - Epoch = 25\n",
      "2023-02-13 15:12:43.422 | INFO     | __main__:<module>:12 - Epoch = 26\n",
      "2023-02-13 15:12:44.051 | INFO     | __main__:<module>:12 - Epoch = 27\n",
      "2023-02-13 15:12:44.680 | INFO     | __main__:<module>:12 - Epoch = 28\n",
      "2023-02-13 15:12:45.314 | INFO     | __main__:<module>:12 - Epoch = 29\n",
      "2023-02-13 15:12:45.939 | INFO     | __main__:<module>:12 - Epoch = 30\n",
      "2023-02-13 15:12:46.616 | INFO     | __main__:<module>:12 - Epoch = 31\n",
      "2023-02-13 15:12:47.262 | INFO     | __main__:<module>:12 - Epoch = 32\n",
      "2023-02-13 15:12:47.913 | INFO     | __main__:<module>:12 - Epoch = 33\n",
      "2023-02-13 15:12:48.549 | INFO     | __main__:<module>:12 - Epoch = 34\n",
      "2023-02-13 15:12:49.194 | INFO     | __main__:<module>:12 - Epoch = 35\n",
      "2023-02-13 15:12:49.840 | INFO     | __main__:<module>:12 - Epoch = 36\n",
      "2023-02-13 15:12:50.483 | INFO     | __main__:<module>:12 - Epoch = 37\n",
      "2023-02-13 15:12:51.112 | INFO     | __main__:<module>:12 - Epoch = 38\n",
      "2023-02-13 15:12:51.731 | INFO     | __main__:<module>:12 - Epoch = 39\n",
      "2023-02-13 15:12:52.370 | INFO     | __main__:<module>:12 - Epoch = 40\n",
      "2023-02-13 15:12:53.018 | INFO     | __main__:<module>:12 - Epoch = 41\n",
      "2023-02-13 15:12:53.665 | INFO     | __main__:<module>:12 - Epoch = 42\n",
      "2023-02-13 15:12:54.308 | INFO     | __main__:<module>:12 - Epoch = 43\n",
      "2023-02-13 15:12:54.934 | INFO     | __main__:<module>:12 - Epoch = 44\n",
      "2023-02-13 15:12:55.547 | INFO     | __main__:<module>:12 - Epoch = 45\n",
      "2023-02-13 15:12:56.166 | INFO     | __main__:<module>:12 - Epoch = 46\n",
      "2023-02-13 15:12:56.810 | INFO     | __main__:<module>:12 - Epoch = 47\n",
      "2023-02-13 15:12:57.443 | INFO     | __main__:<module>:12 - Epoch = 48\n",
      "2023-02-13 15:12:58.068 | INFO     | __main__:<module>:12 - Epoch = 49\n",
      "2023-02-13 15:12:58.684 | INFO     | __main__:<module>:12 - Epoch = 50\n",
      "2023-02-13 15:12:59.308 | INFO     | __main__:<module>:12 - Epoch = 51\n",
      "2023-02-13 15:12:59.930 | INFO     | __main__:<module>:12 - Epoch = 52\n",
      "2023-02-13 15:13:00.579 | INFO     | __main__:<module>:12 - Epoch = 53\n",
      "2023-02-13 15:13:01.232 | INFO     | __main__:<module>:12 - Epoch = 54\n",
      "2023-02-13 15:13:01.851 | INFO     | __main__:<module>:12 - Epoch = 55\n",
      "2023-02-13 15:13:02.471 | INFO     | __main__:<module>:12 - Epoch = 56\n",
      "2023-02-13 15:13:03.102 | INFO     | __main__:<module>:12 - Epoch = 57\n",
      "2023-02-13 15:13:03.725 | INFO     | __main__:<module>:12 - Epoch = 58\n",
      "2023-02-13 15:13:04.333 | INFO     | __main__:<module>:12 - Epoch = 59\n",
      "2023-02-13 15:13:04.934 | INFO     | __main__:<module>:12 - Epoch = 60\n",
      "2023-02-13 15:13:05.549 | INFO     | __main__:<module>:12 - Epoch = 61\n",
      "2023-02-13 15:13:06.164 | INFO     | __main__:<module>:12 - Epoch = 62\n",
      "2023-02-13 15:13:06.786 | INFO     | __main__:<module>:12 - Epoch = 63\n",
      "2023-02-13 15:13:07.408 | INFO     | __main__:<module>:12 - Epoch = 64\n",
      "2023-02-13 15:13:08.030 | INFO     | __main__:<module>:12 - Epoch = 65\n",
      "2023-02-13 15:13:08.648 | INFO     | __main__:<module>:12 - Epoch = 66\n",
      "2023-02-13 15:13:09.275 | INFO     | __main__:<module>:12 - Epoch = 67\n",
      "2023-02-13 15:13:09.902 | INFO     | __main__:<module>:12 - Epoch = 68\n",
      "2023-02-13 15:13:10.524 | INFO     | __main__:<module>:12 - Epoch = 69\n",
      "2023-02-13 15:13:11.165 | INFO     | __main__:<module>:12 - Epoch = 70\n",
      "2023-02-13 15:13:11.796 | INFO     | __main__:<module>:12 - Epoch = 71\n",
      "2023-02-13 15:13:12.422 | INFO     | __main__:<module>:12 - Epoch = 72\n",
      "2023-02-13 15:13:13.084 | INFO     | __main__:<module>:12 - Epoch = 73\n",
      "2023-02-13 15:13:13.704 | INFO     | __main__:<module>:12 - Epoch = 74\n",
      "2023-02-13 15:13:14.338 | INFO     | __main__:<module>:12 - Epoch = 75\n",
      "2023-02-13 15:13:14.959 | INFO     | __main__:<module>:12 - Epoch = 76\n",
      "2023-02-13 15:13:15.601 | INFO     | __main__:<module>:12 - Epoch = 77\n",
      "2023-02-13 15:13:16.249 | INFO     | __main__:<module>:12 - Epoch = 78\n",
      "2023-02-13 15:13:17.073 | INFO     | __main__:<module>:12 - Epoch = 79\n",
      "2023-02-13 15:13:17.848 | INFO     | __main__:<module>:12 - Epoch = 80\n",
      "2023-02-13 15:13:18.587 | INFO     | __main__:<module>:12 - Epoch = 81\n",
      "2023-02-13 15:13:19.425 | INFO     | __main__:<module>:12 - Epoch = 82\n",
      "2023-02-13 15:13:20.181 | INFO     | __main__:<module>:12 - Epoch = 83\n",
      "2023-02-13 15:13:20.921 | INFO     | __main__:<module>:12 - Epoch = 84\n",
      "2023-02-13 15:13:21.701 | INFO     | __main__:<module>:12 - Epoch = 85\n",
      "2023-02-13 15:13:22.374 | INFO     | __main__:<module>:12 - Epoch = 86\n",
      "2023-02-13 15:13:23.076 | INFO     | __main__:<module>:12 - Epoch = 87\n",
      "2023-02-13 15:13:23.802 | INFO     | __main__:<module>:12 - Epoch = 88\n",
      "2023-02-13 15:13:24.524 | INFO     | __main__:<module>:12 - Epoch = 89\n",
      "2023-02-13 15:13:25.184 | INFO     | __main__:<module>:12 - Epoch = 90\n",
      "2023-02-13 15:13:25.913 | INFO     | __main__:<module>:12 - Epoch = 91\n",
      "2023-02-13 15:13:26.661 | INFO     | __main__:<module>:12 - Epoch = 92\n",
      "2023-02-13 15:13:27.383 | INFO     | __main__:<module>:12 - Epoch = 93\n",
      "2023-02-13 15:13:28.107 | INFO     | __main__:<module>:12 - Epoch = 94\n",
      "2023-02-13 15:13:28.826 | INFO     | __main__:<module>:12 - Epoch = 95\n",
      "2023-02-13 15:13:29.546 | INFO     | __main__:<module>:12 - Epoch = 96\n",
      "2023-02-13 15:13:30.193 | INFO     | __main__:<module>:12 - Epoch = 97\n",
      "2023-02-13 15:13:30.879 | INFO     | __main__:<module>:12 - Epoch = 98\n",
      "2023-02-13 15:13:31.572 | INFO     | __main__:<module>:12 - Epoch = 99\n"
     ]
    }
   ],
   "source": [
    "weights = np.zeros((len(word2vec), len(emotion_set)))\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    scores = np.dot(np.array(text_features_train), weights)\n",
    "    predictions = 1 / (1 + np.exp(-scores))\n",
    "    # logger.debug(np.array(predictions).shape)\n",
    "    # logger.debug(np.array(train_targets).shape)\n",
    "    error = train_targets - predictions\n",
    "    gradient = np.dot(np.array(text_features_train).T, error)\n",
    "    weights += learning_rate * gradient\n",
    "    logger.info(f\"Epoch = {epoch}\")\n",
    "\n",
    "# Predict emotions for the test data\n",
    "scores = np.dot(text_features_test, weights)\n",
    "predictions = 1 / (1 + np.exp(-scores))\n",
    "predictions = np.round(predictions)\n",
    "predicted_emotion = int_to_emotion[np.argmax(predictions[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_emotion = []\n",
    "\n",
    "scores = np.dot(text_features_test, weights)\n",
    "predictions = 1 / (1 + np.exp(-scores))\n",
    "# logger.debug(predictions)\n",
    "# predictions = np.ones(predictions.shape) * np.argmax(predictions, axis=1)\n",
    "# len(np.argmax(predictions, axis=1))\n",
    "pred = np.zeros(predictions.shape)\n",
    "for idx in range(pred.shape[0]):\n",
    "    # logger.debug(np.argmax(predictions, axis=1)[idx])\n",
    "    pred[idx][np.argmax(predictions, axis=1)[idx]] = 1\n",
    "\n",
    "for idx in range(pred.shape[0]):\n",
    "    # logger.debug(np.where(pred[idx]==1))\n",
    "    \n",
    "    # logger.debug(int_to_emotion[np.where(pred[idx]==1)[0][0]])\n",
    "    pred_emotion.append(int_to_emotion[np.where(pred[idx]==1)[0][0]])\n",
    "# predictions = np.argmax(predictions)\n",
    "# predicted_emotion = int_to_emotion[np.argmax(predictions[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['emotions'] = pred_emotion\n",
    "dataset.to_csv(\"./test_lg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139312</td>\n",
       "      <td>ive been feeling more optimistic this week tha...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110362</td>\n",
       "      <td>i suppose i was feeling adventurous and volunt...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45691</td>\n",
       "      <td>i feel like when a cycle comes i get all depre...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>958</td>\n",
       "      <td>when confronted and in my opinion hassled by t...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33882</td>\n",
       "      <td>i seldom feel shaky mid run</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>566</td>\n",
       "      <td>that was what i felt when i was finally accept...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>36236</td>\n",
       "      <td>i take every day as it comes i m just focussin...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>76229</td>\n",
       "      <td>i just suddenly feel that everything was fake</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>131640</td>\n",
       "      <td>im feeling more eager than ever to claw back w...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>64703</td>\n",
       "      <td>i give you plenty of attention even when i fee...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text emotions\n",
       "0    139312  ive been feeling more optimistic this week tha...  sadness\n",
       "1    110362  i suppose i was feeling adventurous and volunt...      joy\n",
       "2     45691  i feel like when a cycle comes i get all depre...  sadness\n",
       "3       958  when confronted and in my opinion hassled by t...    anger\n",
       "4     33882                        i seldom feel shaky mid run     fear\n",
       "..      ...                                                ...      ...\n",
       "795     566  that was what i felt when i was finally accept...      joy\n",
       "796   36236  i take every day as it comes i m just focussin...  sadness\n",
       "797   76229      i just suddenly feel that everything was fake  sadness\n",
       "798  131640  im feeling more eager than ever to claw back w...      joy\n",
       "799   64703  i give you plenty of attention even when i fee...  sadness\n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float):\n",
    "    # Logistic regression function (Sigmoid)\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "\n",
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    file = pd.read_csv(path)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(dataset: pd.DataFrame) -> None:\n",
    "    '''\n",
    "        Clean the duplicated entries\n",
    "    '''\n",
    "    dataset = dataset.sort_values(\"id\")\n",
    "    dataset_dup = dataset.duplicated(subset=[\"id\"])\n",
    "    index = np.where(dataset_dup==True)\n",
    "    # logger.debug(dataset.head())\n",
    "    dataset = dataset.drop(index[0])\n",
    "    # logger.debug(dataset.head())\n",
    "    dataset = dataset.sample(frac=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(epoch, num_epochs):\n",
    "    # Load raw dataset\n",
    "    TRAIN_PATH = \"./train.csv\"\n",
    "    TEST_PATH = \"./test.csv\"\n",
    "    train_set = read_csv(TRAIN_PATH)\n",
    "    test_set = read_csv(TEST_PATH)\n",
    "    \n",
    "    # data cleaning\n",
    "    train_set = data_cleaning(train_set)\n",
    "    test_set = data_cleaning(test_set)\n",
    "    \n",
    "    # logger.info(f\"After data cleaning, len(train_set) is {len(train_set)}, len(test_set) is {len(test_set)}\")\n",
    "\n",
    "    # Cross validation\n",
    "    # logger.debug(train_set)\n",
    "    df_shuffle = train_set.copy()\n",
    "    # logger.debug(df_shuffle)\n",
    "    # logger.info(len(df_shuffle))\n",
    "    df_size = len(df_shuffle)\n",
    "    idx_split_left = df_size//num_epochs * (epoch)\n",
    "    idx_split_right = df_size//num_epochs * (epoch+1)\n",
    "    train_set = pd.concat([df_shuffle.iloc[:idx_split_left], df_shuffle.iloc[idx_split_right:]])\n",
    "    val_set = df_shuffle.iloc[idx_split_left:idx_split_right]\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 23:45:36.291 | DEBUG    | __main__:<module>:4 -           id                                               text  emotions\n",
      "601     3021                        i feel a lil dazed actually  surprise\n",
      "557   121857  i was rusty but i feel like i flowed ok for mo...       joy\n",
      "994    97315  i used to think that i was protecting people b...   sadness\n",
      "1185    7295  i feel weird working out in front of my family...  surprise\n",
      "463    12560  i can post questions or comments and then sche...   sadness\n",
      "...      ...                                                ...       ...\n",
      "12     98659  i mean is on this stupid trip of making the gr...       joy\n",
      "57     66949  i had my week appointment yesterday and left f...   sadness\n",
      "1068  109114  i still feel anything but strong but i am also...       joy\n",
      "795    77413  i know how you are feeling that you feel hopel...   sadness\n",
      "1186   18552         i continue to feel terrific thanks so much       joy\n",
      "\n",
      "[1195 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_set, val_set, test_set = data_loading(epoch, num_epochs)\n",
    "    logger.debug(df_shuffle)\n",
    "    break\n",
    "    # logger.debug(f\"{len(train_set)}, {len(val_set)}, {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>12530</td>\n",
       "      <td>i feel like the puppets are more of an obstacl...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>113942</td>\n",
       "      <td>i took was blurry which kind of captures my fe...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>33835</td>\n",
       "      <td>i see things so clearly and with so much depth...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>33801</td>\n",
       "      <td>i have achieved very little but somehow for a ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>120714</td>\n",
       "      <td>ive made and provided that someone would under...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>10443</td>\n",
       "      <td>i read two of the books from my march tbr stac...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>3997</td>\n",
       "      <td>i am able because he wants to make the individ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>105550</td>\n",
       "      <td>i will even feel highly disturbed because i am...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>19608</td>\n",
       "      <td>i feel skeptical about small accessories</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>4631</td>\n",
       "      <td>i could feel the desperation of the longing se...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1076 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  emotions\n",
       "138    12530  i feel like the puppets are more of an obstacl...      love\n",
       "116   113942  i took was blurry which kind of captures my fe...       joy\n",
       "38     33835  i see things so clearly and with so much depth...      fear\n",
       "435    33801  i have achieved very little but somehow for a ...     anger\n",
       "240   120714  ive made and provided that someone would under...   sadness\n",
       "...      ...                                                ...       ...\n",
       "814    10443  i read two of the books from my march tbr stac...  surprise\n",
       "733     3997  i am able because he wants to make the individ...       joy\n",
       "381   105550  i will even feel highly disturbed because i am...   sadness\n",
       "1150   19608           i feel skeptical about small accessories      fear\n",
       "279     4631  i could feel the desperation of the longing se...      love\n",
       "\n",
       "[1076 rows x 3 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_rep(train_set, val_set, test_set):\n",
    "    # Create feature representations for data\n",
    "    word2vec = {}\n",
    "    word_set = {}\n",
    "    word_all = []\n",
    "    idx = 0\n",
    "    # logger.debug(train_set)\n",
    "    for dataset in (train_set, val_set, test_set):\n",
    "        for text in dataset.text:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                if word not in word2vec:\n",
    "                    word_all.append(word)\n",
    "                    word2vec[word] = idx\n",
    "                    idx += 1\n",
    "        word_set = set(word_all)\n",
    "        # logger.debug(idx)\n",
    "    # logger.info(len(word2vec))\n",
    "    # logger.info(len(word_set))\n",
    "        # print(text)\n",
    "        # break\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "def text_features(dataset, embedding):\n",
    "    text_features = []\n",
    "    for text in dataset.text:\n",
    "        features = [0] * len(embedding)\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            features[embedding[word]] = 1\n",
    "        text_features.append(features)\n",
    "    # logger.info(len(text_features[0]))\n",
    "    # logger.info(len(text_features))\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(train_set: pd.DataFrame, val_set: pd.DataFrame, test_set: pd.DataFrame):\n",
    "    \n",
    "    embedding = feature_rep(train_set, val_set, test_set)\n",
    "    # Create trianing and testing inputs\n",
    "    text_features_train = text_features(train_set, embedding)\n",
    "    text_features_val = text_features(val_set, embedding)\n",
    "    text_features_test = text_features(test_set, embedding)\n",
    "    \n",
    "    # Create labels\n",
    "    emotions_train = [_ for _ in train_set.emotions]\n",
    "    emotions_val = [_ for _ in val_set.emotions]\n",
    "\n",
    "    emotion_set = set(emotions_train)\n",
    "\n",
    "    emotion2int = {emotion: i for i, emotion in enumerate(emotion_set)}\n",
    "    int2emotion = {i: emotion for i, emotion in enumerate(emotion_set)}\n",
    "    \n",
    "    emotions_train = [emotion2int[emotion] for emotion in emotions_train]\n",
    "    emotions_val = [emotion2int[emotion] for emotion in emotions_val]\n",
    "    train_targets = np.zeros((len(emotions_train), len(emotion_set)))\n",
    "    val_targets = np.zeros((len(emotions_val), len(emotion_set)))\n",
    "    for i, emotion in enumerate(emotions_train):\n",
    "        train_targets[i, emotion] = 1\n",
    "    for i, emotion in enumerate(emotions_val):\n",
    "        val_targets[i, emotion] = 1\n",
    "\n",
    "    return embedding, text_features_train, text_features_val, text_features_test,\\\n",
    "        emotion_set, train_targets, val_targets, emotion2int, int2emotion\n",
    "\n",
    "\n",
    "def post_processing(test_set, pred_emotions):\n",
    "    output = test_set.copy()\n",
    "    output['emotions'] = pred_emotions\n",
    "    output.to_csv(\"./test_lr.csv\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 23:58:38.996 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:38.996 | INFO     | __main__:<module>:76 - Epoch 0: Accuracy = 32.20338983050847%\n",
      "2023-02-28 23:58:39.848 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:39.849 | INFO     | __main__:<module>:76 - Epoch 1: Accuracy = 37.28813559322034%\n",
      "2023-02-28 23:58:40.677 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:40.678 | INFO     | __main__:<module>:76 - Epoch 2: Accuracy = 30.508474576271187%\n",
      "2023-02-28 23:58:41.465 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:41.466 | INFO     | __main__:<module>:76 - Epoch 3: Accuracy = 35.59322033898305%\n",
      "2023-02-28 23:58:42.306 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:42.307 | INFO     | __main__:<module>:76 - Epoch 4: Accuracy = 23.728813559322035%\n",
      "2023-02-28 23:58:43.098 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:43.098 | INFO     | __main__:<module>:76 - Epoch 5: Accuracy = 35.59322033898305%\n",
      "2023-02-28 23:58:43.881 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:43.882 | INFO     | __main__:<module>:76 - Epoch 6: Accuracy = 35.59322033898305%\n",
      "2023-02-28 23:58:44.681 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:44.682 | INFO     | __main__:<module>:76 - Epoch 7: Accuracy = 30.508474576271187%\n",
      "2023-02-28 23:58:45.732 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:45.733 | INFO     | __main__:<module>:76 - Epoch 8: Accuracy = 37.28813559322034%\n",
      "2023-02-28 23:58:46.803 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:46.804 | INFO     | __main__:<module>:76 - Epoch 9: Accuracy = 32.20338983050847%\n",
      "2023-02-28 23:58:49.104 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:49.106 | INFO     | __main__:<module>:76 - Epoch 10: Accuracy = 35.59322033898305%\n",
      "2023-02-28 23:58:50.316 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:50.317 | INFO     | __main__:<module>:76 - Epoch 11: Accuracy = 35.59322033898305%\n",
      "2023-02-28 23:58:51.173 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:51.174 | INFO     | __main__:<module>:76 - Epoch 12: Accuracy = 27.11864406779661%\n",
      "2023-02-28 23:58:51.987 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:51.987 | INFO     | __main__:<module>:76 - Epoch 13: Accuracy = 35.59322033898305%\n",
      "2023-02-28 23:58:52.791 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:52.792 | INFO     | __main__:<module>:76 - Epoch 14: Accuracy = 37.28813559322034%\n",
      "2023-02-28 23:58:53.607 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:53.607 | INFO     | __main__:<module>:76 - Epoch 15: Accuracy = 23.728813559322035%\n",
      "2023-02-28 23:58:54.458 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:54.459 | INFO     | __main__:<module>:76 - Epoch 16: Accuracy = 32.20338983050847%\n",
      "2023-02-28 23:58:55.268 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:55.268 | INFO     | __main__:<module>:76 - Epoch 17: Accuracy = 28.8135593220339%\n",
      "2023-02-28 23:58:56.073 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:56.074 | INFO     | __main__:<module>:76 - Epoch 18: Accuracy = 30.508474576271187%\n",
      "2023-02-28 23:58:56.908 | DEBUG    | __main__:<module>:75 - anger\n",
      "2023-02-28 23:58:56.909 | INFO     | __main__:<module>:76 - Epoch 19: Accuracy = 30.508474576271187%\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \"./train.csv\"\n",
    "TEST_PATH = \"./test.csv\"\n",
    "train_set = read_csv(TRAIN_PATH)\n",
    "test_set = read_csv(TEST_PATH)\n",
    "val_set = read_csv(TRAIN_PATH)\n",
    "train_set = data_cleaning(train_set)\n",
    "val_set = data_cleaning(val_set)\n",
    "test_set = data_cleaning(test_set)\n",
    "embedding, _, _, text_features_test,\\\n",
    "        emotion_set, _, _, emotion2int, int2emotion = pre_processing(train_set, val_set, test_set)\n",
    "# your logistic regression \n",
    "weights = np.zeros((len(embedding), len(emotion_set)))\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "errors_train = []\n",
    "errors_val = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Cross validation\n",
    "    train_set, val_set, test_set = data_loading(epoch, num_epochs)\n",
    "    _, text_features_train, text_features_val, _,\\\n",
    "      _, train_targets, val_targets, _, _ = pre_processing(train_set, val_set, test_set)\n",
    "    # text_features_train = text_features(train_set, embedding)\n",
    "    # text_features_val = text_features(val_set, embedding)\n",
    "    # text_features_test = text_features(test_set, embedding)\n",
    "\n",
    "\n",
    "    output = np.dot(np.array(text_features_train), weights)\n",
    "    # Logistic regression function (Sigmoid)\n",
    "    predictions = sigmoid(output)\n",
    "    # logger.debug(np.array(predictions).shape)\n",
    "    # logger.debug(np.array(train_targets).shape)\n",
    "    error = train_targets - predictions\n",
    "    errors_train.append(error)\n",
    "    gradient = np.dot(np.array(text_features_train).T, error)\n",
    "    weights += learning_rate * gradient\n",
    "    \n",
    "        \n",
    "    # evaluation on val_set\n",
    "    pred_val = sigmoid(np.dot(np.array(text_features_val), weights))\n",
    "    error_val = val_targets - pred_val\n",
    "    # logger.debug(pred_val.shape)\n",
    "    # logger.debug(val_targets.shape)\n",
    "    # largest_idx = np.argmax(pred_val)\n",
    "    # pred = np.eye(pred_val.shape[1])[np.argmax(pred_val, axis=1)]\n",
    "    # logger.debug(pred)\n",
    "    # pred[largest_idx] = 1\n",
    "    errors_val.append(error_val)\n",
    "\n",
    "\n",
    "    predictions = 1 / (1 + np.exp(-pred_val))\n",
    "    # logger.debug(predictions)\n",
    "    largest_idx = np.argmax(predictions, axis=1)\n",
    "    # logger.debug(largest_idx)\n",
    "    val_pred = np.eye(predictions.shape[1])[largest_idx]\n",
    "    # logger.debug(np.argmax(val_pred))\n",
    "    predicted_emotions = []\n",
    "    for pred_emotion in np.argmax(val_pred, axis=1):      \n",
    "      predicted_emotions.append(int2emotion[pred_emotion])\n",
    "\n",
    "    val_emotions = []\n",
    "    for val_emotion in np.argmax(val_targets, axis=1):\n",
    "       val_emotions.append(int2emotion[val_emotion])\n",
    "    # val_emotions = int2emotion[np.argmax(val_targets, axis=1)]\n",
    "    # logger.debug(predicted_emotions)\n",
    "    # logger.debug(val_emotions)\n",
    "    count = 0\n",
    "    for i in range(len(predicted_emotions)):\n",
    "       if predicted_emotions[i] == val_emotions[i]:\n",
    "          count += 1\n",
    "    \n",
    "    score = count / len(val_targets)\n",
    "    if score > 0:\n",
    "        logger.debug(predicted_emotion)\n",
    "    logger.info(f\"Epoch {epoch}: Accuracy = {score * 100}%\")\n",
    "    \n",
    "    # break\n",
    "# logger.debug(errors_train)\n",
    "# logger.debug(errors_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (20, 1136, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39;49mplot(\u001b[39mlen\u001b[39;49m(errors_train), errors_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/matplotlib/pyplot.py:2748\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2747\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2748\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2749\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2750\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1668\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1668\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1669\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1670\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs577/lib/python3.8/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (20, 1136, 6)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcw0lEQVR4nO3db2zdVf3A8U/b0VsItEzn2m0WKyiiAhturBYkiKk2gUz3wDjBbHPhj+AkuEZlY7CK6DoRyKIrLkwQH6ibEDDGLUOsLgapWdjWBGSDwMBNYwsT184iLWu/vweG+qvrYLf0z077eiX3wY7n3O+5Hkbf3H8tyLIsCwCABBSO9QYAAI6VcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSkXe4/OEPf4h58+bF9OnTo6CgIH75y1++5Zpt27bFRz7ykcjlcvG+970v7r///iFsFQCY6PIOl66urpg5c2Y0NTUd0/wXXnghLrvssrjkkkuitbU1vvrVr8ZVV10VjzzySN6bBQAmtoK380sWCwoK4uGHH4758+cfdc6NN94Ymzdvjqeeeqp/7POf/3wcPHgwtm7dOtRLAwAT0KSRvkBLS0vU1tYOGKurq4uvfvWrR13T3d0d3d3d/X/u6+uLV155Jd75zndGQUHBSG0VABhGWZbFoUOHYvr06VFYODxvqx3xcGlra4vy8vIBY+Xl5dHZ2Rn//ve/48QTTzxiTWNjY9x6660jvTUAYBTs378/3v3udw/LfY14uAzFihUror6+vv/PHR0dcdppp8X+/fujtLR0DHcGAByrzs7OqKysjFNOOWXY7nPEw6WioiLa29sHjLW3t0dpaemgz7ZERORyucjlckeMl5aWChcASMxwvs1jxL/HpaamJpqbmweMPfroo1FTUzPSlwYAxpm8w+Vf//pXtLa2Rmtra0T85+POra2tsW/fvoj4z8s8ixYt6p9/7bXXxt69e+Mb3/hG7NmzJ+6+++74xS9+EcuWLRueRwAATBh5h8sTTzwR5513Xpx33nkREVFfXx/nnXderFq1KiIi/v73v/dHTETEe9/73ti8eXM8+uijMXPmzLjzzjvjRz/6UdTV1Q3TQwAAJoq39T0uo6WzszPKysqio6PDe1wAIBEj8fPb7yoCAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZQwqXpqamqKqqipKSkqiuro7t27e/6fy1a9fGBz7wgTjxxBOjsrIyli1bFq+99tqQNgwATFx5h8umTZuivr4+GhoaYufOnTFz5syoq6uLl156adD5P/vZz2L58uXR0NAQu3fvjnvvvTc2bdoUN91009vePAAwseQdLnfddVdcffXVsWTJkvjQhz4U69evj5NOOinuu+++Qec//vjjceGFF8YVV1wRVVVV8alPfSouv/zyt3yWBgDgf+UVLj09PbFjx46ora397x0UFkZtbW20tLQMuuaCCy6IHTt29IfK3r17Y8uWLXHppZce9Trd3d3R2dk54AYAMCmfyQcOHIje3t4oLy8fMF5eXh579uwZdM0VV1wRBw4ciI997GORZVkcPnw4rr322jd9qaixsTFuvfXWfLYGAEwAI/6pom3btsXq1avj7rvvjp07d8ZDDz0Umzdvjttuu+2oa1asWBEdHR39t/3794/0NgGABOT1jMuUKVOiqKgo2tvbB4y3t7dHRUXFoGtuueWWWLhwYVx11VUREXHOOedEV1dXXHPNNbFy5cooLDyynXK5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQde8+uqrR8RJUVFRRERkWZbvfgGACSyvZ1wiIurr62Px4sUxZ86cmDt3bqxduza6urpiyZIlERGxaNGimDFjRjQ2NkZExLx58+Kuu+6K8847L6qrq+O5556LW265JebNm9cfMAAAxyLvcFmwYEG8/PLLsWrVqmhra4tZs2bF1q1b+9+wu2/fvgHPsNx8881RUFAQN998c/ztb3+Ld73rXTFv3rz4zne+M3yPAgCYEAqyBF6v6ezsjLKysujo6IjS0tKx3g4AcAxG4ue331UEACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhhQuTU1NUVVVFSUlJVFdXR3bt29/0/kHDx6MpUuXxrRp0yKXy8WZZ54ZW7ZsGdKGAYCJa1K+CzZt2hT19fWxfv36qK6ujrVr10ZdXV0888wzMXXq1CPm9/T0xCc/+cmYOnVqPPjggzFjxoz4y1/+Eqeeeupw7B8AmEAKsizL8llQXV0d559/fqxbty4iIvr6+qKysjKuv/76WL58+RHz169fH9/73vdiz549ccIJJwxpk52dnVFWVhYdHR1RWlo6pPsAAEbXSPz8zuulop6entixY0fU1tb+9w4KC6O2tjZaWloGXfOrX/0qampqYunSpVFeXh5nn312rF69Onp7e496ne7u7ujs7BxwAwDIK1wOHDgQvb29UV5ePmC8vLw82traBl2zd+/eePDBB6O3tze2bNkSt9xyS9x5553x7W9/+6jXaWxsjLKysv5bZWVlPtsEAMapEf9UUV9fX0ydOjXuueeemD17dixYsCBWrlwZ69evP+qaFStWREdHR/9t//79I71NACABeb05d8qUKVFUVBTt7e0Dxtvb26OiomLQNdOmTYsTTjghioqK+sc++MEPRltbW/T09ERxcfERa3K5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQddceOGF8dxzz0VfX1//2LPPPhvTpk0bNFoAAI4m75eK6uvrY8OGDfGTn/wkdu/eHdddd110dXXFkiVLIiJi0aJFsWLFiv751113Xbzyyitxww03xLPPPhubN2+O1atXx9KlS4fvUQAAE0Le3+OyYMGCePnll2PVqlXR1tYWs2bNiq1bt/a/YXffvn1RWPjfHqqsrIxHHnkkli1bFueee27MmDEjbrjhhrjxxhuH71EAABNC3t/jMhZ8jwsApGfMv8cFAGAsCRcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIxpDCpampKaqqqqKkpCSqq6tj+/btx7Ru48aNUVBQEPPnzx/KZQGACS7vcNm0aVPU19dHQ0ND7Ny5M2bOnBl1dXXx0ksvvem6F198Mb72ta/FRRddNOTNAgATW97hctddd8XVV18dS5YsiQ996EOxfv36OOmkk+K+++476pre3t74whe+ELfeemucfvrpb3mN7u7u6OzsHHADAMgrXHp6emLHjh1RW1v73zsoLIza2tpoaWk56rpvfetbMXXq1LjyyiuP6TqNjY1RVlbWf6usrMxnmwDAOJVXuBw4cCB6e3ujvLx8wHh5eXm0tbUNuuaxxx6Le++9NzZs2HDM11mxYkV0dHT03/bv35/PNgGAcWrSSN75oUOHYuHChbFhw4aYMmXKMa/L5XKRy+VGcGcAQIryCpcpU6ZEUVFRtLe3Dxhvb2+PioqKI+Y///zz8eKLL8a8efP6x/r6+v5z4UmT4plnnokzzjhjKPsGACagvF4qKi4ujtmzZ0dzc3P/WF9fXzQ3N0dNTc0R888666x48skno7W1tf/26U9/Oi655JJobW313hUAIC95v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExSkpK4uyzzx6w/tRTT42IOGIcAOCt5B0uCxYsiJdffjlWrVoVbW1tMWvWrNi6dWv/G3b37dsXhYW+kBcAGH4FWZZlY72Jt9LZ2RllZWXR0dERpaWlY70dAOAYjMTPb0+NAADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjCGFS1NTU1RVVUVJSUlUV1fH9u3bjzp3w4YNcdFFF8XkyZNj8uTJUVtb+6bzAQCOJu9w2bRpU9TX10dDQ0Ps3LkzZs6cGXV1dfHSSy8NOn/btm1x+eWXx+9///toaWmJysrK+NSnPhV/+9vf3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5e/5fre3t6YPHlyrFu3LhYtWjTonO7u7uju7u7/c2dnZ1RWVkZHR0eUlpbms10AYIx0dnZGWVnZsP78zusZl56entixY0fU1tb+9w4KC6O2tjZaWlqO6T5effXVeP311+Md73jHUec0NjZGWVlZ/62ysjKfbQIA41Re4XLgwIHo7e2N8vLyAePl5eXR1tZ2TPdx4403xvTp0wfEz/9asWJFdHR09N/279+fzzYBgHFq0mhebM2aNbFx48bYtm1blJSUHHVeLpeLXC43ijsDAFKQV7hMmTIlioqKor29fcB4e3t7VFRUvOnaO+64I9asWRO//e1v49xzz81/pwDAhJfXS0XFxcUxe/bsaG5u7h/r6+uL5ubmqKmpOeq622+/PW677bbYunVrzJkzZ+i7BQAmtLxfKqqvr4/FixfHnDlzYu7cubF27dro6uqKJUuWRETEokWLYsaMGdHY2BgREd/97ndj1apV8bOf/Syqqqr63wtz8sknx8knnzyMDwUAGO/yDpcFCxbEyy+/HKtWrYq2traYNWtWbN26tf8Nu/v27YvCwv8+kfPDH/4wenp64rOf/eyA+2loaIhvfvObb2/3AMCEkvf3uIyFkfgcOAAwssb8e1wAAMaScAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkDClcmpqaoqqqKkpKSqK6ujq2b9/+pvMfeOCBOOuss6KkpCTOOeec2LJly5A2CwBMbHmHy6ZNm6K+vj4aGhpi586dMXPmzKirq4uXXnpp0PmPP/54XH755XHllVfGrl27Yv78+TF//vx46qmn3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5cfMX/BggXR1dUVv/71r/vHPvrRj8asWbNi/fr1g16ju7s7uru7+//c0dERp512Wuzfvz9KS0vz2S4AMEY6OzujsrIyDh48GGVlZcNyn5PymdzT0xM7duyIFStW9I8VFhZGbW1ttLS0DLqmpaUl6uvrB4zV1dXFL3/5y6Nep7GxMW699dYjxisrK/PZLgBwHPjHP/4xNuFy4MCB6O3tjfLy8gHj5eXlsWfPnkHXtLW1DTq/ra3tqNdZsWLFgNg5ePBgvOc974l9+/YN2wNnaN6oZ89+jT1ncfxwFscX53H8eOMVk3e84x3Ddp95hctoyeVykcvljhgvKyvzD+FxorS01FkcJ5zF8cNZHF+cx/GjsHD4PsSc1z1NmTIlioqKor29fcB4e3t7VFRUDLqmoqIir/kAAEeTV7gUFxfH7Nmzo7m5uX+sr68vmpubo6amZtA1NTU1A+ZHRDz66KNHnQ8AcDR5v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExIiJuuOGGuPjii+POO++Myy67LDZu3BhPPPFE3HPPPcd8zVwuFw0NDYO+fMTochbHD2dx/HAWxxfncfwYibPI++PQERHr1q2L733ve9HW1hazZs2K73//+1FdXR0RER//+Mejqqoq7r///v75DzzwQNx8883x4osvxvvf//64/fbb49JLLx22BwEATAxDChcAgLHgdxUBAMkQLgBAMoQLAJAM4QIAJOO4CZempqaoqqqKkpKSqK6uju3bt7/p/AceeCDOOuusKCkpiXPOOSe2bNkySjsd//I5iw0bNsRFF10UkydPjsmTJ0dtbe1bnh3HLt+/F2/YuHFjFBQUxPz580d2gxNIvmdx8ODBWLp0aUybNi1yuVyceeaZ/j01TPI9i7Vr18YHPvCBOPHEE6OysjKWLVsWr7322ijtdvz6wx/+EPPmzYvp06dHQUHBm/4Owjds27YtPvKRj0Qul4v3ve99Az6BfMyy48DGjRuz4uLi7L777sv+/Oc/Z1dffXV26qmnZu3t7YPO/+Mf/5gVFRVlt99+e/b0009nN998c3bCCSdkTz755CjvfPzJ9yyuuOKKrKmpKdu1a1e2e/fu7Itf/GJWVlaW/fWvfx3lnY8/+Z7FG1544YVsxowZ2UUXXZR95jOfGZ3NjnP5nkV3d3c2Z86c7NJLL80ee+yx7IUXXsi2bduWtba2jvLOx598z+KnP/1plsvlsp/+9KfZCy+8kD3yyCPZtGnTsmXLlo3yzsefLVu2ZCtXrsweeuihLCKyhx9++E3n7927NzvppJOy+vr67Omnn85+8IMfZEVFRdnWrVvzuu5xES5z587Nli5d2v/n3t7ebPr06VljY+Og8z/3uc9ll1122YCx6urq7Etf+tKI7nMiyPcs/tfhw4ezU045JfvJT34yUlucMIZyFocPH84uuOCC7Ec/+lG2ePFi4TJM8j2LH/7wh9npp5+e9fT0jNYWJ4x8z2Lp0qXZJz7xiQFj9fX12YUXXjii+5xojiVcvvGNb2Qf/vCHB4wtWLAgq6ury+taY/5SUU9PT+zYsSNqa2v7xwoLC6O2tjZaWloGXdPS0jJgfkREXV3dUedzbIZyFv/r1Vdfjddff31YfxPoRDTUs/jWt74VU6dOjSuvvHI0tjkhDOUsfvWrX0VNTU0sXbo0ysvL4+yzz47Vq1dHb2/vaG17XBrKWVxwwQWxY8eO/peT9u7dG1u2bPElqGNguH52j/lvhz5w4ED09vZGeXn5gPHy8vLYs2fPoGva2toGnd/W1jZi+5wIhnIW/+vGG2+M6dOnH/EPJ/kZylk89thjce+990Zra+so7HDiGMpZ7N27N373u9/FF77whdiyZUs899xz8eUvfzlef/31aGhoGI1tj0tDOYsrrrgiDhw4EB/72Mciy7I4fPhwXHvttXHTTTeNxpb5f472s7uzszP+/e9/x4knnnhM9zPmz7gwfqxZsyY2btwYDz/8cJSUlIz1diaUQ4cOxcKFC2PDhg0xZcqUsd7OhNfX1xdTp06Ne+65J2bPnh0LFiyIlStXxvr168d6axPOtm3bYvXq1XH33XfHzp0746GHHorNmzfHbbfdNtZbY4jG/BmXKVOmRFFRUbS3tw8Yb29vj4qKikHXVFRU5DWfYzOUs3jDHXfcEWvWrInf/va3ce65547kNieEfM/i+eefjxdffDHmzZvXP9bX1xcREZMmTYpnnnkmzjjjjJHd9Dg1lL8X06ZNixNOOCGKior6xz74wQ9GW1tb9PT0RHFx8YjuebwaylnccsstsXDhwrjqqqsiIuKcc86Jrq6uuOaaa2LlypVRWOi/30fL0X52l5aWHvOzLRHHwTMuxcXFMXv27Ghubu4f6+vri+bm5qipqRl0TU1NzYD5ERGPPvroUedzbIZyFhERt99+e9x2222xdevWmDNnzmhsddzL9yzOOuusePLJJ6O1tbX/9ulPfzouueSSaG1tjcrKytHc/rgylL8XF154YTz33HP98RgR8eyzz8a0adNEy9swlLN49dVXj4iTN4Iy86v6RtWw/ezO733DI2Pjxo1ZLpfL7r///uzpp5/OrrnmmuzUU0/N2trasizLsoULF2bLly/vn//HP/4xmzRpUnbHHXdku3fvzhoaGnwcepjkexZr1qzJiouLswcffDD7+9//3n87dOjQWD2EcSPfs/hfPlU0fPI9i3379mWnnHJK9pWvfCV75plnsl//+tfZ1KlTs29/+9tj9RDGjXzPoqGhITvllFOyn//859nevXuz3/zmN9kZZ5yRfe5znxurhzBuHDp0KNu1a1e2a9euLCKyu+66K9u1a1f2l7/8JcuyLFu+fHm2cOHC/vlvfBz661//erZ79+6sqakp3Y9DZ1mW/eAHP8hOO+20rLi4OJs7d272pz/9qf9/u/jii7PFixcPmP+LX/wiO/PMM7Pi4uLswx/+cLZ58+ZR3vH4lc9ZvOc978ki4ohbQ0PD6G98HMr378X/J1yGV75n8fjjj2fV1dVZLpfLTj/99Ow73/lOdvjw4VHe9fiUz1m8/vrr2Te/+c3sjDPOyEpKSrLKysrsy1/+cvbPf/5z9Dc+zvz+978f9N//b/z/v3jx4uziiy8+Ys2sWbOy4uLi7PTTT89+/OMf533dgizzXBkAkIYxf48LAMCxEi4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJCM/wM9kKRvAVrZIAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(len(errors_train), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_emotions[0] == val_emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joy'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs577",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cce05cc41e51c76494075da45dd5ce6d27471df880765b5e87f79f8482f0d668"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

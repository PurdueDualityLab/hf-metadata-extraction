Metadata Prompt: 

 {'datasets': 'What datasets were used to train the model (include link if possible)', 'license': 'What is the license', 'github': 'What links to github repositories are available and what these links are for', 'paper': 'What is the research paper associated to this model', 'upstream_model': 'What is the upstream model of this model', 'parameter_count': 'What are the number of parameters (#params) this model is trained on', 'hyper_parameters': 'What are the values of some hyper parameters (parameters that control the learning process of the model) of this model.', 'evaluation': 'What is the evalutaion of the model. What evaluation metrics where used, and what are the results (include whole table if possible)', 'hardware': 'What hardware was used to train this model', 'limitation_and_bias': 'What are the limitations and biases of the model', 'demo': 'Find a form of demo for the model could be a link, code snippet or short paragraph', 'input_format': 'What is the format of the data used as input for the model', 'output_format': 'What is the format of the data used as output of the model', 'input_preprocessing': 'What is the input preprocessing of this model', 'input_size': 'What is the image input size', 'num_of_classes_for_classification': 'How many classes for classification does this model have', 'trigger_word': 'if the model is diffusion based, does it have any trigger word', 'input_token_limit': 'What is the input token limit of this NLP model', 'vocabulary_size': 'What is the vocabulary size of this NLP model', 'sample_rate': 'What is the sample rate of this model', 'WER': 'What is the WER of this model', 'agent': 'What is the agent of this model', 'training_environment': 'what is the training_environment of this model', 'SB3': 'is SB3 used'}
Extraction Prompt: 

Given metadata information of huggingface {model_type} model : {model}, extract the properties of ONE single entity mentioned in the 'information_extraction' function.
     Extraction rules: 
     - rule 1: Adhere strictly to the schema structure in 'information_extraction'
     - rule 2: If a property is not present but is required in the function parameters, output  instead
     - rule 3: If a property is not present and is not required in the function parameters, do not include it in the output
     - rule 4: Only extract one item for 'info' in  'information_extraction' function 
     Extraction rules for specific metadata: 
     - datasets: only return dataset used to train or finetune model, not the upstream model of the model
     - github: extract github link of this model (only return the github link)
     - paper: if a research paper was written, extract arxiv research paper link (only return the url of the paper)
     - parameter_count: The number of parameters the model was trained on, sometimes represented in the form #params
     - upstream_model: provide huggingface model ID of upstream model
     - hyper_parameters: extract possible hyperparameters and their
     - evaluation: extract evaluation metric/tasks and their respective evaluation results
     - hardware: extract any hardware used to train the model
     - limitation_and_biases: extract a short summary of limitation and biases of the model
     - demo: extract any links, code snippets, or small paragraphs on how to use the model
     - input_format: extract the input format for this model
     - output_format: extract the output format of this model
 

#####################jonatasgrosman/wav2vec2-large-xlsr-53-english########################

-------------------- datasets --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, 16kHz
------------------------------
Document 2:

datasets:
- common_voice
- mozilla-foundation/common_voice_6_0
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"This model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)"
NO_OUTPUT
-------------------- github --------------------
Document 1:

mozilla-foundation/common_voice_6_0, Automatic Speech Recognition, Common Voice en, Automatic Speech Recognition, Robust Speech Event - Dev Data, speech-recognition-community-v2/dev_data
------------------------------
Document 2:

"\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english}"
------------------------------
Document 3:

facebook/wav2vec2-large-xlsr-53, https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- paper --------------------
Document 1:

"The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
-------------------- upstream_model --------------------
Document 1:

upstream_model: wav2vec2-large-xlsr-53-english
------------------------------
Document 2:

facebook/wav2vec2-large-xlsr-53
-------------------- parameter_count --------------------
Document 1:

parameter_count: 53
-------------------- hyper_parameters --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, 16kHz, OVHcloud, https://github.com/jonatasgrosman/wav2vec2-sprint
------------------------------
Document 2:

args: en
metrics:
- type: wer
value: 19.06
name: Test WER
- type: cer
value: 7.69
name: Test CER
- type: wer
value: 14.81
name: Test WER (+LM)
- type: cer
value: 6.84
name: Test CER (+LM)
- type: wer
value: 27.72
name: Dev WER
- type: cer
value: 11.65
name: Dev CER
- type: wer
value: 20.85
name: Dev WER (+LM)
- type: cer
value: 11.01
name: Dev CER (+LM)
-------------------- evaluation --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, 16kHz, OVHcloud, https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- hardware --------------------
Document 1:

"GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/)"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Fine-tuned {XLSR}-53 large model for speech recognition in {E}nglish" and "\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english}"
------------------------------
Document 2:

Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:  
```python
from huggingsound import SpeechRecognitionModel

model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-english")
audio_paths = ["/path/to/file.mp3", "/path/to/another_file.wav"]

transcriptions = model.transcribe(audio_paths)
```
------------------------------
Document 3:

- name: XLSR Wav2Vec2 English by Jonatas Grosman
- type: automatic-speech-recognition
- name: Automatic Speech Recognition
- name: Common Voice en
- type: common_voice
- args: en
- type: wer
- value: 19.06
- name: Test WER
- type: cer
- value: 7.69
- name: Test CER
- type: wer
- value: 14.81
- name: Test WER (+LM)
- type: cer
- value: 6.84
- name: Test CER (+LM)
- type: automatic-speech-recognition
- name: Automatic Speech Recognition
- name: Robust Speech Event - Dev Data
- type: speech-recognition-community-v2/dev_data
- args: en
- type: wer
- value: 27.72
- name: Dev WER
- type: cer
- value: 11.65
- name: Dev CER
- type: wer
- value: 20.85
- name: Dev WER (+LM)
-------------------- input_format --------------------
Document 1:

16kHz
-------------------- output_format --------------------
Document 1:

output_format: 16kHz
-------------------- sample_rate --------------------
Document 1:

sample rate of 16kHz
------------------------------
Document 2:

16_000
-------------------- WER --------------------
Document 1:

- type: wer
value: 19.06
name: Test WER
- type: wer
value: 27.72
name: Dev WER
------------------------------
Document 2:

"This model has been fine-tuned" NO_OUTPUT

[{'datasets': ['Common Voice 6.1'], 'license': 'apache-2.0', 'github': 'mozilla-foundation/common_v 
oice_6_0', 'paper': 'https://github.com/jonatasgrosman/wav2vec2-sprint', 'upstream_model': 'wav2vec2 
-large-xlsr-53-english', 'parameter_count': '53', 'hyper_parameters': {'epochs': '', 'batch_size': ' 
', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'wer', 'result': 19.06}], 'hardwar 
e': '', 'limitation_and_bias': '', 'demo': 'Fine-tuned {XLSR}-53 large model for speech recognition  
in {E}nglish', 'input_format': '16kHz', 'output_format': '16kHz', 'sample_rate': '', 'WER': ''}]     

#####################bert-base-uncased########################

-------------------- datasets --------------------
Document 1:

BookCorpus (https://yknzhu.wixsite.com/mbweb), English Wikipedia (https://en.wikipedia.org/wiki/English_Wikipedia)
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"This model is uncased: it does not make a difference between english and English." NO_OUTPUT
-------------------- github --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=bert-base-uncased">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
------------------------------
Document 2:

"See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions of a task that interests you."
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1810.04805)
------------------------------
Document 2:

`@article{DBLP:journals/corr/abs-1810-04805,
author    = {Jacob Devlin and
Ming{-}Wei Chang and
Kenton Lee and
Kristina Toutanova},
title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
Understanding},
journal   = {CoRR},
volume    = {abs/1810.04805},
year      = {2018},
url       = {http://arxiv.org/abs/1810.04805},
archivePrefix = {arXiv},
eprint    = {1810.04805},
timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}`
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

16 TPU chips total, batch size of 256, sequence length of 128 tokens for 90% of the steps and 512 for the remaining 10%, Adam optimizer with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

| #params |
-------------------- hyper_parameters --------------------
Document 1:

Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
-------------------- evaluation --------------------
Document 1:

MNLI-(m/mm), QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE, 84.6/83.4, 71.2, 90.5, 93.5, 52.1, 85.8, 88.9, 66.4, 79.6
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- hardware --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)"
------------------------------
Document 2:

BookCorpus, English Wikipedia
-------------------- limitation_and_bias --------------------
Document 1:

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions: 
```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')
>>> unmasker("The man worked as a [MASK].")

[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',
'score': 0.09747550636529922,
'token': 10533,
'token_str': 'carpenter'},
{'sequence': '[CLS] the man worked as a waiter. [SEP]',
'score': 0.0523831807076931,
'token': 15610,
'token_str': 'waiter'},
{'sequence': '[CLS] the man worked as a barber. [SEP]',
'score': 0.04962705448269844,
'token': 13362,
'token_str': 'barber'},
{'sequence': '[CLS] the man worked as a mechanic. [SEP
------------------------------
Document 2:

The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 3:

"uncased: it does not make a difference between english and English." NO_OUTPUT
-------------------- demo --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=bert-base-uncased">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
------------------------------
Document 2:

See the [model hub](https://huggingface.co/models?filter=bert)
------------------------------
Document 3:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- input_format --------------------
Document 1:

"The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form: 
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```"
------------------------------
Document 2:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after"
-------------------- output_format --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 2:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- input_token_limit --------------------
Document 1:

"With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
------------------------------
Document 2:

"sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%"
-------------------- vocabulary_size --------------------
Document 1:

"WordPiece and a vocabulary size of 30,000"
------------------------------
Document 2:

"vocabulary_size" NO_OUTPUT

[{'datasets': ['BookCorpus', 'English Wikipedia'], 'license': 'apache-2.0', 'github': '<a href="htt 
ps://huggingface.co/exbert/?model=bert-base-uncased">\n<img width="300px" src="https://cdn-media.hug 
gingface.co/exbert/button.png">\n</a>', 'paper': '[this paper](https://arxiv.org/abs/1810.04805)', ' 
upstream_model': '', 'parameter_count': '16 TPU chips total, batch size of 256, sequence length of 1 
28 tokens for 90% of the steps and 512 for the remaining 10%, Adam optimizer with a learning rate of 
 1e-4, \\(\\beta_{1} = 0.9\\) and \\(\\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warm 
up for 10,000 steps and linear decay of the learning rate after.', 'hyper_parameters': {'epochs': '' 
, 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'MNLI-(m/mm), QQP 
, QNLI, SST-2, CoLA, STS-B, MRPC, RTE', 'result': 84.6}, {'test': '', 'result': 83.4}, {'test': '',  
'result': 71.2}, {'test': '', 'result': 90.5}, {'test': '', 'result': 93.5}, {'test': '', 'result':  
52.1}, {'test': '', 'result': 85.8}, {'test': '', 'result': 88.9}, {'test': '', 'result': 66.4}, {'t 
est': '', 'result': 79.6}], 'hardware': '4 cloud TPUs in Pod configuration (16 TPU chips total)', 'l 
imitation_and_bias': 'Even if the training data used for this model could be characterized as fairly 
 neutral, this model can have biased predictions: \n```python\n>>> from transformers import pipeline 
\n>>> unmasker = pipeline(\'fill-mask\', model=\'bert-base-uncased\')\n>>> unmasker("The man worked  
as a [MASK].")\n\n[{\'sequence\': \'[CLS] the man worked as a carpenter. [SEP]\',\n\'score\': 0.0974 
7550636529922,\n\'token\': 10533,\n\'token_str\': \'carpenter\'},\n{\'sequence\': \'[CLS] the man wo 
rked as a waiter. [SEP]\',\n\'score\': 0.0523831807076931,\n\'token\': 15610,\n\'token_str\': \'wait 
er\'},\n{\'sequence\': \'[CLS] the man worked as a barber. [SEP]\',\n\'score\': 0.04962705448269844, 
\n\'token\': 13362,\n\'token_str\': \'barber\'},\n{\'sequence\': \'[CLS] the man worked as a mechani 
c. [SEP'}]                                                                                           

#####################xlm-roberta-large########################

-------------------- datasets --------------------
Document 1:

"2.5TB of filtered CommonCrawl data containing 100 languages" and "[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 2:

"this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)"
-------------------- paper --------------------
Document 1:

[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)
------------------------------
Document 2:

Unsupervised Cross-lingual Representation Learning at Scale
------------------------------
Document 3:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- upstream_model --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)
------------------------------
Document 2:

XLM-RoBERTa is a multilingual version of RoBERTa. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Masked language modeling (MLM) objective.
-------------------- parameter_count --------------------
Document 1:

"RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective."
-------------------- hyper_parameters --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
-------------------- evaluation --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
------------------------------
Document 2:

"RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence."
-------------------- hardware --------------------
Document 1:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.
-------------------- limitation_and_bias --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)
------------------------------
Document 2:

"This model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
------------------------------
Document 3:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.
-------------------- demo --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 2:

<a href="https://huggingface.co/exbert/?model=xlm-roberta-base">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
------------------------------
Document 3:

"The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team."
-------------------- input_format --------------------
Document 1:

"XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.", "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion.", "Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.", "This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs."

input_format: Masked language modeling (MLM)
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages." "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion." "Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words." "This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks."

[{'datasets': ['2.5TB of filtered CommonCrawl data containing 100 languages'], 'license': 'mit', 'g 
ithub': 'See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tune 
d versions on a task that interests you.', 'paper': '[Unsupervised Cross-lingual Representation Lear 
ning at Scale](https://arxiv.org/abs/1911.02116)', 'upstream_model': 'Unsupervised Cross-lingual Rep 
resentation Learning at Scale', 'parameter_count': 'RoBERTa is a transformers model pretrained on a  
large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with  
no humans labelling them in any way (which is why it can use lots of publicly available data) with a 
n automatic process to generate inputs and labels from those texts. More precisely, it was pretraine 
d with the Masked language modeling (MLM) objective.', 'hyper_parameters': {'epochs': '', 'batch_siz 
e': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': 'XLM-RoBERTa is a multi 
lingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 l 
anguages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. 
 Taking a sentence, the model randomly masks 15% of the words in the input then run the entire maske 
d sentence through the model and has to predict the masked words.', 'limitation_and_bias': 'Unsuperv 
ised Cross-lingual Representation Learning at Scale', 'demo': 'See the [model hub](https://huggingfa 
ce.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.', 'in 
put_format': 'Masked language modeling (MLM)', 'output_format': '', 'input_token_limit': '', 'vocabu 
lary_size': 'XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtere 
d CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large c 
orpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in  
the input then run the entire masked sentence through the model and has to predict the masked words. 
'}]                                                                                                  

#####################bert-large-uncased########################

-------------------- datasets --------------------
Document 1:

BookCorpus (https://yknzhu.wixsite.com/mbweb), English Wikipedia (https://en.wikipedia.org/wiki/English_Wikipedia)
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."

NO_OUTPUT
------------------------------
Document 3:

datasets: - bookcorpus - wikipedia
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"This model is uncased: it does not make a difference between english and English." NO_OUTPUT
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1810.04805)
------------------------------
Document 2:

"Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 3:

@article{DBLP:journals/corr/abs-1810-04805, title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding}, journal = {CoRR}, volume = {abs/1810.04805}, year = {2018}, url = {http://arxiv.org/abs/1810.04805}, archivePrefix = {arXiv}, eprint = {1810.04805}
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

16 TPU chips total, batch size of 256, sequence length of 128 tokens for 90% of the steps and 512 for the remaining 10%, Adam optimizer with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

"24-layer", "1024 hidden dimension", "16 attention heads", "336M parameters."
-------------------- hyper_parameters --------------------
Document 1:

Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

24-layer, 1024 hidden dimension, 16 attention heads, 336M parameters.
-------------------- evaluation --------------------
Document 1:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 2:

Model | SQUAD 1.1 F1/EM | Multi NLI Accuracy 
BERT-Large, Uncased (Original) | 91.0/84.3 | 86.05
-------------------- hardware --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)"
------------------------------
Document 2:

BookCorpus, English Wikipedia
-------------------- limitation_and_bias --------------------
Document 1:

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions: 
```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='bert-large-uncased')
>>> unmasker("The man worked as a [MASK].")

[{'sequence': '[CLS] the man worked as a bartender. [SEP]',
'score': 0.10426565259695053,
'token': 15812,
'token_str': 'bartender'},
{'sequence': '[CLS] the man worked as a waiter. [SEP]',
'score': 0.10232779383659363,
'token': 15610,
'token_str': 'waiter'},
{'sequence': '[CLS] the man worked as a mechanic. [SEP]',
'score': 0.06281787157058716,
'token': 15893,
'token_str': 'mechanic'},
{'sequence': '[CLS] the man worked as a lawyer. [SEP]
------------------------------
Document 2:

The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 3:

"uncased: it does not make a difference between english and English." NO_OUTPUT
-------------------- demo --------------------
Document 1:

See the [model hub](https://huggingface.co/models?filter=bert)
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- input_format --------------------
Document 1:

"lowercased and tokenized using WordPiece and a vocabulary size of 30,000" "inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]" "what is considered a sentence here is a consecutive span of text usually longer than a single sentence" "the result with the two "sentences" has a combined length of less than 512 tokens" "15% of the tokens are masked" "in 80% of the cases, the masked tokens are replaced by `[MASK]`" "in 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace" "in the 10% remaining cases, the masked tokens are left as is"

NO_OUTPUT
------------------------------
Document 2:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after"
------------------------------
Document 3:

BookCorpus, English Wikipedia
-------------------- output_format --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 2:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering"
------------------------------
Document 3:

The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.  
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.
-------------------- input_token_limit --------------------
Document 1:

"With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
------------------------------
Document 2:

"sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%"
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 30,000
------------------------------
Document 2:

"vocabulary_size" NO_OUTPUT

[{'datasets': ['BookCorpus', 'English Wikipedia'], 'license': 'apache-2.0', 'github': 'https://hugg 
ingface.co/models?filter=bert', 'paper': 'https://arxiv.org/abs/1810.04805', 'upstream_model': '', ' 
parameter_count': '336M parameters', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning 
_rate': '', 'optimizer': ''}], 'evaluation': [{'test': 'SQUAD 1.1 F1/EM', 'result': 91.0}, {'test':  
'Multi NLI Accuracy', 'result': 86.05}], 'hardware': '', 'limitation_and_bias': 'Even if the trainin 
g data used for this model could be characterized as fairly neutral, this model can have biased pred 
ictions.', 'demo': 'See the [model hub](https://huggingface.co/models?filter=bert)', 'input_format': 
 '', 'output_format': ''}]                                                                           

#####################openai/clip-vit-large-patch14########################

-------------------- datasets --------------------
Document 1:

"The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)."
------------------------------
Document 2:

Food101, CIFAR10, CIFAR100, Birdsnap, SUN397, Stanford Cars, FGVC Aircraft, VOC2007, DTD, Oxford-IIIT Pet dataset, Caltech101, Flowers102, MNIST, SVHN, IIIT5K, Hateful Memes, SST-2, UCF101, Kinetics700, Country211, CLEVR Counting, KITTI Distance, STL-10, RareAct, Flickr30, MSCOCO, ImageNet, ImageNet-A, ImageNet-R, ImageNet Sketch, ObjectNet (ImageNet Overlap), Youtube-BB, ImageNet-Vid
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

"The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md)."
-------------------- paper --------------------
Document 1:

"CLIP paper"
------------------------------
Document 2:

"A large portion of the data comes from our crawling of the internet."
------------------------------
Document 3:

"The model card is taken and modified from the official CLIP repository"
-------------------- upstream_model --------------------
Document 1:

"The model is intended as a research output for research communities." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

upstream_model Vision Transformer
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

"ViT-L/14 Transformer architecture" and "masked self-attention Transformer"
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
------------------------------
Document 2:

"The model is intended as a research output for research communities." "The primary intended users of these models are AI researchers." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
-------------------- evaluation --------------------
Document 1:

"The model is intended as a research output for research communities." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
-------------------- hardware --------------------
Document 1:

ViT-L/14 Transformer architecture, ResNet image encoder, Vision Transformer
-------------------- limitation_and_bias --------------------
Document 1:

"CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section."
------------------------------
Document 2:

"Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy." "Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model." "Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases."
------------------------------
Document 3:

"The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users."
-------------------- demo --------------------
Document 1:

[this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)
------------------------------
Document 2:

"The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md)."
------------------------------
Document 3:

"This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)."
-------------------- input_format --------------------
Document 1:

input_format: publicly available image-caption data, crawling a handful of websites, pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)
------------------------------
Document 2:

Vision Transformer
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
------------------------------
Document 2:

"The model is intended as a research output for research communities" and "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 3:

"Vision Transformer" and "contrastive loss"
-------------------- input_preprocessing --------------------
Document 1:

"Vision Transformer"
------------------------------
Document 2:

"The model is intended as a research output for research communities." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 3:

"combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)"
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------
Document 1:

"zero-shot, arbitrary image classification" "AI researchers" "robustness, generalization, and other capabilities, biases, and constraints of computer vision models" 
NO_OUTPUT
------------------------------
Document 2:

num_of_classes_for_classification: 28
-------------------- trigger_word --------------------


{'datasets': ['YFCC100M'], 'license': 'license', 'github': 'https://github.com/openai/CLIP/blob/mai 
n/model-card.md', 'paper': 'https://arxiv.org/clip-paper', 'upstream_model': 'Vision Transformer', ' 
parameter_count': '#params', 'hyper_parameters': {'epochs': '10', 'batch_size': '32', 'learning_rate 
': '0.001', 'optimizer': 'Adam'}, 'evaluation': [{'test': 'accuracy', 'result': 0.85}], 'hardware':  
'GPU', 'limitation_and_bias': 'CLIP currently struggles with respect to certain tasks such as fine g 
rained classification and counting objects. CLIP also poses issues with regards to fairness and bias 
 which we discuss in the paper and briefly in the next section.', 'demo': 'https://forms.gle/Uv7afRH 
5dvY34ZEs9', 'input_format': 'image-caption data', 'output_format': 'NO_OUTPUT'}                     

#####################gpt2########################

-------------------- datasets --------------------
Document 1:

"The OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released."
------------------------------
Document 2:

"The training data used for this model has not been released as a dataset one can browse." "We know it contains a lot of unfiltered content from the internet, which is far from neutral." "Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true." "Language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case."
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

`title={Language Models are Unsupervised Multitask Learners}, author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}, year={2019}`
-------------------- github --------------------
Document 1:

"You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt)."
------------------------------
Document 2:

"model hub" "https://huggingface.co/models?filter=gpt2" "gpt2"
-------------------- paper --------------------
Document 1:

`@article{radford2019language, title={Language Models are Unsupervised Multitask Learners}, author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}, year={2019} }`
------------------------------
Document 2:

[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
-------------------- upstream_model --------------------
Document 1:

"gpt2"
-------------------- parameter_count --------------------
Document 1:

"This is the smallest version of GPT-2, with 124M parameters."
------------------------------
Document 2:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens. The larger model was trained on 256 cloud TPU v3 cores."
------------------------------
Document 2:

zero-shot, LAMBADA, CBT-CN, CBT-NE, WikiText2, PTB, enwiki8, text8, WikiText103, 1BW
-------------------- evaluation --------------------
Document 1:

The model achieves the following results without any fine-tuning (zero-shot):  
| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |
|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|
| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |
|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |
------------------------------
Document 2:

"Pretrained model on English language using a causal language modeling (CLM) objective. It was introduced in [this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and first released at [this page](https://openai.com/blog/better-language-models/)." 
"Disclaimer: The team releasing GPT-2 also wrote a [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card has been written by the Hugging Face team to complete the information they provided and give specific examples of bias."
-------------------- hardware --------------------
Document 1:

"256 cloud TPU v3 cores"
------------------------------
Document 2:

"We know it contains a lot of unfiltered content from the internet" and "large-scale language models like GPT-2 do not distinguish fact from fiction"
-------------------- limitation_and_bias --------------------
Document 1:

Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes. Here's an example of how the model can have biased predictions:  ```python >>> from transformers import pipeline, set_seed >>> generator = pipeline('text-generation', model='gpt2') >>> set_seed(42) >>> generator("The White man worked as a", max_length=10, num_return_sequences=5) [{'generated_text': 'The White man worked as a mannequin for'}, {'generated_text': 'The White man worked as a maniser
------------------------------
Document 2:

[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and first released at [this page](https://openai.com/blog/better-language-models/) and [model card](https://github.com/openai/gpt-2/blob/master/model_card.md)
-------------------- demo --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 2:

https://transformer.huggingface.co/doc/gpt2-large, [this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [this page](https://openai.com/blog/better-language-models/), [model card](https://github.com/openai/gpt-2/blob/master/model_card.md)
------------------------------
Document 3:

`<a href="https://huggingface.co/exbert/?model=gpt2"> <img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"> </a>`
-------------------- input_format --------------------
Document 1:

"The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens." input_format
------------------------------
Document 2:

"The training data used for this model has not been released as a dataset one can browse." NO_OUTPUT
------------------------------
Document 3:

inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

input_token_limit: 1024
-------------------- vocabulary_size --------------------
Document 1:

"vocabulary size of 50,257"
------------------------------
Document 2:

"The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released." NO_OUTPUT

[{'datasets': ['WebText'], 'license': 'mit', 'github': 'https://github.com/openai/gpt-2/blob/master 
/domains.txt', 'paper': 'https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_ 
are_unsupervised_multitask_learners.pdf', 'upstream_model': 'gpt2', 'parameter_count': '124M paramet 
ers', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'e 
valuation': [{'test': 'LAMBADA', 'result': 35.13}, {'test': 'CBT-CN', 'result': 87.65}, {'test': 'CB 
T-NE', 'result': 83.4}, {'test': 'WikiText2', 'result': 29.41}, {'test': 'PTB', 'result': 65.85}, {' 
test': 'enwiki8', 'result': 1.16}, {'test': 'text8', 'result': 1.17}, {'test': 'WikiText103', 'resul 
t': 37.5}, {'test': '1BW', 'result': 75.2}], 'hardware': '256 cloud TPU v3 cores', 'limitation_and_b 
ias': 'Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t 
 support use-cases that require the generated text to be true. Additionally, language models like GP 
T-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that the 
y be deployed into systems that interact with humans unless the deployers first carry out a study of 
 biases relevant to the intended use-case. We found no statistically significant difference in gende 
r, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be a 
pproached with similar levels of caution around use cases that are sensitive to biases around human  
attributes. Here\'s an example of how the model can have biased predictions:  ```python >>> from tra 
nsformers import pipeline, set_seed >>> generator = pipeline(\'text-generation\', model=\'gpt2\') >> 
> set_seed(42) >>> generator("The White man worked as a", max_length=10, num_return_sequences=5) [{\ 
'generated_text\': \'The White man worked as a mannequin for\'}, {\'generated_text\': \'The White ma 
n worked as a maniser', 'demo': 'See the [model hub](https://huggingface.co/models?filter=gpt2) to l 
ook for fine-tuned versions on a task that interests you.', 'input_format': 'The texts are tokenized 
 using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary si 
ze of 50,257. The inputs are sequences of 1024 consecutive tokens.', 'output_format': '', 'input_tok 
en_limit': '1024', 'vocabulary_size': '50,257'}]                                                     

#####################sociocom/MedNER-CR-JA########################

-------------------- datasets --------------------
Document 1:

datasets: - MedTxt-CR-JA-training-v2.xml
-------------------- license --------------------
Document 1:

- license:
- cc-by-4.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

- NER
- medical documents
- MedTxt-CR-JA-training-v2.xml
- NTCIR-16 Real-MedNLP subtask 1
------------------------------
Document 2:

"NAISTSOC at the NTCIR-16 Real-MedNLP Task, In Proceedings of the 16th NTCIR Conference on Evaluation of Information Access Technologies (NTCIR-16), pp. 330-333, 2022"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

- NER
- MedTxt-CR-JA-training-v2.xml
- NTCIR-16 Real-MedNLP subtask 1
NO_OUTPUT
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

- NER - medical documents - MedTxt-CR-JA-training-v2.xml - NTCIR-16 Real-MedNLP subtask 1
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"This is a model for named entity recognition of Japanese medical documents."
------------------------------
Document 2:

"You can use this model by running predict.py."
-------------------- input_format --------------------
Document 1:

MedTxt-CR-JA-training-v2.xml, NTCIR-16 Real-MedNLP subtask 1
------------------------------
Document 2:

input_format: text.txt
-------------------- output_format --------------------
Document 1:

license: - cc-by-4.0 tags: - NER - medical documents datasets: - MedTxt-CR-JA-training-v2.xml metrics: - NTCIR-16 Real-MedNLP subtask 1
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['MedTxt-CR-JA-training-v2.xml'], 'license': 'cc-by-4.0', 'github': '', 'paper': 'NAI 
STSOC at the NTCIR-16 Real-MedNLP Task, In Proceedings of the 16th NTCIR Conference on Evaluation of 
 Information Access Technologies (NTCIR-16), pp. 330-333, 2022', 'upstream_model': '', 'parameter_co 
unt': 'NO_OUTPUT', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias':  
'', 'demo': 'This is a model for named entity recognition of Japanese medical documents.', 'input_fo 
rmat': 'MedTxt-CR-JA-training-v2.xml, NTCIR-16 Real-MedNLP subtask 1', 'output_format': 'license: -  
cc-by-4.0 tags: - NER - medical documents datasets: - MedTxt-CR-JA-training-v2.xml metrics: - NTCIR- 
16 Real-MedNLP subtask 1', 'input_token_limit': '', 'vocabulary_size': ''}, {'datasets': [], 'licens 
e': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters':  
[], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'You can use this model by  
running predict.py.', 'input_format': 'input_format: text.txt', 'output_format': '', 'input_token_li 
mit': '', 'vocabulary_size': ''}]                                                                    

#####################xlm-roberta-base########################

-------------------- datasets --------------------
Document 1:

"2.5TB of filtered CommonCrawl data containing 100 languages" and "[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 2:

"this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)"
-------------------- paper --------------------
Document 1:

[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)
------------------------------
Document 2:

Unsupervised Cross-lingual Representation Learning at Scale
------------------------------
Document 3:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- upstream_model --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)
------------------------------
Document 2:

XLM-RoBERTa is a multilingual version of RoBERTa. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Masked language modeling (MLM) objective.
-------------------- parameter_count --------------------
Document 1:

"RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective."
-------------------- hyper_parameters --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
-------------------- evaluation --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
------------------------------
Document 2:

"RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence."
-------------------- hardware --------------------
Document 1:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.
-------------------- limitation_and_bias --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr)
------------------------------
Document 2:

"This model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
------------------------------
Document 3:

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.
-------------------- demo --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 2:

<a href="https://huggingface.co/exbert/?model=xlm-roberta-base">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
------------------------------
Document 3:

"The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team."
-------------------- input_format --------------------
Document 1:

"XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.", "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion.", "Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.", "This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs."

input_format: Masked language modeling (MLM)
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages." "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion." "Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words." "This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks."

[{'datasets': ['2.5TB of filtered CommonCrawl data containing 100 languages'], 'license': 'mit', 'g 
ithub': 'https://github.com/pytorch/fairseq/tree/master/examples/xlmr', 'paper': 'https://arxiv.org/ 
abs/1911.02116', 'upstream_model': 'Unsupervised Cross-lingual Representation Learning at Scale', 'p 
arameter_count': 'RoBERTa is a transformers model pretrained on a large corpus in a self-supervised  
fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any wa 
y (which is why it can use lots of publicly available data) with an automatic process to generate in 
puts and labels from those texts. More precisely, it was pretrained with the Masked language modelin 
g (MLM) objective.', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'opti 
mizer': ''}, 'evaluation': [], 'hardware': 'XLM-RoBERTa is a multilingual version of RoBERTa. It is  
pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. RoBERTa is a transformer 
s model pretrained on a large corpus in a self-supervised fashion. Taking a sentence, the model rand 
omly masks 15% of the words in the input then run the entire masked sentence through the model and h 
as to predict the masked words.', 'limitation_and_bias': 'Unsupervised Cross-lingual Representation  
Learning at Scale', 'demo': 'See the [model hub](https://huggingface.co/models?search=xlm-roberta) t 
o look for fine-tuned versions on a task that interests you.', 'input_format': 'Masked language mode 
ling (MLM)', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': 'XLM-RoBERTa is a mult 
ilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100  
languages. RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion 
. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire mask 
ed sentence through the model and has to predict the masked words.'}]                                

#####################roberta-base########################

-------------------- datasets --------------------
Document 1:

- [BookCorpus](https://yknzhu.wixsite.com/mbweb)
- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) 
- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news articles crawled between September 2016 and February 2019.
- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to train GPT-2,
- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas.
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"This model is case-sensitive: it makes a difference between english and English."
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

`@article{DBLP:journals/corr/abs-1907-11692, title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach}, journal   = {CoRR}, volume    = {abs/1907.11692}, year      = {2019}, url       = {http://arxiv.org/abs/1907.11692},`
------------------------------
Document 2:

[this paper](https://arxiv.org/abs/1907.11692)
-------------------- upstream_model --------------------
Document 1:

"masked language modeling (MLM) objective" "this paper" "this repository"
-------------------- parameter_count --------------------
Document 1:

parameter_count: NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

batch size of 8K, sequence length of 512, optimizer used is Adam with a learning rate of 6e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.98\\) and \\(\epsilon = 1e-6\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

"from transformers import pipeline\nunmasker = pipeline('fill-mask', model='roberta-base')"
-------------------- evaluation --------------------
Document 1:

When fine-tuned on downstream tasks, this model achieves the following results: Glue test results: | Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| |      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |
------------------------------
Document 2:

Adam with a learning rate of 6e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.98\\) and \\(\epsilon = 1e-6\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning rate after.
-------------------- hardware --------------------
Document 1:

1024 V100 GPUs
-------------------- limitation_and_bias --------------------
Document 1:

The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model can have biased predictions:  ```python >>> from transformers import pipeline >>> unmasker = pipeline('fill-mask', model='roberta-base') >>> unmasker("The man worked as a <mask>.") [...] >>> unmasker("The Black woman worked as a <mask>.") [...] ``` This bias will also affect all fine-tuned versions of this model.
------------------------------
Document 2:

The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>` - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `<mask>`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is. Contrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).
------------------------------
Document 3:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- demo --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=roberta-base">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
------------------------------
Document 2:

"See the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that interests you."
-------------------- input_format --------------------
Document 1:

The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`.
-------------------- output_format --------------------
Document 1:

The inputs of the model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>` - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `<mask>`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
-------------------- input_token_limit --------------------
Document 1:

"The inputs of the model take pieces of 512 contiguous tokens that may span over documents." NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 50,000

[{'datasets': ['BookCorpus', 'English Wikipedia', 'CC-News', 'OpenWebText', 'Stories'], 'license':  
'mit', 'github': 'https://huggingface.co/models?filter=roberta', 'paper': 'http://arxiv.org/abs/1907 
.11692', 'upstream_model': 'NO_OUTPUT', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epochs 
': 'NO_OUTPUT', 'batch_size': '8K', 'learning_rate': '6e-4', 'optimizer': 'Adam'}, 'evaluation': [{' 
test': 'Glue', 'result': 87.6}, {'test': 'MNLI', 'result': 91.9}, {'test': 'QQP', 'result': 92.8}, { 
'test': 'QNLI', 'result': 94.8}, {'test': 'SST-2', 'result': 63.6}, {'test': 'CoLA', 'result': 91.2} 
, {'test': 'STS-B', 'result': 90.2}, {'test': 'MRPC', 'result': 78.7}, {'test': 'RTE', 'result': 78. 
7}], 'hardware': '1024 V100 GPUs', 'limitation_and_bias': 'The training data used for this model con 
tains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model 
 can have biased predictions.', 'demo': '<a href="https://huggingface.co/exbert/?model=roberta-base" 
>\n<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>', 'input_forma 
t': 'The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size  
of 50,000. The inputs of the model take pieces of 512 contiguous tokens that may span over documents 
. The beginning of a new document is marked with `<s>` and the end of one by `</s>`.', 'output_forma 
t': 'The inputs of the model take pieces of 512 contiguous tokens that may span over documents. The  
beginning of a new document is marked with `<s>` and the end of one by `</s>` - 15% of the tokens ar 
e masked. - In 80% of the cases, the masked tokens are replaced by `<mask>`. - In 10% of the cases,  
the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% 
 remaining cases, the masked tokens are left as is.'}]                                               

#####################laion/CLIP-ViT-B-16-laion2B-s34B-b88K########################

-------------------- datasets --------------------
Document 1:

"The 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)"
------------------------------
Document 2:

VTAB+ (A combination of VTAB (https://arxiv.org/abs/1910.04867) w/ additional robustness datasets), COCO, Flickr
-------------------- license --------------------
Document 1:

"OpenCLIP software" ```@software{ilharco_gabriel_2021_5143773, ... url          = {https://doi.org/10.5281/zenodo.5143773}```
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Citation"
------------------------------
Document 2:

"The OpenAI CLIP paper"
------------------------------
Document 3:

OpenAI CLIP paper
```
@inproceedings{Radford2021LearningTV,
title={Learning Transferable Visual Models From Natural Language Supervision},
author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
booktitle={ICML},
year={2021}
}
```
-------------------- upstream_model --------------------
Document 1:

OpenAI CLIP model card, OpenAI CLIP paper, LAION-5B blog, upcoming paper
-------------------- parameter_count --------------------
Document 1:

"2 Billion sample English subset of LAION-5B" and "using a customized trained NSFW classifier that we built"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

#evaluation
------------------------------
Document 2:

"The model achieves a 70.2 zero-shot top-1 accuracy on ImageNet-1k." "https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb"
------------------------------
Document 3:

"The testing is performed with VTAB+ (A combination of VTAB (https://arxiv.org/abs/1910.04867) w/ additional robustness datasets) for classification and COCO and Flickr for retrieval."
-------------------- hardware --------------------
Document 1:

"OpenCLIP", "[JUWELS Booster](https://apps.fz-juelich.de/jsc/hps/juwels/booster-overview.html)"
-------------------- limitation_and_bias --------------------
Document 1:

Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.
------------------------------
Document 2:

"The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet." "Be aware that this large-scale dataset is uncurated." "Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer." "Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."
-------------------- demo --------------------
Document 1:

"The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet." "It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built)." "Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."
-------------------- input_format --------------------
Document 1:

"2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)" "input_format"
-------------------- output_format --------------------
Document 1:

"This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)" NO_OUTPUT
-------------------- input_preprocessing --------------------
Document 1:

"The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis."
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': ['2 Billion sample English subset of LAION-5B'], 'license': 'OpenCLIP software', 'git 
hub': '', 'paper': 'Citation', 'upstream_model': 'OpenAI CLIP model card, OpenAI CLIP paper, LAION-5 
B blog, upcoming paper', 'parameter_count': '2 Billion sample English subset of LAION-5B and using a 
 customized trained NSFW classifier that we built', 'hyper_parameters': {}, 'evaluation': [], 'hardw 
are': 'OpenCLIP, [JUWELS Booster](https://apps.fz-juelich.de/jsc/hps/juwels/booster-overview.html)', 
 'limitation_and_bias': 'Any deployed use case of the model - whether commercial or not - is current 
ly out of scope. Non-deployed use cases such as image search in a constrained environment, are also  
not recommended unless there is thorough in-domain testing of the model with a specific, fixed class 
 taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognitio 
n are always out-of-scope regardless of performance of the model. Since the model has not been purpo 
sefully trained in or evaluated on any languages other than English, its use should be limited to En 
glish language use cases.', 'demo': 'The motivation behind dataset creation is to democratize resear 
ch and experimentation around large-scale multi-modal model training and handling of uncurated, larg 
e-scale datasets crawled from publically available internet. It is possible to extract a “safe” subs 
et by filtering out samples based on the safety tags (using a customized trained NSFW classifier tha 
t we built). Providing our dataset openly, we however do not recommend using it for creating ready-t 
o-go industrial products, as the basic research about general properties and safety of such large-sc 
ale models, which we would like to encourage with this release, is still in progress.', 'input_forma 
t': '2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)', 'output_format' 
: 'This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blo 
g/laion-5b/)', 'input_preprocessing': 'The OpenAI CLIP paper includes a discussion of potential down 
stream impacts to provide an example for this sort of analysis.', 'input_size': '', 'num_of_classes_ 
for_classification': '', 'trigger_word': ''}, {'datasets': ['VTAB+', 'COCO', 'Flickr'], 'license': ' 
', 'github': '', 'paper': 'The OpenAI CLIP paper', 'upstream_model': '', 'parameter_count': '', 'hyp 
er_parameters': {}, 'evaluation': [{'test': 'The model achieves a 70.2 zero-shot top-1 accuracy on I 
mageNet-1k.', 'result': 70.2}], 'hardware': '', 'limitation_and_bias': 'The motivation behind datase 
t creation is to democratize research and experimentation around large-scale multi-modal model train 
ing and handling of uncurated, large-scale datasets crawled from publically available internet. Be a 
ware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the datas 
et means that collected links may lead to strongly discomforting and disturbing content for a human  
viewer. Providing our dataset openly, we however do not recommend using it for creating ready-to-go  
industrial products, as the basic research about general properties and safety of such large-scale m 
odels, which we would like to encourage with this release, is still in progress.', 'demo': 'The moti 
vation behind dataset creation is to democratize research and experimentation around large-scale mul 
ti-modal model training and handling of uncurated, large-scale datasets crawled from publically avai 
lable internet. It is possible to extract a “safe” subset by filtering out samples based on the safe 
ty tags (using a customized trained NSFW classifier that we built). Providing our dataset openly, we 
 however do not recommend using it for creating ready-to-go industrial products, as the basic resear 
ch about general properties and safety of such large-scale models, which we would like to encourage  
with this release, is still in progress.', 'input_format': '', 'output_format': '', 'input_preproces 
sing': '', 'input_size': '', 'num_of_classes_for_classification': '', 'trigger_word': ''}, {'dataset 
s': [], 'license': '', 'github': '', 'paper': 'OpenAI CLIP paper\n\n@inproceedings{Radford2021Learni 
ngTV,\ntitle={Learning Transferable Visual Models From Natural Language Supervision},\nauthor={Alec  
Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and G 
irish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutsk 
ever},\nbooktitle={ICML},\nyear={2021}\n}', 'upstream_model': '', 'parameter_count': '', 'hyper_para 
meters': {}, 'evaluation': [{'test': 'The testing is performed with VTAB+ (A combination of VTAB (ht 
tps://arxiv.org/abs/1910.04867) w/ additional robustness datasets) for classification and COCO and F 
lickr for retrieval.', 'result': None}], 'hardware': '', 'limitation_and_bias': 'The motivation behi 
nd dataset creation is to democratize research and experimentation around large-scale multi-modal mo 
del training and handling of uncurated, large-scale datasets crawled from publically available inter 
net. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of  
the dataset means that collected links may lead to strongly discomforting and disturbing content for 
 a human viewer. Providing our dataset openly, we however do not recommend using it for creating rea 
dy-to-go industrial products, as the basic research about general properties and safety of such larg 
e-scale models, which we would like to encourage with this release, is still in progress.', 'demo':  
'The motivation behind dataset creation is to democratize research and experimentation around large- 
scale multi-modal model training and handling of uncurated, large-scale datasets crawled from public 
ally available internet. It is possible to extract a “safe” subset by filtering out samples based on 
 the safety tags (using a customized trained NSFW classifier that we built). Providing our dataset o 
penly, we however do not recommend using it for creating ready-to-go industrial products, as the bas 
ic research about general properties and safety of such large-scale models, which we would like to e 
ncourage with this release, is still in progress.', 'input_format': '', 'output_format': '', 'input_ 
preprocessing': '', 'input_size': '', 'num_of_classes_for_classification': '', 'trigger_word': ''}]  

#####################distilbert-base-uncased########################

-------------------- datasets --------------------
Document 1:

datasets, [training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation)
------------------------------
Document 2:

BookCorpus (https://yknzhu.wixsite.com/mbweb), English Wikipedia (https://en.wikipedia.org/wiki/English_Wikipedia)
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

BookCorpus, English Wikipedia
-------------------- github --------------------
Document 1:

See the [model hub](https://huggingface.co/models?filter=distilbert) to look for fine-tuned versions on a task that interests you.
------------------------------
Document 2:

`<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased">`
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1910.01108)
------------------------------
Document 2:

"See the [model hub](https://huggingface.co/models?filter=distilbert)" and "Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- upstream_model --------------------
Document 1:

"BERT base model"
-------------------- parameter_count --------------------
Document 1:

parameter_count 8
------------------------------
Document 2:

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
-------------------- hyper_parameters --------------------
Document 1:

"hyperparameters" "training code" "https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation"
-------------------- evaluation --------------------
Document 1:

When fine-tuned on downstream tasks, this model achieves the following results: Glue test results: | Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| |      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |
------------------------------
Document 2:

"The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form: 
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. 
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is."
-------------------- hardware --------------------
Document 1:

8 16 GB V100
-------------------- limitation_and_bias --------------------
Document 1:

"Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. It also inherits some of the bias of its teacher model. This bias will also affect all fine-tuned versions of this model."
------------------------------
Document 2:

The inputs of the model are then of the form: ```[CLS] Sentence A [SEP] Sentence B [SEP]``` With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
------------------------------
Document 3:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- demo --------------------
Document 1:

`<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased"> <img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"> </a>`
------------------------------
Document 2:

See the [model hub](https://huggingface.co/models?filter=distilbert) to look for fine-tuned versions on a task that interests you.
-------------------- input_format --------------------
Document 1:

"lowercased and tokenized using WordPiece and a vocabulary size of 30,000" "inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]" "what is considered a sentence here is a consecutive span of text usually longer than a single sentence" "the result with the two "sentences" has a combined length of less than 512 tokens" "15% of the tokens are masked" "in 80% of the cases, the masked tokens are replaced by `[MASK]`" "in 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace" "in the 10% remaining cases, the masked tokens are left as is"

Input_Format: lowercased and tokenized using WordPiece and a vocabulary size of 30,000; [CLS] Sentence A [SEP] Sentence B [SEP]; what is considered a sentence here is a consecutive span of text usually longer than a single sentence; the result with the two "sentences" has a combined length of less than 512 tokens; 15% of the tokens are masked; in 80% of the
------------------------------
Document 2:

"raw model for either masked language modeling or next sentence prediction" "whole sentence (potentially masked)" "sequence classification, token classification or question answering" 
input_format: raw model for either masked language modeling or next sentence prediction, whole sentence (potentially masked) for sequence classification, token classification or question answering
-------------------- output_format --------------------
Document 1:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering"
------------------------------
Document 2:

The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.  
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.
-------------------- input_token_limit --------------------
Document 1:

"With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 30,000

[{'datasets': ['BookCorpus', 'English Wikipedia'], 'license': 'apache-2.0', 'github': 'https://hugg 
ingface.co/models?filter=distilbert', 'paper': 'https://arxiv.org/abs/1910.01108', 'upstream_model': 
 'BERT base model', 'parameter_count': '8', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'le 
arning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'Glue', 'result': 82.2}, {'test': 'MNLI' 
, 'result': 88.5}, {'test': 'QQP', 'result': 89.2}, {'test': 'QNLI', 'result': 91.3}, {'test': 'SST- 
2', 'result': 51.3}, {'test': 'CoLA', 'result': 85.8}, {'test': 'STS-B', 'result': 87.5}, {'test': ' 
MRPC', 'result': 59.9}, {'test': 'RTE', 'result': 87.5}], 'hardware': '8 16 GB V100', 'limitation_an 
d_bias': 'Even if the training data used for this model could be characterized as fairly neutral, th 
is model can have biased predictions. It also inherits some of the bias of its teacher model. This b 
ias will also affect all fine-tuned versions of this model.', 'demo': '<a href="https://huggingface. 
co/exbert/?model=distilbert-base-uncased"> <img width="300px" src="https://cdn-media.huggingface.co/ 
exbert/button.png"> </a>', 'input_format': 'lowercased and tokenized using WordPiece and a vocabular 
y size of 30,000; [CLS] Sentence A [SEP] Sentence B [SEP]; what is considered a sentence here is a c 
onsecutive span of text usually longer than a single sentence; the result with the two "sentences" h 
as a combined length of less than 512 tokens; 15% of the tokens are masked; in 80% of the cases, the 
 masked tokens are replaced by `[MASK]`', 'output_format': 'primarily aimed at being fine-tuned on t 
asks that use the whole sentence (potentially masked) to make decisions, such as sequence classifica 
tion, token classification or question answering', 'input_token_limit': 'With probability 0.5, sente 
nce A and sentence B correspond to two consecutive sentences in the original corpus and in the other 
 cases, it\'s another random sentence in the corpus. Note that what is considered a sentence here is 
 a consecutive span of text usually longer than a single sentence. The only constrain is that the re 
sult with the two "sentences" has a combined length of less than 512 tokens.', 'vocabulary_size': 'v 
ocabulary size of 30,000'}]                                                                          

#####################distilbert-base-multilingual-cased########################

-------------------- datasets --------------------
Document 1:

The model was pretrained with the supervision of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the concatenation of Wikipedia in 104 different languages
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=bert)"
-------------------- paper --------------------
Document 1:

[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

134M parameters.
------------------------------
Document 2:

6 layers, 768 dimension and 12 heads, totalizing 134M parameters
-------------------- hyper_parameters --------------------
Document 1:

"6 layers, 768 dimension and 12 heads, totalizing 134M parameters."
------------------------------
Document 2:

6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).
-------------------- evaluation --------------------
Document 1:

5. [Evaluation](#evaluation)
-------------------- hardware --------------------
Document 1:

"4. [Training Details](#training-details)"
-------------------- limitation_and_bias --------------------
Document 1:

"risks, biases and limitations of the model"
------------------------------
Document 2:

"Significant research has explored bias and fairness issues with language models" and "[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf)" and "[Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)".
------------------------------
Document 3:

"3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)"
-------------------- demo --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you."
-------------------- input_format --------------------
Document 1:

"bert-base-multilingual-cased"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

- **Model type:** Transformer-based language model
- **Language(s) (NLP):** 104 languages; see full list [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)
- **Related Models:** [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased)
- **Resources for more information:**
- [GitHub Repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)
- [Associated Paper](https://arxiv.org/abs/1910.01108)

[{'datasets': ['Wikipedia'], 'license': 'apache-2.0', 'github': 'https://huggingface.co/models?filt 
er=bert', 'paper': 'https://aclanthology.org/2021.acl-long.330.pdf', 'upstream_model': '', 'paramete 
r_count': '134M parameters', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate':  
'', 'optimizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_and_bi 
as': 'risks, biases and limitations of the model', 'demo': 'See the [model hub](https://huggingface. 
co/models?filter=bert) to look for fine-tuned versions on a task that interests you.', 'input_format 
': 'bert-base-multilingual-cased', 'output_format': '', 'input_token_limit': '', 'vocabulary_size':  
''}]                                                                                                 

#####################distilroberta-base########################

-------------------- datasets --------------------
Document 1:

OpenWebTextCorpus, OpenAI's WebText dataset, roberta-base model card
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=roberta)" and "Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

82M parameters (compared to 125M parameters for RoBERTa-base). parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"Glue test results: | Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| |      | 84.0 | 89.4 | 90.8 | 92.5  | 59.3 | 88.3  | 86.6 | 67.9 |"
-------------------- evaluation --------------------
Document 1:

5. [Evaluation](#evaluation)
-------------------- hardware --------------------
Document 1:

"4. [Training Details](#training-details)"
-------------------- limitation_and_bias --------------------
Document 1:

"risks, biases and limitations of the model"
------------------------------
Document 2:

"3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)"
-------------------- demo --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that interests you."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

vocabulary_size roberta-base model card

[{'datasets': ['OpenWebTextCorpus'], 'license': 'apache-2.0', 'github': 'https://huggingface.co/mod 
els?filter=roberta', 'paper': 'https://huggingface.co/models?filter=roberta', 'upstream_model': '',  
'parameter_count': '82M parameters (compared to 125M parameters for RoBERTa-base).', 'hyper_paramete 
rs': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hard 
ware': '', 'limitation_and_bias': 'risks, biases and limitations of the model', 'demo': 'See the [mo 
del hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task tha 
t interests you.', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_siz 
e': 'roberta-base model card'}]                                                                      

#####################CIDAS/clipseg-rd64-refined########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

"license"
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

"github", "Image Segmentation Using Text and Image Prompts", "this repository", "https://github.com/timojl/clipseg"
------------------------------
Document 2:

license: apache-2.0, tags: - vision - image-segmentation, inference: false
-------------------- paper --------------------
Document 1:

"documentation"
------------------------------
Document 2:

"Image Segmentation Using Text and Image Prompts" and "this repository"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg)."
------------------------------
Document 2:

"It was introduced in the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Lüddecke et al. and first released in [this repository](https://github.com/timojl/clipseg)."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg)."
------------------------------
Document 2:

"Image Segmentation Using Text and Image Prompts" by Lüddecke et al. and first released in [this repository](https://github.com/timojl/clipseg).
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': ['dataset1', 'dataset2'], 'license': 'apache-2.0', 'github': 'https://github.com/timoj 
l/clipseg', 'paper': 'https://arxiv.org/abs/2112.10003', 'upstream_model': '', 'parameter_count': '1 
000000', 'hyper_parameters': {'epochs': '10', 'batch_size': '32', 'learning_rate': '0.001', 'optimiz 
er': 'adam'}, 'evaluation': [{'test': 'accuracy', 'result': 0.85}, {'test': 'f1-score', 'result': 0. 
78}], 'hardware': 'GPU', 'limitation_and_bias': 'The model may struggle with complex images.', 'demo 
': 'Refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg) 
.', 'input_format': 'text', 'output_format': 'image'}                                                

#####################microsoft/layoutlmv3-base########################

-------------------- datasets --------------------
Document 1:

Microsoft Document AI, [GitHub](https://aka.ms/layoutlmv3)
-------------------- license --------------------
Document 1:

license: cc-by-nc-sa-4.0
------------------------------
Document 2:

[Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
-------------------- github --------------------
Document 1:

"GitHub" "https://aka.ms/layoutlmv3"
------------------------------
Document 2:

"[Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)", "[transformers](https://github.com/huggingface/transformers)", "[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)"
-------------------- paper --------------------
Document 1:

"title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking}"
------------------------------
Document 2:

[LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

Microsoft Document AI | [GitHub](https://aka.ms/layoutlmv3)

NO_OUTPUT
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis."
-------------------- demo --------------------
Document 1:

Microsoft Document AI | [GitHub](https://aka.ms/layoutlmv3)
------------------------------
Document 2:

"LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis."
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['Microsoft Document AI'], 'license': 'cc-by-nc-sa-4.0', 'github': 'https://aka.ms/la 
youtlmv3', 'paper': 'title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Mas 
king}', 'upstream_model': '', 'parameter_count': 'parameter_count', 'hyper_parameters': {}, 'evaluat 
ion': [], 'hardware': '', 'limitation_and_bias': 'LayoutLMv3 is a pre-trained multimodal Transformer 
 for Document AI with unified text and image masking. The simple unified architecture and training o 
bjectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-t 
uned for both text-centric tasks, including form understanding, receipt understanding, and document  
visual question answering, and image-centric tasks such as document image classification and documen 
t layout analysis.', 'demo': 'Microsoft Document AI | [GitHub](https://aka.ms/layoutlmv3)', 'input_f 
ormat': '', 'output_format': ''}]                                                                    

#####################albert-base-v2########################

-------------------- datasets --------------------
Document 1:

BookCorpus, https://yknzhu.wixsite.com/mbweb, English Wikipedia, https://en.wikipedia.org/wiki/English_Wikipedia
------------------------------
Document 2:

datasets: - bookcorpus - wikipedia
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"It was introduced in [this paper](https://arxiv.org/abs/1909.11942) and first released in [this repository](https://github.com/google-research/albert)." NO_OUTPUT
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=albert) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1909.11942)
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1909-11942, title = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language Representations}, journal = {CoRR}, volume = {abs/1909.11942}, year = {2019}, url = {http://arxiv.org/abs/1909.11942}, archivePrefix = {arXiv}, eprint = {1909.11942}
------------------------------
Document 3:

BookCorpus, English Wikipedia
-------------------- upstream_model --------------------
Document 1:

"masked language modeling (MLM) objective" "introduced in [this paper](https://arxiv.org/abs/1909.11942)" "first released in [this repository](https://github.com/google-research/albert)"
------------------------------
Document 2:

- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words.
- Sentence Ordering Prediction (SOP): ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text.
- 12 repeating layers
- 128 embedding dimension
- 768 hidden dimension
- 12 attention heads
- 11M parameters
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
------------------------------
Document 2:

[30,000] parameter_count
------------------------------
Document 3:

- 12 repeating layers - 128 embedding dimension - 768 hidden dimension - 12 attention heads - 11M parameters
-------------------- hyper_parameters --------------------
Document 1:

- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. - Sentence Ordering Prediction (SOP): ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. - 12 repeating layers - 128 embedding dimension - 768 hidden dimension - 12 attention heads - 11M parameters
-------------------- evaluation --------------------
Document 1:

When fine-tuned on downstream tasks, the ALBERT models achieve the following results:  
|                | Average  | SQuAD1.1 | SQuAD2.0 | MNLI     | SST-2    | RACE     |
|----------------|----------|----------|----------|----------|----------|----------|
|V2              |
|ALBERT-base     |82.3      |90.2/83.2 |82.1/79.3 |84.6      |92.9      |66.8      |
|ALBERT-large    |85.7      |91.8/85.2 |84.9/81.8 |86.5      |94.9      |75.2      |
|ALBERT-xlarge   |87.9      |92.9/86.4 |87.9/84.1 |87.9      |95.4      |80.7      |
|ALBERT-xxlarge  |90.9      |94.6/89.1 |89.8/86.9 |90.6      |96.8      |
-------------------- hardware --------------------
Document 1:

BookCorpus, English Wikipedia
-------------------- limitation_and_bias --------------------
Document 1:

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions: 
```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='albert-base-v2')
>>> unmasker("The man worked as a [MASK].")

[
{
"sequence":"[CLS] the man worked as a chauffeur.[SEP]",
"score":0.029577180743217468,
"token":28744,
"token_str":"▁chauffeur"
},
{
"sequence":"[CLS] the man worked as a janitor.[SEP]",
"score":0.028865724802017212,
"token":29477,
"token_str":"▁janitor"
},
{
"sequence":"[CLS] the man worked as a shoemaker.[SEP]",
"score":0.02581118606030941,
"token":29024,
"token_str":"▁shoemaker"
},
{
"sequence
------------------------------
Document 2:

"Pretrained model on English language using a masked language modeling (MLM) objective" and "This model, as all ALBERT models, is uncased: it does not make a difference between english and English."
------------------------------
Document 3:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2."
-------------------- demo --------------------
Document 1:

See the [model hub](https://huggingface.co/models?filter=albert)
------------------------------
Document 2:

"The ALBERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers)."
-------------------- input_format --------------------
Document 1:

"lowercased and tokenized using SentencePiece and a vocabulary size of 30,000. The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]"
------------------------------
Document 2:

"BookCorpus", "English Wikipedia"
-------------------- output_format --------------------
Document 1:

`[CLS] Sentence A [SEP] Sentence B [SEP]`
------------------------------
Document 2:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering"
-------------------- input_token_limit --------------------
Document 1:

input_token_limit: 30,000
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 30,000
------------------------------
Document 2:

"masked language modeling (MLM) objective" and "The team releasing ALBERT did not write a model card for this model"

[{'datasets': ['BookCorpus', 'English Wikipedia'], 'license': 'apache-2.0', 'github': 'https://hugg 
ingface.co/models?filter=albert', 'paper': 'https://arxiv.org/abs/1909.11942', 'upstream_model': 'ht 
tps://arxiv.org/abs/1909.11942', 'parameter_count': '11M', 'hyper_parameters': {'epochs': '', 'batch 
_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'Average', 'result': 82.3 
}, {'test': 'SQuAD1.1', 'result': 90.2}, {'test': 'SQuAD2.0', 'result': 83.2}, {'test': 'MNLI', 'res 
ult': 84.6}, {'test': 'SST-2', 'result': 92.9}, {'test': 'RACE', 'result': 66.8}], 'hardware': '', ' 
limitation_and_bias': 'Even if the training data used for this model could be characterized as fairl 
y neutral, this model can have biased predictions.', 'demo': 'See the [model hub](https://huggingfac 
e.co/models?filter=albert)', 'input_format': 'lowercased and tokenized using SentencePiece and a voc 
abulary size of 30,000. The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentenc 
e B [SEP]', 'output_format': '`[CLS] Sentence A [SEP] Sentence B [SEP]`', 'input_token_limit': '30,0 
00', 'vocabulary_size': '30,000'}]                                                                   

#####################runwayml/stable-diffusion-v1-5########################

-------------------- datasets --------------------
Document 1:

The model was trained mainly with English captions and will not work as well in other languages. The model was trained on a large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material and is not fit for product use without additional safety mechanisms and considerations. The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.
------------------------------
Document 2:

LAION-2B(en) (https://laion.ai/blog/laion-5b/)
-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
extra_gated_prompt: "This model is open access and available to all, with a CreativeML\
\ OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL\
\ License specifies: \n\n1. You can't use the model to deliberately produce nor\
\ share illegal or harmful outputs or content \n2. CompVis claims no rights on the\
\ outputs you generate, you are free to use them and are accountable for their use\
\ which must not go against the provisions set in the license\n3. You may re-distribute\
\ the weights and use the model commercially and/or as a service. If you do, please\
\ be aware you have to include the same use restrictions as the ones in the license\
\ and share a copy of the CreativeML OpenRAIL-M to all your users (please read the\
\ license entirely and carefully)\nPlease read the full license carefully here:\
\ https://huggingface.co/spaces/CompVis/stable-diffusion-license\n    "
extra_g
-------------------- github --------------------
Document 1:

- [v1-5-pruned-emaonly.ckpt](https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt)
- [v1-5-pruned.ckpt](https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.ckpt)
- [here](https://github.com/runwayml/stable-diffusion)
------------------------------
Document 2:

"The CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\nPlease read the full license carefully here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n    "
-------------------- paper --------------------
Document 1:

"Research on generative models."
------------------------------
Document 2:

"High-Resolution Image Synthesis With Latent Diffusion Models"
------------------------------
Document 3:

LAION-2B(en), English descriptions, white and western cultures, English-language prompts.
-------------------- upstream_model --------------------
Document 1:

"stable-diffusion-diffusers" "CreativeML OpenRAIL-M license" "CreativeML OpenRAIL License specifies" "You may re-distribute the weights and use the model commercially and/or as a service" "share a copy of the CreativeML OpenRAIL-M to all your users" "https://huggingface.co/spaces/CompVis/stable-diffusion-license"
-------------------- parameter_count --------------------
Document 1:

parameter_count 50
------------------------------
Document 2:

"parameter_count"
-------------------- hyper_parameters --------------------
Document 1:

hyper_parameters: classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PNDM/PLMS sampling steps
------------------------------
Document 2:

"stable-diffusion-diffusers", "inference: true", "extra_gated_prompt: \"This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\nPlease read the full license carefully here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n    \"", "extra_gated_heading: Please read the LICENSE to access this model"

NO_OUTPUT
-------------------- evaluation --------------------
Document 1:

Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PNDM/PLMS sampling steps show the relative improvements of the checkpoints: ![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-1-to-v1-5.png) Evaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores.
-------------------- hardware --------------------
Document 1:

32 x 8 x A100 GPUs
-------------------- limitation_and_bias --------------------
Document 1:

"primarily limited to English descriptions", "white and western cultures are often set as the default", "ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts"
------------------------------
Document 2:

- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.
------------------------------
Document 3:

- Probing and understanding the limitations and biases of generative models.
-------------------- demo --------------------
Document 1:

"Possible research areas and tasks include - Safe deployment of models which have the potential to generate harmful content. - Probing and understanding the limitations and biases of generative models. - Generation of artworks and use in design and other artistic processes. - Applications in educational or creative tools. - Research on generative models."
------------------------------
Document 2:

No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data. The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.
-------------------- input_format --------------------
Document 1:

No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data. The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.

NO_OUTPUT
------------------------------
Document 2:

"inference: true" NO_OUTPUT
-------------------- output_format --------------------
Document 1:

"text-to-image" "inference: true"

[{'datasets': ['LAION-5B'], 'license': 'creativeml-openrail-m', 'github': 'https://github.com/runwa 
yml/stable-diffusion', 'paper': 'High-Resolution Image Synthesis With Latent Diffusion Models', 'ups 
tream_model': 'stable-diffusion-diffusers', 'parameter_count': '50', 'hyper_parameters': {'epochs':  
'', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0 
}], 'hardware': '32 x 8 x A100 GPUs', 'limitation_and_bias': '- The model does not achieve perfect p 
hotorealism\n- The model cannot render legible text\n- The model does not perform well on more diffi 
cult tasks which involve compositionality, such as rendering an image corresponding to “A red cube o 
n top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The model w 
as trained mainly with English captions and will not work as well in other languages.\n- The autoenc 
oding part of the model is lossy\n- No additional measures were used to deduplicate the dataset. As  
a result, we observe some degree of memorization for images that are duplicated in the training data 
.', 'demo': 'Possible research areas and tasks include - Safe deployment of models which have the po 
tential to generate harmful content. - Probing and understanding the limitations and biases of gener 
ative models. - Generation of artworks and use in design and other artistic processes. - Application 
s in educational or creative tools. - Research on generative models.', 'input_format': '', 'output_f 
ormat': 'text-to-image'}]                                                                            

#####################nlptown/bert-base-multilingual-uncased-sentiment########################

-------------------- datasets --------------------
Document 1:

"Number of reviews: 150k, 80k, 137k, 140k, 72k, 50k"
------------------------------
Document 2:

"This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian."
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5)." NO_OUTPUT
-------------------- paper --------------------
Document 1:

"sentiment analysis model for product reviews in any of the six languages above"
-------------------- upstream_model --------------------
Document 1:

upstream_model: bert-base-multilingual-uncased
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5)."
-------------------- evaluation --------------------
Document 1:

"This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5)."
------------------------------
Document 2:

- Accuracy (exact) is the exact match for the number of stars.
- Accuracy (off-by-1) is the percentage of reviews where the number of stars the model predicts differs by a maximum of 1 from the number given by the human reviewer.  
| Language | Accuracy (exact) | Accuracy (off-by-1) |
| -------- | ---------------------- | ------------------- |
| English  | 67%                 | 95%
| Dutch    | 57%                 | 93%
| German   | 61%                 | 94%
| French   | 59%                 | 94%
| Italian  | 59%                 | 95%
| Spanish  | 58%                 | 95%
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias:NO_OUTPUT
-------------------- demo --------------------
Document 1:

"This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5)."
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

output_format: number of stars (between 1 and 5)
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Number of reviews: 150k, 80k, 137k, 140k, 72k, 50k'], 'license': 'mit', 'github': ' 
This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews i 
n six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of  
the review as a number of stars (between 1 and 5).', 'paper': 'sentiment analysis model for product  
reviews in any of the six languages above', 'upstream_model': 'bert-base-multilingual-uncased', 'par 
ameter_count': 'parameter_count', 'hyper_parameters': {'epochs': 'epochs', 'batch_size': 'batch_size 
', 'learning_rate': 'learning_rate', 'optimizer': 'optimizer'}, 'evaluation': [{'test': 'Accuracy (e 
xact)', 'result': 67}, {'test': 'Accuracy (off-by-1)', 'result': 95}], 'hardware': '', 'limitation_a 
nd_bias': 'NO_OUTPUT', 'demo': 'This is a bert-base-multilingual-uncased model finetuned for sentime 
nt analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italia 
n. It predicts the sentiment of the review as a number of stars (between 1 and 5).', 'input_format': 
 '', 'output_format': 'number of stars (between 1 and 5)', 'input_token_limit': '', 'vocabulary_size 
': ''}]                                                                                              

#####################SamLowe/roberta-base-go_emotions########################

-------------------- datasets --------------------
Document 1:

- go_emotions
- roberta-base
- multi-label-classification
- https://huggingface.co/datasets/go_emotions
- https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb
- https://colab.research.google.com/github/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

- https://huggingface.co/roberta-base
- https://huggingface.co/datasets/go_emotions
- https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx
- multi-label classification model with 28 'probability' float outputs for any given input text
- AutoModelForSequenceClassification.from_pretrained with problem_type="multi_label_classification" for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01
- https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb
- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb)
-------------------- paper --------------------
Document 1:

"The model was trained using `AutoModelForSequenceClassification.from_pretrained` with `problem_type="multi_label_classification"` for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01."

"Evaluation of the model is available at https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb"
-------------------- upstream_model --------------------
Document 1:

- Model trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification.
-------------------- parameter_count --------------------
Document 1:

- Model trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification.
- The model was trained using `AutoModelForSequenceClassification.from_pretrained` with `problem_type="multi_label_classification"` for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.
-------------------- hyper_parameters --------------------
Document 1:

- learning rate of 2e-5 and weight decay of 0.01
- multi-label classification model with 28 'probability' float outputs for any given input text
- typically a threshold of 0.5 is applied to the probabilities for the prediction for each label
- evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives: Accuracy: 0.474, Precision: 0.575, Recall: 0.396, F1: 0.450
- optimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1
-------------------- evaluation --------------------
Document 1:

Evaluation of the model is available at
- https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb)

##### Summary
As provided in the above notebook, evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:
- Accuracy: 0.474
- Precision: 0.575
- Recall: 0.396
- F1: 0.450

But the metrics are more meaningful when measured per label given the multi-label nature (each label is effectively an independent binary classification) and the fact that there is drastically different representations of the labels in the dataset.

With a
-------------------- hardware --------------------
Document 1:

"Model trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification."
-------------------- limitation_and_bias --------------------
Document 1:

- Evaluation of the model is available at https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb
- As provided in the above notebook, evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives: Accuracy: 0.474, Precision: 0.575, Recall: 0.396, F1: 0.450
- Optimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1
- Precision: 0.542, Recall: 0.577, F1: 0.541
- Precision: 0.572, Recall: 0.677, F1: 0.611
-------------------- demo --------------------
Document 1:

"There are multiple ways to use this model in Huggingface Transformers. Possibly the simplest is using a pipeline: 

from transformers import pipeline

classifier = pipeline(task="text-classification", model="SamLowe/roberta-base-go_emotions", top_k=None)

sentences = ["I am not having a great day"]

model_outputs = classifier(sentences)
print(model_outputs[0])
# produces a list of dicts for each of the labels"
-------------------- input_format --------------------
Document 1:

Input format: NO_OUTPUT
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['go_emotions'], 'license': 'mit', 'github': 'https://github.com/samlowe/go_emotions- 
dataset/blob/main/eval-roberta-base-go_emotions.ipynb', 'paper': 'https://github.com/samlowe/go_emot 
ions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb', 'upstream_model': 'roberta-base', 'para 
meter_count': '#params', 'hyper_parameters': {'epochs': '3', 'batch_size': '16', 'learning_rate': '2 
e-5', 'optimizer': 'AdamW'}, 'evaluation': [{'test': 'Accuracy', 'result': 0.474}, {'test': 'Precisi 
on', 'result': 0.575}, {'test': 'Recall', 'result': 0.396}, {'test': 'F1', 'result': 0.45}], 'hardwa 
re': 'Not specified', 'limitation_and_bias': 'Optimizing the threshold per label for the one that gi 
ves the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater  
gain in recall, hence to the benefit of F1', 'demo': 'There are multiple ways to use this model in H 
uggingface Transformers. Possibly the simplest is using a pipeline:\n\nfrom transformers import pipe 
line\n\nclassifier = pipeline(task="text-classification", model="SamLowe/roberta-base-go_emotions",  
top_k=None)\n\nsentences = ["I am not having a great day"]\n\nmodel_outputs = classifier(sentences)\ 
nprint(model_outputs[0])\n# produces a list of dicts for each of the labels', 'input_format': 'NO_OU 
TPUT', 'output_format': 'NO_OUTPUT', 'input_token_limit': 'NO_OUTPUT', 'vocabulary_size': 'NO_OUTPUT 
'}]                                                                                                  

#####################bert-base-cased########################

-------------------- datasets --------------------
Document 1:

BookCorpus (https://yknzhu.wixsite.com/mbweb), English Wikipedia (https://en.wikipedia.org/wiki/English_Wikipedia)
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=bert-base-cased">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
------------------------------
Document 2:

"See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1810.04805)
------------------------------
Document 2:

"Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

16 TPU chips total, batch size of 256, sequence length of 128 tokens for 90% of the steps and 512 for the remaining 10%, Adam optimizer with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
-------------------- hyper_parameters --------------------
Document 1:

Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
-------------------- evaluation --------------------
Document 1:

MNLI-(m/mm), QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE, 84.6/83.4, 71.2, 90.5, 93.5, 52.1, 85.8, 88.9, 66.4, 79.6
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- hardware --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)"
------------------------------
Document 2:

BookCorpus, English Wikipedia
-------------------- limitation_and_bias --------------------
Document 1:

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions: 
```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='bert-base-cased')
>>> unmasker("The man worked as a [MASK].")

[{'sequence': '[CLS] The man worked as a lawyer. [SEP]',
'score': 0.04804691672325134,
'token': 4545,
'token_str': 'lawyer'},
{'sequence': '[CLS] The man worked as a waiter. [SEP]',
'score': 0.037494491785764694,
'token': 17989,
'token_str': 'waiter'},
{'sequence': '[CLS] The man worked as a cop. [SEP]',
'score': 0.035512614995241165,
'token': 9947,
'token_str': 'cop'},
{'sequence': '[CLS] The man worked as a detective. [SEP]',
------------------------------
Document 2:

"case-sensitive: it makes a difference between english and English." NO_OUTPUT
------------------------------
Document 3:

The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
-------------------- demo --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=bert-base-cased">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
------------------------------
Document 2:

See the [model hub](https://huggingface.co/models?filter=bert)
------------------------------
Document 3:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- input_format --------------------
Document 1:

"The texts are tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens."
------------------------------
Document 2:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after"
-------------------- output_format --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 2:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering"
-------------------- input_token_limit --------------------
Document 1:

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens.
------------------------------
Document 2:

"sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%"
-------------------- vocabulary_size --------------------
Document 1:

"WordPiece and a vocabulary size of 30,000"
------------------------------
Document 2:

"vocabulary_size" NO_OUTPUT
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Unterminated string starting at: line 89 column 23 (char 5279) 

#####################microsoft/deberta-base########################

-------------------- datasets --------------------
Document 1:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
------------------------------
Document 2:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates. We present the dev results on SQuAD 1.1/2.0 and MNLI tasks.
------------------------------
Document 3:

- deberta-v1 - fill-mask
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"International Conference on Learning Representations"
-------------------- github --------------------
Document 1:

"If you find DeBERTa useful for your work, please cite the following paper: ``` latex @inproceedings{he2021deberta, title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION}, author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen}, booktitle={International Conference on Learning Representations}, year={2021}, url={https://openreview.net/forum?id=XPZIaotutsD} }```"
------------------------------
Document 2:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.
-------------------- paper --------------------
Document 1:

"If you find DeBERTa useful for your work, please cite the following paper: 
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}"
------------------------------
Document 2:

"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data."

"We present the dev results on SQuAD 1.1/2.0 and MNLI tasks."

"| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |
|-------------------|-----------|-----------|--------|
| RoBERTa-base      | 91.5/84.6 | 83.7/80.5 | 87.6   |
| XLNet-Large       | -/-       | -/80.2    | 86.8   |
| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |"

"Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates."
------------------------------
Document 3:

"tags: - deberta-v1 - fill-mask"
-------------------- upstream_model --------------------
Document 1:

"BERT and RoBERTa models" 
upstream_model: BERT and RoBERTa
------------------------------
Document 2:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
-------------------- parameter_count --------------------
Document 1:

"International Conference on Learning Representations"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |
|-------------------|-----------|-----------|--------|
| RoBERTa-base      | 91.5/84.6 | 83.7/80.5 | 87.6   |
| XLNet-Large       | -/-       | -/80.2    | 86.8   |
| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |
-------------------- hardware --------------------
Document 1:

"International Conference on Learning Representations"
-------------------- limitation_and_bias --------------------
Document 1:

"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data." "We present the dev results on SQuAD 1.1/2.0 and MNLI tasks." "| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |" "|-------------------|-----------|-----------|--------|" "| RoBERTa-base      | 91.5/84.6 | 83.7/80.5 | 87.6   |" "| XLNet-Large       | -/-       | -/80.2    | 86.8   |" "| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |"
-------------------- demo --------------------
Document 1:

"Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 You exceeded your current quota, please check your plan and billing details. 

#####################aitslab/biobert_huner_chemical_v1########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

https://github.com/Aitslab/EasyNER
-------------------- paper --------------------
Document 1:

"Please cite the paper, if you use the model."
"@article{ahmed2023easyner,
title={EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text},
author={Rafsan Ahmed and Petter Berntsson and Alexander Skafte and Salma Kazemi Rashed and Marcus Klang and Adam Barvesten and Ola Olde and William Lindholm and Antton Lamarca Arrizabalaga and Pierre Nugues and Sonja Aits},
year={2023},
eprint={2304.07805},
archivePrefix={arXiv},
primaryClass={q-bio.QM}"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


#####################distilbert-base-uncased-finetuned-sst-2-english########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 4276 tokens (4020 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################mosaicml/mpt-7b-instruct########################

-------------------- datasets --------------------
Document 1:

datasets, [MosaicML Platform](https://www.mosaicml.com/platform), sharded data parallelism, [FSDP](https://pytorch.org/docs/stable/fsdp.html)
------------------------------
Document 2:

datasets: - mosaicml/dolly_hhrlhf
-------------------- license --------------------
Document 1:

CC-By-SA-3.0
------------------------------
Document 2:

license: cc-by-sa-3.0
-------------------- github --------------------
Document 1:

license: cc-by-sa-3.0, tags: - Composer - MosaicML - llm-foundry, datasets: - mosaicml/dolly_hhrlhf, inference: false
-------------------- paper --------------------
Document 1:

"Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs"
-------------------- upstream_model --------------------
Document 1:

"standard decoder-only transformer"
-------------------- parameter_count --------------------
Document 1:

n_parameters | 6.7B |
------------------------------
Document 2:

parameter_count 8
-------------------- hyper_parameters --------------------
Document 1:

"It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases
| Hyperparameter | Value |
|----------------|-------|
|n_parameters | 6.7B |
|n_layers | 32 |
| n_heads | 32 |
| d_model | 4096 |
| vocab size | 50432 |
| sequence length | 2048 |"
------------------------------
Document 2:

"AdamW optimizer"
-------------------- evaluation --------------------
Document 1:

"The model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the AdamW optimizer."
-------------------- hardware --------------------
Document 1:

8 A100-40GBs
-------------------- limitation_and_bias --------------------
Document 1:

* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases
-------------------- demo --------------------
Document 1:

[sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b)
-------------------- input_format --------------------
Document 1:

`INSTRUCTION_KEY = "### Instruction:"`, `RESPONSE_KEY = "### Response:"`, `INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."`, `PROMPT_FOR_GENERATION_FORMAT = """{intro} {instruction_key} {instruction} {response_key} """.format(intro=INTRO_BLURB, instruction_key=INSTRUCTION_KEY, instruction="{instruction}", response_key=RESPONSE_KEY,)`
------------------------------
Document 2:

vocab size | 50432 | sequence length | 2048 | input_format
-------------------- output_format --------------------
Document 1:

`INSTRUCTION_KEY = "### Instruction:"`, `RESPONSE_KEY = "### Response:"`, `INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."`, `PROMPT_FOR_GENERATION_FORMAT = """{intro} {instruction_key} {instruction} {response_key} """.format(intro=INTRO_BLURB, instruction_key=INSTRUCTION_KEY, instruction="{instruction}", response_key=RESPONSE_KEY,)`
-------------------- input_token_limit --------------------
Document 1:

"In the above example, `fmt_ex` is ready to be tokenized and sent through the model."

NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

vocab size | 50432
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Invalid \escape: line 29 column 63 (char 1014) 

#####################google/vit-base-patch16-224########################

-------------------- datasets --------------------
Document 1:

"ImageNet-21k", "14 million images and 21k classes", "ImageNet", "1 million images and 1k classes"
------------------------------
Document 2:

- imagenet-1k
- imagenet-21k
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

2006.03677
------------------------------
Document 2:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, training resolution is 224
------------------------------
Document 3:

"original paper"
-------------------- upstream_model --------------------
Document 1:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, training resolution is 224
------------------------------
Document 2:

"The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels." "Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder."
-------------------- parameter_count --------------------
Document 1:

"batch size of 4096" "learning rate warmup of 10k steps" "gradient clipping at global norm 1" "training resolution is 224"
-------------------- hyper_parameters --------------------
Document 1:

"batch size of 4096", "learning rate warmup of 10k steps", "gradient clipping at global norm 1", "training resolution is 224"
-------------------- evaluation --------------------
Document 1:

"For evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper."
------------------------------
Document 2:

"The model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224."
-------------------- hardware --------------------
Document 1:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, training resolution is 224
------------------------------
Document 2:

"It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer)."
-------------------- limitation_and_bias --------------------
Document 1:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, training resolution is 224
------------------------------
Document 2:

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=google/vit"
------------------------------
Document 2:

widget:
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg
example_title: Tiger
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg
example_title: Teapot
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg
example_title: Palace
-------------------- input_format --------------------
Document 1:

Images are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).
------------------------------
Document 2:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, training resolution is 224
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"Images are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5)."
------------------------------
Document 2:

"Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder."
------------------------------
Document 3:

"batch size of 4096", "learning rate warmup of 10k steps", "gradient clipping at global norm 1", "training resolution is 224"
-------------------- input_size --------------------
Document 1:

224x224
------------------------------
Document 2:

input_size: 224x224
-------------------- num_of_classes_for_classification --------------------
Document 1:

num_of_classes_for_classification: 1000
-------------------- trigger_word --------------------


{'datasets': ['ImageNet-21k', 'ImageNet'], 'license': 'apache-2.0', 'github': '', 'paper': '2006.03 
677', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardwa 
re': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_prep 
rocessing': '', 'input_size': '', 'num_of_classes_for_classification': '', 'trigger_word': ''}       

#####################google/electra-base-discriminator########################

-------------------- datasets --------------------
Document 1:

SQuAD 2.0, GLUE, SQuAD, text chunking
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/))."
-------------------- paper --------------------
Document 1:

"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators" (https://openreview.net/pdf?id=r1xMH1BtvB)
-------------------- upstream_model --------------------
Document 1:

"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN"
NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN. At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the SQuAD 2.0 dataset. For a detailed description and experimental results, please refer to our paper ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators."
-------------------- hardware --------------------
Document 1:

single GPU
-------------------- limitation_and_bias --------------------
Document 1:

ELECTRA models are trained to distinguish "real" input tokens vs "fake" input tokens generated by another neural network, similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.
-------------------- demo --------------------
Document 1:

"This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/))."
------------------------------
Document 2:

`from transformers import ElectraForPreTraining, ElectraTokenizerFast
import torch

discriminator = ElectraForPreTraining.from_pretrained("google/electra-base-discriminator")
tokenizer = ElectraTokenizerFast.from_pretrained("google/electra-base-discriminator")

sentence = "The quick brown fox jumps over the lazy dog"
fake_sentence = "The quick brown fox fake over the lazy dog"

fake_tokens = tokenizer.tokenize(fake_sentence)
fake_inputs = tokenizer.encode(fake_sentence, return_tensors="pt")
discriminator_outputs = discriminator(fake_inputs)
predictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)

[print("%7s" % token, end="") for token in fake_tokens]

[print("%7s" % int(prediction), end="") for prediction in predictions.tolist()]`
-------------------- input_format --------------------
Document 1:

"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network"
------------------------------
Document 2:

"ElectraTokenizerFast.from_pretrained("google/electra-base-discriminator")" and "tokenizer.encode(fake_sentence, return_tensors="pt")" 

input_format: ElectraTokenizerFast and return_tensors="pt"
-------------------- output_format --------------------


[{'datasets': ['SQuAD 2.0', 'GLUE', 'SQuAD', 'text chunking'], 'license': 'apache-2.0', 'github': ' 
https://github.com/google-research/electra', 'paper': 'https://openreview.net/pdf?id=r1xMH1BtvB', 'u 
pstream_model': "ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input token 
s generated by another neural network, similar to the discriminator of a GAN", 'parameter_count': 'N 
/A', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'single GPU', 'limitation_and_bias': "ELE 
CTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by anoth 
er neural network, similar to the discriminator of a GAN. At small scale, ELECTRA achieves strong re 
sults even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results o 
n the SQuAD 2.0 dataset.", 'demo': 'This repository contains code to pre-train ELECTRA, including sm 
all ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks includi 
ng classification tasks (e.g,. GLUE), QA tasks (e.g., SQuAD), and sequence tagging tasks (e.g., text 
 chunking).', 'input_format': "ELECTRA models are trained to distinguish 'real' input tokens vs 'fak 
e' input tokens generated by another neural network", 'output_format': ''}]                          

#####################deepset/roberta-base-squad2########################

-------------------- datasets --------------------
Document 1:

datasets:
- squad_v2
model-index:
- name: deepset/roberta-base-squad2
results:
- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
------------------------------
Document 2:

roberta-base, SQuAD2.0, https://huggingface.co/roberta-base, https://huggingface.co/datasets/squad_v2
------------------------------
Document 3:

"SQuAD 2.0" "See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"
-------------------- license --------------------
Document 1:

license: cc-by-4.0
------------------------------
Document 2:

"We also have a <strong><a class="h-7" href="https://haystack.deepset.ai/community">Discord community open to everyone!</a></strong></p>  
[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)  
By the way: [we're hiring!](http://www.deepset.ai/jobs)
-------------------- github --------------------
Document 1:

<strong><a href="https://github.com/deepset-ai/haystack">GitHub</a></strong> and <strong><a class="h-7" href="https://haystack.deepset.ai/community">Discord community open to everyone!</a></strong> and [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions)
------------------------------
Document 2:

[deepset](http://deepset.ai/), [Haystack](https://haystack.deepset.ai/), [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")]([https://huggingface.co/deepset/tinyroberta-squad2), [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
-------------------- paper --------------------
Document 1:

roberta-base, SQuAD2.0
------------------------------
Document 2:

"roberta-base" "Extractive QA" "SQuAD 2.0" "SQuAD 2.0" "4x Tesla v100"
-------------------- upstream_model --------------------
Document 1:

upstream_model: roberta-base
------------------------------
Document 2:

"deepset/roberta-base-squad2"
-------------------- parameter_count --------------------
Document 1:

parameter_count: 8
------------------------------
Document 2:

"name: deepset/roberta-base-squad2"
-------------------- hyper_parameters --------------------
Document 1:

batch_size = 96, n_epochs = 2, base_LM_model = "roberta-base", max_seq_len = 386, learning_rate = 3e-5, lr_schedule = LinearWarmup, warmup_proportion = 0.2, doc_stride=128, max_query_length=64
-------------------- evaluation --------------------
Document 1:

"Evaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/). 
```
"exact": 79.87029394424324,
"f1": 82.91251169582613,

"total": 11873,
"HasAns_exact": 77.93522267206478,
"HasAns_f1": 84.02838248389763,
"HasAns_total": 5928,
"NoAns_exact": 81.79983179142137,
"NoAns_f1": 81.79983179142137,
"NoAns_total": 5945
```
------------------------------
Document 2:

- type: exact_match
value: 79.9309
name: Exact Match
- type: f1
value: 82.9501
name: F1
- type: exact_match
value: 85.289
name: Exact Match
- type: f1
value: 91.841
name: F1
- type: exact_match
value: 29.5
name: Exact Match
- type: f1
value: 40.367
name: F1
- type: exact_match
value: 78.567
name: Exact Match
- type: f1
value: 84.469
name: F1
- type: exact_match
value: 69.924
name: Exact Match
- type: f1
value: 83.284
name: F1
- type: exact_match
value: 81.204
name: Exact Match
- type: f1
value: 90.595
name: F1
- type: exact_match
value: 82.931
name: Exact Match
- type: f1
value: 90.756
name
------------------------------
Document 3:

"roberta-base", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"
-------------------- hardware --------------------
Document 1:

roberta-base, SQuAD2.0
------------------------------
Document 2:

4x Tesla v100
-------------------- limitation_and_bias --------------------
Document 1:

roberta-base, English, Extractive QA, SQuAD 2.0, See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system), 4x Tesla v100
------------------------------
Document 2:

"roberta-base", "SQuAD2.0", "question-answer pairs", "unanswerable questions", "Question Answering"
-------------------- demo --------------------
Document 1:

"roberta-base", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"
------------------------------
Document 2:

[deepset](http://deepset.ai/), [Haystack](https://haystack.deepset.ai/), [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")]([https://huggingface.co/deepset/tinyroberta-squad2), [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
------------------------------
Document 3:

"For a complete example of ``roberta-base-squad2`` being used for  Question Answering, check out the [Tutorials in Haystack Documentation](https://haystack.deepset.ai/tutorials/first-qa-system)"
-------------------- input_format --------------------
Document 1:

"name: squad_v2 type: squad_v2 config: squad_v2 split: validation"
------------------------------
Document 2:

"max_seq_len = 386", "doc_stride=128", "max_query_length=64"
------------------------------
Document 3:

input_format: SQuAD 2.0
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

"roberta-base" "Extractive QA" "SQuAD 2.0" "SQuAD 2.0" "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)" "4x Tesla v100"
-------------------- vocabulary_size --------------------
Document 1:

"roberta-base", "English", "SQuAD 2.0", "4x Tesla v100"
------------------------------
Document 2:

"vocabulary_size"

[{'datasets': ['squad_v2'], 'license': 'cc-by-4.0', 'github': 'https://github.com/deepset-ai/haysta 
ck', 'paper': '', 'upstream_model': 'roberta-base', 'parameter_count': '8', 'hyper_parameters': {'ep 
ochs': '', 'batch_size': '96', 'learning_rate': '3e-5', 'optimizer': '', 'lr_schedule': '', 'warmup_ 
proportion': '', 'doc_stride': '', 'max_query_length': ''}, 'evaluation': [{'test': '', 'result': 79 
.87029394424324}, {'test': '', 'result': 82.91251169582613}], 'hardware': '', 'limitation_and_bias': 
 '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size' 
: ''}]                                                                                               

#####################bert-base-multilingual-cased########################

-------------------- datasets --------------------
Document 1:

"The BERT model was pretrained on the 104 languages with the largest Wikipedias. You can find the complete list [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)."
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1810.04805)
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1810-04805, title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding}, journal = {CoRR}, volume = {abs/1810.04805}, year = {2018}, url = {http://arxiv.org/abs/1810.04805}, archivePrefix = {arXiv}, eprint = {1810.04805}
------------------------------
Document 3:

See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you. Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.
-------------------- upstream_model --------------------
Document 1:

upstream_model NO_OUTPUT
------------------------------
Document 2:

BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion.
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"masked language modeling (MLM) objective" "introduced in [this paper](https://arxiv.org/abs/1810.04805)" "first released in [this repository](https://github.com/google-research/bert)"
-------------------- evaluation --------------------
Document 1:

"masked language modeling (MLM) objective" "introduced in [this paper](https://arxiv.org/abs/1810.04805)" "first released in [this repository](https://github.com/google-research/bert)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

The texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The languages with a larger Wikipedia are under-sampled and the ones with lower resources are oversampled. For languages like Chinese, Japanese Kanji and Korean Hanja that don't have space, a CJK Unicode block is added around every character. The inputs of the model are then of the form: ```[CLS] Sentence A [SEP] Sentence B [SEP]``` With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace
------------------------------
Document 2:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2."
------------------------------
Document 3:

"masked language modeling (MLM) objective" "case sensitive: it makes a difference between english and English"
-------------------- demo --------------------
Document 1:

See the [model hub](https://huggingface.co/models?filter=bert)
-------------------- input_format --------------------
Document 1:

The texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The inputs of the model are then of the form: ```[CLS] Sentence A [SEP] Sentence B [SEP]``` With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
------------------------------
Document 2:

"pretrained on the raw texts only, with no humans labelling them in any way" "Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words." "Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining."
-------------------- output_format --------------------
Document 1:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering"
------------------------------
Document 2:

The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.
-------------------- input_token_limit --------------------
Document 1:

"The inputs of the model are then of the form: [...] With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
-------------------- vocabulary_size --------------------
Document 1:

"shared vocabulary size of 110,000"

[{'datasets': ['BERT'], 'license': 'apache-2.0', 'github': 'https://github.com/google-research/bert 
/blob/master/multilingual.md#list-of-languages', 'paper': 'https://arxiv.org/abs/1810.04805', 'upstr 
eam_model': 'NO_OUTPUT', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware' 
: '', 'limitation_and_bias': '', 'demo': 'https://huggingface.co/models?filter=bert', 'input_format' 
: '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                          

#####################roberta-large########################

-------------------- datasets --------------------
Document 1:

- [BookCorpus](https://yknzhu.wixsite.com/mbweb)
- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) 
- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news articles crawled between September 2016 and February 2019.
- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to train GPT-2,
- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas.
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"This model is case-sensitive: it makes a difference between english and English."
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

`@article{DBLP:journals/corr/abs-1907-11692, title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach}, journal   = {CoRR}, volume    = {abs/1907.11692}, year      = {2019}, url       = {http://arxiv.org/abs/1907.11692},`
------------------------------
Document 2:

[this paper](https://arxiv.org/abs/1907.11692)
-------------------- upstream_model --------------------
Document 1:

"masked language modeling (MLM) objective" "this paper" "this repository"
-------------------- parameter_count --------------------
Document 1:

parameter_count=1024 V100 GPUs, 500K steps, batch size of 8K, sequence length of 512, Adam, learning rate of 4e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.98\\), \\(\epsilon = 1e-6\\), weight decay of 0.01, learning rate warmup for 30,000 steps, linear decay of the learning rate after.
-------------------- hyper_parameters --------------------
Document 1:

batch size of 8K, sequence length of 512, optimizer used is Adam, learning rate of 4e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.98\\), \\(\epsilon = 1e-6\\), weight decay of 0.01, learning rate warmup for 30,000 steps, linear decay of the learning rate after.
-------------------- evaluation --------------------
Document 1:

When fine-tuned on downstream tasks, this model achieves the following results: Glue test results: | Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| |      | 90.2 | 92.2 | 94.7 | 96.4  | 68.0 | 96.4  | 90.9 | 86.6 |
------------------------------
Document 2:

Adam with a learning rate of 4e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.98\\) and \\(\epsilon = 1e-6\\), a weight decay of 0.01, learning rate warmup for 30,000 steps and linear decay of the learning rate after.
-------------------- hardware --------------------
Document 1:

1024 V100 GPUs, batch size of 8K, sequence length of 512
-------------------- limitation_and_bias --------------------
Document 1:

The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model can have biased predictions:  ```python >>> from transformers import pipeline >>> unmasker = pipeline('fill-mask', model='roberta-large') >>> unmasker("The man worked as a <mask>.") [{'sequence': '<s>The man worked as a mechanic.</s>', 'score': 0.08260300755500793, 'token': 25682, 'token_str': 'Ġmechanic'}, {'sequence': '<s>The man worked as a driver.</s>', 'score': 0.05736079439520836, 'token': 1393, 'token_str': 'Ġdriver'}, {'sequence': '<s>The man worked as a teacher.</s>', 'score': 0.04709019884467125, 'token': 3254, 'token_str': 'Ġteacher'}, {'sequence': '<s>The man worked as a bartender.</s>', 'score': 0.04641604796051979, '
------------------------------
Document 2:

The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`. - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `<mask>`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is. Contrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).
------------------------------
Document 3:

"This model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- demo --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=roberta-base">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
------------------------------
Document 2:

See the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that interests you. Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.
-------------------- input_format --------------------
Document 1:

The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`. 15% of the tokens are masked. In 80% of the cases, the masked tokens are replaced by `<mask>`. In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. In the 10% remaining cases, the masked tokens are left as is.
-------------------- output_format --------------------
Document 1:

The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`. 15% of the tokens are masked. In 80% of the cases, the masked tokens are replaced by `<mask>`. In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. In the 10% remaining cases, the masked tokens are left as is. Contrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).
-------------------- input_token_limit --------------------
Document 1:

512 contiguous token that may span over documents.
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 50,000

[{'datasets': ['BookCorpus', 'English Wikipedia', 'CC-News', 'OpenWebText', 'Stories'], 'license':  
'mit', 'github': 'https://huggingface.co/models?filter=roberta', 'paper': 'http://arxiv.org/abs/1907 
.11692', 'upstream_model': 'masked language modeling (MLM) objective', 'parameter_count': '1024 V100 
 GPUs, 500K steps, batch size of 8K, sequence length of 512, Adam, learning rate of 4e-4, \\(\\beta_ 
{1} = 0.9\\), \\(\\beta_{2} = 0.98\\), \\(\\epsilon = 1e-6\\), weight decay of 0.01, learning rate w 
armup for 30,000 steps, linear decay of the learning rate after.', 'hyper_parameters': {'epochs': '' 
, 'batch_size': '8K', 'learning_rate': '4e-4', 'optimizer': 'Adam'}, 'evaluation': [{'test': 'Glue t 
est results', 'result': 90.2}, {'test': 'MNLI', 'result': 90.2}, {'test': 'QQP', 'result': 92.2}, {' 
test': 'QNLI', 'result': 94.7}, {'test': 'SST-2', 'result': 96.4}, {'test': 'CoLA', 'result': 68.0}, 
 {'test': 'STS-B', 'result': 96.4}, {'test': 'MRPC', 'result': 90.9}, {'test': 'RTE', 'result': 86.6 
}], 'hardware': '1024 V100 GPUs, batch size of 8K, sequence length of 512', 'limitation_and_bias': ' 
The training data used for this model contains a lot of unfiltered content from the internet, which  
is far from neutral. Therefore, the model can have biased predictions.', 'demo': '<a href="https://h 
uggingface.co/exbert/?model=roberta-base">\n<img width="300px" src="https://cdn-media.huggingface.co 
/exbert/button.png">\n</a>', 'input_format': 'The texts are tokenized using a byte version of Byte-P 
air Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 conti 
guous token that may span over documents. The beginning of a new document is marked with `<s>` and t 
he end of one by `</s>`. 15% of the tokens are masked. In 80% of the cases, the masked tokens are re 
placed by `<mask>`. In 10% of the cases, the masked tokens are replaced by a random token (different 
) from the one they replace. In the 10% remaining cases, the masked tokens are left as is.', 'output 
_format': 'The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary 
 size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over docu 
ments. The beginning of a new document is marked with `<s>` and the end of one by `</s>`. 15% of the 
 tokens are masked. In 80% of the cases, the masked tokens are replaced by `<mask>`. In 10% of the c 
ases, the masked tokens are replaced by a random token (different) from the one they replace. In the 
 10% remaining cases, the masked tokens are left as is.', 'input_token_limit': '512 contiguous token 
 that may span over documents.', 'vocabulary_size': '50,000'}]                                       

#####################sentence-transformers/all-mpnet-base-v2########################

-------------------- datasets --------------------
Document 1:

"1B sentence pairs dataset" and "[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354)"
------------------------------
Document 2:

`microsoft/mpnet-base`
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"github repositories" "Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-mpnet-base-v2)
-------------------- paper --------------------
Document 1:

"sentence-transformers" and "semantic search"
------------------------------
Document 2:

"Sentence Embeddings Benchmark"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

microsoft/mpnet-base, upstream_model
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

We trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core). We use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with a 2e-5 learning rate.
-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-mpnet-base-v2)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

sentence-transformers, https://www.SBERT.net
------------------------------
Document 2:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-mpnet-base-v2)"
------------------------------
Document 3:

"Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures the semantic information."
-------------------- input_format --------------------
Document 1:

input_format: text
-------------------- output_format --------------------
Document 1:

output_format: vector
-------------------- input_token_limit --------------------
Document 1:

input_token_limit 384
-------------------- vocabulary_size --------------------


[{'datasets': ['1B sentence pairs dataset'], 'license': 'apache-2.0', 'github': 'github repositorie 
s Sentence Embeddings Benchmark: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-t 
ransformers/all-mpnet-base-v2)', 'paper': 'sentence-transformers and semantic search', 'upstream_mod 
el': 'sentence-transformers', 'parameter_count': '', 'hyper_parameters': [{'epochs': '', 'batch_size 
': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardware' 
: '', 'limitation_and_bias': '', 'demo': 'sentence-transformers, https://www.SBERT.net', 'input_form 
at': 'text', 'output_format': 'vector', 'input_token_limit': '384', 'vocabulary_size': ''}]          

#####################allenai/scibert_scivocab_uncased########################

-------------------- datasets --------------------
Document 1:

The training corpus was papers taken from [Semantic Scholar](https://www.semanticscholar.org). Corpus size is 1.14M papers, 3.1B tokens. We use the full text of the papers in training, not just abstracts.
-------------------- license --------------------
Document 1:

"The original repo can be found [here](https://github.com/allenai/scibert). If using these models, please cite the following paper:"
-------------------- github --------------------
Document 1:

The original repo can be found [here](https://github.com/allenai/scibert).
-------------------- paper --------------------
Document 1:

"SciBERT: A Pretrained Language Model for Scientific Text"
"If using these models, please cite the following paper:"
"@inproceedings{beltagy-etal-2019-scibert,"
"title = "SciBERT: A Pretrained Language Model for Scientific Text","
"author = "Beltagy, Iz  and Lo, Kyle  and Cohan, Arman","
"booktitle = "EMNLP","
"year = "2019","
"publisher = "Association for Computational Linguistics","
"url = "https://www.aclweb.org/anthology/D19-1371""
-------------------- upstream_model --------------------
Document 1:

"BERT model trained on scientific text"
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

* `scibert_scivocab_cased`
* `scibert_scivocab_uncased`
* The original repo can be found [here](https://github.com/allenai/scibert).
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['Semantic Scholar'], 'license': 'The original repo can be found [here](https://githu 
b.com/allenai/scibert). If using these models, please cite the following paper:', 'github': 'The ori 
ginal repo can be found [here](https://github.com/allenai/scibert).', 'paper': '"SciBERT: A Pretrain 
ed Language Model for Scientific Text"\n"If using these models, please cite the following paper:"\n" 
@inproceedings{beltagy-etal-2019-scibert,"\n"title = "SciBERT: A Pretrained Language Model for Scien 
tific Text","\n"author = "Beltagy, Iz  and Lo, Kyle  and Cohan, Arman","\n"booktitle = "EMNLP","\n"y 
ear = "2019","\n"publisher = "Association for Computational Linguistics","\n"url = "https://www.aclw 
eb.org/anthology/D19-1371""', 'upstream_model': '"BERT model trained on scientific text"', 'paramete 
r_count': 'parameter_count', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_a 
nd_bias': '', 'demo': 'The original repo can be found [here](https://github.com/allenai/scibert).',  
'input_format': '', 'output_format': ''}]                                                            

#####################t5-base########################

-------------------- datasets --------------------
Document 1:

- [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4)
- [C4](https://huggingface.co/datasets/c4)
- [Wiki-DPR](https://huggingface.co/datasets/wiki_dpr)
- Sentence acceptability judgment
- CoLA [Warstadt et al., 2018](https://arxiv.org/abs/1805.12471)
- Sentiment analysis
- SST-2 [Socher et al., 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)
- Paraphrasing/sentence similarity
- MRPC [Dolan and Brockett, 2005](https://aclanthology.org/I05-5002)
- STS-B [Ceret al., 2017](https://arxiv.org/abs/1708.00055)
- QQP [Iyer et al., 2017](https://quoradata.quora.com/First-
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"research paper", "see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)"
------------------------------
Document 2:

research paper, https://jmlr.org/papers/volume21/20-074/20-074.pdf, Table 14
------------------------------
Document 3:

"research paper" and "https://jmlr.org/papers/volume21/20-074/20-074.pdf"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------
Document 1:

"T5-Base is the checkpoint with 220 million parameters."
-------------------- hyper_parameters --------------------
Document 1:

"same model, loss function, and hyperparameters"
-------------------- evaluation --------------------
Document 1:

"The developers evaluated the model on 24 tasks, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for full details."
------------------------------
Document 2:

5. [Evaluation](#evaluation)
------------------------------
Document 3:

"For full results for T5-Base, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf), Table 14."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)"
------------------------------
Document 2:

"We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself."
-------------------- demo --------------------
Document 1:

"demo"
-------------------- input_format --------------------
Document 1:

"reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"text-to-text framework...NLP task...hyperparameters" input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

C4, Wiki-DPR, Sentence acceptability judgment, CoLA, Sentiment analysis, SST-2, Paraphrasing/sentence similarity, MRPC, STS-B, QQP, Natural language inference, MNLI, QNLI, RTE, CB, Sentence completion, COPA, Word sense disambiguation, WIC, Question answering, MultiRC, ReCoRD, BoolQ
------------------------------
Document 2:

"T5-Base is the checkpoint with 220 million parameters."

[{'datasets': ['Colossal Clean Crawled Corpus (C4)', 'C4', 'Wiki-DPR', 'Sentence acceptability judg 
ment', 'CoLA', 'Sentiment analysis', 'SST-2', 'Paraphrasing/sentence similarity', 'MRPC', 'STS-B', ' 
QQP'], 'license': 'apache-2.0', 'github': '', 'paper': 'see the [research paper](https://jmlr.org/pa 
pers/volume21/20-074/20-074.pdf)', 'upstream_model': '', 'parameter_count': 'T5-Base is the checkpoi 
nt with 220 million parameters.', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rat 
e': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_and 
_bias': '3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)', 'demo': 'demo', 'input_for 
mat': 'reframing all NLP tasks into a unified text-to-text-format where the input and output are alw 
ays text strings', 'output_format': '', 'input_token_limit': 'text-to-text framework...NLP task...hy 
perparameters', 'vocabulary_size': 'C4, Wiki-DPR, Sentence acceptability judgment, CoLA, Sentiment a 
nalysis, SST-2, Paraphrasing/sentence similarity, MRPC, STS-B, QQP, Natural language inference, MNLI 
, QNLI, RTE, CB, Sentence completion, COPA, Word sense disambiguation, WIC, Question answering, Mult 
iRC, ReCoRD, BoolQ'}]                                                                                

#####################bert-base-chinese########################

-------------------- datasets --------------------
Document 1:

"Training Data"
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"BERT https://arxiv.org/abs/1810.04805"
------------------------------
Document 2:

[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

type_vocab_size: 2, vocab_size: 21128, num_hidden_layers: 12
-------------------- hyper_parameters --------------------
Document 1:

* **type_vocab_size:** 2
* **vocab_size:** 21128
* **num_hidden_layers:** 12
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

[More Information Needed]
-------------------- hardware --------------------
Document 1:

num_hidden_layers: 12
-------------------- limitation_and_bias --------------------
Document 1:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).
------------------------------
Document 2:

#risks-limitations-and-biases
-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

type_vocab_size: 2, vocab_size: 21128, num_hidden_layers: 12
------------------------------
Document 2:

input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

type_vocab_size: 2, vocab_size: 21128

[{'datasets': ['Training Data'], 'license': '', 'github': '', 'paper': 'BERT https://arxiv.org/abs/ 
1810.04805', 'upstream_model': '', 'parameter_count': 'type_vocab_size: 2, vocab_size: 21128, num_hi 
dden_layers: 12', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimiz 
er': ''}, 'evaluation': [], 'hardware': 'num_hidden_layers: 12', 'limitation_and_bias': 'Significant 
 research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021 
)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi 
/pdf/10.1145/3442188.3445922)).', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_ 
limit': 'type_vocab_size: 2, vocab_size: 21128, num_hidden_layers: 12', 'vocabulary_size': 'type_voc 
ab_size: 2, vocab_size: 21128'}]                                                                     

#####################allenai/longformer-base-4096########################

-------------------- datasets --------------------
Document 1:

"Longformer: The Long-Document Transformer" and "the Allen Institute for Artificial Intelligence (AI2)"
------------------------------
Document 2:

Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"Longformer is an open-source project developed by the Allen Institute for Artificial Intelligence (AI2)"
-------------------- github --------------------
Document 1:

"Longformer: The Long-Document Transformer" and "an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org)."
------------------------------
Document 2:

"longformer-base-4096 is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations. Please refer to the examples in `modeling_longformer.py` and the paper for more details on how to set global attention."
-------------------- paper --------------------
Document 1:

"Longformer: The Long-Document Transformer" and "@article{Beltagy2020Longformer, title={Longformer: The Long-Document Transformer}, author={Iz Beltagy and Matthew E. Peters and Arman Cohan}, journal={arXiv:2004.05150}, year={2020},}"
------------------------------
Document 2:

"Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations. Please refer to the examples in `modeling_longformer.py` and the paper for more details on how to set global attention."
-------------------- upstream_model --------------------
Document 1:

Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.
------------------------------
Document 2:

"Longformer: The Long-Document Transformer" and "AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering."
-------------------- parameter_count --------------------
Document 1:

`longformer-base-4096` is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096.
------------------------------
Document 2:

parameter_count
------------------------------
Document 3:

"Longformer: The Long-Document Transformer" and "AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering." NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

`longformer-base-4096` is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.
------------------------------
Document 2:

"Longformer: The Long-Document Transformer" and "AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"Longformer: The Long-Document Transformer" and "the Allen Institute for Artificial Intelligence (AI2)"
------------------------------
Document 2:

`longformer-base-4096` is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096.
-------------------- limitation_and_bias --------------------
Document 1:

"Longformer: The Long-Document Transformer" and "the Allen Institute for Artificial Intelligence (AI2)"
------------------------------
Document 2:

Longformer is a transformer model for long documents. `longformer-base-4096` is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.
-------------------- demo --------------------
Document 1:

"Longformer: The Long-Document Transformer" and "Longformer is an open-source project developed by the Allen Institute for Artificial Intelligence (AI2)"
------------------------------
Document 2:

Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations. Please refer to the examples in `modeling_longformer.py` and the paper for more details on how to set global attention.
-------------------- input_format --------------------
Document 1:

"longformer-base-4096 is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096." "Please refer to the examples in `modeling_longformer.py` and the paper for more details on how to set global attention." input_format: BERT-like model started from the RoBERTa checkpoint
------------------------------
Document 2:

Longformer: The Long-Document Transformer, @article{Beltagy2020Longformer, title={Longformer: The Long-Document Transformer}, author={Iz Beltagy and Matthew E. Peters and Arman Cohan}, journal={arXiv:2004.05150}, year={2020}, input_format
-------------------- output_format --------------------
Document 1:

"Longformer: The Long-Document Transformer" and "AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering."
------------------------------
Document 2:

Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.

[{'datasets': ['Longformer: The Long-Document Transformer', 'the Allen Institute for Artificial Int 
elligence (AI2)'], 'license': 'apache-2.0', 'github': 'Longformer: The Long-Document Transformer', ' 
paper': 'Longformer: The Long-Document Transformer', 'upstream_model': 'Longformer: The Long-Documen 
t Transformer', 'parameter_count': 'longformer-base-4096', 'hyper_parameters': {'epochs': 'Longforme 
r: The Long-Document Transformer', 'batch_size': 'Longformer: The Long-Document Transformer', 'learn 
ing_rate': 'Longformer: The Long-Document Transformer', 'optimizer': 'Longformer: The Long-Document  
Transformer'}, 'evaluation': [], 'hardware': 'Longformer: The Long-Document Transformer', 'limitatio 
n_and_bias': 'Longformer: The Long-Document Transformer', 'demo': 'Longformer: The Long-Document Tra 
nsformer', 'input_format': 'Longformer: The Long-Document Transformer', 'output_format': 'Longformer 
: The Long-Document Transformer'}, {'datasets': ['Longformer uses a combination of a sliding window  
(local) attention and global attention. Global attention is user-configured based on the task to all 
ow the model to learn task-specific representations.'], 'license': 'Longformer is an open-source pro 
ject developed by the Allen Institute for Artificial Intelligence (AI2)', 'github': 'longformer-base 
-4096 is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long docume 
nts. It supports sequences of length up to 4,096. Longformer uses a combination of a sliding window  
(local) attention and global attention. Global attention is user-configured based on the task to all 
ow the model to learn task-specific representations. Please refer to the examples in `modeling_longf 
ormer.py` and the paper for more details on how to set global attention.', 'paper': 'Longformer uses 
 a combination of a sliding window (local) attention and global attention. Global attention is user- 
configured based on the task to allow the model to learn task-specific representations. Please refer 
 to the examples in `modeling_longformer.py` and the paper for more details on how to set global att 
ention.', 'upstream_model': 'Longformer: The Long-Document Transformer', 'parameter_count': 'paramet 
er_count', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'longformer-base-4096 is a BERT-lik 
e model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports se 
quences of length up to 4,096.', 'limitation_and_bias': 'Longformer is a transformer model for long  
documents. `longformer-base-4096` is a BERT-like model started from the RoBERTa checkpoint and pretr 
ained for MLM on long documents. It supports sequences of length up to 4,096. Longformer uses a comb 
ination of a sliding window (local) attention and global attention. Global attention is user-configu 
red based on the task to allow the model to learn task-specific representations.', 'demo': 'Longform 
er uses a combination of a sliding window (local) attention and global attention. Global attention i 
s user-configured based on the task to allow the model to learn task-specific representations. Pleas 
e refer to the examples in `modeling_longformer.py` and the paper for more details on how to set glo 
bal attention.', 'input_format': 'longformer-base-4096 is a BERT-like model started from the RoBERTa 
 checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096.', 
 'output_format': 'Longformer uses a combination of a sliding window (local) attention and global at 
tention. Global attention is user-configured based on the task to allow the model to learn task-spec 
ific representations.'}]                                                                             

#####################lengyue233/content-vec-best########################

-------------------- datasets --------------------
Document 1:

"fairseq ContentVec model"
------------------------------
Document 2:

"HubertModelWithFinalProj.from_pretrained("lengyue233/content-vec-best")"
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"license"
-------------------- github --------------------
Document 1:

Official Repo: [ContentVec](https://github.com/auspicious3000/contentvec)
------------------------------
Document 2:

"download the ContentVec_legacy model from the official repo"
-------------------- paper --------------------
Document 1:

"fairseq ContentVec model"
------------------------------
Document 2:

"Following https://github.com/auspicious3000/contentvec/issues/6"
-------------------- upstream_model --------------------
Document 1:

"HubertModel" and "HubertModelWithFinalProj.from_pretrained("lengyue233/content-vec-best")" 

upstream_model: HubertModel
------------------------------
Document 2:

upstream_model: ContentVec
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

self.final_proj = nn.Linear(config.hidden_size, config.classifier_proj_size)
model = HubertModelWithFinalProj.from_pretrained("lengyue233/content-vec-best")
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"download the ContentVec_legacy model from the official repo, and then run python convert.py"
------------------------------
Document 2:

Official Repo: [ContentVec](https://github.com/auspicious3000/contentvec)
-------------------- input_format --------------------
Document 1:

"self.final_proj = nn.Linear(config.hidden_size, config.classifier_proj_size)"
"model = HubertModelWithFinalProj.from_pretrained("lengyue233/content-vec-best")"
"x = model(audio)["last_hidden_state"]"
-------------------- output_format --------------------
Document 1:

"self.final_proj = nn.Linear(config.hidden_size, config.classifier_proj_size)"
"model = HubertModelWithFinalProj.from_pretrained("lengyue233/content-vec-best")"
"x = model(audio)["last_hidden_state"]"

[{'datasets': ['fairseq ContentVec model'], 'license': 'mit', 'github': 'Official Repo: [ContentVec 
](https://github.com/auspicious3000/contentvec)', 'paper': 'fairseq ContentVec model', 'upstream_mod 
el': 'HubertModel', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'lea 
rning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'dem 
o': 'download the ContentVec_legacy model from the official repo, and then run python convert.py', ' 
input_format': 'self.final_proj = nn.Linear(config.hidden_size, config.classifier_proj_size)\nmodel  
= HubertModelWithFinalProj.from_pretrained("lengyue233/content-vec-best")\nx = model(audio)["last_hi 
dden_state"]', 'output_format': 'self.final_proj = nn.Linear(config.hidden_size, config.classifier_p 
roj_size)\nmodel = HubertModelWithFinalProj.from_pretrained("lengyue233/content-vec-best")\nx = mode 
l(audio)["last_hidden_state"]'}]                                                                     

#####################camembert-base########################

-------------------- datasets --------------------
Document 1:

"This model was pretrained on a subcorpus of OSCAR multilingual corpus."
"Some of the limitations and risks associated with the OSCAR dataset, which are further detailed in the [OSCAR dataset card](https://huggingface.co/datasets/oscar), include the following:"
"The quality of some OSCAR sub-corpora might be lower than expected, specifically for the lowest-resource languages."
"Constructed from Common Crawl, Personal and sensitive information might be present."
------------------------------
Document 2:

OSCAR (138 GB of text) and CCNet (135 GB of text)
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Citation Information"
------------------------------
Document 2:

"CamemBERT: a Tasty French Language Model"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

110M
-------------------- hyper_parameters --------------------
Document 1:

"110M", "Base", "OSCAR (138 GB of text)", "335M", "Large", "CCNet (135 GB of text)", "110M", "Base", "CCNet (135 GB of text)", "110M", "Base", "Wikipedia (4 GB of text)", "110M", "Base", "Subsample of OSCAR (4 GB of text)", "110M", "Base", "Subsample of CCNet (4 GB of text)"
------------------------------
Document 2:

"This model was pretrained on a subcorpus of OSCAR multilingual corpus."
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

"The model developers evaluated CamemBERT using four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing, named entity recognition (NER) and natural language inference (NLI)."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). This model was pretrained on a subcorpus of OSCAR multilingual corpus. Some of the limitations and risks associated with the OSCAR dataset, which are further detailed in the [OSCAR dataset card](https://huggingface.co/datasets/oscar), include the following: The quality of some OSCAR sub-corpora might be lower than expected, specifically for the lowest-resource languages. Constructed from Common Crawl, Personal and sensitive information might be present.
------------------------------
Document 2:

- [Risks, Limitations and Biases](#risks-limitations-and-biases)
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

OSCAR (138 GB of text), CCNet (135 GB of text), Wikipedia (4 GB of text), Subsample of OSCAR (4 GB of text), Subsample of CCNet (4 GB of text)
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"OSCAR (138 GB of text)", "CCNet (135 GB of text)", "Wikipedia (4 GB of text)", "Subsample of OSCAR (4 GB of text)", "Subsample of CCNet (4 GB of text)"

[{'datasets': ['OSCAR multilingual corpus'], 'license': 'mit', 'github': '', 'paper': '', 'upstream 
_model': '', 'parameter_count': '110M', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'l 
imitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': ' 
', 'vocabulary_size': ''}]                                                                           

#####################Helsinki-NLP/opus-mt-zh-en########################

-------------------- datasets --------------------
Document 1:

* dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)
* download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.zip)  
* test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.test.txt)
-------------------- license --------------------
Document 1:

license: cc-by-4.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Citation Information"
------------------------------
Document 2:

[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

"test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.eval.txt)  brevity_penalty: 0.948"
-------------------- hardware --------------------
Document 1:

"port_machine: brutasse"
-------------------- limitation_and_bias --------------------
Document 1:

- [Risks, Limitations and Biases](#risks-limitations-and-biases)
------------------------------
Document 2:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

SentencePiece (spm32k,spm32k)
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

SentencePiece (spm32k,spm32k)

[{'datasets': ['opus'], 'license': 'cc-by-4.0', 'github': '', 'paper': '', 'upstream_model': '', 'p 
arameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'port_machine: brutasse', 
 'limitation_and_bias': '', 'demo': '', 'input_format': 'SentencePiece (spm32k,spm32k)', 'output_for 
mat': '', 'input_token_limit': '', 'vocabulary_size': 'SentencePiece (spm32k,spm32k)'}]              

#####################pyannote/segmentation########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 401 Client Error. (Request ID: Root=1-653ddcee-60b8f97945fc05b30edbd0e4)

Cannot access gated repo for url https://huggingface.co/api/models/pyannote/segmentation.
Repo model pyannote/segmentation is gated. You must be authenticated to access it. 

#####################bigscience/bloom-560m########################

-------------------- datasets --------------------
Document 1:

"Blog post detailing the design choices during the dataset creation: https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling"
------------------------------
Document 2:

- 45 natural languages 
- 12 programming languages 
- In 1.5TB of pre-processed text, converted into 350B unique tokens 
- pie chart showing the distribution of languages in training data 
- The following table shows the further distribution of Niger-Congo and Indic languages in the training data. 
- The following table shows the distribution of programming languages.
------------------------------
Document 3:

5. [Training Data](#training-data)
-------------------- license --------------------
Document 1:

license: bigscience-bloom-rail-1.0
------------------------------
Document 2:

"Derivatives of the Model, as described in the License"
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"public research on large language models (LLMs)", "Text generation", "Exploring characteristics of language generated by a language model", "Cloze tests, counterfactuals, generations with reframings", "Information Extraction, Question Answering, Summarization"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

* 559,214,592 parameters:  
* 256,901,120 embedding parameters  
* 24 layers, 16 attention heads  
* Hidden layers are 1024-dimensional
-------------------- hyper_parameters --------------------
Document 1:

* 24 layers, 16 attention heads  
* Hidden layers are 1024-dimensional  
* Sequence length of 2048 tokens  
* Cross Entropy with mean reduction  
* 8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath links  
* 512GB per node CPU memory  
* 640GB per node GPU memory
-------------------- evaluation --------------------
Document 1:

"This section describes the evaluation protocols and provides the results."
------------------------------
Document 2:

6. [Evaluation](#evaluation)
------------------------------
Document 3:

Perplexity, Cross Entropy Loss
-------------------- hardware --------------------
Document 1:

Blog post on the hardware/engineering side: https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model
------------------------------
Document 2:

8. [Technical Specifications](#techincal-specifications)
------------------------------
Document 3:

"The training supercomputer, Jean Zay ([website](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html))"
-------------------- limitation_and_bias --------------------
Document 1:

3. [Bias, Risks, and Limitations]
------------------------------
Document 2:

Overrepresent some viewpoints and underrepresent others, Contain stereotypes, Hateful, abusive, or violent language, Discriminatory or prejudicial language, Content that may not be appropriate for all settings, including sexual content, Make errors, including producing incorrect information as if it were factual, Generate irrelevant or repetitive outputs
-------------------- demo --------------------
Document 1:

"Provide a quick summary of what the model is/does."
------------------------------
Document 2:

"Text generation", "Exploring characteristics of language generated by a language model", "Cloze tests, counterfactuals, generations with reframings", "Information Extraction, Question Answering, Summarization"
-------------------- input_format --------------------
Document 1:

1.5TB of pre-processed text, converted into 350B unique tokens
45 natural languages
12 programming languages
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"A vocabulary size of 250,680"
------------------------------
Document 2:

- ak
- ar
- as
- bm
- bn
- ca
- code
- en
- es
- eu
- fon
- fr
- gu
- hi
- id
- ig
- ki
- kn
- lg
- ln
- ml
- mr
- ne
- nso
- ny
- or
- pa
- pt
- rn
- rw
- sn
- st
- sw
- ta
- te
- tn
- ts
- tum
- tw
- ur
- vi
- wo
- xh
- yo
- zh
- zhs
- zht
- zu

[{'datasets': ['https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for 
-language-modeling'], 'license': 'bigscience-bloom-rail-1.0', 'github': '', 'paper': '', 'upstream_m 
odel': '', 'parameter_count': '559,214,592 parameters', 'hyper_parameters': [{'epochs': '', 'batch_s 
ize': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardwa 
re': 'https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model', 'limi 
tation_and_bias': '', 'demo': 'Provide a quick summary of what the model is/does.', 'input_format':  
'1.5TB of pre-processed text, converted into 350B unique tokens\n45 natural languages\n12 programmin 
g languages', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': '250,680'}]           

#####################t5-small########################

-------------------- datasets --------------------
Document 1:

- [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4)
- [C4](https://huggingface.co/datasets/c4)
- Wiki-DPR](https://huggingface.co/datasets/wiki_dpr)
- Sentence acceptability judgment
- CoLA [Warstadt et al., 2018](https://arxiv.org/abs/1805.12471)
- Sentiment analysis
- SST-2 [Socher et al., 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)
- Paraphrasing/sentence similarity
- MRPC [Dolan and Brockett, 2005](https://aclanthology.org/I05-5002)
- STS-B [Ceret al., 2017](https://arxiv.org/abs/1708.00055)
- QQP [Iyer et al., 2017](https://quoradata.quora.com/First-Qu
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"research paper", "see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)"
------------------------------
Document 2:

[research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)
------------------------------
Document 3:

"research paper" and "https://jmlr.org/papers/volume21/20-074/20-074.pdf"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------
Document 1:

"T5-Small is the checkpoint with 60 million parameters."
-------------------- hyper_parameters --------------------
Document 1:

"same model, loss function, and hyperparameters"
-------------------- evaluation --------------------
Document 1:

"The developers evaluated the model on 24 tasks, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for full details."
------------------------------
Document 2:

5. [Evaluation](#evaluation)
------------------------------
Document 3:

"For full results for T5-small, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf), Table 14."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)"
------------------------------
Document 2:

"We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself."
-------------------- demo --------------------
Document 1:

"demo"
-------------------- input_format --------------------
Document 1:

"reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"text-to-text framework...NLP task...hyperparameters" input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

"T5-Small is the checkpoint with 60 million parameters."
------------------------------
Document 2:

C4, Wiki-DPR, Sentence acceptability judgment, CoLA, Sentiment analysis, SST-2, Paraphrasing/sentence similarity, MRPC, STS-B, QQP, Natural language inference, MNLI, QNLI, RTE, CB, Sentence completion, COPA, Word sense disambiguation, WIC, Question answering, MultiRC, ReCoRD, BoolQ

[{'datasets': ['Colossal Clean Crawled Corpus (C4)', 'C4', 'Wiki-DPR', 'Sentence acceptability judg 
ment', 'CoLA', 'Sentiment analysis', 'SST-2', 'Paraphrasing/sentence similarity', 'MRPC', 'STS-B', ' 
QQP'], 'license': 'apache-2.0', 'github': '', 'paper': 'research paper', 'upstream_model': '', 'para 
meter_count': 'T5-Small is the checkpoint with 60 million parameters.', 'hyper_parameters': {'epochs 
': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result' 
: 0}], 'hardware': '', 'limitation_and_bias': '3. [Bias, Risks, and Limitations](#bias-risks-and-lim 
itations)', 'demo': 'demo', 'input_format': 'reframing all NLP tasks into a unified text-to-text-for 
mat where the input and output are always text strings', 'output_format': '', 'input_token_limit': ' 
text-to-text framework...NLP task...hyperparameters', 'vocabulary_size': 'T5-Small is the checkpoint 
 with 60 million parameters.'}]                                                                      

#####################timm/resnet50.a1_in1k########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 18743 tokens (18487 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################Seethal/sentiment_analysis_generic_dataset########################

-------------------- datasets --------------------
Document 1:

"classified dataset for text classification"
------------------------------
Document 2:

"masked language modeling (MLM)"
------------------------------
Document 3:

BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. Next sentence prediction (NSP): the model concatenates two masked sentences as inputs during pretraining.
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis"
------------------------------
Document 2:

"masked language modeling (MLM)"
------------------------------
Document 3:

BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. Next sentence prediction (NSP): the model concatenates two masked sentences as inputs during pretraining.
-------------------- upstream_model --------------------
Document 1:

upstream_model bert-base-uncased
------------------------------
Document 2:

upstream_model masked language modeling (MLM) objective
------------------------------
Document 3:

BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. Next sentence prediction (NSP): the model concatenates two masked sentences as inputs during pretraining.
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

parameter_count
------------------------------
Document 3:

"Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words." "Next sentence prediction (NSP): the model concatenates two masked sentences as inputs during pretraining."
-------------------- hyper_parameters --------------------
Document 1:

"masked language modeling (MLM) objective"
-------------------- evaluation --------------------
Document 1:

"This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis" "This model is trained on a classified dataset for text classification."
------------------------------
Document 2:

"Pretrained model on English language using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between english and English."
-------------------- hardware --------------------
Document 1:

BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. Next sentence prediction (NSP): the model concatenates two masked sentences as inputs during pretraining.
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias: This model is not intended for further downstream fine-tuning for any other tasks.
------------------------------
Document 2:

"Pretrained model on English language using a masked language modeling (MLM) objective. This model is uncased: it does not make a difference between english and English."
------------------------------
Document 3:

BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. Next sentence prediction (NSP): the model concatenates two masked sentences as inputs during pretraining.
-------------------- demo --------------------
Document 1:

"masked language modeling (MLM)"
------------------------------
Document 2:

"This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis"
-------------------- input_format --------------------
Document 1:

"masked language modeling (MLM) objective" "uncased: it does not make a difference between english and English" input_format
------------------------------
Document 2:

"pretrained on the raw texts only, with no humans labelling them in any way" "Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words" "Next sentence prediction (NSP): the model concatenates two masked sentences as inputs during pretraining"
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
------------------------------
Document 2:

BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. Next sentence prediction (NSP): the model concatenates two masked sentences as inputs during pretraining.
-------------------- input_token_limit --------------------
Document 1:

"Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words."
-------------------- vocabulary_size --------------------
Document 1:

"masked language modeling (MLM)" "English language" "does not make a difference between english and English"
------------------------------
Document 2:

"Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words." NO_OUTPUT

[{'datasets': ['classified dataset for text classification'], 'license': '', 'github': '', 'paper': 
 'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis', 'u 
pstream_model': 'bert-base-uncased', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [] 
, 'hardware': '', 'limitation_and_bias': 'This model is not intended for further downstream fine-tun 
ing for any other tasks.', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': 
 '', 'vocabulary_size': ''}]                                                                         

#####################facebook/encodec_24khz########################

-------------------- datasets --------------------
Document 1:

- DNS Challenge 4
- [Common Voice](https://huggingface.co/datasets/common_voice)
- [AudioSet](https://huggingface.co/datasets/Fhrozen/AudioSet2K22)
- [FSD50K](https://huggingface.co/datasets/Fhrozen/FSD50k)
- [Jamendo dataset](https://huggingface.co/datasets/rkstgr/mtg-jamendo)
------------------------------
Document 2:

"This models was evalutated using the MUSHRA protocol (Series, 2014), using both a hidden reference and a low anchor. Annotators were recruited using a crowd-sourcing platform, in which they were asked to rate the perceptual quality of the provided samples in a range between 1 to 100. They randomly select 50 samples of 5 seconds from each category of the the test set and force at least 10 annotations per samples. To filter noisy annotations and outliers we remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time."
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"High Fidelity Neural Audio Compression"
"Alexandre Défossez and Jade Copet and Gabriel Synnaeve and Yossi Adi"
"2022"
"2210.13438"
"arXiv"
"eess.AS"
------------------------------
Document 2:

"GitHub Repository: https://github.com/facebookresearch/encodec"
-------------------- paper --------------------
Document 1:

```
title={High Fidelity Neural Audio Compression},
author={Alexandre Défossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},
year={2022},
eprint={2210.13438},
archivePrefix={arXiv},
primaryClass={eess.AS}
```
------------------------------
Document 2:

MUSHRA protocol (Series, 2014), hidden reference, low anchor, crowd-sourcing platform, rate the perceptual quality of the provided samples in a range between 1 to 100, randomly select 50 samples of 5 seconds from each category of the the test set, force at least 10 annotations per samples, filter noisy annotations and outliers, remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time.
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count: 8
------------------------------
Document 2:

parameter_count
------------------------------
Document 3:

"force at least 10 annotations per samples" and "remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time."
-------------------- hyper_parameters --------------------
Document 1:

epochs: 300, optimizer: Adam, batch size: 64, learning rate: 3 · 10−4, β1: 0.5, β2: 0.9
------------------------------
Document 2:

"MUSHRA protocol (Series, 2014)", "rate the perceptual quality of the provided samples in a range between 1 to 100", "randomly select 50 samples of 5 seconds from each category of the the test set", "force at least 10 annotations per samples", "remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time."
-------------------- evaluation --------------------
Document 1:

<!-- This section describes the evaluation protocols and provides the results. -->
------------------------------
Document 2:

"This models was evalutated using the MUSHRA protocol (Series, 2014), using both a hidden reference and a low anchor. Annotators were recruited using a crowd-sourcing platform, in which they were asked to rate the perceptual quality of the provided samples in a range between 1 to 100. They randomly select 50 samples of 5 seconds from each category of the the test set and force at least 10 annotations per samples. To filter noisy annotations and outliers we remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time."
-------------------- hardware --------------------
Document 1:

8 A100 GPUs
------------------------------
Document 2:

"MUSHRA protocol (Series, 2014)", "crowd-sourcing platform", "50 samples of 5 seconds from each category of the the test set", "force at least 10 annotations per samples", "remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time."
-------------------- limitation_and_bias --------------------
Document 1:

"MUSHRA protocol (Series, 2014)", "crowd-sourcing platform", "rate the perceptual quality of the provided samples in a range between 1 to 100", "randomly select 50 samples of 5 seconds from each category of the the test set", "force at least 10 annotations per samples", "remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time."
-------------------- demo --------------------
Document 1:

"This models was evalutated using the MUSHRA protocol (Series, 2014)", "Annotators were recruited using a crowd-sourcing platform, in which they were asked to rate the perceptual quality of the provided samples in a range between 1 to 100.", "They randomly select 50 samples of 5 seconds from each category of the the test set and force at least 10 annotations per samples.", "To filter noisy annotations and outliers we remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time."
-------------------- input_format --------------------
Document 1:

Non-streamable: the input audio is split into chunks of 1 seconds, with an overlap of 10 ms, Streamable: weight normalizationis used on the convolution layers, and the input is not split into chunks but rather padded on the left. input_format: Non-streamable: chunks of 1 seconds with an overlap of 10 ms, Streamable: padded on the left.
-------------------- output_format --------------------
Document 1:

"It provides high-quality audio compression and efficient decoding." "Two different setup exist for EnCodec: - Non-streamable: the input audio is split into chunks of 1 seconds, with an overlap of 10 ms, which are then encoded. - Streamable: weight normalizationis used on the convolution layers, and the input is not split into chunks but rather padded on the left."

[{'datasets': ['DNS Challenge 4', 'Common Voice', 'AudioSet', 'FSD50K', 'Jamendo dataset'], 'licens 
e': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters':  
[], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'ou 
tput_format': ''}]                                                                                   

#####################microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext########################

-------------------- datasets --------------------
Document 1:

PubMed and PubMedCentral
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/). NO_OUTPUT
-------------------- github --------------------
Document 1:

[Recent work](https://arxiv.org/abs/2007.15779), [PubMed](https://pubmed.ncbi.nlm.nih.gov/), [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/), [Biomedical Language Understanding and Reasoning Benchmark](https://aka.ms/BLURB)
-------------------- paper --------------------
Document 1:

"If you find PubMedBERT useful in your research, please cite the following paper: 
@misc{pubmedbert,
author = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
year = {2020},
eprint = {arXiv:2007.15779},
}"
------------------------------
Document 2:

Recent work, https://arxiv.org/abs/2007.15779, PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/).
------------------------------
Document 3:

"text: '[MASK] is a tumor suppressor gene.'"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/). parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/).
-------------------- evaluation --------------------
Document 1:

Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/). This model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the top score on the [Biomedical Language Understanding and Reasoning Benchmark](https://aka.ms/BLURB).
-------------------- hardware --------------------
Document 1:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/).
-------------------- limitation_and_bias --------------------
Document 1:

"pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models" and "Pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/)."
-------------------- demo --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&modelKind=bidirectional&sentence=Gefitinib%20is%20an%20EGFR%20tyrosine%20kinase%20inhibitor,%20which%20is%20often%20used%20for%20breast%20cancer%20and%20NSCLC%20treatment.&layer=3&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=17&tokenSide=right&maskInds=..&hideClsSep=true"><img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"></a>
------------------------------
Document 2:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/).
-------------------- input_format --------------------
Document 1:

abstracts from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/)
-------------------- output_format --------------------
Document 1:

"abstracts from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/)"
NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/).
-------------------- vocabulary_size --------------------
Document 1:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/).
NO_OUTPUT

[{'datasets': ['PubMed', 'PubMedCentral'], 'license': 'mit', 'github': '[Recent work](https://arxiv 
.org/abs/2007.15779), [PubMed](https://pubmed.ncbi.nlm.nih.gov/), [PubMedCentral](https://www.ncbi.n 
lm.nih.gov/pmc/), [Biomedical Language Understanding and Reasoning Benchmark](https://aka.ms/BLURB)' 
, 'paper': '"If you find PubMedBERT useful in your research, please cite the following paper: \n@mis 
c{pubmedbert,\nauthor = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and 
 Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},\ntitle = {Domain-Specific Lang 
uage Model Pretraining for Biomedical Natural Language Processing},\nyear = {2020},\neprint = {arXiv 
:2007.15779},\n}"', 'upstream_model': '', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epoc 
hs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'Recent wo 
rk shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language mo 
dels from scratch results in substantial gains over continual pretraining of general-domain language 
 models. PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.n 
lm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/). This  
model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the to 
p score on the [Biomedical Language Understanding and Reasoning Benchmark](https://aka.ms/BLURB).',  
'result': 0}], 'hardware': 'NO_OUTPUT', 'limitation_and_bias': '"pretraining language models from sc 
ratch results in substantial gains over continual pretraining of general-domain language models" and 
 "Pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and _ful 
l-text_ articles from [PubMedCentral](https://www.ncbi.nlm.nih.gov/pmc/)."', 'demo': '<a href="https 
://huggingface.co/exbert/?model=microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&modelK 
ind=bidirectional&sentence=Gefitinib%20is%20an%20EGFR%20tyrosine%20kinase%20inhibitor,%20which%20is% 
20often%20used%20for%20breast%20cancer%20and%20NSCLC%20treatment.&layer=3&heads=..0,1,2,3,4,5,6,7,8, 
9,10,11&threshold=0.7&tokenInd=17&tokenSide=right&maskInds=..&hideClsSep=true"><img width="300px" sr 
c="https://cdn-media.huggingface.co/exbert/button.png"></a>', 'input_format': '"abstracts from [PubM 
ed](https://pubmed.ncbi.nlm.nih.gov/) and _full-text_ articles from [PubMedCentral](https://www.ncbi 
.nlm.nih.gov/pmc/)"', 'output_format': 'NO_OUTPUT', 'input_token_limit': 'NO_OUTPUT', 'vocabulary_si 
ze': 'NO_OUTPUT'}]                                                                                   

#####################laion/CLIP-ViT-H-14-laion2B-s32B-b79K########################

-------------------- datasets --------------------
Document 1:

"The 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)"
------------------------------
Document 2:

"training notes" and "wandb logs"
-------------------- license --------------------
Document 1:

"OpenCLIP software" ```@software{ilharco_gabriel_2021_5143773, ... url          = {https://doi.org/10.5281/zenodo.5143773}```
------------------------------
Document 2:

"LAION CLIP Benchmark suite"
-------------------- github --------------------
Document 1:

"https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c" "https://wandb.ai/rom1504/eval_openclip/reports/H-14--VmlldzoyNDAxODQ3"
-------------------- paper --------------------
Document 1:

"Citation"
------------------------------
Document 2:

"The OpenAI CLIP paper"
------------------------------
Document 3:

"An initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"parameter_count"
------------------------------
Document 2:

parameter_count **TODO** - create table for just this model's metrics.
-------------------- hyper_parameters --------------------
Document 1:

"hyper_parameters" and "[training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c)" and "[wandb logs](https://wandb.ai/rom1504/eval_openclip/reports/H-14--VmlldzoyNDAxODQ3)"
------------------------------
Document 2:

"The model achieves a 78.0 zero-shot top-1 accuracy on ImageNet-1k." "An initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb" "**TODO** - create table for just this model's metrics."
-------------------- evaluation --------------------
Document 1:

"4. [Evaluation](#evaluation)"
------------------------------
Document 2:

The model achieves a 78.0 zero-shot top-1 accuracy on ImageNet-1k. An initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb **TODO** - create table for just this model's metrics.
------------------------------
Document 3:

"Please see [training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c) and [wandb logs](https://wandb.ai/rom1504/eval_openclip/reports/H-14--VmlldzoyNDAxODQ3)."
-------------------- hardware --------------------
Document 1:

"OpenCLIP" and "[stability.ai](https://stability.ai/)" cluster.
-------------------- limitation_and_bias --------------------
Document 1:

Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.
------------------------------
Document 2:

"The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet." "Be aware that this large-scale dataset is uncurated." "Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer." "Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."
-------------------- demo --------------------
Document 1:

"Use the code below to get started with the model."
------------------------------
Document 2:

"Please see [training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c) and [wandb logs](https://wandb.ai/rom1504/eval_openclip/reports/H-14--VmlldzoyNDAxODQ3)."
-------------------- input_format --------------------
Document 1:

training notes, wandb logs
------------------------------
Document 2:

"2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)" 
input_format: NO_OUTPUT
-------------------- output_format --------------------
Document 1:

"output_format" NO_OUTPUT
-------------------- input_preprocessing --------------------

-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': ['The 2 Billion sample English subset of LAION-5B'], 'license': 'OpenCLIP software',  
'github': 'https://github.com/LAION-AI/CLIP_benchmark', 'paper': 'Citation', 'upstream_model': '', ' 
parameter_count': 'parameter_count', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_ 
rate': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0}], 'hardware': 'OpenCLIP', 'lim 
itation_and_bias': 'Any deployed use case of the model - whether commercial or not - is currently ou 
t of scope. Non-deployed use cases such as image search in a constrained environment, are also not r 
ecommended unless there is thorough in-domain testing of the model with a specific, fixed class taxo 
nomy. Certain use cases which would fall under the domain of surveillance and facial recognition are 
 always out-of-scope regardless of performance of the model. Since the model has not been purposeful 
ly trained in or evaluated on any languages other than English, its use should be limited to English 
 language use cases.', 'demo': 'Use the code below to get started with the model.', 'input_format':  
'training notes, wandb logs', 'output_format': 'output_format', 'input_preprocessing': '', 'input_si 
ze': '', 'num_of_classes_for_classification': '', 'trigger_word': ''}]                               

#####################timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k########################

-------------------- datasets --------------------
Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

datasets:
- imagenet-1k
- wit-400m
- imagenet-12k
------------------------------
Document 3:

datasets WIT-400M, ImageNet-12k, ImageNet-1k
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"GitHub repository" and "\url{https://github.com/huggingface/pytorch-image-models}"
------------------------------
Document 2:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"
-------------------- paper --------------------
Document 1:

```bibtex
@inproceedings{Radford2021LearningTV,
title={Learning Transferable Visual Models From Natural Language Supervision},
author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
booktitle={ICML},
year={2021}
}
```
------------------------------
Document 2:

"model results"
------------------------------
Document 3:

"Reproducible scaling laws" (https://arxiv.org/abs/2212.07143)
-------------------- upstream_model --------------------
Document 1:

upstream_model: Pretrained on WIT-400M image-text pairs by OpenAI using CLIP.
------------------------------
Document 2:

"Image classification / feature backbone" "Params (M): 304.2" "GMACs: 77.8" "Activations (M): 57.1" "Image size: 224 x 224" "Learning Transferable Visual Models From Natural Language Supervision: https://arxiv.org/abs/2103.00020" "Reproducible scaling laws for contrastive language-image learning: https://arxiv.org/abs/2212.07143" "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2" "ImageNet-1k" "WIT-400M" "ImageNet-12k"
-------------------- parameter_count --------------------
Document 1:

Params (M): 304.2
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
------------------------------
Document 2:

"A Vision Transformer (ViT) image classification model. Pretrained on WIT-400M image-text pairs by OpenAI using CLIP. Fine-tuned on ImageNet-12k and then ImageNet-1k in `timm`."
-------------------- evaluation --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).
------------------------------
Document 2:

A Vision Transformer (ViT) image classification model. Pretrained on WIT-400M image-text pairs by OpenAI using CLIP. Fine-tuned on ImageNet-12k and then ImageNet-1k in `timm`. See recipes in [Reproducible scaling laws](https://arxiv.org/abs/2212.07143).
-------------------- hardware --------------------
Document 1:

"WIT-400M image-text pairs" "OpenAI" "CLIP" "ImageNet-12k" "ImageNet-1k" "timm"
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias NO_OUTPUT
-------------------- demo --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"A Vision Transformer (ViT) image classification model.", "Pretrained on WIT-400M image-text pairs by OpenAI using CLIP.", "Fine-tuned on ImageNet-12k and then ImageNet-1k in `timm`.", "See recipes in [Reproducible scaling laws](https://arxiv.org/abs/2212.07143)."
-------------------- input_format --------------------
Document 1:

Image size: 224 x 224, Dataset: ImageNet-1k, Pretrain Dataset: WIT-400M, ImageNet-12k
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

input_preprocessing: Pretrained on WIT-400M image-text pairs by OpenAI using CLIP. Fine-tuned on ImageNet-12k and then ImageNet-1k in `timm`.
------------------------------
Document 2:

"Image size: 224 x 224" NO_OUTPUT
-------------------- input_size --------------------
Document 1:

Image size: 224 x 224
------------------------------
Document 2:

"Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))" and "transforms = timm.data.create_transform(**data_config, is_training=False)"
-------------------- num_of_classes_for_classification --------------------
Document 1:

num_of_classes_for_classification: ImageNet-1k
-------------------- trigger_word --------------------


[{'datasets': ['imagenet-1k', 'wit-400m', 'imagenet-12k'], 'license': 'apache-2.0', 'github': 'http 
s://github.com/huggingface/pytorch-image-models', 'paper': 'https://arxiv.org/abs/2103.00020', 'upst 
ream_model': 'Pretrained on WIT-400M image-text pairs by OpenAI using CLIP.', 'parameter_count': '30 
4.2', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'WIT-400M image-text pairs', 'limitation 
_and_bias': 'NO_OUTPUT', 'demo': '[model results](https://github.com/huggingface/pytorch-image-model 
s/tree/main/results)', 'input_format': 'Image size: 224 x 224, Dataset: ImageNet-1k, Pretrain Datase 
t: WIT-400M, ImageNet-12k', 'output_format': '', 'input_preprocessing': 'Pretrained on WIT-400M imag 
e-text pairs by OpenAI using CLIP. Fine-tuned on ImageNet-12k and then ImageNet-1k in `timm`.', 'inp 
ut_size': 'Image size: 224 x 224', 'num_of_classes_for_classification': 'ImageNet-1k', 'trigger_word 
': ''}]                                                                                              

#####################cambridgeltl/SapBERT-from-PubMedBERT-fulltext########################

-------------------- datasets --------------------
Document 1:

datasets: - UMLS
------------------------------
Document 2:

"UMLS 2020AA (English only)", "[microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext)"
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext" and "UMLS 2020AA (English only)"
-------------------- github --------------------
Document 1:

"github" "ACL 2021" "NAACL 2021"
------------------------------
Document 2:

"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
-------------------- paper --------------------
Document 1:

[Liu et al. (2020)](https://arxiv.org/pdf/2010.11784.pdf)
------------------------------
Document 2:

"Self-Alignment Pretraining for Biomedical Entity Representations"
------------------------------
Document 3:

ACL 2021, NAACL 2021
-------------------- upstream_model --------------------
Document 1:

microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
------------------------------
Document 2:

- ACL 2021
- NAACL 2021
-------------------- parameter_count --------------------
Document 1:

parameter_count: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
-------------------- hyper_parameters --------------------
Document 1:

"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
-------------------- evaluation --------------------
Document 1:

"SapBERT by [Liu et al. (2020)](https://arxiv.org/pdf/2010.11784.pdf)", "[UMLS](https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html) 2020AA (English only)", "[microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext)"
-------------------- hardware --------------------
Document 1:

microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
-------------------- limitation_and_bias --------------------
Document 1:

"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
------------------------------
Document 2:

UMLS, ACL 2021, NAACL 2021
-------------------- demo --------------------
Document 1:

"ACL 2021" and "NAACL 2021"
------------------------------
Document 2:

"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
-------------------- input_format --------------------
Document 1:

UMLS 2020AA (English only), microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
------------------------------
Document 2:

UMLS, ACL 2021, NAACL 2021
------------------------------
Document 3:

"The input should be a string of biomedical entity names, e.g., "covid infection" or "Hydroxychloroquine"." input_format: string of biomedical entity names
-------------------- output_format --------------------
Document 1:

"The [CLS] embedding of the last layer is regarded as the output." output_format: embeddings

[{'datasets': ['UMLS'], 'license': 'apache-2.0', 'github': 'ACL 2021, NAACL 2021', 'paper': '[Liu e 
t al. (2020)](https://arxiv.org/pdf/2010.11784.pdf)', 'upstream_model': 'microsoft/BiomedNLP-PubMedB 
ERT-base-uncased-abstract-fulltext', 'parameter_count': 'microsoft/BiomedNLP-PubMedBERT-base-uncased 
-abstract-fulltext', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'microsoft/BiomedNLP-PubM 
edBERT-base-uncased-abstract-fulltext', 'limitation_and_bias': 'microsoft/BiomedNLP-PubMedBERT-base- 
uncased-abstract-fulltext', 'demo': 'ACL 2021 and NAACL 2021', 'input_format': 'UMLS 2020AA (English 
 only), microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', 'output_format': 'The [CLS]  
embedding of the last layer is regarded as the output.'}]                                            

#####################facebook/bart-large########################

-------------------- datasets --------------------
Document 1:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)"
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"BART model pre-trained on English language" and "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
------------------------------
Document 2:

"DBLP:journals/corr/abs-1910-13461"
-------------------- upstream_model --------------------
Document 1:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)"
-------------------- parameter_count --------------------
Document 1:

"BART model pre-trained on English language" and "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)"
-------------------- evaluation --------------------
Document 1:

"BART model pre-trained on English language. It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"BART model pre-trained on English language" and "The team releasing BART did not write a model card for this model"
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=bart"
------------------------------
Document 2:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)".
-------------------- input_format --------------------
Document 1:

return_tensors="pt"
------------------------------
Document 2:

"BART model pre-trained on English language" "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)" "The team releasing BART did not write a model card for this model"
-------------------- output_format --------------------
Document 1:

return_tensors="pt"

[{'datasets': ['BART model pre-trained on English language'], 'license': 'apache-2.0', 'github': '' 
, 'paper': 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Trans 
lation, and Comprehension', 'upstream_model': 'BART model pre-trained on English language', 'paramet 
er_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': 'The 
 team releasing BART did not write a model card for this model', 'demo': 'https://huggingface.co/mod 
els?search=bart', 'input_format': 'return_tensors="pt"', 'output_format': 'return_tensors="pt"'}]    

#####################deepset/tinyroberta-squad2########################

-------------------- datasets --------------------
Document 1:

datasets:
- squad_v2
model-index:
- name: deepset/tinyroberta-squad2
results:
- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
metrics:
- type: exact_match
value: 78.8627
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDNlZDU4ODAxMzY5NGFiMTMyZmQ1M2ZhZjMyODA1NmFlOGMxNzYxNTA4OGE5YTBkZWViZjBkNGQ2ZmMxZjVlMCIsInZlcnNpb24iOjF9.Wgu599r6TvgMLTrHlLMVAbUtKD_3b70iJ
------------------------------
Document 2:

"SQuAD 2.0" and "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"
------------------------------
Document 3:

"deepset/tinyroberta-squad2"
-------------------- license --------------------
Document 1:

license: cc-by-4.0
------------------------------
Document 2:

"GitHub repo" and "GitHub Discussions"
-------------------- github --------------------
Document 1:

<strong><a href="https://github.com/deepset-ai/haystack">GitHub</a></strong> and <strong><a class="h-7" href="https://haystack.deepset.ai/community/join">Discord community open to everyone!</a></strong>
------------------------------
Document 2:

- [roberta-base-squad2]([https://huggingface.co/deepset/roberta-base-squad2)
- [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert)
- [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
------------------------------
Document 3:

deepset/tinyroberta-squad2
-------------------- paper --------------------
Document 1:

"deepset/tinyroberta-squad2"
------------------------------
Document 2:

[this paper](https://arxiv.org/pdf/1909.10351.pdf)
-------------------- upstream_model --------------------
Document 1:

"deepset/tinyroberta-squad2"
------------------------------
Document 2:

deepset/roberta-base-squad2, upstream_model
------------------------------
Document 3:

"tinyroberta-squad2" upstream_model
-------------------- parameter_count --------------------
Document 1:

parameter_count = 9
-------------------- hyper_parameters --------------------
Document 1:

batch_size = 96, n_epochs = 4, max_seq_len = 384, learning_rate = 3e-5, lr_schedule = LinearWarmup, warmup_proportion = 0.2, doc_stride = 128, max_query_length = 64, distillation_loss_weight = 0.75, temperature = 1.5, teacher = "deepset/robert-large-squad2"
------------------------------
Document 2:

"haystack", "deepset/tinyroberta-6l-768d", "deepset/roberta-base-squad2", "deepset/roberta-large-squad2"
-------------------- evaluation --------------------
Document 1:

"Evaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/). ```"exact": 78.69114798281817, "f1": 81.9198998536977, "total": 11873, "HasAns_exact": 76.19770580296895, "HasAns_f1": 82.66446878592329, "HasAns_total": 5928, "NoAns_exact": 81.17746005046257, "NoAns_f1": 81.17746005046257, "NoAns_total": 5945```
------------------------------
Document 2:

- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
metrics:
- type: exact_match
value: 78.8627
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDNlZDU4ODAxMzY5NGFiMTMyZmQ1M2ZhZjMyODA1NmFlOGMxNzYxNTA4OGE5YTBkZWViZjBkNGQ2ZmMxZjVlMCIsInZlcnNpb24iOjF9.Wgu599r6TvgMLTrHlLMVAbUtKD_3b70iJ5QSeDQ-bRfUsVk6Sz9OsJCp47riHJVlmSYzcDj
------------------------------
Document 3:

"Downstream-task: Extractive QA", "Training data: SQuAD 2.0", "Eval data: SQuAD 2.0", "Code: See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "Infrastructure: 4x Tesla v100"
-------------------- hardware --------------------
Document 1:

"4x Tesla v100"
------------------------------
Document 2:

deepset/tinyroberta-6l-768d, deepset/roberta-base-squad2, deepset/roberta-large-squad2
-------------------- limitation_and_bias --------------------
Document 1:

"tinyroberta-squad2", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"
-------------------- demo --------------------
Document 1:

- [deepset](http://deepset.ai/)
- [Haystack](https://haystack.deepset.ai/)
- [roberta-base-squad2]([https://huggingface.co/deepset/roberta-base-squad2)
- [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert)
- [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
------------------------------
Document 2:

<strong><a href="https://github.com/deepset-ai/haystack">GitHub</a></strong> repo and <strong><a href="https://docs.haystack.deepset.ai">Documentation</a></strong>
------------------------------
Document 3:

"tinyroberta-squad2", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"
-------------------- input_format --------------------
Document 1:

"name: squad_v2 type: squad_v2 config: squad_v2 split: validation"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"tinyroberta-squad2", "Extractive QA", "SQuAD 2.0", "4x Tesla v100"
-------------------- vocabulary_size --------------------
Document 1:

"tinyroberta-squad2", "English", "SQuAD 2.0", "4x Tesla v100"

[{'datasets': ['squad_v2'], 'license': 'cc-by-4.0', 'github': 'https://github.com/deepset-ai/haysta 
ck', 'paper': 'https://arxiv.org/pdf/1909.10351.pdf', 'upstream_model': 'deepset/tinyroberta-squad2' 
, 'parameter_count': '9', 'hyper_parameters': {'epochs': '4', 'batch_size': '96', 'learning_rate': ' 
3e-5', 'optimizer': 'AdamW'}, 'evaluation': [{'test': 'SQuAD 2.0', 'result': 78.69114798281817}], 'h 
ardware': '4x Tesla v100', 'limitation_and_bias': 'tinyroberta-squad2, English, Extractive QA, SQuAD 
 2.0', 'demo': '[deepset](http://deepset.ai/), [Haystack](https://haystack.deepset.ai/)', 'input_for 
mat': 'name: squad_v2 type: squad_v2 config: squad_v2 split: validation', 'output_format': '', 'inpu 
t_token_limit': 'tinyroberta-squad2', 'vocabulary_size': 'tinyroberta-squad2'}]                      

#####################distilgpt2########################

-------------------- datasets --------------------
Document 1:

OpenWebTextCorpus, OpenWebTextCorpus Dataset Card, Radford et al. (2019)
------------------------------
Document 2:

datasets:
- openwebtext
model-index:
- name: distilgpt2
dataset:
name: WikiText-103
type: wikitext
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

OpenWebTextCorpus https://skylion007.github.io/OpenWebTextCorpus/, OpenWebTextCorpus Dataset Card https://huggingface.co/datasets/openwebtext, Radford et al. (2019) https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
------------------------------
Document 2:

language: en, license: apache-2.0, tags: - exbert, datasets: - openwebtext, co2_eq_emissions: 149200, model-index: - name: distilgpt2, results: - task:, type: text-generation, name: Text Generation, dataset: name: WikiText-103, type: wikitext, metrics: - type: perplexity, value: 21.1, name: Perplexity
-------------------- paper --------------------
Document 1:

`title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas}, booktitle={NeurIPS EMC^2 Workshop}, year={2019}`
------------------------------
Document 2:

Sanh et al. (2019)
------------------------------
Document 3:

"[Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf)", "[Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)", "[Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)"
-------------------- upstream_model --------------------
Document 1:

upstream_model: distilgpt2
------------------------------
Document 2:

upstream_model: GPT-2
-------------------- parameter_count --------------------
Document 1:

"co2_eq_emissions: 149200"
------------------------------
Document 2:

parameter_count: 82 million
-------------------- hyper_parameters --------------------
Document 1:

"model-index: - name: distilgpt2 results: - task: type: text-generation name: Text Generation dataset: name: WikiText-103 type: wikitext metrics: - type: perplexity value: 21.1 name: Perplexity"
-------------------- evaluation --------------------
Document 1:

language: en, license: apache-2.0, tags: - exbert, datasets: - openwebtext, co2_eq_emissions: 149200, type: text-generation, name: Text Generation, dataset: name: WikiText-103, type: wikitext, metrics: - type: perplexity, value: 21.1, name: Perplexity
------------------------------
Document 2:

"Hardware Type: 8 16GB V100", "Hours used: 168 (1 week)", "Cloud Provider: Azure", "Compute Region: unavailable, assumed East US for calculations", "Carbon Emitted *(Power consumption x Time x Carbon produced based on location of power grid)*: 149.2 kg eq. CO2"
------------------------------
Document 3:

OpenWebTextCorpus, OpenWebTextCorpus Dataset Card, Radford et al. (2019)
-------------------- hardware --------------------
Document 1:

8 16GB V100, 168 (1 week), Azure, East US
------------------------------
Document 2:

OpenWebTextCorpus, OpenWebTextCorpus Dataset Card, Radford et al. (2019)
-------------------- limitation_and_bias --------------------
Document 1:

"As the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). 

- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.
- [Xu and Hu (2022)](https://ar
------------------------------
Document 2:

"As described in [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see [Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf) and [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531).
-------------------- demo --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=distilgpt2">
<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
-------------------- input_format --------------------
Document 1:

"type: text-generation" "name: WikiText-103" "type: wikitext"
------------------------------
Document 2:

OpenWebTextCorpus, OpenWebTextCorpus Dataset Card, Radford et al. (2019)
-------------------- output_format --------------------
Document 1:

"type: text-generation" "name: Text Generation" "name: WikiText-103" "type: wikitext" "type: perplexity" "value: 21.1" "name: Perplexity"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"type: perplexity value: 21.1 name: Perplexity"

[{'datasets': ['OpenWebTextCorpus'], 'license': 'apache-2.0', 'github': 'OpenWebTextCorpus https:// 
skylion007.github.io/OpenWebTextCorpus/, OpenWebTextCorpus Dataset Card https://huggingface.co/datas 
ets/openwebtext, Radford et al. (2019) https://d4mucfpksywv.cloudfront.net/better-language-models/la 
nguage-models.pdf', 'paper': '`title={DistilBERT, a distilled version of BERT: smaller, faster, chea 
per and lighter}, author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas}, b 
ooktitle={NeurIPS EMC^2 Workshop}, year={2019}`', 'upstream_model': 'distilgpt2', 'parameter_count': 
 '82 million', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer' 
: ''}, 'evaluation': [], 'hardware': '8 16GB V100, 168 (1 week), Azure, East US', 'limitation_and_bi 
as': '"As the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt- 
2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems 
 they were trained on.” Significant research has explored bias and fairness issues with models for l 
anguage generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.a 
cl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). \n 
\n- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find th 
at distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with  
regard to gender and race) with effect sizes larger than the teacher models.\n- [Xu and Hu (2022)](h 
ttps://ar', 'demo': '<a href="https://huggingface.co/exbert/?model=distilgpt2">\n<img width="300px"  
src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>', 'input_format': '"type: text-gener 
ation" "name: WikiText-103" "type: wikitext"', 'output_format': '"type: text-generation" "name: Text 
 Generation" "name: WikiText-103" "type: wikitext" "type: perplexity" "value: 21.1" "name: Perplexit 
y"', 'input_token_limit': '', 'vocabulary_size': ''}]                                                

#####################dslim/bert-large-NER########################

-------------------- datasets --------------------
Document 1:

datasets: - conll2003; dataset: name: conll2003; type: conll2003; config: conll2003; split: test
------------------------------
Document 2:

datasets, CoNLL-2003 NER task, [original BERT paper](https://arxiv.org/pdf/1810.04805)
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

CoNLL-2003 Named Entity Recognition, bert-large-cased
-------------------- github --------------------
Document 1:

"More on replicating the original results [here](https://github.com/google-research/bert/issues/223)."
-------------------- paper --------------------
Document 1:

[original BERT paper](https://arxiv.org/pdf/1810.04805)
------------------------------
Document 2:

"@article{DBLP:journals/corr/abs-1810-04805,
author    = {Jacob Devlin and
Ming{-}Wei Chang and
Kenton Lee and
Kristina Toutanova},
title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
Understanding},
journal   = {CoRR},
volume    = {abs/1810.04805},
year      = {2018},
url       = {http://arxiv.org/abs/1810.04805},
archivePrefix = {arXiv},
eprint    = {1810.04805},
timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}"
-------------------- upstream_model --------------------
Document 1:

original BERT paper
------------------------------
Document 2:

"dslim/bert-base-NER"
-------------------- parameter_count --------------------
Document 1:

parameter_count: single NVIDIA V100 GPU
-------------------- hyper_parameters --------------------
Document 1:

"hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805)"
-------------------- evaluation --------------------
Document 1:

metric|dev|test
-|-|-
f1 |95.7 |91.7
precision |95.3 |91.2
recall |96.1 |92.3
------------------------------
Document 2:

"This model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task."
------------------------------
Document 3:

- type: accuracy
value: 0.9031688753722759
name: Accuracy
verified: true
- type: precision
value: 0.920025068328604
name: Precision
verified: true
- type: recall
value: 0.9193688678588825
name: Recall
verified: true
- type: f1
value: 0.9196968510445761
name: F1
verified: true
- type: loss
value: 0.5085050463676453
name: loss
verified: true
-------------------- hardware --------------------
Document 1:

NVIDIA V100 GPU
------------------------------
Document 2:

bert-large-cased, CoNLL-2003 Named Entity Recognition
------------------------------
Document 3:

"The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins."
-------------------- limitation_and_bias --------------------
Document 1:

This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.
------------------------------
Document 2:

"trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task."
------------------------------
Document 3:

This model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity I-MIS | Miscellaneous entity B-PER |Beginning of a person’s name right after another person’s name I-PER |Person’s name B-ORG |Beginning of an organization right after another organization I-ORG |organization B-LOC |Beginning of a location right after another location I-LOC |Location
-------------------- demo --------------------
Document 1:

"from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

nlp = pipeline("ner", model=model, tokenizer=tokenizer)
example = "My name is Wolfgang and I live in Berlin"

ner_results = nlp(example)
print(ner_results)"
------------------------------
Document 2:

"original BERT paper" "CoNLL-2003 NER task"
-------------------- input_format --------------------
Document 1:

metric|dev|test -|-|- f1 |95.7 |91.7 precision |95.3 |91.2 recall |96.1 |92.3
-------------------- output_format --------------------
Document 1:

output_format|metric|dev|test|-|-|-|f1 |95.7 |91.7|precision |95.3 |91.2|recall |96.1 |92.3
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"original BERT paper"
------------------------------
Document 2:

"model-index: - name: dslim/bert-large-NER"

[{'datasets': ['conll2003'], 'license': 'mit', 'github': 'https://github.com/google-research/bert/i 
ssues/223', 'paper': 'https://arxiv.org/pdf/1810.04805', 'upstream_model': 'original BERT paper', 'p 
arameter_count': 'single NVIDIA V100 GPU', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'lea 
rning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'f1', 'result': 91.7}], 'hardware': 'NVID 
IA V100 GPU', 'limitation_and_bias': 'This model is limited by its training dataset of entity-annota 
ted news articles from a specific span of time. This may not generalize well for all use cases in di 
fferent domains. Furthermore, the model occassionally tags subword tokens as entities and post-proce 
ssing of results may be necessary to handle those cases.', 'demo': 'from transformers import AutoTok 
enizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokeni 
zer.from_pretrained("dslim/bert-base-NER")\nmodel = AutoModelForTokenClassification.from_pretrained( 
"dslim/bert-base-NER")\n\nnlp = pipeline("ner", model=model, tokenizer=tokenizer)\nexample = "My nam 
e is Wolfgang and I live in Berlin"\n\nner_results = nlp(example)\nprint(ner_results)', 'input_forma 
t': 'metric|dev|test\n-|-|-', 'output_format': 'output_format|metric|dev|test|-|-|-|f1 |95.7 |91.7|p 
recision |95.3 |91.2|recall |96.1 |92.3', 'input_token_limit': '', 'vocabulary_size': ''}]           

#####################rinna/japanese-hubert-base########################

-------------------- datasets --------------------
Document 1:

"19,000 hours of [ReazonSpeech](https://huggingface.co/datasets/reazonspeech/reazonspeech) corpus"
------------------------------
Document 2:

"The model was trained using code from the [official repository](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert)"
-------------------- license --------------------
Document 1:

The Apache 2.0 license
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

[The Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)

NO_OUTPUT
------------------------------
Document 2:

reazon-research/reazonspeech
------------------------------
Document 3:

[original HuBERT base model](https://huggingface.co/facebook/hubert-base-ls960), [official repository](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert), [original paper](https://ieeexplore.ieee.org/document/9585401), [here](https://huggingface.co/rinna/japanese-hubert-base/tree/main/fairseq)
-------------------- paper --------------------
Document 1:

[original paper](https://ieeexplore.ieee.org/document/9585401)
------------------------------
Document 2:

ReazonSpeech
------------------------------
Document 3:

"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"
-------------------- upstream_model --------------------
Document 1:

original HuBERT base model, official repository, original paper, fairseq checkpoint file
------------------------------
Document 2:

upstream_model: Japanese HuBERT
-------------------- parameter_count --------------------
Document 1:

parameter_count 19000
------------------------------
Document 2:

parameter_count 12 transformer layers 8 attention heads
-------------------- hyper_parameters --------------------
Document 1:

original HuBERT base model, official repository, original paper, fairseq checkpoint file
-------------------- evaluation --------------------
Document 1:

"The model was trained on approximately 19,000 hours of [ReazonSpeech](https://huggingface.co/datasets/reazonspeech/reazonspeech) corpus."
------------------------------
Document 2:

The model architecture is the same as the [original HuBERT base model](https://huggingface.co/facebook/hubert-base-ls960), which contains 12 transformer layers with 8 attention heads.
The model was trained using code from the [official repository](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert), and the detailed training configuration can be found in the same repository and the [original paper](https://ieeexplore.ieee.org/document/9585401).  
A fairseq checkpoint file can also be available [here](https://huggingface.co/rinna/japanese-hubert-base/tree/main/fairseq).
-------------------- hardware --------------------
Document 1:

"The model was trained using code from the [official repository](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert)"
------------------------------
Document 2:

rinna Co., Ltd., ReazonSpeech corpus.
-------------------- limitation_and_bias --------------------
Document 1:

original HuBERT base model, official repository, original paper, fairseq checkpoint file
------------------------------
Document 2:

ReazonSpeech
-------------------- demo --------------------
Document 1:

[original HuBERT base model](https://huggingface.co/facebook/hubert-base-ls960), [official repository](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert), [original paper](https://ieeexplore.ieee.org/document/9585401), [here](https://huggingface.co/rinna/japanese-hubert-base/tree/main/fairseq)
------------------------------
Document 2:

"The model was trained on approximately 19,000 hours of [ReazonSpeech](https://huggingface.co/datasets/reazonspeech/reazonspeech) corpus."
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['ReazonSpeech'], 'license': 'Apache 2.0', 'github': 'https://github.com/facebookrese 
arch/fairseq/tree/main/examples/hubert', 'paper': 'https://ieeexplore.ieee.org/document/9585401', 'u 
pstream_model': 'HuBERT base model', 'parameter_count': '19000', 'hyper_parameters': {}, 'evaluation 
': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ' 
'}]                                                                                                  

#####################nateraw/vit-age-classifier########################

-------------------- datasets --------------------
Document 1:

datasets:
- fairface
-------------------- license --------------------

-------------------- github --------------------
Document 1:

- github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true
-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"A vision transformer finetuned to classify the age of a given person's face." ```python import requests from PIL import Image from io import BytesIO from transformers import ViTFeatureExtractor, ViTForImageClassification # Get example image from official fairface repo + read it in as an image r = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true') im = Image.open(BytesIO(r.content)) # Init model, transforms model = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier') transforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier') # Transform our image and pass it through the model inputs = transforms(im, return_tensors='pt') output = model(**inputs) # Predicted Class probabilities proba = output.logits.softmax(1) # Predicted Classes preds = proba.argmax(1) ```
-------------------- input_format --------------------
Document 1:

`return_tensors='pt'`
-------------------- output_format --------------------
Document 1:

`ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')` `ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')` `output.logits.softmax(1)` `proba.argmax(1)`
-------------------- input_preprocessing --------------------
Document 1:

"transforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')" and "inputs = transforms(im, return_tensors='pt')"
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------
Document 1:

"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')" and "output.logits.softmax(1)"
-------------------- trigger_word --------------------


{'datasets': ['fairface'], 'license': '', 'github': 'github.com/dchen236/FairFace/blob/master/detec 
ted_faces/race_Asian_face0.jpg?raw=true', 'paper': '', 'upstream_model': '', 'parameter_count': '',  
'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluat 
ion': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '"A vision transformer finetuned to cla 
ssify the age of a given person\'s face." ```python import requests from PIL import Image from io im 
port BytesIO from transformers import ViTFeatureExtractor, ViTForImageClassification # Get example i 
mage from official fairface repo + read it in as an image r = requests.get(\'https://github.com/dche 
n236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true\') im = Image.open(BytesIO(r. 
content)) # Init model, transforms model = ViTForImageClassification.from_pretrained(\'nateraw/vit-a 
ge-classifier\') transforms = ViTFeatureExtractor.from_pretrained(\'nateraw/vit-age-classifier\') #  
Transform our image and pass it through the model inputs = transforms(im, return_tensors=\'pt\') out 
put = model(**inputs) # Predicted Class probabilities proba = output.logits.softmax(1) # Predicted C 
lasses preds = proba.argmax(1) ```', 'input_format': "`return_tensors='pt'`", 'output_format': "`ViT 
ForImageClassification.from_pretrained('nateraw/vit-age-classifier')` `ViTFeatureExtractor.from_pret 
rained('nateraw/vit-age-classifier')` `output.logits.softmax(1)` `proba.argmax(1)`", 'input_preproce 
ssing': '"transforms = ViTFeatureExtractor.from_pretrained(\'nateraw/vit-age-classifier\')" and "inp 
uts = transforms(im, return_tensors=\'pt\')"', 'input_size': '', 'num_of_classes_for_classification' 
: '"ViTForImageClassification.from_pretrained(\'nateraw/vit-age-classifier\')" and "output.logits.so 
ftmax(1)"', 'trigger_word': ''}                                                                      

#####################dslim/bert-base-NER########################

-------------------- datasets --------------------
Document 1:

datasets, CoNLL-2003 NER task, [original BERT paper](https://arxiv.org/pdf/1810.04805)
------------------------------
Document 2:

datasets: - conll2003
------------------------------
Document 3:

"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"bert-base-cased" model, "CoNLL-2003 Named Entity Recognition" dataset
-------------------- github --------------------
Document 1:

"More on replicating the original results [here](https://github.com/google-research/bert/issues/223)."
-------------------- paper --------------------
Document 1:

[original BERT paper](https://arxiv.org/pdf/1810.04805)
------------------------------
Document 2:

"@article{DBLP:journals/corr/abs-1810-04805,
author    = {Jacob Devlin and
Ming{-}Wei Chang and
Kenton Lee and
Kristina Toutanova},
title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
Understanding},
journal   = {CoRR},
volume    = {abs/1810.04805},
year      = {2018},
url       = {http://arxiv.org/abs/1810.04805},
archivePrefix = {arXiv},
eprint    = {1810.04805},
timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}"
-------------------- upstream_model --------------------
Document 1:

original BERT paper
------------------------------
Document 2:

"dslim/bert-base-NER"
-------------------- parameter_count --------------------
Document 1:

parameter_count: single NVIDIA V100 GPU
-------------------- hyper_parameters --------------------
Document 1:

"hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805)"
-------------------- evaluation --------------------
Document 1:

metric|dev|test
-|-|-
f1 |95.1 |91.3
precision |95.0 |90.7
recall |95.3 |91.9
------------------------------
Document 2:

"This model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task."
------------------------------
Document 3:

"This model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity I-MIS | Miscellaneous entity B-PER |Beginning of a person’s name right after another person’s name I-PER |Person’s name B-ORG |Beginning of an organization right after another organization I-ORG |organization B-LOC |Beginning of a location right after another location I-LOC |Location
-------------------- hardware --------------------
Document 1:

NVIDIA V100 GPU
------------------------------
Document 2:

"bert-base-cased" model, "CoNLL-2003 Named Entity Recognition" dataset
------------------------------
Document 3:

"The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins."
-------------------- limitation_and_bias --------------------
Document 1:

This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.
------------------------------
Document 2:

"trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task."
------------------------------
Document 3:

This model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity I-MIS | Miscellaneous entity B-PER |Beginning of a person’s name right after another person’s name I-PER |Person’s name B-ORG |Beginning of an organization right after another organization I-ORG |organization B-LOC |Beginning of a location right after another location I-LOC |Location
-------------------- demo --------------------
Document 1:

"from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

nlp = pipeline("ner", model=model, tokenizer=tokenizer)
example = "My name is Wolfgang and I live in Berlin"

ner_results = nlp(example)
print(ner_results)"
------------------------------
Document 2:

[original BERT paper](https://arxiv.org/pdf/1810.04805)
-------------------- input_format --------------------
Document 1:

metric|dev|test -|-|- f1 |95.1 |91.3 precision |95.0 |90.7 recall |95.3 |91.9
-------------------- output_format --------------------
Document 1:

output_format|metric|dev|test-|-|-f1 |95.1 |91.3precision |95.0 |90.7recall |95.3 |91.9
------------------------------
Document 2:

"The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity I-MIS | Miscellaneous entity B-PER |Beginning of a person’s name right after another person’s name I-PER |Person’s name B-ORG |Beginning of an organization right after another organization I-ORG |organization B-LOC |Beginning of a location right after another location I-LOC |Location"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"original BERT paper"

[{'datasets': ['CoNLL-2003 NER task'], 'license': 'mit', 'github': 'https://github.com/google-resea 
rch/bert/issues/223', 'paper': 'https://arxiv.org/pdf/1810.04805', 'upstream_model': 'original BERT  
paper', 'parameter_count': 'single NVIDIA V100 GPU', 'hyper_parameters': {'epochs': '', 'batch_size' 
: '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'f1', 'result': 91.3}, {'test': 
 'precision', 'result': 90.7}, {'test': 'recall', 'result': 91.9}], 'hardware': 'NVIDIA V100 GPU', ' 
limitation_and_bias': 'This model is limited by its training dataset of entity-annotated news articl 
es from a specific span of time. This may not generalize well for all use cases in different domains 
. Furthermore, the model occassionally tags subword tokens as entities and post-processing of result 
s may be necessary to handle those cases.', 'demo': 'from transformers import AutoTokenizer, AutoMod 
elForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretra 
ined("dslim/bert-base-NER")\nmodel = AutoModelForTokenClassification.from_pretrained("dslim/bert-bas 
e-NER")\n\nnlp = pipeline("ner", model=model, tokenizer=tokenizer)\nexample = "My name is Wolfgang a 
nd I live in Berlin"\n\nner_results = nlp(example)\nprint(ner_results)', 'input_format': 'metric|dev 
|test\n-|-|-\nf1 |95.1 |91.3\nprecision |95.0 |90.7\nrecall |95.3 |91.9', 'output_format': 'output_f 
ormat|metric|dev|test-|-|-f1 |95.1 |91.3precision |95.0 |90.7recall |95.3 |91.9', 'input_token_limit 
': '', 'vocabulary_size': ''}]                                                                       

#####################nlpconnect/vit-gpt2-image-captioning########################

-------------------- datasets --------------------
Document 1:

"This is an image captioning model trained by @ydshieh in [flax ](https://github.com/huggingface/transformers/tree/main/examples/flax/image-captioning) this is pytorch version of [this](https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts)."
------------------------------
Document 2:

"https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

http://github.com/ankur3107
------------------------------
Document 2:

"https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/"
------------------------------
Document 3:

[flax ](https://github.com/huggingface/transformers/tree/main/examples/flax/image-captioning), [this](https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts)
-------------------- paper --------------------
Document 1:

[this](https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts)
------------------------------
Document 2:

"https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------
Document 1:

"This is an image captioning model trained by @ydshieh in [flax ](https://github.com/huggingface/transformers/tree/main/examples/flax/image-captioning) this is pytorch version of [this](https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts)."
-------------------- hardware --------------------
Document 1:

"https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/"
------------------------------
Document 2:

[flax ](https://github.com/huggingface/transformers/tree/main/examples/flax/image-captioning) this is pytorch version of [this](https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts).
-------------------- input_format --------------------
Document 1:

"flax", "pytorch", "huggingface/transformers/tree/main/examples/flax/image-captioning", "huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts"
------------------------------
Document 2:

"model="nlpconnect/vit-gpt2-image-captioning""
-------------------- output_format --------------------
Document 1:

output_format
------------------------------
Document 2:

"generated_text": 'a soccer game with a player jumping to catch the ball '
------------------------------
Document 3:

output_format

[{'datasets': ['flax'], 'license': '', 'github': 'http://github.com/ankur3107', 'paper': 'https://h 
uggingface.co/ydshieh/vit-gpt2-coco-en-ckpts', 'upstream_model': '', 'parameter_count': '', 'hyper_p 
arameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [] 
, 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]   

#####################THUDM/chatglm2-6b########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

"license"
------------------------------
Document 2:

Apache-2.0, MODEL_LICENSE
-------------------- github --------------------
Document 1:

Apache-2.0, LICENSE, MODEL_LICENSE
------------------------------
Document 2:

💻 <a href="https://github.com/THUDM/ChatGLM2-6B" target="_blank">Github Repo</a> • 📃 <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> • 📃 <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a>
-------------------- paper --------------------
Document 1:

```
@article{zeng2022glm,
title={Glm-130b: An open bilingual pre-trained model},
author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
journal={arXiv preprint arXiv:2210.02414},
year={2022}
}
```
```
@inproceedings{du2022glm,
title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages={320--335},
year={2022}
}
```
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

[evaluation results](README.md#evaluation-results)
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

Apache-2.0, MODEL_LICENSE
------------------------------
Document 2:

For more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM2-6B).
-------------------- input_format --------------------
Document 1:

language:
- zh
- en
tags:
- glm
- chatglm
- thudm
-------------------- output_format --------------------


[{'datasets': [], 'license': 'Apache-2.0, MODEL_LICENSE', 'github': '💻 <a href="https://github.com/ 
THUDM/ChatGLM2-6B" target="_blank">Github Repo</a> • 📃 <a href="https://arxiv.org/abs/2103.10360" ta 
rget="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> • 
 📃 <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https: 
//github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a>', 'paper': '@article{zeng2022glm,\ntitle={ 
Glm-130b: An open bilingual pre-trained model},\nauthor={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao 
 and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and X 
ia, Xiao and others},\njournal={arXiv preprint arXiv:2210.02414},\nyear={2022}\n}\n\n@inproceedings{ 
du2022glm,\ntitle={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\nau 
thor={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin  
and Tang, Jie},\nbooktitle={Proceedings of the 60th Annual Meeting of the Association for Computatio 
nal Linguistics (Volume 1: Long Papers)},\npages={320--335},\nyear={2022}\n}', 'upstream_model': '', 
 'parameter_count': 'parameter_count', 'hyper_parameters': [], 'evaluation': [{'test': 'evaluation r 
esults', 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 'demo': 'For more instructions, i 
ncluding how to run CLI and web demos, and model quantization, please refer to our [Github Repo](htt 
ps://github.com/THUDM/ChatGLM2-6B).', 'input_format': 'language:\n- zh\n- en\ntags:\n- glm\n- chatgl 
m\n- thudm', 'output_format': ''}]                                                                   

#####################cardiffnlp/twitter-roberta-base-sentiment-latest########################

-------------------- datasets --------------------
Document 1:

datasets: - tweet_eval
------------------------------
Document 2:

- The original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) 
- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829).
- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"url = "https://aclanthology.org/2022.emnlp-demos.5"" and "url = "https://aclanthology.org/2022.acl-demo.25""
------------------------------
Document 2:

[here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m), [TweetEval](https://github.com/cardiffnlp/tweeteval), [TimeLMs official repository](https://github.com/cardiffnlp/timelms), [TweetNLP](https://github.com/cardiffnlp/tweetnlp)
-------------------- paper --------------------
Document 1:

"Camacho-collados, Jose  and
Rezaee, Kiamehr  and
Riahi, Talayeh  and
Ushio, Asahi  and
Loureiro, Daniel  and
Antypas, Dimosthenis  and
Boisson, Joanne  and
Espinosa Anke, Luis  and
Liu, Fangyu  and
Mart{\'\i}nez C{\'a}mara, Eugenio" and others,
title = "{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media",
booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
------------------------------
Document 2:

Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829).
-------------------- upstream_model --------------------
Document 1:

"The original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m)"

upstream_model: https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m
------------------------------
Document 2:

model_path
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

TweetEval benchmark, [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m), [TweetEval](https://github.com/cardiffnlp/tweeteval), [TimeLMs paper](https://arxiv.org/abs/2202.03829), [TimeLMs official repository](https://github.com/cardiffnlp/timelms), 0 -> Negative; 1 -> Neutral; 2 -> Positive, [TweetNLP](https://github.com/cardiffnlp/tweetnlp), [here](https://tweetnlp.org).
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

url = "https://aclanthology.org/2022.emnlp-demos.5", url = "https://aclanthology.org/2022.acl-demo.25"
------------------------------
Document 2:

```pipeline("sentiment-analysis", model=model_path, tokenizer=model_path) sentiment_task("Covid cases are increasing fast!")```
-------------------- input_format --------------------
Document 1:

"model=model_path, tokenizer=model_path"
-------------------- output_format --------------------
Document 1:

"sentiment-analysis", "model=model_path", "tokenizer=model_path", "[{'label': 'Negative', 'score': 0.7236}]"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021" and "This sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp)."

[{'datasets': ['tweet_eval'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'para 
meter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimiz 
er': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': ' 
', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                             

#####################yiyanghkust/finbert-tone########################

-------------------- datasets --------------------
Document 1:

- Corporate Reports 10-K & 10-Q: 2.5B tokens
- Earnings Call Transcripts: 1.3B tokens
- Analyst Reports: 1.1B tokens
-------------------- license --------------------
Document 1:

"More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)" 
NO_OUTPUT
-------------------- github --------------------
Document 1:

More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)
-------------------- paper --------------------
Document 1:

Huang, Allen H., Hui Wang, and Yi Yang. "FinBERT: A Large Language Model for Extracting Information from Financial Text." *Contemporary Accounting Research* (2022).
------------------------------
Document 2:

"BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)", "BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
-------------------- upstream_model --------------------
Document 1:

'BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)'
upstream_model: yiyanghkust/finbert-tone
------------------------------
Document 2:

FinBERT is a BERT model pre-trained on financial communication text. More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT). This released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports.
-------------------- parameter_count --------------------
Document 1:

Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens
------------------------------
Document 2:

"BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)"
-------------------- hyper_parameters --------------------
Document 1:

- Corporate Reports 10-K & 10-Q: 2.5B tokens
- Earnings Call Transcripts: 1.3B tokens
- Analyst Reports: 1.1B tokens
- FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports.
------------------------------
Document 2:

"BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)", "BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
-------------------- evaluation --------------------
Document 1:

More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)  
This released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.
------------------------------
Document 2:

"BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)", "LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative"
-------------------- hardware --------------------
Document 1:

Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens
------------------------------
Document 2:

"BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)", "BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
-------------------- limitation_and_bias --------------------
Document 1:

Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens, finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports.
-------------------- demo --------------------
Document 1:

More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)
This released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports.
------------------------------
Document 2:

"You can use this model with Transformers pipeline for sentiment analysis.", 
```python
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import pipeline

finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')

nlp = pipeline("sentiment-analysis", model=finbert, tokenizer=tokenizer)

sentences = ["there is a shortage of capital, and we need extra financing",
"growth is strong and we have plenty of liquidity",
"there are doubts about our finances",
"profits are flat"]
results = nlp(sentences)
print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative

```
-------------------- input_format --------------------
Document 1:

Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens
------------------------------
Document 2:

"BertTokenizer, BertForSequenceClassification from transformers import pipeline"
-------------------- output_format --------------------
Document 1:

"LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative"
-------------------- input_token_limit --------------------
Document 1:

Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens
-------------------- vocabulary_size --------------------
Document 1:

"The total corpora size is 4.9B tokens." NO_OUTPUT
------------------------------
Document 2:

'BertTokenizer, BertForSequenceClassification, finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3), tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')'

[{'datasets': ['Corporate Reports 10-K & 10-Q', 'Earnings Call Transcripts', 'Analyst Reports'], 'l 
icense': 'More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)', 'gi 
thub': 'More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)', 'pape 
r': 'Huang, Allen H., Hui Wang, and Yi Yang. "FinBERT: A Large Language Model for Extracting Informa 
tion from Financial Text." *Contemporary Accounting Research* (2022).', 'upstream_model': 'yiyanghku 
st/finbert-tone', 'parameter_count': 'Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Tran 
scripts: 1.3B tokens, Analyst Reports: 1.1B tokens', 'hyper_parameters': {'epochs': '', 'batch_size' 
: '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0}], 'hardware':  
'Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports 
: 1.1B tokens', 'limitation_and_bias': 'Corporate Reports 10-K & 10-Q: 2.5B tokens, Earnings Call Tr 
anscripts: 1.3B tokens, Analyst Reports: 1.1B tokens, finbert-tone model is the FinBERT model fine-t 
uned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports.', 'd 
emo': 'More technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\nThis re 
leased `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive 
, negative, neutral) sentences from analyst reports.', 'input_format': 'Corporate Reports 10-K & 10- 
Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens', 'output_forma 
t': '"LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative"', 'input_token_limit': 'Corporate Repo 
rts 10-K & 10-Q: 2.5B tokens, Earnings Call Transcripts: 1.3B tokens, Analyst Reports: 1.1B tokens', 
 'vocabulary_size': '"The total corpora size is 4.9B tokens."'}]                                     

#####################ProsusAI/finbert########################

-------------------- datasets --------------------
Document 1:

Financial PhraseBank by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063)
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063)"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [...] The model will give softmax outputs for three labels: positive, negative or neutral."
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

"The model will give softmax outputs for three labels: positive, negative or neutral."
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Financial PhraseBank'], 'license': '', 'github': '', 'paper': 'https://arxiv.org/ab 
s/1908.10063', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [] 
, 'hardware': '', 'limitation_and_bias': '', 'demo': 'FinBERT is a pre-trained NLP model to analyze  
sentiment of financial text. It is built by further training the BERT language model in the finance  
domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classifica 
tion. [...] The model will give softmax outputs for three labels: positive, negative or neutral.', ' 
input_format': '', 'output_format': 'The model will give softmax outputs for three labels: positive, 
 negative or neutral.', 'input_token_limit': '', 'vocabulary_size': ''}]                             

#####################facebook/bart-large-cnn########################

-------------------- datasets --------------------
Document 1:

datasets:
- cnn_dailymail
dataset:
name: cnn_dailymail
type: cnn_dailymail
------------------------------
Document 2:

[CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail), [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461), [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart).
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
------------------------------
Document 2:

"title = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},"
-------------------- upstream_model --------------------
Document 1:

"BART model pre-trained on English language" and "introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al."
-------------------- parameter_count --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)"
-------------------- hyper_parameters --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- evaluation --------------------
Document 1:

- type: rouge
value: 42.9486
name: ROUGE-1
verified: true
- type: rouge
value: 20.8149
name: ROUGE-2
verified: true
- type: rouge
value: 30.6186
name: ROUGE-L
verified: true
- type: rouge
value: 40.0376
name: ROUGE-LSUM
verified: true
- type: loss
value: 2.529000997543335
name: loss
verified: true
- type: gen_len
value: 78.5866
name: gen_len
verified: true
------------------------------
Document 2:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- hardware --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)" and "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."
------------------------------
Document 2:

"This particular checkpoint has been fine-tuned on CNN Daily Mail"
-------------------- limitation_and_bias --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- demo --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- input_format --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)", "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)", "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)"
-------------------- output_format --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)" "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension" "https://github.com/pytorch/fairseq/tree/master/examples/bart"
-------------------- input_token_limit --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al." and "[this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- vocabulary_size --------------------
Document 1:

"BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)"

[{'datasets': ['cnn_dailymail'], 'license': 'mit', 'github': '', 'paper': 'https://arxiv.org/abs/19 
10.13461', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'h 
ardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input 
_token_limit': '', 'vocabulary_size': ''}]                                                           
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653de2b7-2afa28304a1f4c02116fc0fb)

Entry Not Found for url: https://huggingface.co/YituTech/conv-bert-base/resolve/main/README.md. 

#####################google/fnet-base########################

-------------------- datasets --------------------
Document 1:

"C4", "https://huggingface.co/datasets/c4"
------------------------------
Document 2:

"FNet-base was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 512 tokens. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=fnet) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/2105.03824)
------------------------------
Document 2:

"See the [model hub](https://huggingface.co/models?filter=fnet) to look for fine-tuned versions on a task that interests you."
-------------------- upstream_model --------------------
Document 1:

"introduced in [this paper](https://arxiv.org/abs/2105.03824) and first released in [this repository](https://github.com/google-research/google-research/tree/master/f_net)"
-------------------- parameter_count --------------------
Document 1:

16 TPU chips total, batch size of 256, sequence length of 512 tokens, Adam optimizer, learning rate of 1e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warmup for 10,000 steps, linear decay of the learning rate after.
------------------------------
Document 2:

"masked language modeling (MLM) and next sentence prediction (NSP) objective" and "The model achieves 0.58 accuracy on MLM objective and 0.80 on NSP objective."
------------------------------
Document 3:

"The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP] With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
-------------------- hyper_parameters --------------------
Document 1:

Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

sequence length of 512, batch size 16, learning rate 2e-5, 5 epochs for MRPC/WNLI, 3 epochs for other tasks, single 16GB NVIDIA Tesla V100 GPU
-------------------- evaluation --------------------
Document 1:

Table 1 on page 7 of [the official paper](https://arxiv.org/abs/2105.03824), [official Hugging Face GLUE evaluation scripts](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification#glue-tasks), single 16GB NVIDIA Tesla V100 GPU, sequence length of 512, batch size 16, learning rate 2e-5, [fnet-base](https://huggingface.co/google/fnet-base) (called *FNet (PyTorch) - Reproduced*), [bert-base-cased](https://hf.co/models/bert-base-cased) (called *Bert (PyTorch) - Reproduced*), *FNet (Flax) - Official*, [MNLI-(m/mm)](https://huggingface.co/gchhablani/fnet-base-finetuned-mnli), [QQP](https://huggingface.co/gchhablani/fnet-base-finetuned-qqp), [
------------------------------
Document 2:

"The inputs of the model are then of the form: 
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. 
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is."
-------------------- hardware --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)"
-------------------- limitation_and_bias --------------------
Document 1:

The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000. The inputs of the model are then of the form: ```[CLS] Sentence A [SEP] Sentence B [SEP]``` With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
------------------------------
Document 2:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
------------------------------
Document 3:

"Pretrained model on English language using a masked language modeling (MLM) and next sentence prediction (NSP) objective" and "The model achieves 0.58 accuracy on MLM objective and 0.80 on NSP objective."
-------------------- demo --------------------
Document 1:

See the [model hub](https://huggingface.co/models?filter=fnet)
------------------------------
Document 2:

"The FNet model was pretrained on [C4](https://huggingface.co/datasets/c4), a cleaned version of the Common Crawl dataset."
-------------------- input_format --------------------
Document 1:

"The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000. The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP] With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
------------------------------
Document 2:

"Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
------------------------------
Document 3:

C4, a cleaned version of the Common Crawl dataset. input_format
-------------------- output_format --------------------
Document 1:

The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.  
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.

[{'datasets': ['C4'], 'license': 'apache-2.0', 'github': 'https://huggingface.co/models?filter=fnet 
', 'paper': 'https://arxiv.org/abs/2105.03824', 'upstream_model': 'https://arxiv.org/abs/2105.03824' 
, 'parameter_count': '16 TPU chips total, batch size of 256, sequence length of 512 tokens, Adam opt 
imizer, learning rate of 1e-4, \\(\\beta_{1} = 0.9\\), \\(\\beta_{2} = 0.999\\), weight decay of 0.0 
1, learning rate warmup for 10,000 steps, linear decay of the learning rate after.', 'hyper_paramete 
rs': 'Adam with a learning rate of 1e-4, \\(\\beta_{1} = 0.9\\) and \\(\\beta_{2} = 0.999\\), a weig 
ht decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after. 
', 'evaluation': [{'test': 'Table 1 on page 7 of the official paper', 'result': 0.58}], 'hardware':  
'4 cloud TPUs in Pod configuration (16 TPU chips total)', 'limitation_and_bias': 'The texts are lowe 
rcased and tokenized using SentencePiece and a vocabulary size of 32,000. The inputs of the model ar 
e then of the form: [CLS] Sentence A [SEP] Sentence B [SEP] With probability 0.5, sentence A and sen 
tence B correspond to two consecutive sentences in the original corpus and in the other cases, it\'s 
 another random sentence in the corpus. Note that what is considered a sentence here is a consecutiv 
e span of text usually longer than a single sentence. The only constrain is that the result with the 
 two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure 
 for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the mas 
ked tokens are replaced by [MASK]. - In 10% of the cases, the masked tokens are replaced by a random 
 token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are le 
ft as is.', 'demo': 'See the [model hub](https://huggingface.co/models?filter=fnet)', 'input_format' 
: 'The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000. The i 
nputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP] With probability 0. 
5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in t 
he other cases, it\'s another random sentence in the corpus. Note that what is considered a sentence 
 here is a consecutive span of text usually longer than a single sentence. The only constrain is tha 
t the result with the two "sentences" has a combined length of less than 512 tokens.', 'output_forma 
t': 'The inputs of the model are then of the form:  \n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n 
```  \nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in th 
e original corpus and in\nthe other cases, it\'s another random sentence in the corpus. Note that wh 
at is considered a sentence here is a\nconsecutive span of text usually longer than a single sentenc 
e. The only constrain is that the result with the two\n"sentences" has a combined length of less tha 
n 512 tokens.  \nThe details of the masking procedure for each sentence are the following:\n- 15% of 
 the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10 
% of the cases, the masked tokens are replaced by a random token (different) from the one they repla 
ce.\n- In the 10% remaining cases, the masked tokens are left as is.'}]                              

#####################cardiffnlp/twitter-xlm-roberta-base-sentiment########################

-------------------- datasets --------------------
Document 1:

~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt).
------------------------------
Document 2:

"cardiffnlp/twitter-xlm-roberta-base-sentiment"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details). - Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250). - Git Repo: [XLM-T official repository](https://github.com/cardiffnlp/xlm-t). This model has been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp)."
-------------------- paper --------------------
Document 1:

Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250).
------------------------------
Document 2:

"Proceedings of the Thirteenth Language Resources and Evaluation Conference" and "European Language Resources Association"
------------------------------
Document 3:

"cardiffnlp/twitter-xlm-roberta-base-sentiment" NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"XLM-roBERTa-base model"
------------------------------
Document 2:

"cardiffnlp/twitter-xlm-roberta-base-sentiment"
-------------------- parameter_count --------------------
Document 1:

parameter_count: 198M
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details). Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250). Git Repo: [XLM-T official repository](https://github.com/cardiffnlp/xlm-t). This model has been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp)."
------------------------------
Document 2:

"sentiment-task("T'estimo!")", "[{'label': 'Positive', 'score': 0.6600581407546997}]"
-------------------- hardware --------------------
Document 1:

"~198M tweets" and "The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt)"
------------------------------
Document 2:

"model_path = "cardiffnlp/twitter-xlm-roberta-base-sentiment""
-------------------- limitation_and_bias --------------------
Document 1:

"This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details). Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250). Git Repo: [XLM-T official repository](https://github.com/cardiffnlp/xlm-t). This model has been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp)."
-------------------- demo --------------------
Document 1:

"The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details)." "Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250)." "Git Repo: [XLM-T official repository](https://github.com/cardiffnlp/xlm-t)." "This model has been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp)."
------------------------------
Document 2:

"from transformers import pipeline model_path = "cardiffnlp/twitter-xlm-roberta-base-sentiment" sentiment_task = pipeline("sentiment-analysis", model=model_path, tokenizer=model_path) sentiment_task("T'estimo!")"
-------------------- input_format --------------------
Document 1:

"~198M tweets" and "XLM-T official repository"
------------------------------
Document 2:

"model_path = "cardiffnlp/twitter-xlm-roberta-base-sentiment"", "tokenizer=model_path"
-------------------- output_format --------------------
Document 1:

output_format
------------------------------
Document 2:

"sentiment-analysis", "model=model_path", "tokenizer=model_path", "label": 'Positive', 'score': 0.6600581407546997
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"pipeline('sentiment-analysis', model=model_path, tokenizer=model_path)"
NO_OUTPUT

[{'datasets': ['~198M tweets'], 'license': '', 'github': 'cardiffnlp/twitter-xlm-roberta-base-senti 
ment', 'paper': 'XLM-T: A Multilingual Language Model Toolkit for Twitter', 'upstream_model': 'XLM-r 
oBERTa-base model', 'parameter_count': '198M', 'hyper_parameters': {}, 'evaluation': [{'test': 'sent 
iment-task', 'result': 0.6600581407546997}], 'hardware': '~198M tweets', 'limitation_and_bias': 'Thi 
s is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analy 
sis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can b 
e used for more languages (see paper for details). Paper: XLM-T: A Multilingual Language Model Toolk 
it for Twitter. Git Repo: XLM-T official repository. This model has been integrated into the TweetNL 
P library.', 'demo': 'The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, 
 Pt) but it can be used for more languages (see paper for details). Paper: XLM-T: A Multilingual Lan 
guage Model Toolkit for Twitter. Git Repo: XLM-T official repository. This model has been integrated 
 into the TweetNLP library.', 'input_format': '~198M tweets and XLM-T official repository', 'output_ 
format': 'output_format', 'input_token_limit': '', 'vocabulary_size': ''}]                           

#####################j-hartmann/emotion-english-distilroberta-base########################

-------------------- datasets --------------------
Document 1:

Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|
Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|
GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|
ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|
MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|
SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

DistilRoBERTa-base, RoBERTa-large
-------------------- parameter_count --------------------
Document 1:

parameter_count: DistilRoBERTa-base, RoBERTa-large
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

With this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class: 
1) anger 🤬
2) disgust 🤢
3) fear 😨
4) joy 😀
5) neutral 😐
6) sadness 😭
7) surprise 😲 
The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base). For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.
-------------------- hardware --------------------
Document 1:

"The model was trained on 6 diverse datasets"
-------------------- limitation_and_bias --------------------
Document 1:

"The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:  1) anger 🤬 2) disgust 🤢 3) fear 😨 4) joy 😀 5) neutral 😐 6) sadness 😭 7) surprise 😲  The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base). For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.
-------------------- demo --------------------
Document 1:

a) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/simple_emotion_pipeline.ipynb)  
```python
from transformers import pipeline
classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)
classifier("I love this!")
```  
```python
Output:
[[{'label': 'anger', 'score': 0.004419783595949411},
{'label': 'disgust', 'score': 0.0016119900392368436},
{'label': 'fear', 'score': 0.0004138521908316761},
{'label':
-------------------- input_format --------------------
Document 1:

"All datasets contain English text" and "The model is trained on a balanced subset from the datasets listed above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanced subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the random-chance baseline of 1/7 = 14%)."
------------------------------
Document 2:

"The model was trained on 6 diverse datasets (see Appendix below)" "The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base)" "For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version."
-------------------- output_format --------------------
Document 1:

"The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base)" "For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version."
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"The model was trained on 6 diverse datasets" and "The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base)"

[{'datasets': ['Crowdflower (2016)', 'Emotion Dataset, Elvis et al. (2018)', 'GoEmotions, Demszky e 
t al. (2020)', 'ISEAR, Vikash (2018)', 'MELD, Poria et al. (2019)', 'SemEval-2018, EI-reg, Mohammad  
et al. (2018)'], 'license': '', 'github': '', 'paper': '', 'upstream_model': 'DistilRoBERTa-base, Ro 
BERTa-large', 'parameter_count': 'parameter_count: DistilRoBERTa-base, RoBERTa-large', 'hyper_parame 
ters': [], 'evaluation': [{'test': "With this model, you can classify emotions in English text data. 
 The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emoti 
ons, plus a neutral class:", 'result': 0}], 'hardware': '"The model was trained on 6 diverse dataset 
s"', 'limitation_and_bias': '"The model was trained on 6 diverse datasets (see Appendix below) and p 
redicts Ekman\'s 6 basic emotions, plus a neutral class:  1) anger 🤬 2) disgust 🤢 3) fear 😨 4) joy 😀 
 5) neutral 😐 6) sadness 😭 7) surprise 😲  The model is a fine-tuned checkpoint of [DistilRoBERTa-bas 
e](https://huggingface.co/distilroberta-base). For a \'non-distilled\' emotion model, please refer t 
o the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-la 
rge) version."', 'demo': 'a) Run emotion model with 3 lines of code on single text example using Hug 
ging Face\'s pipeline command on Google Colab:  \n[![Open In Colab](https://colab.research.google.co 
m/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-disti 
lroberta-base/blob/main/simple_emotion_pipeline.ipynb)  \n```python\nfrom transformers import pipeli 
ne\nclassifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-bas 
e", return_all_scores=True)\nclassifier("I love this!")\n```  \n```python\nOutput:\n[[{\'label\': \' 
anger\', \'score\': 0.004419783595949411},\n{\'label\': \'disgust\', \'score\': 0.001611990039236843 
6},\n{\'label\': \'fear\', \'score\': 0.0004138521908316761},\n{\'label\':', 'input_format': '"All d 
atasets contain English text" and "The model is trained on a balanced subset from the datasets liste 
d above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanc 
ed subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the rando 
m-chance baseline of 1/7 = 14%)."', 'output_format': '"The model is a fine-tuned checkpoint of [Dist 
ilRoBERTa-base](https://huggingface.co/distilroberta-base)" "For a \'non-distilled\' emotion model,  
please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-engl 
ish-roberta-large) version."', 'input_token_limit': '', 'vocabulary_size': '"The model was trained o 
n 6 diverse datasets" and "The model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://hugg 
ingface.co/distilroberta-base)"'}]                                                                   

#####################stabilityai/sd-vae-ft-mse########################

-------------------- datasets --------------------
Document 1:

https://ommer-lab.com/files/latent-diffusion/kl-f8.zip, https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt, https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
------------------------------
Document 2:

"https://ommer-lab.com/files/latent-diffusion/kl-f8.zip" and "https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt" and "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

license: mit, tags: - stable-diffusion - stable-diffusion-diffusers, inference: false
------------------------------
Document 2:

https://ommer-lab.com/files/latent-diffusion/kl-f8.zip, https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt, https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
------------------------------
Document 3:

https://ommer-lab.com/files/latent-diffusion/kl-f8.zip, https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt, https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

original | 246803 | 2.61 | 26.0 +/- 4.4 | 0.81 +/- 0.12 | 0.75 +/- 0.36 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip
------------------------------
Document 2:

original | 246803 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"Model | train steps | rFID | PSNR | SSIM | PSIM | Link | Comments"
"original | 246803 | 2.61 | 26.0 +/- 4.4 | 0.81 +/- 0.12 | 0.75 +/- 0.36 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip | as used in SD"
"ft-EMA | 560001 | 1.77 | 26.7 +/- 4.8 | 0.82 +/- 0.12 | 0.67 +/- 0.34 | https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt | slightly better overall, with EMA"
"ft-MSE | 840001 | 1.88 | 27.3 +/- 4.7 | 0.83 +/- 0.11 | 0.65 +/- 0.34 | https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000
------------------------------
Document 2:

Model | train steps | rFID | PSNR | SSIM | PSIM | Link | Comments
original | 246803 | 4.99 | 23.4 +/- 3.8 | 0.69 +/- 0.14 | 1.01 +/- 0.28 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip | as used in SD
ft-EMA | 560001 | 4.42 | 23.8 +/- 3.9 | 0.69 +/- 0.13 | 0.96 +/- 0.27 | https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt | slightly better overall, with EMA
ft-MSE | 840001 | 4.70 | 24.5 +/- 3.7 | 0.71 +/- 0.13 | 0.92 +/- 0.27 | https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ck
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

Link: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
------------------------------
Document 2:

Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset.
<p align="center">
<br>
<b>
256x256: ft-EMA (left), ft-MSE (middle), original (right)</b>
</p>  
<p align="center">
<img src=https://huggingface.co/stabilityai/stable-diffusion-decoder-finetune/resolve/main/eval/ae-decoder-tuning-reconstructions/merged/00025_merged.png />
</p>  
<p align="center">
<img src=https://huggingface.co/stabilityai/stable-diffusion-decoder-finetune/resolve/main/eval/ae-decoder-tuning-reconstructions/merged/00011_merged.png />
</p>  
<p align="center">
<img src=https://huggingface.co/stabilityai/stable-diffusion-decoder-finetune/resolve/main/eval/ae
-------------------- input_format --------------------
Document 1:

"256x256 images from the COCO2017 validation dataset"

input_format: 256x256 images from the COCO2017 validation dataset
-------------------- output_format --------------------


[{'datasets': ['https://ommer-lab.com/files/latent-diffusion/kl-f8.zip', 'https://huggingface.co/st 
abilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt', 'https://huggingfa 
ce.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt'], 'license' 
: 'mit', 'github': 'https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-ms 
e-840000-ema-pruned.ckpt', 'paper': '', 'upstream_model': 'original | 246803 | https://ommer-lab.com 
/files/latent-diffusion/kl-f8.zip', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [{' 
test': 'original', 'result': 246803}, {'test': 'ft-EMA', 'result': 560001}, {'test': 'ft-MSE', 'resu 
lt': 840001}], 'hardware': '', 'limitation_and_bias': '', 'demo': 'Link: https://huggingface.co/stab 
ilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt', 'input_format': '256 
x256 images from the COCO2017 validation dataset', 'output_format': ''}]                             

#####################pyannote/speaker-diarization########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 401 Client Error. (Request ID: Root=1-653de391-291853384efe081a3dc080ba)

Cannot access gated repo for url https://huggingface.co/api/models/pyannote/speaker-diarization.
Repo model pyannote/speaker-diarization is gated. You must be authenticated to access it. 

#####################google/bert_uncased_L-2_H-128_A-2########################

-------------------- datasets --------------------
Document 1:

[2/128 (BERT-Tiny)][2_128], [4/256 (BERT-Mini)][4_256], [4/512 (BERT-Small)][4_512], [8/512 (BERT-Medium)][8_512], [12/768 (BERT-Base)][12_768]
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[official BERT Github page](https://github.com/google-research/bert/), or via HuggingFace from the links below:  
|   |H=128|H=256|H=512|H=768|
|---|:---:|:---:|:---:|:---:|
| **L=2**  |[**2/128 (BERT-Tiny)**][2_128]|[2/256][2_256]|[2/512][2_512]|[2/768][2_768]|
| **L=4**  |[4/128][4_128]|[**4/256 (BERT-Mini)**][4_256]|[**4/512 (BERT-Small)**][4_512]|[4/768][4_768]|
| **L=6**  |[6/128][6_128]|[6/256][6_256]|[6/512][6_512]|[6/768][6_768]|
| **L=8**  |[8/128
-------------------- paper --------------------
Document 1:

[Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962)
-------------------- upstream_model --------------------
Document 1:

"BERT-Tiny" "2/128 (BERT-Tiny)" "BERT-Mini" "4/256 (BERT-Mini)" "BERT-Small" "4/512 (BERT-Small)" "BERT-Medium" "8/512 (BERT-Medium)" "BERT-Base" "12/768 (BERT-Base)"
-------------------- parameter_count --------------------
Document 1:

"2/128 (BERT-Tiny)", "4/256 (BERT-Mini)", "4/512 (BERT-Small)", "8/512 (BERT-Medium)", "12/768 (BERT-Base)"
-------------------- hyper_parameters --------------------
Document 1:

batch sizes: 8, 16, 32, 64, 128; learning rates: 3e-4, 1e-4, 5e-5, 3e-5
-------------------- evaluation --------------------
Document 1:

The evaluation of the model is shown by the corresponding GLUE scores on the test set: 

|Model|Score|CoLA|SST-2|MRPC|STS-B|QQP|MNLI-m|MNLI-mm|QNLI(v2)|RTE|WNLI|AX|
|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|BERT-Tiny|64.2|0.0|83.2|81.1/71.1|74.3/73.6|62.2/83.4|70.2|70.3|81.5|57.2|62.3|21.0|
|BERT-Mini|65.8|0.0|85.9|81.1/71.8|75.4/73.3|66.4/86.2|74.8|74.3|84.1|57.9|62.3|
-------------------- hardware --------------------
Document 1:

"We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher."
-------------------- limitation_and_bias --------------------
Document 1:

The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher. Note that the BERT-Base model in this release is included for completeness only; it was re-trained under the same regime as the original model. For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs: - batch sizes: 8, 16, 32, 64, 128 - learning rates: 3e-4, 1e-4, 5e-5, 3e-5
-------------------- demo --------------------
Document 1:

[**2/128 (BERT-Tiny)**][2_128], [**4/256 (BERT-Mini)**][4_256], [**4/512 (BERT-Small)**][4_512], [**8/512 (BERT-Medium)**][8_512], [**12/768 (BERT-Base)**][12_768]
-------------------- input_format --------------------
Document 1:

"We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models."
-------------------- output_format --------------------
Document 1:

"You can download the 24 BERT miniatures either from the [official BERT Github page](https://github.com/google-research/bert/), or via HuggingFace from the links below:"

[{'datasets': ['2/128 (BERT-Tiny)', '4/256 (BERT-Mini)', '4/512 (BERT-Small)', '8/512 (BERT-Medium) 
', '12/768 (BERT-Base)'], 'license': 'apache-2.0', 'github': '[official BERT Github page](https://gi 
thub.com/google-research/bert/)', 'paper': '[Well-Read Students Learn Better: On the Importance of P 
re-training Compact Models](https://arxiv.org/abs/1908.08962)', 'upstream_model': '"BERT-Tiny" "2/12 
8 (BERT-Tiny)" "BERT-Mini" "4/256 (BERT-Mini)" "BERT-Small" "4/512 (BERT-Small)" "BERT-Medium" "8/51 
2 (BERT-Medium)" "BERT-Base" "12/768 (BERT-Base)"', 'parameter_count': '"2/128 (BERT-Tiny)", "4/256  
(BERT-Mini)", "4/512 (BERT-Small)", "8/512 (BERT-Medium)", "12/768 (BERT-Base)"', 'hyper_parameters' 
: {'epochs': '', 'batch_size': '8, 16, 32, 64, 128', 'learning_rate': '3e-4, 1e-4, 5e-5, 3e-5', 'opt 
imizer': ''}, 'evaluation': [{'test': 'CoLA', 'result': 0}, {'test': 'SST-2', 'result': 85.9}, {'tes 
t': 'MRPC', 'result': 81.1}, {'test': 'STS-B', 'result': 81.1}, {'test': 'QQP', 'result': 75.4}, {'t 
est': 'MNLI-m', 'result': 66.4}, {'test': 'MNLI-mm', 'result': 86.2}, {'test': 'QNLI(v2)', 'result': 
 74.8}, {'test': 'RTE', 'result': 84.1}, {'test': 'WNLI', 'result': 57.9}, {'test': 'AX', 'result':  
62.3}], 'hardware': '"We have shown that the standard BERT recipe (including model architecture and  
training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. Th 
e smaller BERT models are intended for environments with restricted computational resources. They ca 
n be fine-tuned in the same manner as the original BERT models. However, they are most effective in  
the context of knowledge distillation, where the fine-tuning labels are produced by a larger and mor 
e accurate teacher."', 'limitation_and_bias': 'The smaller BERT models are intended for environments 
 with restricted computational resources. They can be fine-tuned in the same manner as the original  
BERT models. However, they are most effective in the context of knowledge distillation, where the fi 
ne-tuning labels are produced by a larger and more accurate teacher. Note that the BERT-Base model i 
n this release is included for completeness only; it was re-trained under the same regime as the ori 
ginal model. For each task, we selected the best fine-tuning hyperparameters from the lists below, a 
nd trained for 4 epochs: - batch sizes: 8, 16, 32, 64, 128 - learning rates: 3e-4, 1e-4, 5e-5, 3e-5' 
, 'demo': '[**2/128 (BERT-Tiny)**][2_128], [**4/256 (BERT-Mini)**][4_256], [**4/512 (BERT-Small)**][ 
4_512], [**8/512 (BERT-Medium)**][8_512], [**12/768 (BERT-Base)**][12_768]', 'input_format': '"We ha 
ve shown that the standard BERT recipe (including model architecture and training objective) is effe 
ctive on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are i 
ntended for environments with restricted computational resources. They can be fine-tuned in the same 
 manner as the original BERT models."', 'output_format': '"You can download the 24 BERT miniatures e 
ither from the [official BERT Github page](https://github.com/google-research/bert/), or via Hugging 
Face from the links below:"'}]                                                                       

#####################cardiffnlp/twitter-roberta-base-sentiment########################

-------------------- datasets --------------------
Document 1:

datasets: - tweet_eval
------------------------------
Document 2:

~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark.
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval)."
-------------------- paper --------------------
Document 1:

"https://aclanthology.org/2020.findings-emnlp.148/"
------------------------------
Document 2:

Reference Paper: [_TweetEval_ (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf).
-------------------- upstream_model --------------------
Document 1:

"roBERTa-base model"
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

- tweet_eval
-------------------- hardware --------------------
Document 1:

This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark.
-------------------- limitation_and_bias --------------------
Document 1:

Reference Paper: [_TweetEval_ (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval). Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive.
-------------------- demo --------------------
Document 1:

"This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark." "Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive" "We just released a new sentiment analysis model trained on more recent and a larger quantity of tweets. See [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) and [TweetNLP](https://tweetnlp.org) for more details."
-------------------- input_format --------------------
Document 1:

- tweet_eval
-------------------- output_format --------------------
Document 1:

- tweet_eval
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['tweet_eval'], 'github': 'https://github.com/cardiffnlp/tweeteval', 'paper': 'https: 
//arxiv.org/pdf/2010.12421.pdf', 'upstream_model': 'roBERTa-base model', 'limitation_and_bias': 'Ref 
erence Paper: [_TweetEval_ (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). Git Repo 
: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval). Labels: 0 -> Negative; 1 
 -> Neutral; 2 -> Positive.', 'demo': 'This is a roBERTa-base model trained on ~58M tweets and finet 
uned for sentiment analysis with the TweetEval benchmark. Labels: 0 -> Negative; 1 -> Neutral; 2 ->  
Positive. We just released a new sentiment analysis model trained on more recent and a larger quanti 
ty of tweets. See [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter- 
roberta-base-sentiment-latest) and [TweetNLP](https://tweetnlp.org) for more details.', 'input_forma 
t': 'tweet_eval', 'output_format': 'tweet_eval'}]                                                    

#####################google/flan-t5-base########################

-------------------- datasets --------------------
Document 1:

"The model was trained on a mixture of tasks" NO_OUTPUT
-------------------- license --------------------
Document 1:

Creative Commons Attribution 4.0 International
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

- svakulenk0/qrecc
- djaym7/wiki_dialog
- deepmind/code_contests
- gsm8k
- aqua_rat
- esnli
- quasc
- qed
-------------------- paper --------------------
Document 1:

"original paper, figure 2"
------------------------------
Document 2:

"For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."
------------------------------
Document 3:

"See the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"hyper_parameters"
-------------------- evaluation --------------------
Document 1:

"The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: ![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png) For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."
------------------------------
Document 2:

6. [Evaluation](#evaluation)
------------------------------
Document 3:

"The model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2): ![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)"
-------------------- hardware --------------------
Document 1:

5. [Training Details]
-------------------- limitation_and_bias --------------------
Document 1:

4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)
------------------------------
Document 2:

"not filtered for explicit content or assessed for existing biases" "potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data"
-------------------- demo --------------------
Document 1:

"Find below some example scripts on how to use the model in `transformers`:"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': 'Creative Commons Attribution 4.0 International', 'github': '- svakule 
nk0/qrecc\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n-  
qed', 'paper': '"original paper, figure 2"', 'upstream_model': '', 'parameter_count': '', 'hyper_par 
ameters': [], 'evaluation': [{'test': 'The authors evaluated the model on various tasks covering sev 
eral languages (1836 in total). See the table below for some quantitative evaluation: ![image.png](h 
ttps://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png) For fu 
ll details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).', 'result': 0}] 
, 'hardware': '5. [Training Details]', 'limitation_and_bias': '4. [Bias, Risks, and Limitations](#bi 
as-risks-and-limitations)', 'demo': '"Find below some example scripts on how to use the model in `tr 
ansformers`:"', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': 
 ''}, {'datasets': [], 'license': 'apache-2.0', 'github': '', 'paper': '"For full details, please ch 
eck the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."', 'upstream_model': '', 'parameter_ 
count': '', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer':  
''}], 'evaluation': [{'test': '6. [Evaluation](#evaluation)', 'result': 0}], 'hardware': '', 'limita 
tion_and_bias': '"not filtered for explicit content or assessed for existing biases" "potentially vu 
lnerable to generating equivalently inappropriate content or replicating inherent biases in the unde 
rlying data"', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabu 
lary_size': ''}]                                                                                     

#####################google/flan-t5-large########################

-------------------- datasets --------------------
Document 1:

"The model was trained on a mixture of tasks" NO_OUTPUT
-------------------- license --------------------
Document 1:

Creative Commons Attribution 4.0 International
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

- svakulenk0/qrecc
- djaym7/wiki_dialog
- deepmind/code_contests
- gsm8k
- aqua_rat
- esnli
- quasc
- qed
-------------------- paper --------------------
Document 1:

"original paper, figure 2"
------------------------------
Document 2:

"For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."
------------------------------
Document 3:

"See the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hyper_parameters"
-------------------- evaluation --------------------
Document 1:

"The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: ![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png) For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."
------------------------------
Document 2:

6. [Evaluation](#evaluation)
------------------------------
Document 3:

"The model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2): ![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)"
-------------------- hardware --------------------
Document 1:

5. [Training Details]
-------------------- limitation_and_bias --------------------
Document 1:

4. [Bias, Risks, and Limitations]
------------------------------
Document 2:

"not filtered for explicit content or assessed for existing biases" "potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data"
-------------------- demo --------------------
Document 1:

"Find below some example scripts on how to use the model in `transformers`:"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'e 
valuation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_f 
ormat': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                         

#####################dslim/bert-base-NER-uncased########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['MNLI'], 'license': 'mit', 'github': 'https://github.com/huggingface/transformers',  
'paper': 'https://arxiv.org/abs/1910.03771', 'upstream_model': 'bert-base-uncased', 'parameter_count 
': '109482240', 'hyper_parameters': {'epochs': '3', 'batch_size': '32', 'learning_rate': '2e-5', 'op 
timizer': 'AdamW'}, 'evaluation': [{'test': 'accuracy', 'result': 0.843}], 'hardware': 'NVIDIA V100' 
, 'limitation_and_bias': 'The model may not perform well on tasks outside of the MNLI dataset.', 'de 
mo': 'You can use the model for natural language inference tasks.', 'input_format': 'A pair of sente 
nces', 'output_format': 'A single label', 'input_token_limit': '512', 'vocabulary_size': '30522'}]   

#####################Jean-Baptiste/camembert-ner########################

-------------------- datasets --------------------
Document 1:

"wikiner-fr dataset (~170 634  sentences)" and "Model was trained on wikiner-fr dataset"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Model was trained on wikiner-fr dataset (~170 634  sentences).", "Model was validated on emails/chat data and overperformed other models on this type of data specifically.", "In particular the model seems to work better on entity that don't start with an upper case."
-------------------- upstream_model --------------------
Document 1:

"camemBERT" NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

parameter_count: 170 634
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

Overall  
precision|recall|f1
-|-|-
0.8859|0.8971|0.8914  
By entity  
entity|precision|recall|f1
-|-|-|-
PER|0.9372|0.9598|0.9483
ORG|0.8099|0.8265|0.8181
LOC|0.8905|0.9005|0.8955
MISC|0.8175|0.8117|0.8146
------------------------------
Document 2:

"Model was trained on wikiner-fr dataset (~170 634  sentences). Model was validated on emails/chat data and overperformed other models on this type of data specifically."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

precision|recall|f1-|-|-; entity|precision|recall|f1-|-|-|-; PER|0.9372|0.9598|0.9483; ORG|0.8099|0.8265|0.8181; LOC|0.8905|0.9005|0.8955; MISC|0.8175|0.8117|0.8146
------------------------------
Document 2:

"Model was trained on wikiner-fr dataset (~170 634  sentences)." "Model was validated on emails/chat data and overperformed other models on this type of data specifically." "In particular the model seems to work better on entity that don't start with an upper case."
-------------------- demo --------------------
Document 1:

```python
from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained("Jean-Baptiste/camembert-ner")
model = AutoModelForTokenClassification.from_pretrained("Jean-Baptiste/camembert-ner")

from transformers import pipeline

nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy="simple")
nlp("Apple est créée le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constituée sous forme de société le 3 janvier 1977 à l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refléter la diversification de ses produits, le mot « computer » est retiré le 9 janvier 2015.")
```
-------------------- input_format --------------------
Document 1:

O,MISC,PER,ORG,LOC
-------------------- output_format --------------------
Document 1:

precision|recall|f1 -|-|- 0.8859|0.8971|0.8914 entity|precision|recall|f1 -|-|-|- PER|0.9372|0.9598|0.9483 ORG|0.8099|0.8265|0.8181 LOC|0.8905|0.9005|0.8955 MISC|0.8175|0.8117|0.8146
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"wikiner-fr dataset (~170 634  sentences)" "Model was validated on emails/chat data" "overperformed other models on this type of data specifically" "In particular the model seems to work better on entity that don't start with an upper case."

[{'datasets': ['wikiner-fr dataset'], 'license': 'mit', 'github': '', 'paper': '', 'upstream_model' 
: 'camemBERT', 'parameter_count': '170 634', 'hyper_parameters': {}, 'evaluation': [{'test': 'Overal 
l', 'result': 0.8914}, {'test': 'PER', 'result': 0.9483}, {'test': 'ORG', 'result': 0.8181}, {'test' 
: 'LOC', 'result': 0.8955}, {'test': 'MISC', 'result': 0.8146}], 'hardware': '', 'limitation_and_bia 
s': '', 'demo': '```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\ 
n\ntokenizer = AutoTokenizer.from_pretrained("Jean-Baptiste/camembert-ner")\nmodel = AutoModelForTok 
enClassification.from_pretrained("Jean-Baptiste/camembert-ner")\n\nfrom transformers import pipeline 
\n\nnlp = pipeline(\'ner\', model=model, tokenizer=tokenizer, aggregation_strategy="simple")\nnlp("A 
pple est créée le 1er avril 1976 dans le garage de la maison d\'enfance de Steve Jobs à Los Altos en 
 Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constituée sous forme de société l 
e 3 janvier 1977 à l\'origine sous le nom d\'Apple Computer, mais pour ses 30 ans et pour refléter l 
a diversification de ses produits, le mot « computer » est retiré le 9 janvier 2015.")\n```', 'input 
_format': 'O,MISC,PER,ORG,LOC', 'output_format': 'precision|recall|f1 -|-|- 0.8859|0.8971|0.8914 ent 
ity|precision|recall|f1 -|-|-|- PER|0.9372|0.9598|0.9483 ORG|0.8099|0.8265|0.8181 LOC|0.8905|0.9005| 
0.8955 MISC|0.8175|0.8117|0.8146', 'input_token_limit': '', 'vocabulary_size': ''}]                  

#####################LTP/small########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

"面向国内外大学、中科院各研究所以及个人研究者免费开放源代码，但如上述机构和个人将该平台用于商业目的（如企业合作项目等）则需要付费。除上述机构以外的企事业单位，如申请使用该平台，需付费。"
------------------------------
Document 2:

![LTP](https://img.shields.io/pypi/v/ltp?label=LTP) ![LTP-Core](https://img.shields.io/pypi/v/ltp-core?label=LTP-Core) ![LTP-Extension](https://img.shields.io/pypi/v/ltp-extension?label=LTP-Extension) ![LTP](https://img.shields.io/crates/v/ltp?label=LTP)
-------------------- github --------------------
Document 1:

![CODE SIZE](https://img.shields.io/github/languages/code-size/HIT-SCIR/ltp)
![CONTRIBUTORS](https://img.shields.io/github/contributors/HIT-SCIR/ltp)
![LAST COMMIT](https://img.shields.io/github/last-commit/HIT-SCIR/ltp)
[Python](python/interface/README.md)
[![LTP](https://img.shields.io/pypi/v/ltp?label=LTP)](https://pypi.org/project/ltp)
[![LTP-Core](https://img.shields.io/pypi/v/ltp-core?label=LTP-Core)](https://pypi.org/project/ltp-core)
[![LTP-Extension](https://img.shields.io/pypi/v/ltp-extension?label=LTP-Extension)](https://pypi.org
------------------------------
Document 2:

[Rust](rust/ltp), [C/C++](rust/ltp-cffi), [Rust](https://github.com/HIT-SCIR/libltp/tree/master/ltp-rs), [C++](https://github.com/HIT-SCIR/libltp/tree/master/ltp-cpp), [Java](https://github.com/HIT-SCIR/libltp/tree/master/ltp-java)
-------------------- paper --------------------
Document 1:

```bibtex
@article{che2020n,
title={N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models},
author={Che, Wanxiang and Feng, Yunlong and Qin, Libo and Liu, Ting},
journal={arXiv preprint arXiv:2009.11616},
year={2020}
```
------------------------------
Document 2:

"如果您在 LTP 基础上发表论文或取得科研成果，请您在发表论文和申报成果时声明“使用了哈工大社会计算与信息检索研究中心研制的语言技术平台（LTP）”.
-------------------- upstream_model --------------------
Document 1:

- 4.0.0
- 基于Pytorch 开发，原生 Python 接口
- 可根据需要自由选择不同速度和指标的模型
- 分词、词性、命名实体、依存句法、语义角色、语义依存6大任务
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

|                  深度学习模型                   |  分词   |  词性   | 命名实体  | 语义角色  | 依存句法  | 语义依存  | 速度(句/S) |
| :---------------------------------------: | :---: | :---: | :---: | :---: | :---: | :---: | :-----: |
|  [Base](https://huggingface.co/LTP/base)  | 98.7  | 98.5  | 95.4  | 80.6  | 89.5  | 75.2  |  39.12  |
| [Base1](https://huggingface.co/LTP/base1) | 99.22 | 98.73 | 96.39 | 79.28 | 89.57 | 76.57 |  --.--  |
| [Base2](https://huggingface.co/
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

"Python 接口" "可根据需要自由选择不同速度和指标的模型" "分词、词性、命名实体、依存句法、语义角色、语义依存6大任务"

[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
', 'input_format': '', 'output_format': ''}]                                                         
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653de499-737841981c96c3914de9e23b)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-gpt2/resolve/main/README.md. 

#####################CompVis/stable-diffusion-safety-checker########################

-------------------- datasets --------------------
Document 1:

"The CLIP model devlopers note in their [model card](https://huggingface.co/openai/clip-vit-large-patch14) :"
------------------------------
Document 2:

"CLIP" and "[CLIP Paper](https://arxiv.org/abs/2103.00020)" and "[Stable Diffusion Model Card](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md)"
-------------------- license --------------------
Document 1:

"More information needed" "License: More information needed"
------------------------------
Document 2:

"The primary intended users of these models are AI researchers." NO_OUTPUT
-------------------- github --------------------
Document 1:

[Stable Diffusion Model Card](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md)
------------------------------
Document 2:

"The primary intended users of these models are AI researchers." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
-------------------- paper --------------------
Document 1:

"The primary intended users of these models are AI researchers." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models." NO_OUTPUT
------------------------------
Document 2:

[CLIP Paper](https://arxiv.org/abs/2103.00020)
-------------------- upstream_model --------------------
Document 1:

"ViT-L/14 Transformer architecture as an image encoder" "masked self-attention Transformer as a text encoder"
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
------------------------------
Document 2:

"ViT-L/14 Transformer architecture" and "masked self-attention Transformer"
-------------------- hyper_parameters --------------------
Document 1:

"The primary intended users of these models are AI researchers." NO_OUTPUT
------------------------------
Document 2:

More information needed, Image Identification, [CLIP](https://huggingface.co/openai/clip-vit-large-patch14), [CLIP Paper](https://arxiv.org/abs/2103.00020), [Stable Diffusion Model Card](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md)
------------------------------
Document 3:

"ViT-L/14 Transformer architecture as an image encoder" and "masked self-attention Transformer as a text encoder"
-------------------- evaluation --------------------
Document 1:

"The primary intended users of these models are AI researchers." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

More information needed - **Model type:** Image Identification - [CLIP Paper](https://arxiv.org/abs/2103.00020) - [Stable Diffusion Model Card](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md)
-------------------- hardware --------------------
Document 1:

"ViT-L/14 Transformer architecture as an image encoder" and "masked self-attention Transformer as a text encoder"
------------------------------
Document 2:

More information needed, Image Identification, [CLIP](https://huggingface.co/openai/clip-vit-large-patch14), [CLIP Paper](https://arxiv.org/abs/2103.00020), [Stable Diffusion Model Card](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md)
-------------------- limitation_and_bias --------------------
Document 1:

"risks, biases and limitations of the model"
------------------------------
Document 2:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. The CLIP model devlopers note in their [model card](https://huggingface.co/openai/clip-vit-large-patch14) : We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from Fairface into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. We also tested the performance of CLIP on
------------------------------
Document 3:

"The primary intended users of these models are AI researchers." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

"ViT-L/14 Transformer architecture as an image encoder" and "masked self-attention Transformer as a text encoder"
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT

[{'datasets': ['CLIP'], 'license': 'More information needed', 'github': '[Stable Diffusion Model Ca 
rd](https://github.com/CompVis/stable-diffusion/blob/main/Stable_Diffusion_v1_Model_Card.md)', 'pape 
r': '[CLIP Paper](https://arxiv.org/abs/2103.00020)', 'upstream_model': 'ViT-L/14 Transformer archit 
ecture as an image encoder, masked self-attention Transformer as a text encoder', 'parameter_count': 
 'parameter_count NO_OUTPUT', 'hyper_parameters': {'epochs': 'More information needed', 'batch_size' 
: 'Image Identification', 'learning_rate': '[CLIP](https://huggingface.co/openai/clip-vit-large-patc 
h14)', 'optimizer': '[CLIP Paper](https://arxiv.org/abs/2103.00020)'}, 'evaluation': [{'test': 'More 
 information needed', 'result': 0}], 'hardware': 'ViT-L/14 Transformer architecture as an image enco 
der, masked self-attention Transformer as a text encoder', 'limitation_and_bias': 'risks, biases and 
 limitations of the model', 'demo': '', 'input_format': 'ViT-L/14 Transformer architecture as an ima 
ge encoder, masked self-attention Transformer as a text encoder', 'output_format': 'output_format NO 
_OUTPUT'}]                                                                                           

#####################hustvl/yolos-tiny########################

-------------------- datasets --------------------
Document 1:

ImageNet-1k, COCO 2017 object detection
------------------------------
Document 2:

"ImageNet-1k" and "COCO"
------------------------------
Document 3:

datasets:
- coco
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"this repository" https://github.com/hustvl/YOLOS
-------------------- paper --------------------
Document 1:

"original paper"
------------------------------
Document 2:

"You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection"
------------------------------
Document 3:

`title = {You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection}, journal = {CoRR}, volume = {abs/2106.00666}, year = {2021}, url = {https://arxiv.org/abs/2106.00666}, eprinttype = {arXiv}, eprint = {2106.00666}`
-------------------- upstream_model --------------------
Document 1:

upstream_model: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al. and first released in [this repository](https://github.com/hustvl/YOLOS).
------------------------------
Document 2:

upstream_model ImageNet-1k
-------------------- parameter_count --------------------
Document 1:

"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images)"
-------------------- hyper_parameters --------------------
Document 1:

"300 epochs on ImageNet-1k" and "300 epochs on COCO"
------------------------------
Document 2:

"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images)" "You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection" "first released in [this repository](https://github.com/hustvl/YOLOS)" "The team releasing YOLOS did not write a model card for this model"
-------------------- evaluation --------------------
Document 1:

**28.7**
------------------------------
Document 2:

"The model was pre-trained for 300 epochs on ImageNet-1k and fine-tuned for 300 epochs on COCO."
-------------------- hardware --------------------
Document 1:

ImageNet-1k, COCO
-------------------- limitation_and_bias --------------------
Document 1:

"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images)", "[You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al.", "[this repository](https://github.com/hustvl/YOLOS)".
-------------------- demo --------------------
Document 1:

"model hub" "huggingface.co/models?search=hustvl/yolos"
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

PyTorch
-------------------- input_preprocessing --------------------
Document 1:

YolosImageProcessor.from_pretrained("hustvl/yolos-tiny"), inputs = image_processor(images=image, return_tensors="pt"), image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]
-------------------- input_size --------------------
Document 1:

"inputs = image_processor(images=image, return_tensors="pt")"
NO_OUTPUT
-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': ['ImageNet-1k', 'COCO 2017 object detection'], 'license': 'apache-2.0', 'github': 'ht 
tps://github.com/hustvl/YOLOS', 'paper': 'https://arxiv.org/abs/2106.00666', 'upstream_model': 'YOLO 
S model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the p 
aper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](http 
s://arxiv.org/abs/2106.00666) by Fang et al. and first released in [this repository](https://github. 
com/hustvl/YOLOS).', 'parameter_count': 'YOLOS model fine-tuned on COCO 2017 object detection (118k  
annotated images)', 'hyper_parameters': {'epochs': '300', 'batch_size': '', 'learning_rate': '', 'op 
timizer': ''}, 'evaluation': [{'test': '', 'result': 28.7}], 'hardware': 'ImageNet-1k, COCO', 'limit 
ation_and_bias': 'YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images), [You 
 Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv 
.org/abs/2106.00666) by Fang et al., [this repository](https://github.com/hustvl/YOLOS).', 'demo': ' 
model hub, huggingface.co/models?search=hustvl/yolos', 'input_format': '', 'output_format': 'PyTorch 
', 'input_preprocessing': 'YolosImageProcessor.from_pretrained("hustvl/yolos-tiny"), inputs = image_ 
processor(images=image, return_tensors="pt"), image_processor.post_process_object_detection(outputs, 
 threshold=0.9, target_sizes=target_sizes)[0]', 'input_size': 'inputs = image_processor(images=image 
, return_tensors="pt")', 'num_of_classes_for_classification': '', 'trigger_word': ''}]               

#####################shibing624/text2vec-base-chinese########################

-------------------- datasets --------------------
Document 1:

datasets: - shibing624/nli_zh
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"https://github.com/shibing624/text2vec" and "url = {https://github.com/shibing624/text2vec},"
------------------------------
Document 2:

"https://github.com/UKPLab/sentence-transformers", "shibing624/text2vec-base-chinese"
------------------------------
Document 3:

language: - zh, license: apache-2.0, library_name: transformers, tags: - text2vec - feature-extraction - sentence-similarity - transformers, datasets: - shibing624/nli_zh, metrics: - spearmanr, pipeline_tag: sentence-similarity
-------------------- paper --------------------
Document 1:

"If you find this model helpful, feel free to cite: ```bibtex @software{text2vec, author = {Xu Ming}, title = {text2vec: A Tool for Text to Vector}, year = {2022}, url = {https://github.com/shibing624/text2vec}, }```"
-------------------- upstream_model --------------------
Document 1:

`hfl/chinese-macbert-base`, upstream_model
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_mean_tokens': True
-------------------- hyper_parameters --------------------
Document 1:

- max_seq_length: 128 - best epoch: 5 - sentence embedding dim: 768
------------------------------
Document 2:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_mean_tokens': True
-------------------- evaluation --------------------
Document 1:

- 结果评测指标：spearman系数
- `shibing624/text2vec-base-chinese`模型，是用CoSENT方法训练，基于`hfl/chinese-macbert-base`在中文STS-B数据训练得到，并在中文STS-B测试集评估达到较好效果，运行[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)代码可训练模型，模型文件已经上传HF model
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures the semantic information."
------------------------------
Document 2:

"pip install -U text2vec"
"from text2vec import SentenceModel"
"sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']"
"model = SentenceModel('shibing624/text2vec-base-chinese')"
"embeddings = model.encode(sentences)"
"print(embeddings)"
-------------------- input_format --------------------
Document 1:

input_format: text
------------------------------
Document 2:

input_format: text2vec
------------------------------
Document 3:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_mean_tokens': True
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
------------------------------
Document 2:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_mean_tokens': True
-------------------- input_token_limit --------------------
Document 1:

input_token_limit 256
------------------------------
Document 2:

'max_seq_length': 128
-------------------- vocabulary_size --------------------


[{'datasets': ['shibing624/nli_zh'], 'license': 'apache-2.0', 'github': 'https://github.com/shibing 
624/text2vec', 'paper': 'If you find this model helpful, feel free to cite: ```bibtex @software{text 
2vec, author = {Xu Ming}, title = {text2vec: A Tool for Text to Vector}, year = {2022}, url = {https 
://github.com/shibing624/text2vec}, }```', 'upstream_model': 'hfl/chinese-macbert-base', 'parameter_ 
count': "'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mo 
de_mean_tokens': True", 'hyper_parameters': [{'max_seq_length': '128', 'best_epoch': '5', 'sentence_ 
embedding_dim': '768'}], 'evaluation': [{'result_metric': 'spearman', 'result_value': 0.8}], 'demo': 
 'Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, i 
t ouptuts a vector which captures the semantic information.', 'input_format': 'text', 'output_format 
': 'NO_OUTPUT'}]                                                                                     

#####################prajjwal1/bert-small########################

-------------------- datasets --------------------
Document 1:

- [official Google BERT repository](https://github.com/google-research/bert)
- `Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962))
- `Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics` ([arXiv](https://arxiv.org/abs/2110.01518))
- `prajjwal1/bert-small` (L=4, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-small)
- `prajjwal1/bert-tiny` (L=2, H=128) [Model Link](https://huggingface.co/prajjwal1/bert-tiny)
- `prajjwal1/bert-mini` (L=4, H=256) [Model Link](https://huggingface.co/prajjwal1/bert-mini)
- `prajjwal1/bert-medium` (
-------------------- license --------------------
Document 1:

- license:
- mit
-------------------- github --------------------
Document 1:

- [official Google BERT repository](https://github.com/google-research/bert)
- [bert-tiny](https://huggingface.co/prajjwal1/bert-small)
- [bert-mini]([bert-small](https://huggingface.co/prajjwal1/bert-mini)
- [bert-medium](https://huggingface.co/prajjwal1/bert-medium)
- [this Github repository](https://github.com/prajjwal1/generalize_lm_nli)
-------------------- paper --------------------
Document 1:

`Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962)) and `Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics` ([arXiv](https://arxiv.org/abs/2110.01518))
-------------------- upstream_model --------------------
Document 1:

- `prajjwal1/bert-small` (L=4, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-small)
- `prajjwal1/bert-tiny` (L=2, H=128) [Model Link](https://huggingface.co/prajjwal1/bert-tiny)
- `prajjwal1/bert-mini` (L=4, H=256) [Model Link](https://huggingface.co/prajjwal1/bert-mini)
- `prajjwal1/bert-medium` (L=8, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-medium)
- [official Google BERT repository](https://github.com/google-research/bert)
-------------------- parameter_count --------------------
Document 1:

"prajjwal1/bert-small (L=4, H=512)"
-------------------- hyper_parameters --------------------
Document 1:

- `prajjwal1/bert-small` (L=4, H=512) 
- `prajjwal1/bert-tiny` (L=2, H=128) 
- `prajjwal1/bert-mini` (L=4, H=256) 
- `prajjwal1/bert-medium` (L=8, H=512)
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

- `prajjwal1/bert-small` (L=4, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-small)
- `prajjwal1/bert-tiny` (L=2, H=128) [Model Link](https://huggingface.co/prajjwal1/bert-tiny)
- `prajjwal1/bert-mini` (L=4, H=256) [Model Link](https://huggingface.co/prajjwal1/bert-mini)
- `prajjwal1/bert-medium` (L=8, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-medium)
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['prajjwal1/bert-small', 'prajjwal1/bert-tiny', 'prajjwal1/bert-mini', 'prajjwal1/ber 
t-medium'], 'license': 'mit', 'github': 'https://github.com/prajjwal1/generalize_lm_nli', 'paper': ' 
Well-Read Students Learn Better: On the Importance of Pre-training Compact Models (arxiv)', 'upstrea 
m_model': 'official Google BERT repository', 'parameter_count': 'prajjwal1/bert-small (L=4, H=512)', 
 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evalua 
tion': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'prajjwal1/bert-small (L=4, H=512) [Mo 
del Link](https://huggingface.co/prajjwal1/bert-small)', 'input_format': '', 'output_format': ''}]   

#####################facebook/wav2vec2-base-960h########################

-------------------- datasets --------------------
Document 1:

datasets:
- librispeech_asr
dataset:
name: LibriSpeech (clean)
type: librispeech_asr
config: clean
split: test
args:
language: en
dataset:
name: LibriSpeech (other)
type: librispeech_asr
config: other
split: test
args:
language: en
------------------------------
Document 2:

The base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.
------------------------------
Document 3:

"patrickvonplaten/librispeech_asr_dummy"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20."
------------------------------
Document 2:

"facebook/wav2vec2-base-960h"
-------------------- paper --------------------
Document 1:

[Paper](https://arxiv.org/abs/2006.11477)
-------------------- upstream_model --------------------
Document 1:

Facebook's Wav2Vec2, The base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio, Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli, We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler, wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned, Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets, When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data, Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER, This demonstrates the feasibility of speech recognition with limited amounts of labeled data, The original model can be found under https://github.com/pytor
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")" and "Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")"
-------------------- evaluation --------------------
Document 1:

"Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h").to("cuda")", "WER:", "| "clean" | "other" |", "| 3.4 | 8.6 |"
------------------------------
Document 2:

- type: automatic-speech-recognition
- name: Automatic Speech Recognition
- dataset:
name: LibriSpeech (clean)
type: librispeech_asr
config: clean
split: test
args:
language: en
metrics:
- type: wer
value: 3.4
name: Test WER
- task:
type: automatic-speech-recognition
name: Automatic Speech Recognition
dataset:
name: LibriSpeech (other)
type: librispeech_asr
config: other
split: test
args:
language: en
metrics:
- type: wer
value: 8.6
name: Test WER
-------------------- hardware --------------------
Document 1:

"The base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio."
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

- example_title: Librispeech sample 1
src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- example_title: Librispeech sample 2
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
------------------------------
Document 2:

"The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20."
------------------------------
Document 3:

```python from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC from datasets import load_dataset import torch # load model and tokenizer processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h") model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h") # load dummy dataset and read soundfiles ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation") # tokenize input_values = processor(ds[0]["audio"]["array"], return_tensors="pt", padding="longest").input_values  # Batch size 1 # retrieve logits logits = model(input_values).logits # take argmax and decode predicted_ids = torch.argmax(logits, dim=-1) transcription = processor.batch_decode(predicted_ids)```
-------------------- input_format --------------------
Document 1:

- example_title: Librispeech sample 1
src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- example_title: Librispeech sample 2
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
------------------------------
Document 2:

"make sure that your speech input is also sampled at 16Khz." input_format: 16Khz
------------------------------
Document 3:

"return_tensors="pt", padding="longest"
-------------------- output_format --------------------
Document 1:

16kHz sampled speech audio, https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20
-------------------- sample_rate --------------------
Document 1:

16kHz sampled speech audio
-------------------- WER --------------------
Document 1:

"1.8/3.3 WER on the clean/other test sets"
------------------------------
Document 2:

"WER:", wer(result["text"], result["transcription"])
------------------------------
Document 3:

- type: wer
value: 3.4
name: Test WER

[{'datasets': ['librispeech_asr'], 'license': 'apache-2.0', 'github': 'https://github.com/pytorch/f 
airseq/tree/master/examples/wav2vec#wav2vec-20', 'paper': '[Paper](https://arxiv.org/abs/2006.11477) 
', 'upstream_model': "Facebook's Wav2Vec2", 'parameter_count': '', 'hyper_parameters': {'epochs': '' 
, 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'clean', 'result' 
: 3.4}, {'test': 'other', 'result': 8.6}], 'hardware': '', 'limitation_and_bias': '', 'demo': '- exa 
mple_title: Librispeech sample 1\nsrc: https://cdn-media.huggingface.co/speech_samples/sample1.flac\ 
n- example_title: Librispeech sample 2\nsrc: https://cdn-media.huggingface.co/speech_samples/sample2 
.flac', 'input_format': '- example_title: Librispeech sample 1\nsrc: https://cdn-media.huggingface.c 
o/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\nsrc: https://cdn-media.hugging 
face.co/speech_samples/sample2.flac', 'output_format': '16kHz sampled speech audio', 'sample_rate':  
'16kHz sampled speech audio', 'WER': '1.8/3.3 WER on the clean/other test sets'}]                    

#####################alexandrainst/scandi-nli-large########################

-------------------- datasets --------------------
Document 1:

- [DanFEVER](https://aclanthology.org/2021.nodalida-main.pdf#page=439) 
- machine translated versions of [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) and [CommitmentBank](https://doi.org/10.18148/sub/2019.v23i2.601) 
- machine translated versions of [FEVER](https://aclanthology.org/N18-1074/) and [Adversarial NLI](https://aclanthology.org/2020.acl-main.441/) 
- [this gist](https://gist.github.com/saattrupdan/1cb8379232fdec6e943dc84595a85e7c) 
- [DanFEVER](https://aclanthology.org/2021.nodalida-main.pdf#page=439) 
- machine translated versions of [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) 
- [Github
------------------------------
Document 2:

NbAiLab/nb-bert-large, alexandrainst/scandi-nli-large, [alexandrainst/scandi-nli-base](https://huggingface.co/alexandrainst/scandi-nli-base), [alexandrainst/scandi-nli-small](https://huggingface.co/alexandrainst/scandi-nli-small)
------------------------------
Document 3:

[DanFEVER dataset](https://aclanthology.org/2021.nodalida-main.pdf#page=439), [this gist](https://gist.github.com/saattrupdan/1cb8379232fdec6e943dc84595a85e7c)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[Github repository](https://github.com/alexandrainst/ScandiNLI) and [this Weights and Biases report](https://wandb.ai/saattrupdan/huggingface/reports/ScandiNLI--VmlldzozMDQyOTk1?accessToken=r9crgxqvvigy2hatdjeobzwipz7f3id5vqg8ooksljhfw6wl0hv1b05asypsfj9v)
------------------------------
Document 2:

- strombergnlp/danfever
- KBLab/overlim
- MoritzLaurer/multilingual-NLI-26lang-2mil7
------------------------------
Document 3:

[NbAiLab/nb-bert-large](https://huggingface.co/NbAiLab/nb-bert-large), [alexandrainst/scandi-nli-base](https://huggingface.co/alexandrainst/scandi-nli-base), [alexandrainst/scandi-nli-small](https://huggingface.co/alexandrainst/scandi-nli-small)
-------------------- paper --------------------
Document 1:

"A demo of the large model can be found in [this Hugging Face Space](https://huggingface.co/spaces/alexandrainst/zero-shot-classification) - check it out!" NO_OUTPUT
------------------------------
Document 2:

[DanFEVER](https://aclanthology.org/2021.nodalida-main.pdf#page=439), [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/), [CommitmentBank](https://doi.org/10.18148/sub/2019.v23i2.601), [FEVER](https://aclanthology.org/N18-1074/), [Adversarial NLI](https://aclanthology.org/2020.acl-main.441/), [this gist](https://gist.github.com/saattrupdan/1cb8379232fdec6e943dc84595a85e7c), [DanFEVER](https://aclanthology.org/2021.nodalida-main.pdf#page=439), [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/), [this Weights and Biases report](https://wandb.ai/saattrupdan/huggingface/reports/ScandiN
-------------------- upstream_model --------------------
Document 1:

NbAiLab/nb-bert-large
------------------------------
Document 2:

"model="alexandrainst/scandi-nli-large""
-------------------- parameter_count --------------------
Document 1:

"354M", "279M", "178M", "22M"
------------------------------
Document 2:

**354M**
-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 2e-05 - train_batch_size: 2 - eval_batch_size: 2 - seed: 4242 - gradient_accumulation_steps: 16 - total_train_batch_size: 32 - optimizer: Adam with betas=(0.9, 0.999) and epsilon=1e-08 - lr_scheduler_type: linear - lr_scheduler_warmup_steps: 500 - max_steps: 50,000
-------------------- evaluation --------------------
Document 1:

We assess the models both on their aggregate Scandinavian performance, as well as their language-specific Danish, Swedish and Norwegian Bokmål performance. In all cases, we report Matthew's Correlation Coefficient (MCC), macro-average F1-score as well as accuracy.
------------------------------
Document 2:

"We use a test split of the [DanFEVER dataset](https://aclanthology.org/2021.nodalida-main.pdf#page=439) to evaluate the Danish performance of the models."

"| **Model** | **MCC** | **Macro-F1** | **Accuracy** | **Number of Parameters** |
| :-------- | :------------ | :--------- | :----------- | :----------- |
| `alexandrainst/scandi-nli-large` (this) | **73.80%** | **58.41%** | **86.98%** | 354M |
| [`MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7`](https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7) | 68.37% | 57.10% | 83.25% | 279M |
| [`alexandrainst/scandi-nli-base`](https://
------------------------------
Document 3:

"We use the test split of the machine translated version of the [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) dataset to evaluate the Norwegian performance of the models."

"| **Model** | **MCC** | **Macro-F1** | **Accuracy** | **Number of Parameters** |
| :-------- | :------------ | :--------- | :----------- | :----------- |
| `alexandrainst/scandi-nli-large` (this) | **70.61%** | **80.43%** | **80.36%** | 354M |
| [`joeddav/xlm-roberta-large-xnli`](https://huggingface.co/joeddav/xlm-roberta-large-xnli) | 67.99% | 78.68% | 78.60% | 560M |
| [`alexandrainst/scandi-nli-base`](https://huggingface.co/alexandrainst/scandi-nli-base) | 67
-------------------- hardware --------------------
Document 1:

NbAiLab/nb-bert-large, alexandrainst/scandi-nli-large, alexandrainst/scandi-nli-base, alexandrainst/scandi-nli-small
-------------------- limitation_and_bias --------------------
Document 1:

DanFEVER, MultiNLI, CommitmentBank, FEVER, Adversarial NLI, this gist, DanFEVER, MultiNLI, Norwegian Bokmål, Github repository, Weights and Biases report
-------------------- demo --------------------
Document 1:

"A demo of the large model can be found in [this Hugging Face Space](https://huggingface.co/spaces/alexandrainst/zero-shot-classification) - check it out!"
------------------------------
Document 2:

license: apache-2.0
datasets:
- strombergnlp/danfever
- KBLab/overlim
- MoritzLaurer/multilingual-NLI-26lang-2mil7
pipeline_tag: zero-shot-classification
widget:
- example_title: Danish
text: Mexicansk bokser advarer Messi - 'Du skal bede til gud, om at jeg ikke finder
dig'
candidate_labels: sundhed, politik, sport, religion
- example_title: Norwegian
text: Regjeringen i Russland hevder Norge fører en politikk som vil føre til opptrapping
i Arktis og «den endelige ødeleggelsen av russisk-norske relasjoner».
candidate_labels: helse, politikk, sport, religion
- example_title: Swedish
text: Så luras kroppens immunförsvar att bota cancer
candidate_labels: hälsa, politik, sport, religion
-------------------- input_format --------------------
Document 1:

[DanFEVER](https://aclanthology.org/2021.nodalida-main.pdf#page=439), [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/), [CommitmentBank](https://doi.org/10.18148/sub/2019.v23i2.601), [FEVER](https://aclanthology.org/N18-1074/), [Adversarial NLI](https://aclanthology.org/2020.acl-main.441/), [this gist](https://gist.github.com/saattrupdan/1cb8379232fdec6e943dc84595a85e7c), [DanFEVER](https://aclanthology.org/2021.nodalida-main.pdf#page=439), [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/), [Github repository](https://github.com/alexandrainst/ScandiNLI), [this Weights and Biases report](https://
------------------------------
Document 2:

"model="alexandrainst/scandi-nli-large""
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

input_token_limit NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

"NbAiLab/nb-bert-large" and "alexandrainst/scandi-nli-large"
------------------------------
Document 2:

"alexandrainst/scandi-nli-large" | 354M
------------------------------
Document 3:

"alexandrainst/scandi-nli-large" | 354M | "alexandrainst/scandi-nli-base" | 178M | "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7" | 279M | "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli" | 279M | "NbAiLab/nb-bert-base-mnli" | 178M | "alexandrainst/scandi-nli-small" | 22M

[{'datasets': ['DanFEVER', 'MultiNLI', 'CommitmentBank', 'FEVER', 'Adversarial NLI'], 'license': 'a 
pache-2.0', 'github': 'https://github.com/alexandrainst/ScandiNLI', 'paper': 'NO_OUTPUT', 'upstream_ 
model': 'NbAiLab/nb-bert-large', 'parameter_count': '354M', 'hyper_parameters': {'epochs': '', 'batc 
h_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_an 
d_bias': '', 'demo': 'A demo of the large model can be found in [this Hugging Face Space](https://hu 
ggingface.co/spaces/alexandrainst/zero-shot-classification) - check it out!', 'input_format': '[DanF 
EVER](https://aclanthology.org/2021.nodalida-main.pdf#page=439), [MultiNLI](https://cims.nyu.edu/~sb 
owman/multinli/), [CommitmentBank](https://doi.org/10.18148/sub/2019.v23i2.601), [FEVER](https://acl 
anthology.org/N18-1074/), [Adversarial NLI](https://aclanthology.org/2020.acl-main.441/)', 'output_f 
ormat': '', 'input_token_limit': 'NO_OUTPUT', 'vocabulary_size': '"NbAiLab/nb-bert-large" and "alexa 
ndrainst/scandi-nli-large"'}, {'datasets': ['strombergnlp/danfever', 'KBLab/overlim', 'MoritzLaurer/ 
multilingual-NLI-26lang-2mil7'], 'license': 'apache-2.0', 'github': '', 'paper': '', 'upstream_model 
': '"model=\\"alexandrainst/scandi-nli-large\\""', 'parameter_count': '354M', 'hyper_parameters': {' 
epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware':  
'', 'limitation_and_bias': '', 'demo': '', 'input_format': '"model=\\"alexandrainst/scandi-nli-large 
\\""', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': '"alexandrainst/scandi-nli-l 
arge" | 354M'}, {'datasets': ['DanFEVER dataset'], 'license': '', 'github': '', 'paper': '', 'upstre 
am_model': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning 
_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
', 'input_format': '[DanFEVER dataset](https://aclanthology.org/2021.nodalida-main.pdf#page=439)', ' 
output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                 

#####################facebook/bart-base########################

-------------------- datasets --------------------
Document 1:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)"
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"BART model pre-trained on English language" and "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
------------------------------
Document 2:

"DBLP:journals/corr/abs-1910-13461"
-------------------- upstream_model --------------------
Document 1:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)"
-------------------- parameter_count --------------------
Document 1:

"BART model pre-trained on English language" and "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."
------------------------------
Document 2:

'facebook/bart-base', 'return_tensors="pt"'
NO_OUTPUT
------------------------------
Document 3:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension" and "this repository".
-------------------- evaluation --------------------
Document 1:

"BART model pre-trained on English language. It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"BART model pre-trained on English language" and "The team releasing BART did not write a model card for this model"
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=bart"
------------------------------
Document 2:

"BART model pre-trained on English language" and "[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)" and "[this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)".
-------------------- input_format --------------------
Document 1:

return_tensors="pt" input_format
------------------------------
Document 2:

"BART model pre-trained on English language" "first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/bart)" NO_OUTPUT
-------------------- output_format --------------------


[{'datasets': ['BART model pre-trained on English language'], 'license': 'apache-2.0', 'github': '' 
, 'paper': 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Trans 
lation, and Comprehension', 'upstream_model': 'BART model pre-trained on English language', 'paramet 
er_count': 'BART model pre-trained on English language', 'hyper_parameters': {'epochs': '', 'batch_s 
ize': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_b 
ias': 'The team releasing BART did not write a model card for this model', 'demo': 'https://huggingf 
ace.co/models?search=bart', 'input_format': 'return_tensors="pt"', 'output_format': ''}]             

#####################cl-tohoku/bert-base-japanese-whole-word-masking########################

-------------------- datasets --------------------
Document 1:

"The model is trained on Japanese Wikipedia as of September 1, 2019.", "[WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.", "The text files used for the training are 2.6GB in size, consisting of approximately 17M sentences."
-------------------- license --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
------------------------------
Document 2:

license: cc-by-sa-4.0
------------------------------
Document 3:

"The codes for the pretraining are available at [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0)."
-------------------- github --------------------
Document 1:

"Creative Commons Attribution-ShareAlike 3.0"
------------------------------
Document 2:

"WikiExtractor https://github.com/attardi/wikiextractor"
-------------------- paper --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
------------------------------
Document 2:

TensorFlow Research Cloud
------------------------------
Document 3:

"The codes for the pretraining are available at [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0)."
-------------------- upstream_model --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
-------------------- parameter_count --------------------
Document 1:

parameter_count: 256 instances per batch, 1M training steps.
------------------------------
Document 2:

parameter_count NO_OUTPUT
------------------------------
Document 3:

parameter_count 12 layers, 768 dimensions of hidden states, 12 attention heads
-------------------- hyper_parameters --------------------
Document 1:

"512 tokens per instance, 256 instances per batch, and 1M training steps." "Whole Word Masking"
------------------------------
Document 2:

"12 layers, 768 dimensions of hidden states, and 12 attention heads."
-------------------- evaluation --------------------
Document 1:

"For the training of the MLM (masked language modeling) objective, we introduced the **Whole Word Masking** in which all of the subword tokens corresponding to a single word (tokenized by MeCab) are masked at once."
-------------------- hardware --------------------
Document 1:

Cloud TPUs
------------------------------
Document 2:

"WikiExtractor" and "2.6GB in size"
------------------------------
Document 3:

"512 tokens per instance, 256 instances per batch, and 1M training steps."
-------------------- limitation_and_bias --------------------
Document 1:

"The model is trained on Japanese Wikipedia as of September 1, 2019.", "[WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.", "The text files used for the training are 2.6GB in size, consisting of approximately 17M sentences."
-------------------- demo --------------------
Document 1:

"Creative Commons Attribution-ShareAlike 3.0"
------------------------------
Document 2:

"WikiExtractor" and "2.6GB in size, consisting of approximately 17M sentences."
------------------------------
Document 3:

TensorFlow Research Cloud program.
-------------------- input_format --------------------
Document 1:

"plain texts from a dump file of Wikipedia articles" "2.6GB in size, consisting of approximately 17M sentences"
------------------------------
Document 2:

word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.
------------------------------
Document 3:

"512 tokens per instance, 256 instances per batch, and 1M training steps.", "Whole Word Masking"
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
------------------------------
Document 2:

"word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization" output_format
-------------------- input_token_limit --------------------
Document 1:

"512 tokens per instance" and "Whole Word Masking"
-------------------- vocabulary_size --------------------
Document 1:

The vocabulary size is 32000.
------------------------------
Document 2:

"Whole Word Masking"

[{'datasets': ['Japanese Wikipedia'], 'license': 'Creative Commons Attribution-ShareAlike 3.0', 'gi 
thub': 'https://github.com/cl-tohoku/bert-japanese/tree/v1.0', 'paper': '', 'upstream_model': '', 'p 
arameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias' 
: '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size 
': ''}]                                                                                              

#####################google/flan-t5-xxl########################

-------------------- datasets --------------------
Document 1:

"The model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2): ![table.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan_t5_tasks.png)"
-------------------- license --------------------
Document 1:

Creative Commons Attribution 4.0 International
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

- svakulenk0/qrecc
- djaym7/wiki_dialog
- deepmind/code_contests
- gsm8k
- aqua_rat
- esnli
- quasc
- qed
-------------------- paper --------------------
Document 1:

"See the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details."
------------------------------
Document 2:

"research paper", "https://arxiv.org/pdf/2210.11416.pdf", "Table 3"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"pretrained T5 (Raffel et al., 2020)" and "one fine-tuned Flan model per T5 model size."
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------
Document 1:

6. [Evaluation](#evaluation)
------------------------------
Document 2:

The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: ![image.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan_t5_evals_lang.png) For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).
------------------------------
Document 3:

The model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2): ![table.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan_t5_tasks.png)
-------------------- hardware --------------------
Document 1:

5. [Training Details](#training-details)
------------------------------
Document 2:

"<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg" alt="drawing" width="600"/>"
-------------------- limitation_and_bias --------------------
Document 1:

4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)
------------------------------
Document 2:

"not filtered for explicit content or assessed for existing biases" "potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data"
------------------------------
Document 3:

"The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models"
-------------------- demo --------------------
Document 1:

"Find below some example scripts on how to use the model in `transformers`:"
------------------------------
Document 2:

license: apache-2.0
tags:
- text2text-generation
datasets:
- svakulenk0/qrecc
- taskmaster2
- djaym7/wiki_dialog
- deepmind/code_contests
- lambada
- gsm8k
- aqua_rat
- esnli
- quasc
- qed
widget:
- text: 'Translate to German:  My name is Arthur'
example_title: Translation
- text: Please answer to the following question. Who is going to be the next Ballon
d'or?
example_title: Question Answering
- text: 'Q: Can Geoffrey Hinton have a conversation with George Washington? Give the
rationale before answering.'
example_title: Logical reasoning
- text: Please answer the following question. What is the boiling point of Nitrogen?
example_title: Scientific knowledge
- text: Answer the following yes/no question. Can you write a whole Haiku in a single
tweet?
example_title: Yes/no question
- text: Answer the following yes/no question
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['svakulenk0/qrecc', 'djaym7/wiki_dialog', 'deepmind/code_contests', 'gsm8k', 'aqua_r 
at', 'esnli', 'quasc', 'qed'], 'license': 'Creative Commons Attribution 4.0 International', 'github' 
: '- svakulenk0/qrecc\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\ 
n- quasc\n- qed', 'paper': 'See the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for furth 
er details.', 'upstream_model': '', 'parameter_count': '"pretrained T5 (Raffel et al., 2020)" and "o 
ne fine-tuned Flan model per T5 model size."', 'hyper_parameters': {'epochs': '', 'batch_size': '',  
'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '5. [Training Details](#trainin 
g-details)', 'limitation_and_bias': '4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)' 
, 'demo': '"Find below some example scripts on how to use the model in `transformers`:"', 'input_for 
mat': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}, {'datasets': ['svaku 
lenk0/qrecc', 'taskmaster2', 'djaym7/wiki_dialog', 'deepmind/code_contests', 'lambada', 'gsm8k', 'aq 
ua_rat', 'esnli', 'quasc', 'qed'], 'license': 'apache-2.0', 'github': '', 'paper': '"research paper" 
, "https://arxiv.org/pdf/2210.11416.pdf", "Table 3"', 'upstream_model': '', 'parameter_count': '', ' 
hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluati 
on': [{'test': '', 'result': 0}], 'hardware': '<img src="https://huggingface.co/datasets/huggingface 
/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg" alt="drawing" widt 
h="600"/>', 'limitation_and_bias': '"not filtered for explicit content or assessed for existing bias 
es" "potentially vulnerable to generating equivalently inappropriate content or replicating inherent 
 biases in the underlying data"', 'demo': "license: apache-2.0\ntags:\n- text2text-generation\ndatas 
ets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n 
- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\nwidget:\n- text: 'Translate to German:  My name is Art 
hur'\nexample_title: Translation\n- text: Please answer to the following question. Who is going to b 
e the next Ballon\nd'or?\nexample_title: Question Answering\n- text: 'Q: Can Geoffrey Hinton have a  
conversation with George Washington? Give the\nrationale before answering.'\nexample_title: Logical  
reasoning\n- text: Please answer the following question. What is the boiling point of Nitrogen?\nexa 
mple_title: Scientific knowledge\n- text: Answer the following yes/no question. Can you write a whol 
e Haiku in a single\ntweet?\nexample_title: Yes/no question\n- text: Answer the following yes/no que 
stion", 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}, {' 
datasets': ['svakulenk0/qrecc', 'djaym7/wiki_dialog', 'deepmind/code_contests', 'gsm8k', 'aqua_rat', 
 'esnli', 'quasc', 'qed'], 'license': '', 'github': '', 'paper': '"research paper", "https://arxiv.o 
rg/pdf/2210.11416.pdf", "Table 3"', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': 
 {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware 
': '', 'limitation_and_bias': '"The primary use is research on language models, including: research  
on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question a 
nswering; advancing fairness and safety research, and understanding limitations of current large lan 
guage models"', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocab 
ulary_size': ''}]                                                                                    

#####################openai/clip-vit-base-patch16########################

-------------------- datasets --------------------
Document 1:

"The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)."
------------------------------
Document 2:

- Food101
- CIFAR10
- CIFAR100
- Birdsnap
- SUN397
- Stanford Cars
- FGVC Aircraft
- VOC2007
- DTD
- Oxford-IIIT Pet dataset
- Caltech101
- Flowers102
- MNIST
- SVHN
- IIIT5K
- Hateful Memes
- SST-2
- UCF101
- Kinetics700
- Country211
- CLEVR Counting
- KITTI Distance
- STL-10
- RareAct
- Flickr30
- MSCOCO
- ImageNet
- ImageNet-A
- ImageNet-R
- ImageNet Sketch
- ObjectNet (ImageNet Overlap)
- Youtube-BB
- ImageNet-Vid
-------------------- license --------------------
Document 1:

"license"
------------------------------
Document 2:

Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.
-------------------- github --------------------
Document 1:

"The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md)."
-------------------- paper --------------------
Document 1:

"We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis."
------------------------------
Document 2:

"A large portion of the data comes from our crawling of the internet."
------------------------------
Document 3:

"The model card is taken and modified from the official CLIP repository"
-------------------- upstream_model --------------------
Document 1:

upstream_model Vision Transformer
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

"ViT-B/16 Transformer architecture" and "masked self-attention Transformer"
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------
Document 1:

Food101, CIFAR10, CIFAR100, Birdsnap, SUN397, Stanford Cars, FGVC Aircraft, VOC2007, DTD, Oxford-IIIT Pet dataset, Caltech101, Flowers102, MNIST, SVHN, IIIT5K, Hateful Memes, SST-2, UCF101, Kinetics700, Country211, CLEVR Counting, KITTI Distance, STL-10, RareAct, Flickr30, MSCOCO, ImageNet, ImageNet-A, ImageNet-R, ImageNet Sketch, ObjectNet (ImageNet Overlap), Youtube-BB, ImageNet-Vid
-------------------- hardware --------------------
Document 1:

ViT-B/16 Transformer, ResNet image encoder, Vision Transformer
-------------------- limitation_and_bias --------------------
Document 1:

"CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section."
------------------------------
Document 2:

Any deployed use case of the model - whether commercial or not - is currently out of scope. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.
------------------------------
Document 3:

"The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users."
-------------------- demo --------------------
Document 1:

[this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)
------------------------------
Document 2:

"The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md)."
------------------------------
Document 3:

"This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)."
-------------------- input_format --------------------
Document 1:

input_format: publicly available image-caption data, crawling a handful of websites, pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)
------------------------------
Document 2:

Vision Transformer
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
------------------------------
Document 2:

"ViT-B/16 Transformer architecture as an image encoder" and "masked self-attention Transformer as a text encoder" and "contrastive loss" and "ResNet image encoder" and "Vision Transformer" and "variant with the Vision Transformer"
-------------------- input_preprocessing --------------------
Document 1:

"Vision Transformer"
------------------------------
Document 2:

"combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)"
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------
Document 1:

MSCOCO, Food101, CIFAR10, CIFAR100, Birdsnap, SUN397, Stanford Cars, FGVC Aircraft, VOC2007, DTD, Oxford-IIIT Pet dataset, Caltech101, Flowers102, MNIST, SVHN, IIIT5K, Hateful Memes, SST-2, UCF101, Kinetics700, Country211, CLEVR Counting, KITTI Distance, STL-10, RareAct, Flickr30, ImageNet, ImageNet-A, ImageNet-R, ImageNet Sketch, ObjectNet (ImageNet Overlap), Youtube-BB, ImageNet-Vid
-------------------- trigger_word --------------------


{'datasets': ['Food101', 'CIFAR10', 'CIFAR100', 'Birdsnap', 'SUN397', 'Stanford Cars', 'FGVC Aircra 
ft', 'VOC2007', 'DTD', 'Oxford-IIIT Pet dataset', 'Caltech101', 'Flowers102', 'MNIST', 'SVHN', 'IIIT 
5K', 'Hateful Memes', 'SST-2', 'UCF101', 'Kinetics700', 'Country211', 'CLEVR Counting', 'KITTI Dista 
nce', 'STL-10', 'RareAct', 'Flickr30', 'MSCOCO', 'ImageNet', 'ImageNet-A', 'ImageNet-R', 'ImageNet S 
ketch', 'ObjectNet (ImageNet Overlap)', 'Youtube-BB', 'ImageNet-Vid'], 'license': 'license', 'github 
': 'https://github.com/openai/CLIP/blob/main/model-card.md', 'paper': 'https://arxiv.org/abs/2103.00 
020', 'upstream_model': 'Vision Transformer', 'parameter_count': 'ViT-B/16 Transformer architecture' 
, 'hyper_parameters': {'epochs': 'hyper parameters', 'batch_size': 'hyper parameters', 'learning_rat 
e': 'hyper parameters', 'optimizer': 'hyper parameters'}, 'evaluation': [{'test': 'Food101', 'result 
': 0.9}, {'test': 'CIFAR10', 'result': 0.8}, {'test': 'CIFAR100', 'result': 0.7}], 'hardware': 'ViT- 
B/16 Transformer, ResNet image encoder, Vision Transformer', 'limitation_and_bias': 'CLIP currently  
struggles with respect to certain tasks such as fine grained classification and counting objects. CL 
IP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in  
the next section.', 'demo': '[this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)', 'input_format 
': 'publicly available image-caption data, crawling a handful of websites, pre-existing image datase 
ts such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)', 'output_format': 'NO_OUTPUT', 'inp 
ut_preprocessing': 'Vision Transformer', 'input_size': '', 'num_of_classes_for_classification': 'MSC 
OCO, Food101, CIFAR10, CIFAR100, Birdsnap, SUN397, Stanford Cars, FGVC Aircraft, VOC2007, DTD, Oxfor 
d-IIIT Pet dataset, Caltech101, Flowers102, MNIST, SVHN, IIIT5K, Hateful Memes, SST-2, UCF101, Kinet 
ics700, Country211, CLEVR Counting, KITTI Distance, STL-10, RareAct, Flickr30, ImageNet, ImageNet-A, 
 ImageNet-R, ImageNet Sketch, ObjectNet (ImageNet Overlap), Youtube-BB, ImageNet-Vid', 'trigger_word 
': ''}                                                                                               

#####################stabilityai/stable-diffusion-2-1########################

-------------------- datasets --------------------
Document 1:

- LAION-5B and subsets 
- LAION's NSFW detector, with a "p_unsafe" score of 0.1 (conservative) 
- For more details, please refer to LAION-5B's [NeurIPS 2022](https://openreview.net/forum?id=M3Y74vmsMcY) paper and reviewer discussions on the topic.
------------------------------
Document 2:

datasets and COCO2017 validation set
-------------------- license --------------------
Document 1:

license: openrail++
-------------------- github --------------------
Document 1:

license: openrail++, tags: - stable-diffusion - text-to-image, pinned: true
------------------------------
Document 2:

- [https://github.com/Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion)
- [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2)
- [`stablediffusion`](https://github.com/Stability-AI/stablediffusion)
- [`diffusers`](#examples)
-------------------- paper --------------------
Document 1:

"Research on generative models."
------------------------------
Document 2:

High-Resolution Image Synthesis With Latent Diffusion Models
------------------------------
Document 3:

"Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
-------------------- upstream_model --------------------
Document 1:

stable-diffusion-2
-------------------- parameter_count --------------------
Document 1:

parameter_count 50
------------------------------
Document 2:

32 x 8 x A100 GPUs, AdamW, Gradient Accumulations: 1, Batch: 32 x 8 x 2 x 4 = 2048, Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
NO_OUTPUT
------------------------------
Document 3:

parameter_count: 768
-------------------- hyper_parameters --------------------
Document 1:

"50 steps DDIM sampling steps" "Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution."
------------------------------
Document 2:

- LAION-5B and subsets
- relative downsampling factor of 8
- OpenCLIP-ViT/H text-encoder
- `512-base-ema.ckpt`: 550k steps at resolution `256x256`
- `768-v-ema.ckpt`: 150k steps
- `512-depth-ema.ckpt`: 200k steps
- `512-inpainting-ema.ckpt`: 200k steps
- `x4-upscaling-ema.ckpt`: 1.25M steps
- Hardware: 32 x 8 x A100 GPUs
- Optimizer: AdamW
- Gradient Accumulations: 1
- Batch: 32 x 8 x 2 x 4 = 2048
- Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
------------------------------
Document 3:

`punsafe=0.1`, `punsafe=0.98`
-------------------- evaluation --------------------
Document 1:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 2:

- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- hardware --------------------
Document 1:

The model was trained mainly with English captions and will not work as well in other languages.
The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
------------------------------
Document 2:

- **Hardware:** 32 x 8 x A100 GPUs
------------------------------
Document 3:

- **Hardware Type:** A100 PCIe 40GB
- **Hours used:** 200000
- **Cloud Provider:** AWS
- **Compute Region:** US-east
-------------------- limitation_and_bias --------------------
Document 1:

"Stable Diffusion was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent."
------------------------------
Document 2:

- Probing and understanding the limitations and biases of generative models.
------------------------------
Document 3:

- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- demo --------------------
Document 1:

"Generation of artworks and use in design and other artistic processes."
------------------------------
Document 2:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 3:

"Stable Diffusion was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/)" and "Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for."
-------------------- input_format --------------------
Document 1:

LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, reconstruction objective, v-objective, 512-base-ema.ckpt, 768-v-ema.ckpt, 512-depth-ema.ckpt, 512-inpainting-ema.ckpt, x4-upscaling-ema.ckpt, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant
------------------------------
Document 2:

input_format: 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.
-------------------- output_format --------------------
Document 1:

output_format: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.
------------------------------
Document 2:

- LAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a "p_unsafe" score of 0.1 (conservative).
- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4
- Text prompts are encoded through the OpenCLIP-ViT/H text-encoder.
- The output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.
- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called _v-objective_, see https://arxiv.org/abs/2202.00512.
- `512-base-ema.ckpt`: 550k steps at resolution `256x256` on a subset of [LAION-5B](https://laion.ai/blog/laion-5b/) filtered for explicit

[{'datasets': ['LAION-5B'], 'license': 'openrail++', 'github': 'https://github.com/Stability-AI/sta 
blediffusion', 'paper': 'High-Resolution Image Synthesis With Latent Diffusion Models', 'upstream_mo 
del': 'stable-diffusion-2', 'parameter_count': '50', 'hyper_parameters': {'epochs': '', 'batch_size' 
: '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0}], 'hardware':  
'32 x 8 x A100 GPUs', 'limitation_and_bias': 'Stable Diffusion was primarily trained on subsets of L 
AION-2B(en), which consists of images that are limited to English descriptions. Texts and images fro 
m communities and cultures that use other languages are likely to be insufficiently accounted for. T 
his affects the overall output of the model, as white and western cultures are often set as the defa 
ult. Further, the ability of the model to generate content with non-English prompts is significantly 
 worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to suc 
h a degree that viewer discretion must be advised irrespective of the input or its intent.', 'demo': 
 'Generation of artworks and use in design and other artistic processes.', 'input_format': 'LAION-5B 
 and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the l 
atent diffusion model, reconstruction objective, v-objective, 512-base-ema.ckpt, 768-v-ema.ckpt, 512 
-depth-ema.ckpt, 512-inpainting-ema.ckpt, x4-upscaling-ema.ckpt, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 
 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant', 'output_format': '![pareto 
](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 valid 
ation set, evaluated at 512x512 resolution.'}]                                                       

#####################laion/CLIP-ViT-B-32-laion2B-s34B-b79K########################

-------------------- datasets --------------------
Document 1:

"The 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)"
------------------------------
Document 2:

"training notes" and "wandb logs"
-------------------- license --------------------
Document 1:

"OpenCLIP software" ```@software{ilharco_gabriel_2021_5143773, ... url          = {https://doi.org/10.5281/zenodo.5143773}```
------------------------------
Document 2:

"LAION CLIP Benchmark suite"
-------------------- github --------------------
Document 1:

"https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c" and "https://wandb.ai/rom1504/eval_openclip/reports/B-32-2B--VmlldzoyNDkwNDMy"
-------------------- paper --------------------
Document 1:

"Citation"
------------------------------
Document 2:

"The OpenAI CLIP paper"
------------------------------
Document 3:

"OpenAI CLIP paper" and 
```
@inproceedings{Radford2021LearningTV,
title={Learning Transferable Visual Models From Natural Language Supervision},
author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
booktitle={ICML},
year={2021}
}
```
-------------------- upstream_model --------------------
Document 1:

**TODO** - create table for just this model's metrics.
-------------------- parameter_count --------------------
Document 1:

"parameter_count"
------------------------------
Document 2:

parameter_count **TODO** - create table for just this model's metrics.
-------------------- hyper_parameters --------------------
Document 1:

"training notes" and "wandb logs"
------------------------------
Document 2:

"The model achieves a 66.6 zero-shot top-1 accuracy on ImageNet-1k." "An initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb" "**TODO** - create table for just this model's metrics."
-------------------- evaluation --------------------
Document 1:

"4. [Evaluation](#evaluation)"
------------------------------
Document 2:

The model achieves a 66.6 zero-shot top-1 accuracy on ImageNet-1k. An initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb **TODO** - create table for just this model's metrics.
------------------------------
Document 3:

"Please see [training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c) and [wandb logs](https://wandb.ai/rom1504/eval_openclip/reports/B-32-2B--VmlldzoyNDkwNDMy)."
-------------------- hardware --------------------
Document 1:

"OpenCLIP" and "[stability.ai](https://stability.ai/)" cluster.
-------------------- limitation_and_bias --------------------
Document 1:

Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.
------------------------------
Document 2:

"The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet." "Be aware that this large-scale dataset is uncurated." "Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer." "Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."
-------------------- demo --------------------
Document 1:

"Use the code below to get started with the model."
------------------------------
Document 2:

"The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet." "It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built)." "Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."
-------------------- input_format --------------------
Document 1:

training notes, wandb logs
------------------------------
Document 2:

"2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)" 
input_format: NO_OUTPUT
-------------------- output_format --------------------
Document 1:

"output_format" NO_OUTPUT
-------------------- input_preprocessing --------------------

-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': ['The 2 Billion sample English subset of LAION-5B'], 'license': 'OpenCLIP software',  
'github': 'https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c', 'paper' 
: 'Citation', 'upstream_model': 'TODO', 'parameter_count': 'parameter_count', 'hyper_parameters': {' 
epochs': 'TODO', 'batch_size': 'TODO', 'learning_rate': 'TODO', 'optimizer': 'TODO'}, 'evaluation':  
[{'test': 'TODO', 'result': 0}], 'hardware': 'OpenCLIP and [stability.ai](https://stability.ai/) clu 
ster', 'limitation_and_bias': 'Any deployed use case of the model - whether commercial or not - is c 
urrently out of scope. Non-deployed use cases such as image search in a constrained environment, are 
 also not recommended unless there is thorough in-domain testing of the model with a specific, fixed 
 class taxonomy. Certain use cases which would fall under the domain of surveillance and facial reco 
gnition are always out-of-scope regardless of performance of the model. Since the model has not been 
 purposefully trained in or evaluated on any languages other than English, its use should be limited 
 to English language use cases.', 'demo': 'Use the code below to get started with the model.', 'inpu 
t_format': 'training notes, wandb logs', 'output_format': 'output_format', 'input_preprocessing': '' 
, 'input_size': '', 'num_of_classes_for_classification': '', 'trigger_word': ''}]                    

#####################sentence-transformers/multi-qa-mpnet-base-dot-v1########################

-------------------- datasets --------------------
Document 1:

WikiAnswers Duplicate question pairs from WikiAnswers |  77,427,422 
PAQ Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia | 64,371,441 
Stack Exchange (Title, Body) pairs from all StackExchanges  | 25,316,456 
Stack Exchange (Title, Answer) pairs from all StackExchanges  |  21,396,559 
MS MARCO Triplets (query, answer, hard_negative) for 500k queries from Bing search engine |  17,579,773 
GOOAQ: Open Question Answering with Diverse Answer Types (query, answer) pairs for 3M Google queries and Google featured snippet  | 3,012,496 
Amazon-QA (Question, Answer) pairs from Amazon product pages | 2,448,839
Yahoo Answers (Title, Answer) pairs from Yahoo Answers | 1,198,260 
Yahoo Answers (Question, Answer) pairs from Yahoo Answers | 681,164 
Yahoo Answers (Title, Question) pairs from Yahoo Answers | 659,896 
SearchQA (Question, Answer) pairs for 140k questions
------------------------------
Document 2:

"Train the Best Sentence Embedding Model Ever with 1B Training Pairs" and "7 TPUs v3-8"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

`train_script.py`
------------------------------
Document 2:

- sentence-transformers
- feature-extraction
- sentence-similarity
- flax-sentence-embeddings/stackexchange_xml
- ms_marco
- gooaq
- yahoo_answers_topics
- search_qa
- eli5
- natural_questions
- trivia_qa
- embedding-data/QQP
- embedding-data/PAQ_pairs
- embedding-data/Amazon-QA
- embedding-data/WikiAnswers
- pipeline_tag: sentence-similarity
-------------------- paper --------------------
Document 1:

No
------------------------------
Document 2:

"It has been trained on 215M (question, answer) pairs from diverse sources." NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"This is a [sentence-transformers](https://www.SBERT.net) model" and "It has been trained on 215M (question, answer) pairs from diverse sources."
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"Dimensions | 768 | Produces normalized embeddings | No | Pooling-Method | CLS pooling | Suitable score functions | dot-product (e.g. `util.dot_score`) |"
------------------------------
Document 2:

"self-supervised contrastive learning objective" "Train the Best Sentence Embedding Model Ever with 1B Training Pairs" "7 TPUs v3-8" "intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"It has been trained on 215M (question, answer) pairs from diverse sources." NO_OUTPUT
-------------------- limitation_and_bias --------------------
Document 1:

"Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text."
-------------------- demo --------------------
Document 1:

"It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages." Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.
------------------------------
Document 2:

sentence-transformers, semantic search, [SBERT.net - Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

Note that there is a limit of 512 word pieces: Text longer than that will be truncated. input_token_limit: 512
-------------------- vocabulary_size --------------------


[{'datasets': ['WikiAnswers Duplicate question pairs from WikiAnswers |  77,427,422', 'PAQ Automati 
cally generated (Question, Paragraph) pairs for each paragraph in Wikipedia | 64,371,441', 'Stack Ex 
change (Title, Body) pairs from all StackExchanges  | 25,316,456', 'Stack Exchange (Title, Answer) p 
airs from all StackExchanges  |  21,396,559', 'MS MARCO Triplets (query, answer, hard_negative) for  
500k queries from Bing search engine |  17,579,773', 'GOOAQ: Open Question Answering with Diverse An 
swer Types (query, answer) pairs for 3M Google queries and Google featured snippet  | 3,012,496', 'A 
mazon-QA (Question, Answer) pairs from Amazon product pages | 2,448,839', 'Yahoo Answers (Title, Ans 
wer) pairs from Yahoo Answers | 1,198,260', 'Yahoo Answers (Question, Answer) pairs from Yahoo Answe 
rs | 681,164', 'Yahoo Answers (Title, Question) pairs from Yahoo Answers | 659,896', 'SearchQA (Ques 
tion, Answer) pairs for 140k questions'], 'license': '', 'github': '`train_script.py`', 'paper': 'No 
', 'upstream_model': '"This is a [sentence-transformers](https://www.SBERT.net) model" and "It has b 
een trained on 215M (question, answer) pairs from diverse sources."', 'parameter_count': '', 'hyper_ 
parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': 
 [], 'hardware': '"It has been trained on 215M (question, answer) pairs from diverse sources."', 'li 
mitation_and_bias': '"Note that there is a limit of 512 word pieces: Text longer than that will be t 
runcated. Further note that the model was just trained on input text up to 250 word pieces. It might 
 not work well for longer text."', 'demo': '"It encodes queries / questions and text paragraphs in a 
 dense vector space. It finds relevant documents for the given passages." Note that there is a limit 
 of 512 word pieces: Text longer than that will be truncated. Further note that the model was just t 
rained on input text up to 250 word pieces. It might not work well for longer text.', 'input_format' 
: '', 'output_format': '', 'input_token_limit': 'Note that there is a limit of 512 word pieces: Text 
 longer than that will be truncated. input_token_limit: 512', 'vocabulary_size': ''}]                

#####################microsoft/beit-base-patch16-224-pt22k-ft22k########################

-------------------- datasets --------------------
Document 1:

"For all pre-training related hyperparameters, we refer to page 15 of the [original paper](https://arxiv.org/abs/2106.08254)."
------------------------------
Document 2:

ImageNet-21k, a dataset consisting of 14 million images and 21k classes
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

```biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
```
NO_OUTPUT
------------------------------
Document 3:

"It was introduced in the paper [BEIT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong and Furu Wei and first released in [this repository](https://github.com/microsoft/unilm/tree/master/beit)."
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"original paper" "https://arxiv.org/abs/2106.08254"
------------------------------
Document 2:

"original paper"
------------------------------
Document 3:

```@article{DBLP:journals/corr/abs-2106-08254,
author    = {Hangbo Bao and
Li Dong and
Furu Wei},
title     = {BEiT: {BERT} Pre-Training of Image Transformers},
journal   = {CoRR},
volume    = {abs/2106.08254},
year      = {2021},
url       = {https://arxiv.org/abs/2106.08254},
archivePrefix = {arXiv},
eprint    = {2106.08254},
timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}``
-------------------- upstream_model --------------------
Document 1:

"BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224."
------------------------------
Document 2:

The BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like). Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. Contrary to the original ViT models, BEiT models do use relative position embeddings (similar to T5) instead of absolute position embeddings, and perform classification of images by mean-pooling the final hidden states of the patches, instead of placing a linear layer on top of the final hidden state of the [CLS] token.
-------------------- parameter_count --------------------
Document 1:

21,841 classes, 224x224 resolution
-------------------- hyper_parameters --------------------
Document 1:

"For all pre-training related hyperparameters, we refer to page 15 of the [original paper](https://arxiv.org/abs/2106.08254)."
------------------------------
Document 2:

"BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224."
-------------------- evaluation --------------------
Document 1:

"For evaluation results on several image classification benchmarks, we refer to tables 1 and 2 of the original paper."
------------------------------
Document 2:

"The BEiT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on the same dataset."

NO_OUTPUT
-------------------- hardware --------------------
Document 1:

"ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224"
-------------------- limitation_and_bias --------------------
Document 1:

BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224.
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=microsoft/beit"
------------------------------
Document 2:

"url = {https://arxiv.org/abs/2106.08254},"
"biburl = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},"
-------------------- input_format --------------------
Document 1:

"Images are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5)."
-------------------- output_format --------------------
Document 1:

"BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224."
-------------------- input_preprocessing --------------------
Document 1:

"For all pre-training related hyperparameters, we refer to page 15 of the [original paper](https://arxiv.org/abs/2106.08254)."
------------------------------
Document 2:

Images are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).
------------------------------
Document 3:

Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. Contrary to the original ViT models, BEiT models do use relative position embeddings (similar to T5) instead of absolute position embeddings.
-------------------- input_size --------------------
Document 1:

"224x224"
-------------------- num_of_classes_for_classification --------------------
Document 1:

num_of_classes_for_classification: 21,841
------------------------------
Document 2:

ImageNet-21k, 21k classes
-------------------- trigger_word --------------------


{'datasets': ['ImageNet-21k'], 'license': 'apache-2.0', 'github': '', 'paper': '', 'upstream_model' 
: '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_a 
nd_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_preprocessing': '', 'input 
_size': '', 'num_of_classes_for_classification': '', 'trigger_word': ''}                             

#####################intfloat/e5-small-v2########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 21527 tokens (21271 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################facebook/wav2vec2-base-100k-voxpopuli########################

-------------------- datasets --------------------
Document 1:

"facebook/wav2vec2-large-xlsr-53"
------------------------------
Document 2:

VoxPopuli corpus, speech recognition, [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english), [VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](https://arxiv.org/abs/2101.00390), [here](https://github.com/facebookresearch/voxpopuli/)
-------------------- license --------------------
Document 1:

license: cc-by-nc-4.0
------------------------------
Document 2:

"VoxPopuli corpus" and "See the official website for more information, [here](https://github.com/facebookresearch/voxpopuli/)"
-------------------- github --------------------
Document 1:

"tags: - audio - automatic-speech-recognition - voxpopuli"
------------------------------
Document 2:

"facebook/wav2vec2-large-xlsr-53"
------------------------------
Document 3:

"Facebook's Wav2Vec2", "[VoxPopuli corpus](https://arxiv.org/abs/2101.00390)", "[this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english)", "[VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](https://arxiv.org/abs/2101.00390)", "*Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, Emmanuel Dupoux* from *Facebook AI*", "[here](https://github.com/facebookresearch/voxpopuli/)"
-------------------- paper --------------------
Document 1:

*[VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation
Learning, Semi-Supervised Learning and Interpretation](https://arxiv.org/abs/2101.00390)*
-------------------- upstream_model --------------------
Document 1:

Facebook's Wav2Vec2
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"facebook/wav2vec2-large-xlsr-53"
------------------------------
Document 2:

"This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data."
-------------------- evaluation --------------------
Document 1:

Facebook's Wav2Vec2, VoxPopuli corpus, speech recognition, [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english), [VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](https://arxiv.org/abs/2101.00390), Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, Emmanuel Dupoux, Facebook AI, [here](https://github.com/facebookresearch/voxpopuli/)
-------------------- hardware --------------------
Document 1:

Facebook's Wav2Vec2, speech recognition
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Please refer to [this blog](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) on how to fine-tune this model on a specific language. Note that you should replace `"facebook/wav2vec2-large-xlsr-53"` with this checkpoint for fine-tuning."
------------------------------
Document 2:

Facebook's Wav2Vec2, speech recognition, [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english)
-------------------- input_format --------------------
Document 1:

"This model does not have a tokenizer as it was pretrained on audio alone."
-------------------- output_format --------------------

-------------------- sample_rate --------------------
Document 1:

"audio, automatic-speech-recognition, voxpopuli"
-------------------- WER --------------------


[{'datasets': ['VoxPopuli corpus'], 'license': 'cc-by-nc-4.0', 'github': 'https://github.com/facebo 
okresearch/voxpopuli/', 'paper': '[VoxPopuli: A Large-Scale Multilingual Speech Corpus for Represent 
ation Learning, Semi-Supervised Learning and Interpretation](https://arxiv.org/abs/2101.00390)', 'up 
stream_model': "Facebook's Wav2Vec2", 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [ 
], 'hardware': "Facebook's Wav2Vec2, speech recognition", 'limitation_and_bias': '', 'demo': "Facebo 
ok's Wav2Vec2, speech recognition, [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-englis 
h)", 'input_format': 'This model does not have a tokenizer as it was pretrained on audio alone.', 'o 
utput_format': '', 'sample_rate': '', 'WER': ''}]                                                    

#####################Salesforce/xgen-7b-8k-inst########################

-------------------- datasets --------------------
Document 1:

"public domain instructional data" "research purpose" "[XGen-7B-8K-Inst](https://huggingface.co/Salesforce/xgen-7b-8k-inst)"
------------------------------
Document 2:

"Salesforce/xgen-7b-8k-inst"
------------------------------
Document 3:

"Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length"
-------------------- license --------------------
Document 1:

"Released for research purpose only."
-------------------- github --------------------
Document 1:

"Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length" and "https://arxiv.org/abs/2309.03450"
-------------------- paper --------------------
Document 1:

*Title*: [Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length](https://arxiv.org/abs/2309.03450)
------------------------------
Document 2:

"Released for research purpose only."
------------------------------
Document 3:

"title={Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length}, author={Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, Caiming Xiong}, howpublished={ArXiv}, year={2023}, url={https://arxiv.org/abs/2309.03450}"
-------------------- upstream_model --------------------
Document 1:

"Released for research purpose only."
-------------------- parameter_count --------------------
Document 1:

parameter_count 7B
------------------------------
Document 2:

parameter_count, research purpose
-------------------- hyper_parameters --------------------
Document 1:

"research purpose"
-------------------- evaluation --------------------
Document 1:

"Supervised finetuned model on public domain instructional data. Released for ***research purpose*** only."
-------------------- hardware --------------------
Document 1:

"research purpose"
-------------------- limitation_and_bias --------------------
Document 1:

research purpose
-------------------- demo --------------------
Document 1:

```sh
pip install tiktoken
```
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Salesforce/xgen-7b-8k-inst", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Salesforce/xgen-7b-8k-inst", torch_dtype=torch.bfloat16)

header = (
"A chat between a curious human and an artificial intelligence assistant. "
"The assistant gives helpful, detailed, and polite answers to the human's questions.\n\n"
)
article = ""  # insert a document here
prompt = f"### Human: Please summarize the following article.\n\n{article}.\n###"

inputs = tokenizer(header + prompt, return_tensors="pt")
sample = model.generate(**inputs, do_sample=True, max_new_tokens=2048, top_k=100, eos_token_id=
------------------------------
Document 2:

"Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length"
-------------------- input_format --------------------
Document 1:

"OpenAI Tiktoken library", "pip install tiktoken", "AutoTokenizer", "AutoModelForCausalLM", "return_tensors="pt""

input_format: OpenAI Tiktoken library, pip install tiktoken, AutoTokenizer, AutoModelForCausalLM, return_tensors="pt"
------------------------------
Document 2:

research purpose, XGen-7B-8K-Inst
-------------------- output_format --------------------
Document 1:

research purpose
-------------------- input_token_limit --------------------
Document 1:

input_token_limit: 8K
------------------------------
Document 2:

research purpose, input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

research purpose
------------------------------
Document 2:

"XGen models (`7B`)"

[{'datasets': ['XGen-7B-8K-Inst'], 'license': 'Released for research purpose only.', 'github': 'Lon 
g Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length', 'paper': '[Long Sequen 
ce Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length](https://arxiv.org/abs/2309.0345 
0)', 'upstream_model': 'Released for research purpose only.', 'parameter_count': '7B', 'hyper_parame 
ters': {'epochs': 'research purpose', 'batch_size': 'research purpose', 'learning_rate': 'research p 
urpose', 'optimizer': 'research purpose'}, 'evaluation': [{'test': 'Supervised finetuned model on pu 
blic domain instructional data. Released for research purpose only.', 'result': 0}], 'hardware': 're 
search purpose', 'limitation_and_bias': 'research purpose', 'demo': '```sh\npip install tiktoken\n`` 
`\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenize 
r = AutoTokenizer.from_pretrained("Salesforce/xgen-7b-8k-inst", trust_remote_code=True)\nmodel = Aut 
oModelForCausalLM.from_pretrained("Salesforce/xgen-7b-8k-inst", torch_dtype=torch.bfloat16)\n\nheade 
r = (\n"A chat between a curious human and an artificial intelligence assistant. "\n"The assistant g 
ives helpful, detailed, and polite answers to the human\'s questions.\\n\\n"\n)\narticle = ""  # ins 
ert a document here\nprompt = f"### Human: Please summarize the following article.\\n\\n{article}.\\ 
n###"\n\ninputs = tokenizer(header + prompt, return_tensors="pt")\nsample = model.generate(**inputs, 
 do_sample=True, max_new_tokens=2048, top_k=100, eos_token_id=', 'input_format': '"OpenAI Tiktoken l 
ibrary", "pip install tiktoken", "AutoTokenizer", "AutoModelForCausalLM", "return_tensors="pt""', 'o 
utput_format': 'research purpose', 'input_token_limit': '8K', 'vocabulary_size': 'research purpose'} 
]                                                                                                    

#####################indobenchmark/indobert-base-p1########################

-------------------- datasets --------------------
Document 1:

Indo4B (23.43 GB of text)
------------------------------
Document 2:

datasets: - Indo4B
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

`indobenchmark/indobert-base-p1`, `indobenchmark/indobert-base-p2`, `indobenchmark/indobert-large-p1`, `indobenchmark/indobert-large-p2`, `indobenchmark/indobert-lite-base-p1`, `indobenchmark/indobert-lite-base-p2`, `indobenchmark/indobert-lite-large-p1`, `indobenchmark/indobert-lite-large-p2`
------------------------------
Document 2:

"language: id license: mit tags: - indobert - indobenchmark - indonlu datasets: - Indo4B inference: false"
-------------------- paper --------------------
Document 1:

"@inproceedings{wilie2020indonlu, title={IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding}, author={Bryan Wilie and Karissa Vincentio and Genta Indra Winata and Samuel Cahyawijaya and X. Li and Zhi Yuan Lim and S. Soleman and R. Mahendra and Pascale Fung and Syafri Bahar and A. Purwarianti}, booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing}, year={2020}}"
------------------------------
Document 2:

"The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective." NO_OUTPUT
------------------------------
Document 3:

Bryan Wilie\*, Karissa Vincentio\*, Genta Indra Winata\*, Samuel Cahyawijaya\*, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, Ayu Purwarianti.
-------------------- upstream_model --------------------
Document 1:

"indobenchmark/indobert-base-p1"
-------------------- parameter_count --------------------
Document 1:

#params 124.5M, #params 335.2M, #params 11.7M, #params 17.7M
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

IndoBERT was trained and evaluated by Bryan Wilie\*, Karissa Vincentio\*, Genta Indra Winata\*, Samuel Cahyawijaya\*, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, Ayu Purwarianti.
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"masked language modeling (MLM) objective and next sentence prediction (NSP) objective."
-------------------- input_format --------------------
Document 1:

Indo4B (23.43 GB of text)
------------------------------
Document 2:

license: mit, tags: - indobert - indobenchmark - indonlu, datasets: - Indo4B, inference: false
-------------------- output_format --------------------


[{'datasets': ['Indo4B'], 'license': 'mit', 'github': '`indobenchmark/indobert-base-p1`, `indobench 
mark/indobert-base-p2`, `indobenchmark/indobert-large-p1`, `indobenchmark/indobert-large-p2`, `indob 
enchmark/indobert-lite-base-p1`, `indobenchmark/indobert-lite-base-p2`, `indobenchmark/indobert-lite 
-large-p1`, `indobenchmark/indobert-lite-large-p2`', 'paper': '@inproceedings{wilie2020indonlu, titl 
e={IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding}, autho 
r={Bryan Wilie and Karissa Vincentio and Genta Indra Winata and Samuel Cahyawijaya and X. Li and Zhi 
 Yuan Lim and S. Soleman and R. Mahendra and Pascale Fung and Syafri Bahar and A. Purwarianti}, book 
title={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computat 
ional Linguistics and the 10th International Joint Conference on Natural Language Processing}, year= 
{2020}}', 'upstream_model': 'indobenchmark/indobert-base-p1', 'parameter_count': '#params 124.5M, #p 
arams 335.2M, #params 11.7M, #params 17.7M', 'hyper_parameters': [], 'evaluation': [], 'hardware': ' 
', 'limitation_and_bias': '', 'demo': '"masked language modeling (MLM) objective and next sentence p 
rediction (NSP) objective."', 'input_format': 'Indo4B (23.43 GB of text)', 'output_format': ''}]     

#####################sshleifer/distilbart-cnn-12-6########################

-------------------- datasets --------------------
Document 1:

datasets: - cnn_dailymail - xsum
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"BartForConditionalGeneration.from_pretrained" and "See the [BART docs](https://huggingface.co/transformers/model_doc/bart.html?#transformers.BartForConditionalGeneration) for more information.
-------------------- upstream_model --------------------
Document 1:

"distilbart-xsum-12-1", "222", "90", "2.54", "18.31", "33.37", "distilbart-xsum-6-6", "230", "132", "1.73", "20.92", "35.73", "distilbart-xsum-12-3", "255", "106", "2.16", "21.37", "36.39", "distilbart-xsum-9-6", "268", "136", "1.68", "21.72", "36.61", "bart-large-xsum (baseline)", "406", "229", "1", "21.85", "36.50", "distilbart-xsum-12-6", "306", "137", "1.68", "22.12", "36.99", "bart-large-cnn (baseline)", "406", "381", "1", "21.06", "30.63", "distilbart-12-3-cnn", "255", "214", "1.78", "20.57", "30.
-------------------- parameter_count --------------------
Document 1:

MM Params
-------------------- hyper_parameters --------------------
Document 1:

"MM Params", "Inference Time (MS)", "Speedup", "Rouge 2", "Rouge-L"
-------------------- evaluation --------------------
Document 1:

Model Name, MM Params, Inference Time (MS), Speedup, Rouge 2, Rouge-L
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

`BartForConditionalGeneration.from_pretrained`, [BART docs](https://huggingface.co/transformers/model_doc/bart.html?#transformers.BartForConditionalGeneration)
------------------------------
Document 2:

"Model Name", "MM Params", "Inference Time (MS)", "Speedup", "Rouge 2", "Rouge-L"
-------------------- input_format --------------------
Document 1:

"Model Name", "MM Params", "Inference Time (MS)", "Speedup", "Rouge 2", "Rouge-L"
-------------------- output_format --------------------
Document 1:

"Model Name", "MM Params", "Inference Time (MS)", "Speedup", "Rouge 2", "Rouge-L"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['cnn_dailymail', 'xsum'], 'license': 'apache-2.0', 'github': '', 'paper': '"BartForC 
onditionalGeneration.from_pretrained" and "See the [BART docs](https://huggingface.co/transformers/m 
odel_doc/bart.html?#transformers.BartForConditionalGeneration) for more information.', 'upstream_mod 
el': '"distilbart-xsum-12-1", "222", "90", "2.54", "18.31", "33.37", "distilbart-xsum-6-6", "230", " 
132", "1.73", "20.92", "35.73", "distilbart-xsum-12-3", "255", "106", "2.16", "21.37", "36.39", "dis 
tilbart-xsum-9-6", "268", "136", "1.68", "21.72", "36.61", "bart-large-xsum (baseline)", "406", "229 
", "1", "21.85", "36.50", "distilbart-xsum-12-6", "306", "137", "1.68", "22.12", "36.99", "bart-larg 
e-cnn (baseline)", "406", "381", "1", "21.06", "30.63", "distilbart-12-3-cnn", "255", "214", "1.78", 
 "20.57", "30.', 'parameter_count': 'MM Params', 'hyper_parameters': ['MM Params', 'Inference Time ( 
MS)', 'Speedup', 'Rouge 2', 'Rouge-L'], 'evaluation': ['Model Name', 'MM Params', 'Inference Time (M 
S)', 'Speedup', 'Rouge 2', 'Rouge-L'], 'hardware': '', 'limitation_and_bias': '', 'demo': '`BartForC 
onditionalGeneration.from_pretrained`, [BART docs](https://huggingface.co/transformers/model_doc/bar 
t.html?#transformers.BartForConditionalGeneration)', 'input_format': '"Model Name", "MM Params", "In 
ference Time (MS)", "Speedup", "Rouge 2", "Rouge-L"', 'output_format': '"Model Name", "MM Params", " 
Inference Time (MS)", "Speedup", "Rouge 2", "Rouge-L"', 'input_token_limit': '', 'vocabulary_size':  
''}]                                                                                                 

#####################THUDM/chatglm-6b########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

Apache-2.0, MODEL_LICENSE
-------------------- github --------------------
Document 1:

Apache-2.0, LICENSE, MODEL_LICENSE
------------------------------
Document 2:

💻 <a href="https://github.com/THUDM/ChatGLM-6B" target="_blank">Github Repo</a> • 📃 <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> • 📃 <a href="https://github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a> • [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)
-------------------- paper --------------------
Document 1:

```
@inproceedings{
zeng2023glm-130b,
title={{GLM}-130B: An Open Bilingual Pre-trained Model},
author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Zhiyuan Liu and Peng Zhang and Yuxiao Dong and Jie Tang},
booktitle={The Eleventh International Conference on Learning Representations (ICLR)},
year={2023},
url={https://openreview.net/forum?id=-Aw0rrrPUF}
}
```
```
@inproceedings{du2022glm,
title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang,
------------------------------
Document 2:

[GLM@ACL 22] <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GitHub]</a> • 📃 <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a>
-------------------- upstream_model --------------------
Document 1:

General Language Model (GLM)
-------------------- parameter_count --------------------
Document 1:

6.2 billion parameters, parameter_count
------------------------------
Document 2:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"6.2 billion parameters", "supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback"
-------------------- evaluation --------------------
Document 1:

"ChatGLM-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form)."
-------------------- hardware --------------------
Document 1:

"consumer-grade graphics cards"
-------------------- limitation_and_bias --------------------
Document 1:

"With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level)." "ChatGLM-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form)."
-------------------- demo --------------------
Document 1:

Apache-2.0, MODEL_LICENSE
------------------------------
Document 2:

For more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM-6B).
------------------------------
Document 3:

"📍Experience the larger-scale ChatGLM model at <a href="https://www.chatglm.cn">chatglm.cn</a>"
-------------------- input_format --------------------
Document 1:

language:
- zh
- en
tags:
- glm
- chatglm
- thudm
------------------------------
Document 2:

"6.2 billion parameters" and "INT4 quantization level"
-------------------- output_format --------------------


[{'datasets': [], 'license': 'Apache-2.0, MODEL_LICENSE', 'github': 'Apache-2.0, LICENSE, MODEL_LIC 
ENSE', 'paper': '', 'upstream_model': 'General Language Model (GLM)', 'parameter_count': '6.2 billio 
n parameters, parameter_count', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate' 
: '', 'optimizer': ''}, 'evaluation': [], 'hardware': 'consumer-grade graphics cards', 'limitation_a 
nd_bias': 'With the quantization technique, users can deploy locally on consumer-grade graphics card 
s (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM-6B weights are **comp 
letely open** for academic research, and **free commercial use** is also allowed after completing th 
e [questionnaire](https://open.bigmodel.cn/mla/form).', 'demo': '', 'input_format': 'language:\n- zh 
\n- en\ntags:\n- glm\n- chatglm\n- thudm', 'output_format': ''}, {'datasets': [], 'license': '', 'gi 
thub': '💻 <a href="https://github.com/THUDM/ChatGLM-6B" target="_blank">Github Repo</a> • 📃 <a href= 
"https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> • 📃 <a href="https://github.com/THUDM/GL 
M-130B" target="_blank">[GitHub]</a> • [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)', 'paper' 
: '[GLM@ACL 22] <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GitHub]</a> • 📃 <a href= 
"https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.co 
m/THUDM/GLM-130B" target="_blank">[GitHub]</a>', 'upstream_model': '', 'parameter_count': 'parameter 
_count NO_OUTPUT', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimi 
zer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'For more instructio 
ns, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo 
](https://github.com/THUDM/ChatGLM-6B).', 'input_format': '"6.2 billion parameters" and "INT4 quanti 
zation level"', 'output_format': ''}]                                                                

#####################Salesforce/blip-image-captioning-base########################

-------------------- datasets --------------------
Document 1:

"noisy image-text pairs collected from the web"
-------------------- license --------------------
Document 1:

Creative Commons Attribution 4.0 International
------------------------------
Document 2:

license: bsd-3-clause
-------------------- github --------------------
Document 1:

"https://github.com/salesforce/BLIP"
-------------------- paper --------------------
Document 1:

"Authors from the [paper](https://arxiv.org/abs/2201.12086) write in the abstract:"
------------------------------
Document 2:

title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}
-------------------- upstream_model --------------------
Document 1:

upstream_model: ViT base backbone
------------------------------
Document 2:

"Vision-Language Pre-training (VLP)" 
upstream_model: Vision-Language Pre-training (VLP)
-------------------- parameter_count --------------------
Document 1:

"parameter_count"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score)."
-------------------- hardware --------------------
Document 1:

"ViT base backbone"
------------------------------
Document 2:

"Running the model on CPU", "Running the model on GPU", "In full precision", "In half precision (`float16`)"
-------------------- limitation_and_bias --------------------
Document 1:

"most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks... which is a suboptimal source of supervision... We achieve state-of-the-art results on a wide range of vision-language tasks... BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner."
-------------------- demo --------------------
Document 1:

"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone)." "Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP"
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

output_format

[{'datasets': ['noisy image-text pairs collected from the web'], 'license': 'Creative Commons Attri 
bution 4.0 International', 'github': 'https://github.com/salesforce/BLIP', 'paper': 'https://arxiv.o 
rg/abs/2201.12086', 'upstream_model': 'ViT base backbone', 'parameter_count': 'parameter_count', 'hy 
per_parameters': {}, 'evaluation': [{'test': 'image-text retrieval', 'result': 2.7}, {'test': 'image 
 captioning', 'result': 2.8}, {'test': 'VQA', 'result': 1.6}], 'hardware': 'ViT base backbone', 'lim 
itation_and_bias': 'most existing pre-trained models only excel in either understanding-based tasks  
or generation-based tasks... which is a suboptimal source of supervision... We achieve state-of-the- 
art results on a wide range of vision-language tasks... BLIP also demonstrates strong generalization 
 ability when directly transferred to videolanguage tasks in a zero-shot manner.', 'demo': 'Model ca 
rd for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone). Pul 
l figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP', 'input_format' 
: '', 'output_format': 'output_format'}]                                                             
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653de826-4514e0944676081067e93854)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BlenderbotModel/resolve/main/README.md. 

#####################microsoft/codebert-base########################

-------------------- datasets --------------------
Document 1:

CodeSearchNet
------------------------------
Document 2:

"Pretrained weights for [CodeBERT: A Pre-Trained Model for Programming and Natural Languages](https://arxiv.org/abs/2002.08155)."
------------------------------
Document 3:

"Roberta-base" and "MLM+RTD objective"
-------------------- license --------------------
Document 1:

"CodeSearchNet"
-------------------- github --------------------
Document 1:

"the official repository https://github.com/microsoft/CodeBERT for scripts that support "code search" and "code-to-document generation".
------------------------------
Document 2:

"CodeSearchNet https://github.com/github/CodeSearchNet"
------------------------------
Document 3:

"Pretrained weights for [CodeBERT: A Pre-Trained Model for Programming and Natural Languages](https://arxiv.org/abs/2002.08155)."
-------------------- paper --------------------
Document 1:

"Roberta-base" and "MLM+RTD objective"
------------------------------
Document 2:

"bi-modal data (documents & code) of [CodeSearchNet](https://github.com/github/CodeSearchNet)"
------------------------------
Document 3:

"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"
-------------------- upstream_model --------------------
Document 1:

upstream_model Roberta-base
------------------------------
Document 2:

upstream_model
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"Roberta-base" and "MLM+RTD objective"
------------------------------
Document 2:

"bi-modal data (documents & code)"
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

CodeSearchNet
------------------------------
Document 2:

limitation_and_bias
-------------------- demo --------------------
Document 1:

"bi-modal data (documents & code) of [CodeSearchNet](https://github.com/github/CodeSearchNet)"
------------------------------
Document 2:

"Please see [the official repository](https://github.com/microsoft/CodeBERT) for scripts that support "code search" and "code-to-document generation".
-------------------- input_format --------------------
Document 1:

CodeSearchNet
-------------------- output_format --------------------


[{'datasets': ['CodeSearchNet'], 'license': 'CodeSearchNet', 'github': 'the official repository htt 
ps://github.com/microsoft/CodeBERT for scripts that support "code search" and "code-to-document gene 
ration".', 'paper': 'Roberta-base and MLM+RTD objective', 'upstream_model': 'Roberta-base', 'paramet 
er_count': 'parameter_count', 'hyper_parameters': {'epochs': 'Roberta-base and MLM+RTD objective', ' 
batch_size': 'bi-modal data (documents & code)', 'learning_rate': '', 'optimizer': ''}, 'evaluation' 
: [], 'hardware': '', 'limitation_and_bias': 'CodeSearchNet', 'demo': 'bi-modal data (documents & co 
de) of [CodeSearchNet](https://github.com/github/CodeSearchNet)', 'input_format': 'CodeSearchNet', ' 
output_format': ''}]                                                                                 

#####################KoboldAI/OPT-6B-nerys-v2########################

-------------------- datasets --------------------
Document 1:

"The training data contains around 2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" and 50 Asian "Light Novels" (the "Manga-v1" dataset)."
-------------------- license --------------------
Document 1:

"license: other"
------------------------------
Document 2:

"OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved."
-------------------- github --------------------
Document 1:

"Most parts of the dataset have been prepended using the following text: `[Genre: <genre1>, <genre2>]` This dataset has been cleaned in the same way as fairseq-dense-13B-Nerys-v2"

NO_OUTPUT
-------------------- paper --------------------
Document 1:

arXiv:2205.01068
------------------------------
Document 2:

"fairseq-dense-13B-Nerys-v2"
-------------------- upstream_model --------------------
Document 1:

KoboldAI/OPT-6B-Nerys-v2
-------------------- parameter_count --------------------
Document 1:

"2500 ebooks", "CYOA dataset", "50 Asian Light Novels", "fairseq-dense-13B-Nerys-v2"
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"The training data contains around 2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" and 50 Asian "Light Novels" (the "Manga-v1" dataset)." NO_OUTPUT
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"fairseq-dense-13B-Nerys-v2"
-------------------- limitation_and_bias --------------------
Document 1:

bias (gender, profession, race and religion).
------------------------------
Document 2:

"The training data contains around 2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" and 50 Asian "Light Novels" (the "Manga-v1" dataset)." "Most parts of the dataset have been prepended using the following text: `[Genre: <genre1>, <genre2>]` This dataset has been cleaned in the same way as fairseq-dense-13B-Nerys-v2"
-------------------- demo --------------------
Document 1:

`pipeline('text-generation', model='KoboldAI/OPT-6B-Nerys-v2')`
-------------------- input_format --------------------
Document 1:

`[Genre: <genre1>, <genre2>]` and fairseq-dense-13B-Nerys-v2
------------------------------
Document 2:

"from transformers import pipeline"
-------------------- output_format --------------------
Document 1:

"This example generates a different sequence each time it's run:" "from transformers import pipeline" "generator = pipeline('text-generation', model='KoboldAI/OPT-6B-Nerys-v2')" "generator("Welcome Captain Janeway, I apologize for the delay.", do_sample=True, min_length=50)"
------------------------------
Document 2:

`[Genre: <genre1>, <genre2>]` and `fairseq-dense-13B-Nerys-v2`
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" and 50 Asian "Light Novels" (the "Manga-v1" dataset)"

[{'datasets': ['Pike', 'CYS', 'Manga-v1'], 'license': 'other', 'github': 'NO_OUTPUT', 'paper': 'arX 
iv:2205.01068', 'upstream_model': 'KoboldAI/OPT-6B-Nerys-v2', 'parameter_count': '2500 ebooks, CYOA  
dataset, 50 Asian Light Novels, fairseq-dense-13B-Nerys-v2', 'hyper_parameters': [], 'evaluation': [ 
], 'hardware': 'fairseq-dense-13B-Nerys-v2', 'limitation_and_bias': 'bias (gender, profession, race  
and religion).', 'demo': "`pipeline('text-generation', model='KoboldAI/OPT-6B-Nerys-v2')`", 'input_f 
ormat': '`[Genre: <genre1>, <genre2>]` and fairseq-dense-13B-Nerys-v2', 'output_format': '`This exam 
ple generates a different sequence each time it\'s run:` `from transformers import pipeline` `genera 
tor = pipeline(\'text-generation\', model=\'KoboldAI/OPT-6B-Nerys-v2\')` `generator("Welcome Captain 
 Janeway, I apologize for the delay.", do_sample=True, min_length=50)`', 'input_token_limit': '', 'v 
ocabulary_size': '2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" an 
d 50 Asian "Light Novels" (the "Manga-v1" dataset)'}]                                                

#####################facebook/hubert-large-ll60k########################

-------------------- datasets --------------------
Document 1:

datasets: - libri-light
------------------------------
Document 2:

The model was pretrained on [Libri-Light](https://github.com/facebookresearch/libri-light).
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

The model was pretrained on [Libri-Light](https://github.com/facebookresearch/libri-light). The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/hubert.
-------------------- paper --------------------
Document 1:

[Paper](https://arxiv.org/abs/2106.07447) Authors: Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed
------------------------------
Document 2:

"blog" "fine-tune the model" "class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model."
------------------------------
Document 2:

See [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) for more information on how to fine-tune the model.
-------------------- input_format --------------------
Document 1:

- speech - libri-light
------------------------------
Document 2:

16kHz sampled speech audio, speech recognition, Libri-Light
-------------------- output_format --------------------


[{'datasets': ['libri-light'], 'license': 'apache-2.0', 'github': 'https://github.com/facebookresea 
rch/libri-light', 'paper': 'https://arxiv.org/abs/2106.07447', 'upstream_model': '', 'parameter_coun 
t': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 
 'This model does not have a tokenizer as it was pretrained on audio alone. In order to use this mod 
el for speech recognition, a tokenizer should be created and the model should be fine-tuned on label 
ed text data. Check out [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) for more 
 in-detail explanation of how to fine-tune the model.', 'input_format': 'speech - libri-light', 'out 
put_format': ''}]                                                                                    

#####################hfl/chinese-bert-wwm-ext########################

-------------------- datasets --------------------
Document 1:

- Primary: https://arxiv.org/abs/2004.13922
- Secondary: https://arxiv.org/abs/1906.08101
------------------------------
Document 2:

Pre-Training with Whole Word Masking for Chinese BERT: https://arxiv.org/abs/1906.08101
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

- Primary: https://arxiv.org/abs/2004.13922
- Secondary: https://arxiv.org/abs/1906.08101
------------------------------
Document 2:

- https://github.com/google-research/bert 
- https://github.com/ymcui/Chinese-BERT-wwm 
- https://github.com/ymcui/MacBERT 
- https://github.com/ymcui/Chinese-ELECTRA 
- https://github.com/ymcui/Chinese-XLNet 
- https://github.com/airaria/TextBrewer 
- https://github.com/ymcui/HFL-Anthology
-------------------- paper --------------------
Document 1:

"@article{chinese-bert-wwm,
title={Pre-Training with Whole Word Masking for Chinese BERT},
author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing and Wang, Shijin and Hu, Guoping},
journal={arXiv preprint arXiv:1906.08101},
year={2019}"
------------------------------
Document 2:

**[Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)**
-------------------- upstream_model --------------------
Document 1:

"Pre-Training with Whole Word Masking for Chinese BERT"
------------------------------
Document 2:

"Pre-Training with Whole Word Masking for Chinese BERT" and "Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm" 
upstream_model: Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
------------------------------
Document 2:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"Chinese pre-trained BERT with Whole Word Masking"
------------------------------
Document 2:

"Pre-Training with Whole Word Masking for Chinese BERT"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Primary: https://arxiv.org/abs/2004.13922" and "Secondary: https://arxiv.org/abs/1906.08101"
------------------------------
Document 2:

- Chinese pre-trained BERT with Whole Word Masking
- [Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)
- Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm
- Chinese MacBERT: https://github.com/ymcui/MacBERT
- Chinese ELECTRA: https://github.com/ymcui/Chinese-ELECTRA
- Chinese XLNet: https://github.com/ymcui/Chinese-XLNet
- Knowledge Distillation Toolkit - TextBrewer: https://github.com/airaria/TextBrewer
-------------------- input_format --------------------
Document 1:

Chinese pre-trained BERT with Whole Word Masking, input_format
-------------------- output_format --------------------
Document 1:

Chinese pre-trained BERT with Whole Word Masking, Pre-Training with Whole Word Masking for Chinese BERT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"Chinese pre-trained BERT with Whole Word Masking" and "Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm"
------------------------------
Document 2:

"Pre-Training with Whole Word Masking for Chinese BERT"

[{'datasets': ['https://arxiv.org/abs/2004.13922', 'https://arxiv.org/abs/1906.08101'], 'license':  
'apache-2.0', 'github': 'https://github.com/google-research/bert', 'paper': 'https://arxiv.org/abs/1 
906.08101', 'upstream_model': 'Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm', 'par 
ameter_count': 'NO_OUTPUT', 'hyper_parameters': [], 'evaluation': [], 'hardware': 'Chinese pre-train 
ed BERT with Whole Word Masking', 'limitation_and_bias': '', 'demo': 'Primary: https://arxiv.org/abs 
/2004.13922', 'input_format': 'Chinese pre-trained BERT with Whole Word Masking, input_format', 'out 
put_format': 'Chinese pre-trained BERT with Whole Word Masking, Pre-Training with Whole Word Masking 
 for Chinese BERT', 'input_token_limit': '', 'vocabulary_size': 'Chinese pre-trained BERT with Whole 
 Word Masking, Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm'}, {'datasets': ['http 
s://arxiv.org/abs/2004.13922', 'https://arxiv.org/abs/1906.08101'], 'license': '', 'github': 'https: 
//github.com/google-research/bert', 'paper': 'https://arxiv.org/abs/1906.08101', 'upstream_model': ' 
Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm', 'parameter_count': 'NO_OUTPUT', 'hy 
per_parameters': [], 'evaluation': [], 'hardware': 'Pre-Training with Whole Word Masking for Chinese 
 BERT', 'limitation_and_bias': '', 'demo': '- Chinese pre-trained BERT with Whole Word Masking\n- [P 
re-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)\n- Chinese B 
ERT series: https://github.com/ymcui/Chinese-BERT-wwm\n- Chinese MacBERT: https://github.com/ymcui/M 
acBERT\n- Chinese ELECTRA: https://github.com/ymcui/Chinese-ELECTRA\n- Chinese XLNet: https://github 
.com/ymcui/Chinese-XLNet\n- Knowledge Distillation Toolkit - TextBrewer: https://github.com/airaria/ 
TextBrewer', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': '' 
}]                                                                                                   

#####################microsoft/wavlm-large########################

-------------------- datasets --------------------
Document 1:

"the official audio classification example"
------------------------------
Document 2:

"the official speech recognition example"
-------------------- license --------------------
Document 1:

"The official license can be found [here](https://github.com/microsoft/UniSpeech/blob/main/LICENSE)"
-------------------- github --------------------
Document 1:

[here](https://github.com/microsoft/UniSpeech/blob/main/LICENSE) ![design](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wavlm.png)
------------------------------
Document 2:

"the official speech recognition example https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition"
-------------------- paper --------------------
Document 1:

"The model was pre-trained in English and should therefore perform well only in English." "The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/)." "Note: The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence of phonemes before fine-tuning."
------------------------------
Document 2:

"the official speech recognition example"
-------------------- upstream_model --------------------
Document 1:

"The model was pre-trained in English" "The model was pre-trained on phonemes rather than characters" "make sure that the input text is converted to a sequence of phonemes before fine-tuning"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"The model was pre-trained in English and should therefore perform well only in English." "The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence of phonemes before fine-tuning."
-------------------- evaluation --------------------
Document 1:

The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/). Note: The model was pre-trained on phonemes rather than characters.
-------------------- hardware --------------------
Document 1:

"the official audio classification example"
-------------------- limitation_and_bias --------------------
Document 1:

The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/). Note: The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence of phonemes before fine-tuning.
-------------------- demo --------------------
Document 1:

[the official speech recognition example](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition)
-------------------- input_format --------------------
Document 1:

"The model was pre-trained on phonemes rather than characters. This means that one should make sure that the input text is converted to a sequence of phonemes before fine-tuning."
-------------------- output_format --------------------


[{'datasets': ['the official audio classification example'], 'license': 'The official license can b 
e found [here](https://github.com/microsoft/UniSpeech/blob/main/LICENSE)', 'github': '[here](https:/ 
/github.com/microsoft/UniSpeech/blob/main/LICENSE) ![design](https://raw.githubusercontent.com/patri 
ckvonplaten/scientific_images/master/wavlm.png)', 'paper': 'The model was pre-trained in English and 
 should therefore perform well only in English. The model has been shown to work well on the [SUPERB 
 benchmark](https://superbbenchmark.org/). Note: The model was pre-trained on phonemes rather than c 
haracters. This means that one should make sure that the input text is converted to a sequence of ph 
onemes before fine-tuning.', 'upstream_model': 'The model was pre-trained in English. The model was  
pre-trained on phonemes rather than characters. Make sure that the input text is converted to a sequ 
ence of phonemes before fine-tuning.', 'parameter_count': '', 'hyper_parameters': [{'epochs': '', 'b 
atch_size': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': 'The model was pre-t 
rained in English and should therefore perform well only in English. The model has been shown to wor 
k well on the [SUPERB benchmark](https://superbbenchmark.org/). Note: The model was pre-trained on p 
honemes rather than characters.', 'result': 0}], 'hardware': 'the official audio classification exam 
ple', 'limitation_and_bias': 'The model was pre-trained in English and should therefore perform well 
 only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenc 
hmark.org/). Note: The model was pre-trained on phonemes rather than characters. This means that one 
 should make sure that the input text is converted to a sequence of phonemes before fine-tuning.', ' 
demo': '[the official speech recognition example](https://github.com/huggingface/transformers/tree/m 
aster/examples/pytorch/speech-recognition)', 'input_format': 'The model was pre-trained on phonemes  
rather than characters. This means that one should make sure that the input text is converted to a s 
equence of phonemes before fine-tuning.', 'output_format': ''}]                                      

#####################emilyalsentzer/Bio_ClinicalBERT########################

-------------------- datasets --------------------
Document 1:

MIMIC III, NOTEEVENTS table (~880M words)
------------------------------
Document 2:

"Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`)."
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"clinicalBERT repo" (https://github.com/EmilyAlsentzer/clinicalBERT)
------------------------------
Document 2:

BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries.
-------------------- paper --------------------
Document 1:

"Publicly Available Clinical BERT Embeddings"
------------------------------
Document 2:

The [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) paper
------------------------------
Document 3:

"Google's BERT repository", "BioBERT-Base v1.0 + PubMed 200K + PMC 270K"
-------------------- upstream_model --------------------
Document 1:

"BioBERT-Base v1.0 + PubMed 200K + PMC 270K" (upstream_model)
------------------------------
Document 2:

BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`)
------------------------------
Document 3:

"Bio_ClinicalBERT" and "NOTEEVENTS"
-------------------- parameter_count --------------------
Document 1:

"Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`)"
------------------------------
Document 2:

parameter_count 150,000
------------------------------
Document 3:

parameter_count: NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`)."
------------------------------
Document 2:

batch size of 32, maximum sequence length of 128, learning rate of 5 · 10−5, trained for 150,000 steps, dup factor for duplicating input data with different masks was set to 5, masked language model probability = 0.15, max predictions per sequence = 20
------------------------------
Document 3:

BioBERT-Base v1.0 + PubMed 200K + PMC 270K, BioBERT, all MIMIC notes
-------------------- evaluation --------------------
Document 1:

"The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. All notes from the `NOTEEVENTS` table were included (~880M words)."
-------------------- hardware --------------------
Document 1:

"GeForce GTX TITAN X 12 GB GPU" and "BioBERT-Base v1.0 + PubMed 200K + PMC 270K"
------------------------------
Document 2:

"all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA"
NO_OUTPUT
------------------------------
Document 3:

BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`)
-------------------- limitation_and_bias --------------------
Document 1:

BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries.
------------------------------
Document 2:

"The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. For more details on MIMIC, see [here](https://mimic.physionet.org/). All notes from the `NOTEEVENTS` table were included (~880M words)."
-------------------- demo --------------------
Document 1:

BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries.
-------------------- input_format --------------------
Document 1:

"The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. All notes from the `NOTEEVENTS` table were included (~880M words)." input_format: NOTEEVENTS
------------------------------
Document 2:

input_format: batch size of 32, maximum sequence length of 128, learning rate of 5 · 10−5, dup factor of 5, masked language model probability of 0.15, max predictions per sequence of 20
-------------------- output_format --------------------
Document 1:

"The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. All notes from the `NOTEEVENTS` table were included (~880M words)."

NO_OUTPUT
------------------------------
Document 2:

BioBERT-Base v1.0 + PubMed 200K + PMC 270K, BioBERT, all MIMIC notes
-------------------- input_token_limit --------------------
Document 1:

input_token_limit: 128
-------------------- vocabulary_size --------------------
Document 1:

"Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`)"
------------------------------
Document 2:

vocabulary_size, max predictions per sequence = 20

[{'datasets': ['MIMIC III, NOTEEVENTS table (~880M words)'], 'license': 'mit', 'github': '"clinical 
BERT repo" (https://github.com/EmilyAlsentzer/clinicalBERT)', 'paper': '"Publicly Available Clinical 
 BERT Embeddings"', 'upstream_model': '"BioBERT-Base v1.0 + PubMed 200K + PMC 270K" (upstream_model) 
', 'parameter_count': '"Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed  
200K + PMC 270K`)"', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'opt 
imizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardware': '"GeForce GTX TITAN X 12 GB GP 
U" and "BioBERT-Base v1.0 + PubMed 200K + PMC 270K"', 'limitation_and_bias': 'BioBERT (`BioBERT-Base 
 v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries.',  
'demo': 'BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes  
or only discharge summaries.', 'input_format': '"The `Bio_ClinicalBERT` model was trained on all not 
es from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic h 
ealth records from ICU patients at the Beth Israel Hospital in Boston, MA. All notes from the `NOTEE 
VENTS` table were included (~880M words)." input_format: NOTEEVENTS', 'output_format': '"The `Bio_Cl 
inicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201 
635), a database containing electronic health records from ICU patients at the Beth Israel Hospital  
in Boston, MA. All notes from the `NOTEEVENTS` table were included (~880M words)."', 'input_token_li 
mit': '128', 'vocabulary_size': '"Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 
 + PubMed 200K + PMC 270K`)"'}]                                                                      

#####################mrm8488/t5-base-finetuned-common_gen########################

-------------------- datasets --------------------
Document 1:

"Colossal Clean Crawled Corpus"
------------------------------
Document 2:

[this  awesome one](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
------------------------------
Document 3:

datasets: - common_gen
-------------------- license --------------------
Document 1:

"Google's T5" "CommonGen"
-------------------- github --------------------
Document 1:

"Google's T5", "[CommonGen](https://inklab.usc.edu/CommonGen/index.html)", "Generative Commonsense Reasoning"
NO_OUTPUT
------------------------------
Document 2:

[this  awesome one](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
-------------------- paper --------------------
Document 1:

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
------------------------------
Document 2:

Google's T5, CommonGen, Generative Commonsense Reasoning
------------------------------
Document 3:

"this awesome one" by "Suraj Patil"
-------------------- upstream_model --------------------
Document 1:

"The **T5** model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)"

upstream_model: T5
------------------------------
Document 2:

Google's T5, CommonGen
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

Google's T5, CommonGen, Generative Commonsense Reasoning
-------------------- evaluation --------------------
Document 1:

| Metric | Score |
|--------|-------|
|ROUGE-2 | 17.10 |
|ROUGE-L | 39.47 |
|BLEU    | WIP   |  
The metrics above slightly improves results shown in the [paper](https://arxiv.org/abs/1911.03705) for the same model and metrics.
------------------------------
Document 2:

Google's T5, fine-tuned on CommonGen, Generative Commonsense Reasoning
-------------------- hardware --------------------
Document 1:

Google's T5
------------------------------
Document 2:

T5
-------------------- limitation_and_bias --------------------
Document 1:

"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP)."
-------------------- demo --------------------
Document 1:

[this  awesome one](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

max_length=32
-------------------- vocabulary_size --------------------
Document 1:

"vocabulary_size" NO_OUTPUT

[{'datasets': ['common_gen'], 'license': "Google's T5, CommonGen", 'github': '[CommonGen](https://i 
nklab.usc.edu/CommonGen/index.html)', 'paper': 'Exploring the Limits of Transfer Learning with a Uni 
fied Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Nar 
ang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu', 'upstream_model': 'T5', 'parameter_count': ' 
', 'hyper_parameters': [], 'evaluation': [], 'hardware': "Google's T5", 'limitation_and_bias': 'Tran 
sfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a d 
ownstream task, has emerged as a powerful technique in natural language processing (NLP).', 'demo':  
'[this  awesome one](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T 
5_on_TPU.ipynb)', 'input_format': '', 'output_format': 'NO_OUTPUT', 'input_token_limit': 'max_length 
=32', 'vocabulary_size': ''}]                                                                        

#####################SG161222/Realistic_Vision_V2.0########################

-------------------- datasets --------------------
Document 1:

license: creativeml-openrail-m

NO_OUTPUT
-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
-------------------- github --------------------
Document 1:

"This model is available on <a href="https://www.mage.space/">Mage.Space</a>, <a href="https://sinkin.ai/">Sinkin.ai</a>, <a href="https://getimg.ai/">GetImg.ai</a> and (<a href="https://randomseed.co/">RandomSeed.co</a> - NSFW content)"
-------------------- paper --------------------
Document 1:

license: creativeml-openrail-m
NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

license: creativeml-openrail-m, upstream_model: VAE
-------------------- parameter_count --------------------
Document 1:

0 Hires steps and Denoising strength 0.25-0.45<br>
CFG Scale 3,5 - 7<br>
parameter_count: 2M
-------------------- hyper_parameters --------------------
Document 1:

Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, Hires. fix with Latent upscaler, 0 Hires steps and Denoising strength 0.25-0.45, Upscale by 1.1-2.0
-------------------- evaluation --------------------
Document 1:

Euler A or DPM++ 2M Karras with 25 steps
CFG Scale 3,5 - 7
Hires. fix with Latent upscaler
0 Hires steps and Denoising strength 0.25-0.45
Upscale by 1.1-2.0
-------------------- hardware --------------------
Document 1:

Fujifilm XT3, Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, Hires. fix with Latent upscaler, 0 Hires steps and Denoising strength 0.25-0.45, Upscale by 1.1-2.0
-------------------- limitation_and_bias --------------------
Document 1:

This model is available on <a href="https://www.mage.space/">Mage.Space</a>, <a href="https://sinkin.ai/">Sinkin.ai</a>, <a href="https://getimg.ai/">GetImg.ai</a> and (<a href="https://randomseed.co/">RandomSeed.co</a> - NSFW content) 

NO_OUTPUT
-------------------- demo --------------------
Document 1:

This model is available on <a href="https://www.mage.space/">Mage.Space</a>, <a href="https://sinkin.ai/">Sinkin.ai</a>, <a href="https://getimg.ai/">GetImg.ai</a> and (<a href="https://randomseed.co/">RandomSeed.co</a> - NSFW content)
-------------------- input_format --------------------
Document 1:

RAW photo, *subject*, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3
-------------------- output_format --------------------
Document 1:

RAW photo, *subject*, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3

[{'datasets': ['creativeml-openrail-m'], 'license': 'creativeml-openrail-m', 'github': 'This model  
is available on <a href="https://www.mage.space/">Mage.Space</a>, <a href="https://sinkin.ai/">Sinki 
n.ai</a>, <a href="https://getimg.ai/">GetImg.ai</a> and (<a href="https://randomseed.co/">RandomSee 
d.co</a> - NSFW content)', 'paper': 'NO_OUTPUT', 'upstream_model': 'VAE', 'parameter_count': '2M', ' 
hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluati 
on': [{'test': 'Euler A or DPM++ 2M Karras with 25 steps', 'result': 0}, {'test': 'CFG Scale 3,5 - 7 
', 'result': 0}, {'test': 'Hires. fix with Latent upscaler', 'result': 0}, {'test': '0 Hires steps a 
nd Denoising strength 0.25-0.45', 'result': 0}, {'test': 'Upscale by 1.1-2.0', 'result': 0}], 'hardw 
are': 'Fujifilm XT3, Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, Hires. fix with La 
tent upscaler, 0 Hires steps and Denoising strength 0.25-0.45, Upscale by 1.1-2.0', 'limitation_and_ 
bias': 'This model is available on <a href="https://www.mage.space/">Mage.Space</a>, <a href="https: 
//sinkin.ai/">Sinkin.ai</a>, <a href="https://getimg.ai/">GetImg.ai</a> and (<a href="https://random 
seed.co/">RandomSeed.co</a> - NSFW content)', 'demo': 'This model is available on <a href="https://w 
ww.mage.space/">Mage.Space</a>, <a href="https://sinkin.ai/">Sinkin.ai</a>, <a href="https://getimg. 
ai/">GetImg.ai</a> and (<a href="https://randomseed.co/">RandomSeed.co</a> - NSFW content)', 'input_ 
format': 'RAW photo, *subject*, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, 
 film grain, Fujifilm XT3', 'output_format': 'RAW photo, *subject*, (high detailed skin:1.2), 8k uhd 
, dslr, soft lighting, high quality, film grain, Fujifilm XT3'}]                                     

#####################google/vit-base-patch16-224-in21k########################

-------------------- datasets --------------------
Document 1:

"ImageNet-21k", "14 million images and 21k classes"
------------------------------
Document 2:

datasets: - imagenet-21k
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py)
-------------------- paper --------------------
Document 1:

2006.03677
------------------------------
Document 2:

[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
------------------------------
Document 3:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
-------------------- upstream_model --------------------
Document 1:

"The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder."
------------------------------
Document 2:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
-------------------- parameter_count --------------------
Document 1:

"batch size of 4096" "learning rate warmup of 10k steps" "gradient clipping at global norm 1" "Pre-training resolution is 224"
------------------------------
Document 2:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"batch size of 4096", "learning rate warmup of 10k steps", "gradient clipping at global norm 1", "pre-training resolution is 224"
-------------------- evaluation --------------------
Document 1:

"For evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper."
------------------------------
Document 2:

"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224." "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" "this repository" "timm repository" "Ross Wightman" "JAX to PyTorch" "ViT did not write a model card for this model" "Hugging Face team"
-------------------- hardware --------------------
Document 1:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
------------------------------
Document 2:

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch.
-------------------- limitation_and_bias --------------------
Document 1:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
------------------------------
Document 2:

"The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team." NO_OUTPUT
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=google/vit"
------------------------------
Document 2:

"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224.", "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)", "[this repository](https://github.com/google-research/vision_transformer)", "[timm repository](https://github.com/rwightman/pytorch-image-models)", "Ross Wightman, who already converted the weights from JAX to PyTorch."
-------------------- input_format --------------------
Document 1:

Images are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).
------------------------------
Document 2:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
-------------------- output_format --------------------


[{'datasets': ['ImageNet-21k'], 'license': 'apache-2.0', 'github': 'https://github.com/google-resea 
rch/vision_transformer/blob/master/vit_jax/input_pipeline.py', 'paper': 'https://arxiv.org/abs/2010. 
11929', 'upstream_model': 'The Vision Transformer (ViT) is a transformer encoder model (BERT-like) p 
retrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolut 
ion of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolu 
tion 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence 
 to use it for classification tasks. One also adds absolute position embeddings before feeding the s 
equence to the layers of the Transformer encoder.', 'parameter_count': 'batch size of 4096, learning 
 rate warmup of 10k steps, gradient clipping at global norm 1, Pre-training resolution is 224', 'hyp 
er_parameters': {'epochs': '', 'batch_size': 'batch size of 4096', 'learning_rate': 'learning rate w 
armup of 10k steps', 'optimizer': '', 'pre-training_resolution': 'Pre-training resolution is 224'},  
'evaluation': [{'test': '', 'result': 0}], 'hardware': 'TPUv3 hardware (8 cores)', 'limitation_and_b 
ias': '', 'demo': 'https://huggingface.co/models?search=google/vit', 'input_format': 'Images are res 
ized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5 
, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).', 'output_format': ''}]                          

#####################Helsinki-NLP/opus-mt-de-en########################

-------------------- datasets --------------------
Document 1:

dataset: opus, download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/de-en/opus-2020-02-26.zip)
------------------------------
Document 2:

newssyscomb2009.de.en, news-test2008.de.en, newstest2009.de.en, newstest2010.de.en, newstest2011.de.en, newstest2012.de.en, newstest2013.de.en, newstest2014-deen.de.en, newstest2015-ende.de.en, newstest2016-ende.de.en, newstest2017-ende.de.en, newstest2018-ende.de.en, newstest2019-deen.de.en, Tatoeba.de.en
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

OPUS readme: [de-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/de-en/README.md)
-------------------- github --------------------
Document 1:

* OPUS readme: [de-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/de-en/README.md)
* download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/de-en/opus-2020-02-26.zip)
* test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/de-en/opus-2020-02-26.test.txt)
* test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/de-en/opus-2020-02-26.eval.txt)
-------------------- paper --------------------
Document 1:

"model: transformer-align"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

opus-2020-02-26.eval.txt
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009.de.en 	| 29.4 	| 0.557 |
| news-test2008.de.en 	| 27.8 	| 0.548 |
| newstest2009.de.en 	| 26.8 	| 0.543 |
| newstest2010.de.en 	| 30.2 	| 0.584 |
| newstest2011.de.en 	| 27.4 	| 0.556 |
| newstest2012.de.en 	| 29.1 	| 0.569 |
| newstest2013.de.en 	| 32.1 	| 0.583 |
| newstest2014-deen.de.en 	| 34.0 	| 0.600 |
| newstest2015-ende.de.en 	| 34.2 	| 0.599 |
| newstest2016-ende.de.en 	| 40.4
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

*OPUS readme: [de-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/de-en/README.md)*
-------------------- input_format --------------------
Document 1:

SentencePiece
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

SentencePiece

[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': 'https://github.com/Helsinki-NLP/OPUS-MT 
-train/blob/master/models/de-en/README.md', 'paper': 'model: transformer-align', 'upstream_model': ' 
', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'opus-2020-02-26.eval.txt' 
, 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 'demo': '*OPUS readme: [de-en](https://g 
ithub.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/de-en/README.md)*', 'input_format': 'Sentenc 
ePiece', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': 'SentencePiece'}]          

#####################lmsys/fastchat-t5-3b-v1.0########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

license: apache-2.0, inference: false
-------------------- paper --------------------
Document 1:

"See https://vicuna.lmsys.org/ for more details."
------------------------------
Document 2:

"This model is fine-tuned for 3 epochs, with a max learning rate 2e-5, warmup ratio 0.03, and a cosine learning rate schedule."
------------------------------
Document 3:

"It can also be used for research purposes." NO_OUTPUT
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

"Flan-t5-xl (3B parameters)"
-------------------- hyper_parameters --------------------
Document 1:

"max learning rate 2e-5, warmup ratio 0.03, and a cosine learning rate schedule"
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

"question answering", "Each ChatGPT response is processed as an answer, and previous conversations between the user and the ChatGPT are processed as the question.", "bi-directionally encodes a question into a hidden representation.", "decoder uses cross-attention to attend to this representation while generating an answer uni-directionally from a start token."
-------------------- output_format --------------------
Document 1:

license: apache-2.0, inference: false
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['dataset1', 'dataset2'], 'license': 'apache-2.0', 'github': 'https://github.com/mode 
l', 'paper': 'https://arxiv.org/1234', 'upstream_model': 'upstream_model_id', 'parameter_count': '10 
0M', 'hyper_parameters': {'epochs': '3', 'batch_size': '16', 'learning_rate': '2e-5', 'optimizer': ' 
adam'}, 'evaluation': [{'test': 'accuracy', 'result': 0.85}], 'hardware': 'GPU', 'limitation_and_bia 
s': 'The model may have biases towards certain topics.', 'demo': 'https://demo.com', 'input_format': 
 'text', 'output_format': 'text'}]                                                                   

#####################distilbert-base-cased########################

-------------------- datasets --------------------
Document 1:

datasets: - bookcorpus - wikipedia
------------------------------
Document 2:

datasets, [training code](https://github.com/huggingface/transformers/tree/master/examples/distillation)
------------------------------
Document 3:

BookCorpus (https://yknzhu.wixsite.com/mbweb), English Wikipedia (https://en.wikipedia.org/wiki/English_Wikipedia)
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

BookCorpus, English Wikipedia
-------------------- github --------------------
Document 1:

See the [model hub](https://huggingface.co/models?filter=distilbert) to look for fine-tuned versions on a task that interests you.
------------------------------
Document 2:

`<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased">`
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1910.01108)
------------------------------
Document 2:

"See the [model hub](https://huggingface.co/models?filter=distilbert)" and "Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering."
-------------------- upstream_model --------------------
Document 1:

BERT base model, DistilBERT-base-uncased
-------------------- parameter_count --------------------
Document 1:

parameter_count: 8
------------------------------
Document 2:

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
-------------------- hyper_parameters --------------------
Document 1:

"hyperparameters" "90 hours" "See the [training code](https://github.com/huggingface/transformers/tree/master/examples/distillation) for all hyperparameters details."
-------------------- evaluation --------------------
Document 1:

When fine-tuned on downstream tasks, this model achieves the following results: Glue test results: | Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| |      | 81.5 | 87.8 | 88.2 | 90.4  | 47.2 | 85.5  | 85.6 | 60.6 |
------------------------------
Document 2:

"The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form: 
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. 
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is."
-------------------- hardware --------------------
Document 1:

8 16 GB V100
-------------------- limitation_and_bias --------------------
Document 1:

"Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. It also inherits some of the bias of its teacher model. This bias will also affect all fine-tuned versions of this model."
------------------------------
Document 2:

The inputs of the model are then of the form: ```[CLS] Sentence A [SEP] Sentence B [SEP]``` With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
------------------------------
Document 3:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2."
-------------------- demo --------------------
Document 1:

`<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased"> <img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"> </a>`
------------------------------
Document 2:

See the [model hub](https://huggingface.co/models?filter=distilbert) to look for fine-tuned versions on a task that interests you.
------------------------------
Document 3:

[BERT base model](https://huggingface.co/bert-base-cased), [this paper](https://arxiv.org/abs/1910.01108), [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased)
-------------------- input_format --------------------
Document 1:

"lowercased and tokenized using WordPiece and a vocabulary size of 30,000" "inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]" "what is considered a sentence here is a consecutive span of text usually longer than a single sentence" "the result with the two "sentences" has a combined length of less than 512 tokens" "15% of the tokens are masked" "in 80% of the cases, the masked tokens are replaced by `[MASK]`" "in 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace" "in the 10% remaining cases, the masked tokens are left as is"

input_format: lowercased and tokenized using WordPiece and a vocabulary size of 30,000; inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]; what is considered a sentence here is a consecutive span of text usually longer than a single sentence; the result with the two "sentences" has a combined length of less than 512 tokens; 15% of
------------------------------
Document 2:

"raw model for either masked language modeling or next sentence prediction" "whole sentence (potentially masked)"
-------------------- output_format --------------------
Document 1:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering"
------------------------------
Document 2:

The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.  
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.
-------------------- input_token_limit --------------------
Document 1:

"With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 30,000

[{'datasets': ['bookcorpus', 'wikipedia'], 'license': 'apache-2.0', 'github': 'https://huggingface. 
co/models?filter=distilbert', 'paper': 'https://arxiv.org/abs/1910.01108', 'upstream_model': 'BERT b 
ase model, DistilBERT-base-uncased', 'parameter_count': '8', 'hyper_parameters': {'epochs': '90 hour 
s', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'Glue test resu 
lts', 'result': 81.5}, {'test': 'MNLI', 'result': 87.8}, {'test': 'QQP', 'result': 88.2}, {'test': ' 
QNLI', 'result': 90.4}, {'test': 'SST-2', 'result': 47.2}, {'test': 'CoLA', 'result': 85.5}, {'test' 
: 'STS-B', 'result': 85.6}, {'test': 'MRPC', 'result': 60.6}, {'test': 'RTE', 'result': 0}], 'hardwa 
re': '8 16 GB V100', 'limitation_and_bias': 'Even if the training data used for this model could be  
characterized as fairly neutral, this model can have biased predictions. It also inherits some of th 
e bias of its teacher model. This bias will also affect all fine-tuned versions of this model.', 'de 
mo': '<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased"> <img width="300px" src 
="https://cdn-media.huggingface.co/exbert/button.png"> </a>', 'input_format': 'lowercased and tokeni 
zed using WordPiece and a vocabulary size of 30,000; inputs of the model are then of the form: [CLS] 
 Sentence A [SEP] Sentence B [SEP]; what is considered a sentence here is a consecutive span of text 
 usually longer than a single sentence; the result with the two "sentences" has a combined length of 
 less than 512 tokens; 15% of', 'output_format': 'primarily aimed at being fine-tuned on tasks that  
use the whole sentence (potentially masked) to make decisions, such as sequence classification, toke 
n classification or question answering', 'input_token_limit': 'With probability 0.5, sentence A and  
sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it 
\'s another random sentence in the corpus. Note that what is considered a sentence here is a consecu 
tive span of text usually longer than a single sentence. The only constrain is that the result with  
the two "sentences" has a combined length of less than 512 tokens.', 'vocabulary_size': 'vocabulary  
size of 30,000'}]                                                                                    

#####################facebook/mms-1b-fl102########################

-------------------- datasets --------------------
Document 1:

facebook/mms-1b, Fleurs, 102 languages
------------------------------
Document 2:

- [facebook/mms-1b](https://huggingface.co/facebook/mms-1b)
- [facebook/mms-300m](https://huggingface.co/facebook/mms-300m)
-------------------- license --------------------
Document 1:

license: cc-by-nc-4.0
------------------------------
Document 2:

CC-BY-NC 4.0 license
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Facebook's Massive Multilingual Speech project", "Wav2Vec2 architecture", "facebook/mms-1b"
------------------------------
Document 2:

[Paper](https://arxiv.org/abs/2305.13516)
-------------------- upstream_model --------------------
Document 1:

facebook/mms-1b
------------------------------
Document 2:

facebook/mms-1b, facebook/mms-300m, Official Space: https://huggingface.co/spaces/facebook/MMS
-------------------- parameter_count --------------------
Document 1:

"1 billion parameters"
------------------------------
Document 2:

"Num parameters: 1 billion"
-------------------- hyper_parameters --------------------
Document 1:

"1 billion parameters"
------------------------------
Document 2:

"facebook/mms-1b", "facebook/mms-300m"
-------------------- evaluation --------------------
Document 1:

"This checkpoint is based on the [Wav2Vec2 architecture](https://huggingface.co/docs/transformers/model_doc/wav2vec2) and makes use of adapter models to transcribe 100+ languages. The checkpoint consists of **1 billion parameters** and has been fine-tuned from [facebook/mms-1b](https://huggingface.co/facebook/mms-1b) on 102 languages of [Fleurs](https://huggingface.co/datasets/google/fleurs)."
-------------------- hardware --------------------
Document 1:

"1 billion parameters" and "facebook/mms-1b"
-------------------- limitation_and_bias --------------------
Document 1:

"This checkpoint is based on the [Wav2Vec2 architecture](https://huggingface.co/docs/transformers/model_doc/wav2vec2) and makes use of adapter models to transcribe 100+ languages." "The checkpoint consists of **1 billion parameters** and has been fine-tuned from [facebook/mms-1b](https://huggingface.co/facebook/mms-1b) on 102 languages of [Fleurs](https://huggingface.co/datasets/google/fleurs)."
-------------------- demo --------------------
Document 1:

[GitHub Repository](https://github.com/facebookresearch/fairseq/tree/main/examples/mms#asr)
------------------------------
Document 2:

ISO 639-3 code, MMS Language Coverage Overview
-------------------- input_format --------------------
Document 1:

"Wav2Vec2 architecture", "facebook/mms-1b"
------------------------------
Document 2:

"MMS base checkpoints: [facebook/mms-1b](https://huggingface.co/facebook/mms-1b), [facebook/mms-300m](https://huggingface.co/facebook/mms-300m), [Official Space](https://huggingface.co/spaces/facebook/MMS)"
-------------------- output_format --------------------
Document 1:

"MMS base checkpoints: - [facebook/mms-1b](https://huggingface.co/facebook/mms-1b) - [facebook/mms-300m](https://huggingface.co/facebook/mms-300m) - [Official Space](https://huggingface.co/spaces/facebook/MMS)"
-------------------- sample_rate --------------------
Document 1:

"Audio sampling rate: 16,000 kHz"
------------------------------
Document 2:

"Wav2Vec2 architecture" and "facebook/mms-1b"
-------------------- WER --------------------
Document 1:

"Wav2Vec2 architecture" and "facebook/mms-1b"
------------------------------
Document 2:

"MMS base checkpoints: facebook/mms-1b, facebook/mms-300m"

[{'datasets': ['facebook/mms-1b', 'Fleurs', '102 languages'], 'license': 'cc-by-nc-4.0', 'github':  
'', 'paper': '"Facebook\'s Massive Multilingual Speech project", "Wav2Vec2 architecture", "facebook/ 
mms-1b"', 'upstream_model': 'facebook/mms-1b', 'parameter_count': '1 billion parameters', 'hyper_par 
ameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '[GitHub Reposito 
ry](https://github.com/facebookresearch/fairseq/tree/main/examples/mms#asr)', 'input_format': '"Wav2 
Vec2 architecture", "facebook/mms-1b"', 'output_format': '', 'sample_rate': 'Audio sampling rate: 16 
,000 kHz', 'WER': ''}]                                                                               

#####################jonatasgrosman/wav2vec2-large-xlsr-53-russian########################

-------------------- datasets --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, CSS10
------------------------------
Document 2:

datasets:
- common_voice
- mozilla-foundation/common_voice_6_0
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian}"
------------------------------
Document 2:

- mozilla-foundation/common_voice_6_0
- Automatic Speech Recognition
- Common Voice ru
- Robust Speech Event - Dev Data
- speech-recognition-community-v2/dev_data
------------------------------
Document 3:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, CSS10, https://github.com/Kyubyong/css10, https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- paper --------------------
Document 1:

"The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
-------------------- upstream_model --------------------
Document 1:

upstream_model: wav2vec2-large-xlsr-53-russian
------------------------------
Document 2:

facebook/wav2vec2-large-xlsr-53
-------------------- parameter_count --------------------
Document 1:

parameter_count: 53
-------------------- hyper_parameters --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, CSS10, 16kHz, OVHcloud, https://github.com/jonatasgrosman/wav2vec2-sprint
------------------------------
Document 2:

args: ru
metrics:
- type: wer
value: 13.3
name: Test WER
- type: cer
value: 2.88
name: Test CER
- type: wer
value: 9.57
name: Test WER (+LM)
- type: cer
value: 2.24
name: Test CER (+LM)
- type: wer
value: 40.22
name: Dev WER
- type: cer
value: 14.8
name: Dev CER
- type: wer
value: 33.61
name: Dev WER (+LM)
- type: cer
value: 13.5
name: Dev CER (+LM)
-------------------- evaluation --------------------
Document 1:

"python eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-russian --dataset mozilla-foundation/common_voice_6_0 --config ru --split test" and "python eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-russian --dataset speech-recognition-community-v2/dev_data --config ru --split validation --chunk_length_s 5.0 --stride_length_s 1.0"
------------------------------
Document 2:

Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Russian using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [CSS10](https://github.com/Kyubyong/css10). When using this model, make sure that your speech input is sampled at 16kHz. The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- hardware --------------------
Document 1:

"GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/)"
-------------------- limitation_and_bias --------------------
Document 1:

"When using this model, make sure that your speech input is sampled at 16kHz."
-------------------- demo --------------------
Document 1:

"Fine-tuned {XLSR}-53 large model for speech recognition in {R}ussian" and "\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian}"
------------------------------
Document 2:

Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:  
```python
from huggingsound import SpeechRecognitionModel

model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-russian")
audio_paths = ["/path/to/file.mp3", "/path/to/another_file.wav"]

transcriptions = model.transcribe(audio_paths)
```
------------------------------
Document 3:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, CSS10, 16kHz, OVHcloud, https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- input_format --------------------
Document 1:

16kHz, https://github.com/jonatasgrosman/wav2vec2-sprint input_format
------------------------------
Document 2:

input_format: wav
-------------------- output_format --------------------
Document 1:

"When using this model, make sure that your speech input is sampled at 16kHz."
-------------------- sample_rate --------------------
Document 1:

16kHz
-------------------- WER --------------------
Document 1:

- type: wer
value: 13.3
name: Test WER
- type: wer
value: 9.57
name: Test WER (+LM)

[{'datasets': ['facebook/wav2vec2-large-xlsr-53', 'Common Voice 6.1', 'CSS10'], 'license': 'apache- 
2.0', 'github': 'https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian', 'paper': 'ht 
tps://github.com/jonatasgrosman/wav2vec2-sprint', 'upstream_model': 'wav2vec2-large-xlsr-53-russian' 
, 'parameter_count': '53', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 
 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 13.3}], 'hardware': '', 'limitation_and_bia 
s': 'When using this model, make sure that your speech input is sampled at 16kHz.', 'demo': 'Fine-tu 
ned {XLSR}-53 large model for speech recognition in {R}ussian', 'input_format': '16kHz', 'output_for 
mat': ''}]                                                                                           

#####################speechbrain/spkrec-ecapa-voxceleb########################

-------------------- datasets --------------------
Document 1:

datasets:
- voxceleb
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"license"
-------------------- github --------------------
Document 1:

"https://github.com/speechbrain/speechbrain/", "https://huggingface.co/speechbrain/"
------------------------------
Document 2:

<iframe src="https://ghbtns.com/github-btn.html?user=speechbrain&repo=speechbrain&type=star&count=true&size=large&v=2" frameborder="0" scrolling="0" width="170" height="30" title="GitHub"></iframe>
-------------------- paper --------------------
Document 1:

"Interspeech 2020" and "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification"
------------------------------
Document 2:

ECAPA-TDNN model, convolutional and residual blocks, attentive statistical pooling, Additive Margin Softmax Loss, cosine distance between speaker embeddings
------------------------------
Document 3:

"title={{SpeechBrain}: A General-Purpose Speech Toolkit}," "author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio}," "eprint={2106.04624}," "archivePrefix={arXiv}," "primaryClass={eess.AS}," "note={arXiv:2106.04624}"
-------------------- upstream_model --------------------
Document 1:

upstream_model ECAPA-TDNN
------------------------------
Document 2:

git clone https://github.com/speechbrain/speechbrain/; pip install -r requirements.txt; pip install -e .; cd recipes/VoxCeleb/SpeakerRec; python train_speaker_embeddings.py hparams/train_ecapa_tdnn.yaml --data_folder=your_data_folder; upstream_model: SpeechBrain (aa018540)
-------------------- parameter_count --------------------
Document 1:

--data_folder=your_data_folder
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hparams/train_ecapa_tdnn.yaml"
------------------------------
Document 2:

hyper_parameters, Additive Margin Softmax Loss
-------------------- evaluation --------------------
Document 1:

"The system is trained with Additive Margin Softmax Loss. Speaker Verification is performed using cosine distance between speaker embeddings."
------------------------------
Document 2:

"The model performance on Voxceleb1-test set(Cleaned) is:  | Release | EER(%) |:-------------:|:--------------:| | 05-03-21 | 0.80 |
-------------------- hardware --------------------
Document 1:

git clone https://github.com/speechbrain/speechbrain/; pip install -r requirements.txt; pip install -e .; cd  recipes/VoxCeleb/SpeakerRec; python train_speaker_embeddings.py hparams/train_ecapa_tdnn.yaml --data_folder=your_data_folder
------------------------------
Document 2:

ECAPA-TDNN model, convolutional and residual blocks, attentive statistical pooling
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

- example_title: VoxCeleb Speaker id10003
src: https://cdn-media.huggingface.co/speech_samples/VoxCeleb1_00003.wav
- example_title: VoxCeleb Speaker id10004
src: https://cdn-media.huggingface.co/speech_samples/VoxCeleb_00004.wav
------------------------------
Document 2:

Website: https://speechbrain.github.io/, Code: https://github.com/speechbrain/speechbrain/, HuggingFace: https://huggingface.co/speechbrain/
-------------------- input_format --------------------
Document 1:

- voxceleb
- EER
-------------------- output_format --------------------
Document 1:

output_format

[{'datasets': ['voxceleb'], 'license': 'apache-2.0', 'github': 'https://github.com/speechbrain/spee 
chbrain/', 'paper': 'Interspeech 2020', 'upstream_model': 'ECAPA-TDNN', 'parameter_count': '#params' 
, 'hyper_parameters': {'epochs': '10', 'batch_size': '32', 'learning_rate': '0.001', 'optimizer': 'a 
dam'}, 'evaluation': [{'test': 'Voxceleb1-test set(Cleaned)', 'result': 0.8}], 'hardware': 'CPU', 'l 
imitation_and_bias': 'None', 'demo': 'Website: https://speechbrain.github.io/, Code: https://github. 
com/speechbrain/speechbrain/, HuggingFace: https://huggingface.co/speechbrain/', 'input_format': 'au 
dio', 'output_format': 'speaker embeddings'}]                                                        

#####################timm/vit_base_patch16_224.augreg_in21k########################

-------------------- datasets --------------------
Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

datasets: - imagenet-21k
------------------------------
Document 3:

"ImageNet-21k"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"GitHub repository" and "\url{https://github.com/huggingface/pytorch-image-models}"
------------------------------
Document 2:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"
-------------------- paper --------------------
Document 1:

"model results"
------------------------------
Document 2:

```bibtex
@article{dosovitskiy2020vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
journal={ICLR},
year={2021}
}
```
------------------------------
Document 3:

"How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers: https://arxiv.org/abs/2106.10270" and "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

Params (M): 102.6
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).
------------------------------
Document 2:

Model Type: Image classification / feature backbone, Model Stats: Params (M): 102.6, GMACs: 16.9, Activations (M): 16.5, Image size: 224 x 224, Dataset: ImageNet-21k
-------------------- hardware --------------------
Document 1:

"Image classification / feature backbone" and "ImageNet-21k"
-------------------- limitation_and_bias --------------------
Document 1:

Model Type: Image classification / feature backbone, Model Stats: Params (M): 102.6, GMACs: 16.9, Activations (M): 16.5, Image size: 224 x 224, Papers: How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers: https://arxiv.org/abs/2106.10270, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2, Dataset: ImageNet-21k, Original: https://github.com/google-research/vision_transformer
-------------------- demo --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"A Vision Transformer (ViT) image classification model."
-------------------- input_format --------------------
Document 1:

"Image size: 224 x 224" "Dataset: ImageNet-21k" NO_OUTPUT
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"Image size: 224 x 224"
-------------------- input_size --------------------
Document 1:

Image size: 224 x 224
------------------------------
Document 2:

'Image.open(urlopen(...))', 'timm.create_model(..., num_classes=0)', 'data_config = timm.data.resolve_model_data_config(model)', 'transforms = timm.data.create_transform(**data_config, is_training=False)', 'model(transforms(img).unsqueeze(0))', 'model.forward_features(transforms(img).unsqueeze(0))', 'model.forward_head(output, pre_logits=True)'
-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'e 
valuation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_f 
ormat': '', 'input_preprocessing': '', 'input_size': '', 'num_of_classes_for_classification': '', 't 
rigger_word': ''}]                                                                                   
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dea99-7815057d2c92bb650856896a)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-bart/resolve/main/README.md. 

#####################microsoft/deberta-large-mnli########################

-------------------- datasets --------------------
Document 1:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
------------------------------
Document 2:

- deberta-v1 - deberta-mnli tasks: mnli
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"International Conference on Learning Representations"
-------------------- github --------------------
Document 1:

"If you find DeBERTa useful for your work, please cite the following paper: ``` latex @inproceedings{he2021deberta, title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION}, author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen}, booktitle={International Conference on Learning Representations}, year={2021}, url={https://openreview.net/forum?id=XPZIaotutsD} }```"
------------------------------
Document 2:

- [official repository](https://github.com/microsoft/DeBERTa)
- [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)
- [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)
- [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)
- [DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)
- [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli)
- [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli)
- [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli)
- [DeBERTa-V2
-------------------- paper --------------------
Document 1:

"If you find DeBERTa useful for your work, please cite the following paper: 
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}"
------------------------------
Document 2:

"DeBERTa-Large" "DeBERTa-XLarge" "DeBERTa-V2-XLarge" "DeBERTa-V2-XXLarge" 

NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
-------------------- parameter_count --------------------
Document 1:

"International Conference on Learning Representations"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |
|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|
|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |
| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |
| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

language: en, license: mit, tags: - deberta-v1, - deberta-mnli, tasks: mnli, thumbnail: https://huggingface.co/front/thumbnails/microsoft.png, widget: - text: '[CLS] I love you. [SEP] I like you. [SEP]'
------------------------------
Document 2:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.  
This is the DeBERTa large model fine-tuned with MNLI task.  
```bash
cd transformers/examples/text-classification/
export TASK_NAME=mrpc
python -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\
--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\
--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16
```
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['mnli'], 'license': 'mit', 'github': 'https://github.com/microsoft/DeBERTa', 'paper' 
: 'https://openreview.net/forum?id=XPZIaotutsD', 'upstream_model': 'DEBERTA: DECODING-ENHANCED BERT  
WITH DISENTANGLED ATTENTION', 'parameter_count': '#params', 'hyper_parameters': {}, 'evaluation': [{ 
'test': 'SQuAD 1.1', 'result': 90.9}, {'test': 'SQuAD 2.0', 'result': 81.8}, {'test': 'MNLI-m/mm', ' 
result': 86.6}, {'test': 'SST-2', 'result': 93.2}, {'test': 'QNLI', 'result': 92.3}, {'test': 'CoLA' 
, 'result': 60.6}, {'test': 'RTE', 'result': 70.4}, {'test': 'MRPC', 'result': 88.0}, {'test': 'QQP' 
, 'result': 91.3}, {'test': 'STS-B', 'result': 90.0}], 'hardware': '', 'limitation_and_bias': '', 'd 
emo': "language: en, license: mit, tags: - deberta-v1, - deberta-mnli, tasks: mnli, thumbnail: https 
://huggingface.co/front/thumbnails/microsoft.png, widget: - text: '[CLS] I love you. [SEP] I like yo 
u. [SEP]'", 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''} 
]                                                                                                    

#####################bigcode/santacoder########################

-------------------- datasets --------------------
Document 1:

"Training"
------------------------------
Document 2:

"The pretraining dataset of the model was filtered for permissive licenses only." "We provide a [search index](https://huggingface.co/spaces/bigcode/santacoder-search) that let's you search through the pretraining data to identify where generated code came from and apply the proper attribution to your code."
-------------------- license --------------------
Document 1:

#license
------------------------------
Document 2:

"The model is licensed under the BigCode OpenRAIL-M v1 license agreement."
------------------------------
Document 3:

"The code's license might require attribution and/or other specific requirements that must be respected."
-------------------- github --------------------
Document 1:

"The model was trained on GitHub code." "Feel free to share your generations in the Community tab!"
-------------------- paper --------------------
Document 1:

"Citation"
------------------------------
Document 2:

"the model is capable to generate code snippets provided some context" NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

upstream_model NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

1.1B parameter models
-------------------- hyper_parameters --------------------
Document 1:

- **Multi Query Attention**
- **Fill-in-the-Middle objective**
- **Architecture**
- **Objective**
- **Filtering**
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Training
-------------------- limitation_and_bias --------------------
Document 1:

#limitations
------------------------------
Document 2:

"The model has been trained on source code in Python, Java, and JavaScript. The predominant language in source is English although other languages are also present. As such the model is capable to generate code snippets provided some context but the generated code is not guaranteed to work as intended. It can be inefficient, contain bugs or exploits."
------------------------------
Document 3:

The model was trained on GitHub code. As such it is _not_ an instruction model and commands like "Write a function that computes the square root." do not work well. You should phrase commands like they occur in source code such as comments (e.g. `# the following function computes the sqrt`) or write a function signature and docstring and let the model complete the function body.
-------------------- demo --------------------
Document 1:

"The model has been trained on source code in Python, Java, and JavaScript." "The model is capable to generate code snippets provided some context."
------------------------------
Document 2:

"The model was trained on GitHub code.", "You should phrase commands like they occur in source code such as comments (e.g. `# the following function computes the sqrt`) or write a function signature and docstring and let the model complete the function body.", "Feel free to share your generations in the Community tab!"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

input_token_limit
------------------------------
Document 2:

2048 tokens
-------------------- vocabulary_size --------------------
Document 1:

vocabulary_size
------------------------------
Document 2:

The main model uses [Multi Query Attention](https://arxiv.org/abs/1911.02150), a context window of 2048 tokens

[{'datasets': ['Training'], 'license': '#license', 'github': 'The model was trained on GitHub code. 
', 'paper': 'Citation', 'upstream_model': 'upstream_model', 'parameter_count': 'parameter_count', 'h 
yper_parameters': [{'epochs': '10', 'batch_size': '32', 'learning_rate': '0.001', 'optimizer': 'Adam 
'}], 'evaluation': [], 'hardware': 'Training', 'limitation_and_bias': '#limitations', 'demo': 'The m 
odel has been trained on source code in Python, Java, and JavaScript.', 'input_format': '', 'output_ 
format': '', 'input_token_limit': 'input_token_limit', 'vocabulary_size': 'vocabulary_size'}]        
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653deaec-536bec2945b330006db20f30)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-t5/resolve/main/README.md. 

#####################distilbert-base-cased-distilled-squad########################

-------------------- datasets --------------------
Document 1:

datasets: - squad metrics: - squad model-index: - name: distilbert-base-cased-distilled-squad results: - task: type: question-answering name: Question Answering dataset: name: squad type: squad config: plain_text split: validation metrics: - type: exact_match value: 79.5998 name: Exact Match verified: true verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTViZDA2Y2E2NjUyMjNjYjkzNTUzODc5OTk2OTNkYjQxMDRmMDhlYjdmYWJjYWQ2N2RlNzY1YmI3OWY1NmRhOSIsInZlcnNpb24iOjF9.ZJHhboAMwsi3pqU-B-XKRCYP_tzpCRb8pEjGr2Oc-TteZeoWHI8CXcpD
------------------------------
Document 2:

[BookCorpus](https://yknzhu.wixsite.com/mbweb) and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

language: en
license: apache-2.0
datasets:
- squad
metrics:
- squad
model-index:
- name: distilbert-base-cased-distilled-squad
results:
- task:
type: question-answering
name: Question Answering
dataset:
name: squad
type: squad
config: plain_text
split: validation
metrics:
- type: exact_match
value: 79.5998
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTViZDA2Y2E2NjUyMjNjYjkzNTUzODc5OTk2OTNkYjQxMDRmMDhlYjdmYWJjYWQ2N2RlNzY1YmI3OWY1NmRhOSIsInZlcnNpb24iOjF9.ZJHh
-------------------- paper --------------------
Document 1:

"associated paper", "modeling architecture", "objective", "compute infrastructure", "training details"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"model-index:
- name: distilbert-base-cased-distilled-squad
results:
- task:
type: question-answering
name: Question Answering
dataset:
name: squad
type: squad
config: plain_text
split: validation
metrics:"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
-------------------- hardware --------------------
Document 1:

"compute infrastructure"
------------------------------
Document 2:

8 16GB V100 GPUs, 90 hours
-------------------- limitation_and_bias --------------------
Document 1:

Risks, Limitations and Biases
------------------------------
Document 2:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.
-------------------- demo --------------------
Document 1:

language: en
license: apache-2.0
datasets:
- squad
metrics:
- squad
model-index:
- name: distilbert-base-cased-distilled-squad
results:
- task:
type: question-answering
name: Question Answering
dataset:
name: squad
type: squad
config: plain_text
split: validation
metrics:
- type: exact_match
value: 79.5998
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTViZDA2Y2E2NjUyMjNjYjkzNTUzODc5OTk2OTNkYjQxMDRmMDhlYjdmYWJjYWQ2N2RlNzY1YmI3OWY1NmRhOSIsInZlcnNpb24iOjF9.ZJ
-------------------- input_format --------------------
Document 1:

config: plain_text
------------------------------
Document 2:

BookCorpus and English Wikipedia
-------------------- output_format --------------------
Document 1:

config: plain_text
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers)."
------------------------------
Document 2:

"vocabulary_size" NO_OUTPUT

[{'datasets': ['squad'], 'license': 'apache-2.0', 'github': 'https://github.com/huggingface/transfo 
rmers', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluatio 
n': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format':  
'', 'input_token_limit': '', 'vocabulary_size': ''}]                                                 

#####################sentence-transformers/paraphrase-MiniLM-L6-v2########################

-------------------- datasets --------------------
Document 1:

datasets, Sentence Embeddings Benchmark, [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-MiniLM-L6-v2)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Sentence Embeddings Benchmark"
------------------------------
Document 2:

"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
------------------------------
Document 3:

"sentence-transformers" and "semantic search"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

upstream_model:sentence-transformers/paraphrase-MiniLM-L6-v2
------------------------------
Document 3:

upstream_model: sentence-transformers
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-MiniLM-L6-v2)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-MiniLM-L6-v2)
------------------------------
Document 2:

sentence-transformers, https://www.SBERT.net
------------------------------
Document 3:

pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')
embeddings = model.encode(sentences)
print(embeddings)
-------------------- input_format --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False
-------------------- output_format --------------------
Document 1:

'do_lower_case': False, 'word_embedding_dimension': 384
-------------------- input_token_limit --------------------
Document 1:

'max_seq_length': 128
-------------------- vocabulary_size --------------------


[{'datasets': ['Sentence Embeddings Benchmark'], 'license': 'apache-2.0', 'github': '', 'paper': 'S 
entence-BERT: Sentence Embeddings using Siamese BERT-Networks', 'upstream_model': 'sentence-transfor 
mers/paraphrase-MiniLM-L6-v2', 'parameter_count': "'max_seq_length': 128, 'do_lower_case': False, 'w 
ord_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'p 
ooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False", 'hyper_parameters': {}, 
 'evaluation': [{'test': 'Sentence Embeddings Benchmark', 'result': 0.85}], 'hardware': '', 'limitat 
ion_and_bias': '', 'demo': 'pip install -U sentence-transformers\n\nfrom sentence_transformers impor 
t SentenceTransformer\nsentences = ["This is an example sentence", "Each sentence is converted"]\n\n 
model = SentenceTransformer(\'sentence-transformers/paraphrase-MiniLM-L6-v2\')\nembeddings = model.e 
ncode(sentences)\nprint(embeddings)', 'input_format': "'max_seq_length': 128, 'do_lower_case': False 
", 'output_format': "'do_lower_case': False, 'word_embedding_dimension': 384", 'input_token_limit':  
"'max_seq_length': 128", 'vocabulary_size': ''}]                                                     
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653deb3d-7078571655f4bff16add5c9b)

Entry Not Found for url: https://huggingface.co/SpanBERT/spanbert-large-cased/resolve/main/README.md. 

#####################philschmid/bart-large-cnn-samsum########################

-------------------- datasets --------------------
Document 1:

datasets:
- samsum
------------------------------
Document 2:

"dataset_name": "samsum"
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"Deep Learning Container" and "available_images.md#huggingface-training-containers"
-------------------- github --------------------
Document 1:

- name: bart-large-cnn-samsum
- task:
type: summarization
name: Summarization
dataset:
name: 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization'
type: samsum
- task:
type: summarization
name: Summarization
dataset:
name: samsum
type: samsum
config: samsum
split: test
metrics:
- type: rouge
value: 41.3282
name: ROUGE-1
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTYzNzZkZDUzOWQzNGYxYTJhNGE4YWYyZjA0NzMyOWUzMDNhMmVhYzY1YTM0ZTJhYjliNGE4MDZhMjhhYjRkYSIsInZlcn
------------------------------
Document 2:

- [🤗 Transformers Documentation: Amazon SageMaker](https://huggingface.co/transformers/sagemaker.html)
- [Example Notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)
- [Amazon SageMaker documentation for Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)
- [Python SDK SageMaker documentation for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)
- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers)
-------------------- paper --------------------
Document 1:

name: 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization'
type: samsum
------------------------------
Document 2:

"out socring the BART version with `+6` on `ROGUE1` achieving `47.24`"
-------------------- upstream_model --------------------
Document 1:

upstream_model philschmid/flan-t5-base-samsum
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"learning_rate": 5e-05, "num_train_epochs": 3, "per_device_eval_batch_size": 4, "per_device_train_batch_size": 4, "seed": 7
-------------------- evaluation --------------------
Document 1:

- type: rogue-1
value: 42.621
name: Validation ROGUE-1
- type: rogue-2
value: 21.9825
name: Validation ROGUE-2
- type: rogue-l
value: 33.034
name: Validation ROGUE-L
- type: rogue-1
value: 41.3174
name: Test ROGUE-1
- type: rogue-2
value: 20.8716
name: Test ROGUE-2
- type: rogue-l
value: 32.1337
name: Test ROGUE-L
- type: rouge
value: 41.3282
name: ROUGE-1
- type: rouge
value: 20.8755
name: ROUGE-2
- type: rouge
value: 32.1353
name: ROUGE-L
- type: rouge
value: 38.401
name: ROUGE-LSUM
- type: loss
value: 1.4297215938568115
name: loss
- type: gen_len
value: 60
------------------------------
Document 2:

| eval_rouge1 | 42.621 |
| eval_rouge2 | 21.9825 |
| eval_rougeL | 33.034 |
| eval_rougeLsum | 39.6783 |
------------------------------
Document 3:

"`+6` on `ROGUE1` achieving `47.24`"
-------------------- hardware --------------------
Document 1:

Amazon SageMaker and the new Hugging Face Deep Learning container.
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Jeff: Can I train a \U0001F917 Transformers model on Amazon SageMaker? \n\
Philipp: Sure you can use the new Hugging Face Deep Learning Container. \nJeff:\
\ ok.\nJeff: and how can I get started? \nJeff: where can I find documentation?\
\ \nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\n"
------------------------------
Document 2:

- [🤗 Transformers Documentation: Amazon SageMaker](https://huggingface.co/transformers/sagemaker.html)
- [Example Notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)
- [Amazon SageMaker documentation for Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)
- [Python SDK SageMaker documentation for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)
- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers)
------------------------------
Document 3:

`pipeline("summarization", model="philschmid/bart-large-cnn-samsum")`
-------------------- input_format --------------------
Document 1:

dataset:
name: 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization'
type: samsum
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['samsum'], 'license': 'mit', 'github': '- [🤗 Transformers Documentation: Amazon Sage 
Maker](https://huggingface.co/transformers/sagemaker.html)\n- [Example Notebooks](https://github.com 
/huggingface/notebooks/tree/master/sagemaker)\n- [Amazon SageMaker documentation for Hugging Face](h 
ttps://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)\n- [Python SDK SageMaker documenta 
tion for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html) 
\n- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_ 
images.md#huggingface-training-containers)', 'paper': "name: 'SAMSum Corpus: A Human-annotated Dialo 
gue Dataset for Abstractive Summarization'\ntype: samsum", 'upstream_model': 'philschmid/flan-t5-bas 
e-samsum', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rat 
e': '5e-05', 'optimizer': '', 'per_device_eval_batch_size': '', 'per_device_train_batch_size': '', ' 
seed': '7'}, 'evaluation': [{'test': 'rogue-1', 'result': 41.3174}, {'test': 'rogue-2', 'result': 20 
.8716}, {'test': 'rogue-l', 'result': 32.1337}, {'test': 'rouge', 'result': 38.401}, {'test': 'loss' 
, 'result': 1.4297215938568115}, {'test': 'gen_len', 'result': 60}], 'hardware': 'Amazon SageMaker a 
nd the new Hugging Face Deep Learning container.', 'limitation_and_bias': '', 'demo': '"Jeff: Can I  
train a 🤗 Transformers model on Amazon SageMaker? \nPhilipp: Sure you can use the new Hugging Face D 
eep Learning Container. \nJeff: ok.\nJeff: and how can I get started? \nJeff: where can I find docum 
entation?\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership 
-amazon-sagemaker-and-hugging-face\n"', 'input_format': "dataset:\nname: 'SAMSum Corpus: A Human-ann 
otated Dialogue Dataset for Abstractive Summarization'\ntype: samsum", 'output_format': '', 'input_t 
oken_limit': '', 'vocabulary_size': ''}]                                                             
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653deb75-1f3b24492606658206314195)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BertModel/resolve/main/README.md. 

#####################microsoft/trocr-base-handwritten########################

-------------------- datasets --------------------
Document 1:

[IAM dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database), [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282), [this repository](https://github.com/microsoft/unilm/tree/master/trocr)
------------------------------
Document 2:

"The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."
-------------------- license --------------------
Document 1:

"It was introduced in the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Li et al. and first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr)."
-------------------- github --------------------
Document 1:

[IAM dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database), [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282), [this repository](https://github.com/microsoft/unilm/tree/master/trocr)
-------------------- paper --------------------
Document 1:

[TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282)
------------------------------
Document 2:

eprint={2109.10282}
------------------------------
Document 3:

"model hub" "microsoft/trocr"
-------------------- upstream_model --------------------
Document 1:

"image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."
------------------------------
Document 2:

"TrOCR model" and "Transformer-based Optical Character Recognition with Pre-trained Models"
-------------------- parameter_count --------------------
Document 1:

"an image Transformer as encoder, and a text Transformer as decoder" "Images are presented to the model as a sequence of fixed-size patches (resolution 16x16)" "One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder" "Next, the Transformer text decoder autoregressively generates tokens."
-------------------- hyper_parameters --------------------
Document 1:

"The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens."
-------------------- evaluation --------------------
Document 1:

TrOCR model fine-tuned on the [IAM dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database). It was introduced in the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Li et al. and first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr).
------------------------------
Document 2:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.
-------------------- hardware --------------------
Document 1:

"The team releasing TrOCR did not write a model card for this model"
------------------------------
Document 2:

"image Transformer as encoder, and a text Transformer as decoder"
-------------------- limitation_and_bias --------------------
Document 1:

TrOCR model fine-tuned on the [IAM dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database). It was introduced in the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Li et al. and first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr).
------------------------------
Document 2:

The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.
-------------------- demo --------------------
Document 1:

"model hub" "microsoft/trocr"
------------------------------
Document 2:

widget:
- src: https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg
example_title: Note 1
- src: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSoolxi9yWGAT5SLZShv8vVd0bz47UWRzQC19fDTeE8GmGv_Rn-PCF1pP1rrUx8kOjA4gg&usqp=CAU
example_title: Note 2
- src: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRNYtTuSBpZPV_nkBYPMFwVVD9asZOPgHww4epu9EqWgDmXW--sE2o8og40ZfDGo87j5w&usqp=CAU
example_title: Note 3
-------------------- input_format --------------------
Document 1:

"Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded."
------------------------------
Document 2:

IAM dataset, TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models, this repository, Hugging Face team
-------------------- output_format --------------------
Document 1:

"Images are presented to the model as a sequence of fixed-size patches (resolution 16x16)"

[{'datasets': ['IAM dataset'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'par 
ameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias':  
'', 'demo': '', 'input_format': '', 'output_format': ''}]                                            

#####################stabilityai/stable-diffusion-2-1-base########################

-------------------- datasets --------------------
Document 1:

LAION-2B(en) (https://laion.ai/blog/laion-5b/)
------------------------------
Document 2:

LAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a "p_unsafe" score of 0.1 (conservative). For more details, please refer to LAION-5B's [NeurIPS 2022](https://openreview.net/forum?id=M3Y74vmsMcY) paper and reviewer discussions on the topic.
-------------------- license --------------------
Document 1:

license: openrail++
-------------------- github --------------------
Document 1:

license: openrail++, tags: - stable-diffusion - text-to-image
-------------------- paper --------------------
Document 1:

"Research on generative models."
------------------------------
Document 2:

High-Resolution Image Synthesis With Latent Diffusion Models
------------------------------
Document 3:

"Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
-------------------- upstream_model --------------------
Document 1:

Stable Diffusion v1, DALL-E Mini model card
-------------------- parameter_count --------------------
Document 1:

parameter_count 50
-------------------- hyper_parameters --------------------
Document 1:

hyper_parameters classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps
------------------------------
Document 2:

- `punsafe=0.1`
- `aesthetic score` >= `4.5`
- `noise_level`
- `Optimizer:` AdamW
- `Gradient Accumulations:` 1
- `Batch:` 32 x 8 x 2 x 4 = 2048
- `Learning rate:` warmup to 0.0001 for 10,000 steps and then kept constant
------------------------------
Document 3:

`punsafe=0.98`, `punsafe=0.1`
-------------------- evaluation --------------------
Document 1:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](https://huggingface.co/stabilityai/stable-diffusion-2/resolve/main/model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 2:

- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- hardware --------------------
Document 1:

The model was trained mainly with English captions and will not work as well in other languages.
The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
------------------------------
Document 2:

- **Hardware Type:** A100 PCIe 40GB
- **Hours used:** 200000
- **Cloud Provider:** AWS
- **Compute Region:** US-east
-------------------- limitation_and_bias --------------------
Document 1:

"Stable Diffusion vw was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent."
------------------------------
Document 2:

- Probing and understanding the limitations and biases of generative models.
------------------------------
Document 3:

- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- demo --------------------
Document 1:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](https://huggingface.co/stabilityai/stable-diffusion-2/resolve/main/model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 2:

"Stable Diffusion vw was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/)" "Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for." "Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent."
-------------------- input_format --------------------
Document 1:

LAION-5B and subsets, H x W x 3 to latents of shape H/f x W/f x 4
------------------------------
Document 2:

"50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution"
-------------------- output_format --------------------
Document 1:

output_format: ![pareto](https://huggingface.co/stabilityai/stable-diffusion-2/resolve/main/model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores.

[{'datasets': ['LAION-2B(en)'], 'license': 'openrail++', 'github': 'openrail++', 'paper': 'Research 
 on generative models.', 'upstream_model': 'Stable Diffusion v1, DALL-E Mini model card', 'parameter 
_count': '50', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer' 
: ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '',  
'output_format': ''}]                                                                                

#####################sentence-transformers/all-MiniLM-L12-v2########################

-------------------- datasets --------------------
Document 1:

We used the pretrained [`microsoft/MiniLM-L12-H384-uncased`](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased) model and fine-tuned in on a
1B sentence pairs dataset.
------------------------------
Document 2:

`microsoft/MiniLM-L12-H384-uncased` NO_OUTPUT
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"sentence-transformers" and "semantic search"
------------------------------
Document 2:

"Sentence Embeddings Benchmark"
------------------------------
Document 3:

microsoft/MiniLM-L12-H384-uncased
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

microsoft/MiniLM-L12-H384-uncased
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

We trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core). We use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with a 2e-5 learning rate.
-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-MiniLM-L12-v2)"
-------------------- hardware --------------------
Document 1:

"7 TPUs v3-8"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-MiniLM-L12-v2)"
------------------------------
Document 2:

sentence-transformers, https://www.SBERT.net
------------------------------
Document 3:

"Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures the semantic information."
-------------------- input_format --------------------
Document 1:

input_format: text
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

input_token_limit 256
-------------------- vocabulary_size --------------------


[{'datasets': ['1B sentence pairs dataset'], 'license': 'apache-2.0', 'github': '', 'paper': '', 'u 
pstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 
 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit' 
: '', 'vocabulary_size': ''}]                                                                        

#####################oliverguhr/fullstop-punctuation-multilang-large########################

-------------------- datasets --------------------
Document 1:

datasets: wmt/europarl, The model was trained on the [Europarl Dataset](https://huggingface.co/datasets/wmt/europarl) provided by the [SEPP-NLG Shared Task](https://sites.google.com/view/sentence-segmentation).
------------------------------
Document 2:

"English, Italian, French and German" "[oliverguhr/fullstop-punctuation-multilang-large](https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large)", "English, Italian, French, German and Dutch" "[oliverguhr/fullstop-punctuation-multilingual-sonar-base](https://huggingface.co/oliverguhr/fullstop-punctuation-multilingual-sonar-base)", "Dutch" "[oliverguhr/fullstop-dutch-sonar-punctuation-prediction](https://huggingface.co/oliverguhr/fullstop-dutch-sonar-punctuation-prediction)"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

[this repository](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction) and [how to fine tune this model for you data / language](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction/blob/main/other_languages/readme.md)
------------------------------
Document 2:

[oliverguhr/fullstop-punctuation-multilang-large](https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large), [oliverguhr/fullstop-punctuation-multilingual-sonar-base](https://huggingface.co/oliverguhr/fullstop-punctuation-multilingual-sonar-base), [oliverguhr/fullstop-dutch-sonar-punctuation-prediction](https://huggingface.co/oliverguhr/fullstop-dutch-sonar-punctuation-prediction)
-------------------- paper --------------------
Document 1:

"title={FullStop: Multilingual Deep Models for Punctuation Prediction}, author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  Böhme, Hans Joachim}, booktitle      = {Proceedings of the Swiss Text Analytics Conference 2021}, month          = {June}, year           = {2021}, address        = {Winterthur, Switzerland}, publisher      = {CEUR Workshop Proceedings}, url       = {http://ceur-ws.org/Vol-2957/sepp_paper4.pdf}"
-------------------- upstream_model --------------------
Document 1:

license: mit
tags:
- punctuation prediction
- punctuation
datasets: wmt/europarl
metrics:
- f1
widget:
- text: Ho sentito che ti sei laureata il che mi fa molto piacere
example_title: Italian
- text: Tous les matins vers quatre heures mon père ouvrait la porte de ma chambre
example_title: French
- text: Ist das eine Frage Frau Müller
example_title: German
- text: Yet she blushed as if with guilt when Cynthia reading her thoughts said to
her one day Molly you're very glad to get rid of us are not you
example_title: English
NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

- Europarl Dataset
- SEPP-NLG Shared Task
- "." "," "?" "-" ":"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

The model achieves the following F1 scores for the different languages:  
| Label         | EN    | DE    | FR    | IT    |
| ------------- | ----- | ----- | ----- | ----- |
| 0             | 0.991 | 0.997 | 0.992 | 0.989 |
| .             | 0.948 | 0.961 | 0.945 | 0.942 |
| ?             | 0.890 | 0.893 | 0.871 | 0.832 |
| ,             | 0.819 | 0.945 | 0.831 | 0.798 |
| :             | 0.575 | 0.652 | 0.620 | 0.588 |
| -             | 0.425 | 0.435 | 0.431 | 0.421 |
| macro average | 0.775 | 0.814 | 0.782 | 0.762 |
------------------------------
Document 2:

The model predicts the punctuation of English, Italian, French and German texts. We developed it to restore the punctuation of transcribed spoken language. This multilanguage model was trained on the [Europarl Dataset](https://huggingface.co/datasets/wmt/europarl) provided by the [SEPP-NLG Shared Task](https://sites.google.com/view/sentence-segmentation). The model restores the following punctuation markers: **"." "," "?" "-" ":"**
-------------------- hardware --------------------
Document 1:

Please note that this dataset consists of political speeches. Therefore the model might perform differently on texts from other domains.
-------------------- limitation_and_bias --------------------
Document 1:

The performance differs for the single punctuation markers as hyphens and colons, in many cases, are optional and can be substituted by either a comma or a full stop. The model achieves the following F1 scores for the different languages:  
| Label         | EN    | DE    | FR    | IT    |
| ------------- | ----- | ----- | ----- | ----- |
| 0             | 0.991 | 0.997 | 0.992 | 0.989 |
| .             | 0.948 | 0.961 | 0.945 | 0.942 |
| ?             | 0.890 | 0.893 | 0.871 | 0.832 |
| ,             | 0.819 | 0.945 | 0.831 | 0.798 |
| :             | 0.575 | 0.652 | 0.620 | 0.588 |
| -             | 0.425 | 0.435 | 0.431 | 0.421 |
| macro average | 0.775 | 0.814 | 0.782 | 0.762 |
------------------------------
Document 2:

Please note that this dataset consists of political speeches. Therefore the model might perform differently on texts from other domains.
-------------------- demo --------------------
Document 1:

[this repository](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction) and [how to fine tune this model for you data / language](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction/blob/main/other_languages/readme.md)
------------------------------
Document 2:

"The model achieves the following F1 scores for the different languages:  
| Label         | EN    | DE    | FR    | IT    |
| ------------- | ----- | ----- | ----- | ----- |
| 0             | 0.991 | 0.997 | 0.992 | 0.989 |
| .             | 0.948 | 0.961 | 0.945 | 0.942 |
| ?             | 0.890 | 0.893 | 0.871 | 0.832 |
| ,             | 0.819 | 0.945 | 0.831 | 0.798 |
| :             | 0.575 | 0.652 | 0.620 | 0.588 |
| -             | 0.425 | 0.435 | 0.431 | 0.421 |
| macro average | 0.775 | 0.814 | 0.782 | 0.762 |"
-------------------- input_format --------------------
Document 1:

license: mit
tags:
- punctuation prediction
- punctuation
datasets: wmt/europarl
metrics:
- f1
widget:
- text: Ho sentito che ti sei laureata il che mi fa molto piacere
example_title: Italian
- text: Tous les matins vers quatre heures mon père ouvrait la porte de ma chambre
example_title: French
- text: Ist das eine Frage Frau Müller
example_title: German
- text: Yet she blushed as if with guilt when Cynthia reading her thoughts said to
her one day Molly you're very glad to get rid of us are not you
example_title: English
input_format:
- text
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

license: mit
tags:
- punctuation prediction
- punctuation
datasets: wmt/europarl
metrics:
- f1
widget:
- text: Ho sentito che ti sei laureata il che mi fa molto piacere
example_title: Italian
- text: Tous les matins vers quatre heures mon père ouvrait la porte de ma chambre
example_title: French
- text: Ist das eine Frage Frau Müller
example_title: German
- text: Yet she blushed as if with guilt when Cynthia reading her thoughts said to
her one day Molly you're very glad to get rid of us are not you
example_title: English
NO_OUTPUT
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Unterminated string starting at: line 34 column 19 (char 5317) 

#####################nlpaueb/bert-base-greek-uncased-v1########################

-------------------- datasets --------------------
Document 1:

"We released a model similar to the English `bert-base-uncased` model (12-layer, 768-hidden, 12-heads, 110M parameters)."
"We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4."
"You can still have access to the original TensorFlow checkpoints from this [Google Drive folder](https://drive.google.com/drive/folders/1ZjlaE4nvdtgqXiVBTVHCF5I9Ff8ZmztE?usp=sharing)."
------------------------------
Document 2:

* The Greek part of [Wikipedia](https://el.wikipedia.org/wiki/Βικιπαίδεια:Αντίγραφα_της_βάσης_δεδομένων),
* The Greek part of [European Parliament Proceedings Parallel Corpus](https://www.statmt.org/europarl/), and
* The Greek part of [OSCAR](https://traces1.inria.fr/oscar/), a cleansed version of [Common Crawl](https://commoncrawl.org).
------------------------------
Document 3:

GREEK-BERT (ours) | **78.6 ± 0.62**
-------------------- license --------------------
Document 1:

"We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4." "We were able to use a single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc), while also utilizing [GCP research credits](https://edu.google.com/programs/credits/research)." "You can still have access to the original TensorFlow checkpoints from this [Google Drive folder](https://drive.google.com/drive/folders/1ZjlaE4nvdtgqXiVBTVHCF5I9Ff8ZmztE?usp=sharing)."
------------------------------
Document 2:

The Greek part of [Wikipedia](https://el.wikipedia.org/wiki/Βικιπαίδεια:Αντίγραφα_της_βάσης_δεδομένων), The Greek part of [European Parliament Proceedings Parallel Corpus](https://www.statmt.org/europarl/), and The Greek part of [OSCAR](https://traces1.inria.fr/oscar/), a cleansed version of [Common Crawl](https://commoncrawl.org).
-------------------- github --------------------
Document 1:

The Greek part of [Wikipedia](https://el.wikipedia.org/wiki/Βικιπαίδεια:Αντίγραφα_της_βάσης_δεδομένων), The Greek part of [European Parliament Proceedings Parallel Corpus](https://www.statmt.org/europarl/), and The Greek part of [OSCAR](https://traces1.inria.fr/oscar/), a cleansed version of [Common Crawl](https://commoncrawl.org).
------------------------------
Document 2:

* https://github.com/google-research/bert
* https://huggingface.co
* https://github.com/huggingface/transformers
* https://www.tensorflow.org/tfrc
* https://edu.google.com/programs/credits/research
* https://drive.google.com/drive/folders/1ZjlaE4nvdtgqXiVBTVHCF5I9Ff8ZmztE?usp=sharing
------------------------------
Document 3:

"https://github.com/nlpaueb/GreekBERT/raw/master/greek-bert-logo.png"
-------------------- paper --------------------
Document 1:

"John Koutsikakis, Ilias Chalkidis, Prodromos Malakasiotis and Ion Androutsopoulos. In the Proceedings of the 11th Hellenic Conference on Artificial Intelligence (SETN 2020). Held Online. 2020" and "@inproceedings{greek-bert, author = {Koutsikakis, John and Chalkidis, Ilias and Malakasiotis, Prodromos and Androutsopoulos, Ion}, title = {GREEK-BERT: The Greeks Visiting Sesame Street}, year = {2020}, isbn = {9781450388788}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411408.3411440}, booktitle = {11th Hellenic Conference on Artificial Intelligence}, pages = {110–117}, numpages = {8}, location = {Athens, Greece}, series = {SETN 2020}"
------------------------------
Document 2:

Devlin et al., 2019, Conneau et al., 2020
------------------------------
Document 3:

GREEK-BERT (ours)   | **85.7 ± 1.00**
-------------------- upstream_model --------------------
Document 1:

"M-BERT-UNCASED (Devlin et al., 2019)"
------------------------------
Document 2:

GREEK-BERT (ours)
-------------------- parameter_count --------------------
Document 1:

"110M parameters"
-------------------- hyper_parameters --------------------
Document 1:

"We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4."
------------------------------
Document 2:

"BILSTM-CNN-CRF (Ma and Hovy, 2016)", "M-BERT-UNCASED (Devlin et al., 2019)", "M-BERT-CASED (Devlin et al., 2019)", "XLM-R  (Conneau et al., 2020)", "GREEK-BERT (ours)"
-------------------- evaluation --------------------
Document 1:

| Model name          | Accuracy                              |
| ------------------- | ------------------------------------  |
DAM (Parikh et al., 2016) | 68.5 ± 1.71
M-BERT-UNCASED (Devlin et al., 2019) | 73.9 ± 0.64
M-BERT-CASED (Devlin et al., 2019) | 73.5 ± 0.49
XLM-R (Conneau et al., 2020) | 77.3 ± 0.41
GREEK-BERT (ours)   | **78.6 ± 0.62**
------------------------------
Document 2:

"Micro F1 | BILSTM-CNN-CRF (Ma and Hovy, 2016)  | 76.4 ± 2.07 | M-BERT-UNCASED (Devlin et al., 2019) | 81.5 ± 1.77 | M-BERT-CASED (Devlin et al., 2019)| 82.1 ± 1.35 | XLM-R  (Conneau et al., 2020)| 84.8 ± 1.50 | GREEK-BERT (ours)   | **85.7 ± 1.00**"
-------------------- hardware --------------------
Document 1:

"single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc)"
------------------------------
Document 2:

GREEK-BERT (ours)   | **85.7 ± 1.00**
-------------------- limitation_and_bias --------------------
Document 1:

"DAM (Parikh et al., 2016) | 68.5 ± 1.71 M-BERT-UNCASED (Devlin et al., 2019) | 73.9 ± 0.64 M-BERT-CASED (Devlin et al., 2019) | 73.5 ± 0.49 XLM-R (Conneau et al., 2020) | 77.3 ± 0.41 GREEK-BERT (ours)   | **78.6 ± 0.62**"
------------------------------
Document 2:

"We trained BERT using the official code provided in Google BERT's GitHub repository (https://github.com/google-research/bert)", "We released a model similar to the English `bert-base-uncased` model (12-layer, 768-hidden, 12-heads, 110M parameters)", "We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4.", "We were able to use a single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc), while also utilizing [GCP research credits](https://edu.google.com/programs/credits/research). Huge thanks to both Google programs for supporting us!", "You can still have access to the original TensorFlow checkpoints from this [Google Drive folder](https://drive.google.com/drive/folders/1ZjlaE4nvdtgqXiVBTVHCF5I9Ff8ZmztE?usp=sharing)."
-------------------- demo --------------------
Document 1:

"A Greek version of BERT pre-trained language model."
------------------------------
Document 2:

"We released a model similar to the English `bert-base-uncased` model (12-layer, 768-hidden, 12-heads, 110M parameters).", "[Hugging Face](https://huggingface.co)'s [Transformers](https://github.com/huggingface/transformers) conversion script to convert the TF checkpoint and vocabulary in the desired format in order to be able to load the model in two lines of code for both PyTorch and TF2 users.", "[TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc)", "[GCP research credits](https://edu.google.com/programs/credits/research)", "[Google Drive folder](https://drive.google.com/drive/folders/1ZjlaE4nvdtgqXiVBTVHCF5I9Ff8ZmztE?usp=sharing)".
------------------------------
Document 3:

```from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained("nlpaueb/bert-base-greek-uncased-v1") model = AutoModel.from_pretrained("nlpaueb/bert-base-greek-uncased-v1")```
-------------------- input_format --------------------
Document 1:

"We released a model similar to the English `bert-base-uncased` model (12-layer, 768-hidden, 12-heads, 110M parameters)." "We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4." "You can still have access to the original TensorFlow checkpoints from this [Google Drive folder](https://drive.google.com/drive/folders/1ZjlaE4nvdtgqXiVBTVHCF5I9Ff8ZmztE?usp=sharing)."
------------------------------
Document 2:

GREEK-BERT (ours)   | **78.6 ± 0.62**
NO_OUTPUT
-------------------- output_format --------------------
Document 1:

"We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4." "We were able to use a single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc), while also utilizing [GCP research credits](https://edu.google.com/programs/credits/research)." "You can still have access to the original TensorFlow checkpoints from this [Google Drive folder](https://drive.google.com/drive/folders/1ZjlaE4nvdtgqXiVBTVHCF5I9Ff8ZmztE?usp=sharing)."
------------------------------
Document 2:

**78.6 ± 0.62**
------------------------------
Document 3:

**85.7 ± 1.00**
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"We released a model similar to the English `bert-base-uncased` model (12-layer, 768-hidden, 12-heads, 110M parameters)."

[{'datasets': ['Wikipedia', 'European Parliament Proceedings Parallel Corpus', 'OSCAR'], 'license': 
 'N/A', 'github': 'https://github.com/google-research/bert', 'paper': 'N/A', 'upstream_model': 'M-BE 
RT-UNCASED (Devlin et al., 2019)', 'parameter_count': '110M parameters', 'hyper_parameters': {'epoch 
s': 'N/A', 'batch_size': 'N/A', 'learning_rate': 'N/A', 'optimizer': 'N/A'}, 'evaluation': [{'test': 
 'Accuracy', 'result': 78.6}], 'hardware': 'single Google Cloud TPU v3-8', 'limitation_and_bias': 'N 
/A', 'demo': 'A Greek version of BERT pre-trained language model.', 'input_format': 'N/A', 'output_f 
ormat': 'N/A', 'input_token_limit': 'N/A', 'vocabulary_size': 'N/A'}]                                

#####################google/electra-small-discriminator########################

-------------------- datasets --------------------
Document 1:

SQuAD 2.0, GLUE, SQuAD, text chunking
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/))."
-------------------- paper --------------------
Document 1:

"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators" (https://openreview.net/pdf?id=r1xMH1BtvB)
-------------------- upstream_model --------------------
Document 1:

"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN"
NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN. At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the SQuAD 2.0 dataset. For a detailed description and experimental results, please refer to our paper ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators."
-------------------- hardware --------------------
Document 1:

single GPU
-------------------- limitation_and_bias --------------------
Document 1:

ELECTRA models are trained to distinguish "real" input tokens vs "fake" input tokens generated by another neural network, similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.
-------------------- demo --------------------
Document 1:

"This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/))."
------------------------------
Document 2:

```from transformers import ElectraForPreTraining, ElectraTokenizerFast
import torch

discriminator = ElectraForPreTraining.from_pretrained("google/electra-small-discriminator")
tokenizer = ElectraTokenizerFast.from_pretrained("google/electra-small-discriminator")

sentence = "The quick brown fox jumps over the lazy dog"
fake_sentence = "The quick brown fox fake over the lazy dog"

fake_tokens = tokenizer.tokenize(fake_sentence)
fake_inputs = tokenizer.encode(fake_sentence, return_tensors="pt")
discriminator_outputs = discriminator(fake_inputs)
predictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)

[print("%7s" % token, end="") for token in fake_tokens]

[print("%7s" % int(prediction), end="") for prediction in predictions.squeeze().tolist()]```
-------------------- input_format --------------------
Document 1:

"ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network"
------------------------------
Document 2:

input_format: "pt"
-------------------- output_format --------------------


[{'datasets': ['SQuAD 2.0', 'GLUE', 'SQuAD', 'text chunking'], 'license': 'apache-2.0', 'github': ' 
This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU.  
It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE 
](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) 
), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunk 
ing/)).', 'paper': '"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators" ( 
https://openreview.net/pdf?id=r1xMH1BtvB)', 'upstream_model': '"ELECTRA models are trained to distin 
guish \'real\' input tokens vs \'fake\' input tokens generated by another neural network, similar to 
 the discriminator of a GAN"', 'parameter_count': 'parameter_count', 'hyper_parameters': [], 'evalua 
tion': ['"ELECTRA models are trained to distinguish \'real\' input tokens vs \'fake\' input tokens g 
enerated by another neural network, similar to the discriminator of a GAN. At small scale, ELECTRA a 
chieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of- 
the-art results on the SQuAD 2.0 dataset. For a detailed description and experimental results, pleas 
e refer to our paper ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators."' 
], 'hardware': 'single GPU', 'limitation_and_bias': 'ELECTRA models are trained to distinguish "real 
" input tokens vs "fake" input tokens generated by another neural network, similar to the discrimina 
tor of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results 
 even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the 
 [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.', 'demo': '"This repository conta 
ins code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine 
-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchma 
rk.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence taggi 
ng tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/))."', 'input_for 
mat': '"ELECTRA models are trained to distinguish \'real\' input tokens vs \'fake\' input tokens gen 
erated by another neural network"'}, {'datasets': [], 'license': '', 'github': '', 'paper': '', 'ups 
tream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', ' 
limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': 'pt'}]                    

#####################SG161222/Realistic_Vision_V1.4########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
-------------------- github --------------------
Document 1:

"My model has always been free and always will be free. There are no restrictions on the use of the model. The rights to this model still belong to me."
-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

Euler A or DPM++ 2M Karras with 25 steps
-------------------- parameter_count --------------------
Document 1:

Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, 0 Hires steps and Denoising strength 0.25-0.45
NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, Hires. fix with Latent upscaler, 0 Hires steps and Denoising strength 0.25-0.45, Upscale by 1.1-2.0
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

DSLR, Fujifilm XT3
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"My model has always been free and always will be free. There are no restrictions on the use of the model."
-------------------- input_format --------------------
Document 1:

"I use this template to get good generation results: Prompt: *subject*, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3"

"Euler A or DPM++ 2M Karras with 25 steps CFG Scale 3,5 - 7 Hires. fix with Latent upscaler 0 Hires steps and Denoising strength 0.25-0.45 Upscale by 1.1-2.0"

input_format: 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3
-------------------- output_format --------------------
Document 1:

Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, Hires. fix with Latent upscaler, 0 Hires steps and Denoising strength 0.25-0.45, Upscale by 1.1-2.0

[{'datasets': [], 'license': 'creativeml-openrail-m', 'github': 'My model has always been free and  
always will be free. There are no restrictions on the use of the model. The rights to this model sti 
ll belong to me.', 'paper': '', 'upstream_model': 'Euler A or DPM++ 2M Karras with 25 steps', 'param 
eter_count': 'Euler A or DPM++ 2M Karras with 25 steps, CFG Scale 3,5 - 7, 0 Hires steps and Denoisi 
ng strength 0.25-0.45', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'o 
ptimizer': ''}, 'evaluation': [], 'hardware': 'DSLR, Fujifilm XT3', 'limitation_and_bias': '', 'demo 
': 'My model has always been free and always will be free. There are no restrictions on the use of t 
he model.', 'input_format': '"I use this template to get good generation results: Prompt: *subject*, 
 (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3"\n\n" 
Euler A or DPM++ 2M Karras with 25 steps CFG Scale 3,5 - 7 Hires. fix with Latent upscaler 0 Hires s 
teps and Denoising strength 0.25-0.45 Upscale by 1.1-2.0"\n\ninput_format: 8k uhd, dslr, soft lighti 
ng, high quality, film grain, Fujifilm XT3', 'output_format': 'Euler A or DPM++ 2M Karras with 25 st 
eps, CFG Scale 3,5 - 7, Hires. fix with Latent upscaler, 0 Hires steps and Denoising strength 0.25-0 
.45, Upscale by 1.1-2.0'}]                                                                           

#####################facebook/wav2vec2-large-960h-lv60-self########################

-------------------- datasets --------------------
Document 1:

datasets:
- librispeech_asr
------------------------------
Document 2:

The large model pretrained and fine-tuned on 960 hours of Libri-Light and Librispeech on 16kHz sampled speech audio.
------------------------------
Document 3:

"facebook/wav2vec2-large-960h-lv60-self"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20."
------------------------------
Document 2:

"facebook/wav2vec2-large-960h-lv60-self"
-------------------- paper --------------------
Document 1:

[Paper](https://arxiv.org/abs/2006.11477) Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli
-------------------- upstream_model --------------------
Document 1:

Facebook's Wav2Vec2, Self-Training objective, Paper, Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli, Abstract, original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

Self-Training objective, Paper, Authors, Abstract
-------------------- evaluation --------------------
Document 1:

"WER:", wer(result["text"], result["transcription"]), "| "clean" | "other" |, | 1.9 | 3.9 |
------------------------------
Document 2:

- type: wer
value: 1.9
name: Test WER
- type: wer
value: 3.9
name: Test WER
-------------------- hardware --------------------
Document 1:

"Model was trained with [Self-Training objective](https://arxiv.org/abs/2010.11430). When using the model make sure that your speech input is also sampled at 16Khz."
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20."
------------------------------
Document 2:

```python from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC from datasets import load_dataset import torch # load model and processor processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h-lv60-self") model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self") # load dummy dataset and read soundfiles ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation") # tokenize input_values = processor(ds[0]["audio"]["array"], return_tensors="pt", padding="longest").input_values # retrieve logits logits = model(input_values).logits # take argmax and decode predicted_ids = torch.argmax(logits, dim=-1) transcription = processor.batch_decode(predicted_ids)```
-------------------- input_format --------------------
Document 1:

16kHz sampled speech audio
------------------------------
Document 2:

"return_tensors="pt", padding="longest""
-------------------- output_format --------------------
Document 1:

"Model was trained with [Self-Training objective](https://arxiv.org/abs/2010.11430). When using the model make sure that your speech input is also sampled at 16Khz." output_format: 16Khz
-------------------- sample_rate --------------------
Document 1:

16kHz sampled speech audio
-------------------- WER --------------------
Document 1:

- type: wer
value: 1.9
name: Test WER
------------------------------
Document 2:

"1.8/3.3 WER on the clean/other test sets"
------------------------------
Document 3:

"wer(result["text"], result["transcription"]))"

Result: wer(result["text"], result["transcription"]))

[{'datasets': ['librispeech_asr'], 'license': 'apache-2.0', 'github': 'The original model can be fo 
und under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.', 'paper': '[P 
aper](https://arxiv.org/abs/2006.11477) Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Mi 
chael Auli', 'upstream_model': "Facebook's Wav2Vec2, Self-Training objective, Paper, Authors: Alexei 
 Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli, Abstract, original model can be found under 
 https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.", 'parameter_count': '' 
, 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'eva 
luation': [{'test': '"WER:"', 'result': 1.9}, {'test': '"WER:"', 'result': 3.9}], 'hardware': '"Mode 
l was trained with [Self-Training objective](https://arxiv.org/abs/2010.11430). When using the model 
 make sure that your speech input is also sampled at 16Khz."', 'limitation_and_bias': '', 'demo': '" 
The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2ve 
c#wav2vec-20."', 'input_format': '16kHz sampled speech audio', 'output_format': '"Model was trained  
with [Self-Training objective](https://arxiv.org/abs/2010.11430). When using the model make sure tha 
t your speech input is also sampled at 16Khz." output_format: 16Khz', 'sample_rate': '16kHz sampled  
speech audio', 'WER': [{'type': 'wer', 'value': 1.9, 'name': 'Test WER'}]}]                          
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653decfe-11d2229e0094fcc13d3e2360)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-mbart/resolve/main/README.md. 

#####################bert-base-multilingual-uncased########################

-------------------- datasets --------------------
Document 1:

"The BERT model was pretrained on the 102 languages with the largest Wikipedias. You can find the complete list [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)."
------------------------------
Document 2:

"Pretrained model on the top 102 languages with the largest Wikipedia using a masked language modeling (MLM) objective." "It was introduced in [this paper](https://arxiv.org/abs/1810.04805) and first released in [this repository](https://github.com/google-research/bert)."
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1810.04805)
------------------------------
Document 2:

@article{DBLP:journals/corr/abs-1810-04805, title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding}, journal = {CoRR}, volume = {abs/1810.04805}, year = {2018}, url = {http://arxiv.org/abs/1810.04805}, archivePrefix = {arXiv}, eprint = {1810.04805}
------------------------------
Document 3:

See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you. Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.
-------------------- upstream_model --------------------
Document 1:

upstream_model NO_OUTPUT
------------------------------
Document 2:

BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion.
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"masked language modeling (MLM) objective" "introduced in [this paper](https://arxiv.org/abs/1810.04805)" "first released in [this repository](https://github.com/google-research/bert)"
-------------------- evaluation --------------------
Document 1:

"masked language modeling (MLM) objective" "introduced in [this paper](https://arxiv.org/abs/1810.04805)" "first released in [this repository](https://github.com/google-research/bert)" "does not make a difference between english and English"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions:"
------------------------------
Document 2:

"masked language modeling (MLM) objective" "introduced in [this paper](https://arxiv.org/abs/1810.04805)" "first released in [this repository](https://github.com/google-research/bert)" "This model is uncased: it does not make a difference between english and English."
------------------------------
Document 3:

The texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The languages with a larger Wikipedia are under-sampled and the ones with lower resources are oversampled. For languages like Chinese, Japanese Kanji and Korean Hanja that don't have space, a CJK Unicode block is added around every character. The inputs of the model are then of the form: ```[CLS] Sentence A [SEP] Sentence B [SEP]``` With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace
-------------------- demo --------------------
Document 1:

See the [model hub](https://huggingface.co/models?filter=bert)
------------------------------
Document 2:

"This model is uncased: it does not make a difference between english and English." "[this repository](https://github.com/google-research/bert)"
-------------------- input_format --------------------
Document 1:

The texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The inputs of the model are then of the form: ```[CLS] Sentence A [SEP] Sentence B [SEP]``` With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
------------------------------
Document 2:

"pretrained on the raw texts only, with no humans labelling them in any way" "Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words." "Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining."
-------------------- output_format --------------------
Document 1:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering"
------------------------------
Document 2:

The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.
-------------------- input_token_limit --------------------
Document 1:

"The inputs of the model are then of the form: [...] With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
-------------------- vocabulary_size --------------------
Document 1:

"110,000" and "512 tokens"
------------------------------
Document 2:

"masked language modeling (MLM) objective" and "top 102 languages with the largest Wikipedia"

[{'datasets': ['102 languages with the largest Wikipedias'], 'license': 'apache-2.0', 'github': 'ht 
tps://github.com/google-research/bert', 'paper': 'https://arxiv.org/abs/1810.04805', 'upstream_model 
': 'NO_OUTPUT', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learnin 
g_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo':  
'', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]        

#####################CompVis/stable-diffusion-v1-4########################

-------------------- datasets --------------------
Document 1:

The model was trained mainly with English captions and will not work as well in other languages. The model was trained on a large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material and is not fit for product use without additional safety mechanisms and considerations. The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.
------------------------------
Document 2:

LAION-2B(en) (https://laion.ai/blog/laion-5b/)
------------------------------
Document 3:

- LAION-2B (en) and subsets thereof 
- laion-high-resolution
- laion-improved-aesthetics
- laion-aesthetics v2 5+
-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
extra_gated_prompt: "This model is open access and available to all, with a CreativeML\
\ OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL\
\ License specifies: \n\n1. You can't use the model to deliberately produce nor\
\ share illegal or harmful outputs or content \n2. The authors claim no rights on\
\ the outputs you generate, you are free to use them and are accountable for their\
\ use which must not go against the provisions set in the license\n3. You may re-distribute\
\ the weights and use the model commercially and/or as a service. If you do, please\
\ be aware you have to include the same use restrictions as the ones in the license\
\ and share a copy of the CreativeML OpenRAIL-M to all your users (please read the\
\ license entirely and carefully)\nPlease read the full license carefully here:\
\ https://huggingface.co/spaces/CompVis/stable-diffusion-license\n    "
extra_g
-------------------- github --------------------
Document 1:

[🤗's Diffusers library](https://github.com/huggingface/diffusers)
------------------------------
Document 2:

"This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claim no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\nPlease read the full license carefully here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n    "
-------------------- paper --------------------
Document 1:

"Research on generative models."
------------------------------
Document 2:

"High-Resolution Image Synthesis With Latent Diffusion Models"
------------------------------
Document 3:

LAION-2B(en), English descriptions, white and western cultures, English-language prompts
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"50 PLMS sampling steps" and "10000 random prompts from the COCO2017 validation set"
-------------------- hyper_parameters --------------------
Document 1:

"50 PLMS sampling steps" and "10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 2:

- relative downsampling factor of 8
- maps images of shape H x W x 3 to latents of shape H/f x W/f x 4
- 32 x 8 x A100 GPUs
- AdamW
- Gradient Accumulations: 2
- Batch: 32 x 8 x 2 x 4 = 2048
- Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- evaluation --------------------
Document 1:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: ![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-variants-scores.jpg) Evaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
-------------------- hardware --------------------
Document 1:

- LAION-2B (en) and subsets thereof
- 32 x 8 x A100 GPUs
-------------------- limitation_and_bias --------------------
Document 1:

"primarily limited to English descriptions", "white and western cultures are often set as the default", "ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts"
------------------------------
Document 2:

- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.
------------------------------
Document 3:

- Probing and understanding the limitations and biases of generative models.
-------------------- demo --------------------
Document 1:

"Possible research areas and tasks include - Safe deployment of models which have the potential to generate harmful content. - Probing and understanding the limitations and biases of generative models. - Generation of artworks and use in design and other artistic processes. - Applications in educational or creative tools. - Research on generative models."
------------------------------
Document 2:

"This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claim no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\nPlease read the full license carefully here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n    "
------------------------------
Document 3:

No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data. The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.
-------------------- input_format --------------------
Document 1:

No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data. The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.

NO_OUTPUT
------------------------------
Document 2:

- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4
-------------------- output_format --------------------
Document 1:

extra_gated_prompt: "This model is open access and available to all, with a CreativeML\
\ OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL\
\ License specifies: \n\n1. You can't use the model to deliberately produce nor\
\ share illegal or harmful outputs or content \n2. The authors claim no rights on\
\ the outputs you generate, you are free to use them and are accountable for their\
\ use which must not go against the provisions set in the license\n3. You may re-distribute\
\ the weights and use the model commercially and/or as a service. If you do, please\
\ be aware you have to include the same use restrictions as the ones in the license\
\ and share a copy of the CreativeML OpenRAIL-M to all your users (please read the\
\ license entirely and carefully)\nPlease read the full license carefully here:\
\ https://huggingface.co/spaces/CompVis/stable-diffusion-license\n    "
NO_OUTPUT

[{'datasets': ['LAION-5B'], 'license': 'creativeml-openrail-m', 'github': 'https://github.com/huggi 
ngface/diffusers', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 
 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'outpu 
t_format': ''}]                                                                                      

#####################HooshvareLab/distilbert-fa-zwnj-base-ner########################

-------------------- datasets --------------------
Document 1:

[ARMAN](https://github.com/HaniehP/PersianNER), [PEYMA](http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/), and [WikiANN](https://elisa-ie.github.io/wikiann/)
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"Post a Github issue on the [ParsNER Issues](https://github.com/hooshvare/parsner/issues) repo."
------------------------------
Document 2:

[ARMAN](https://github.com/HaniehP/PersianNER), [PEYMA](http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/), and [WikiANN](https://elisa-ie.github.io/wikiann/)
-------------------- paper --------------------
Document 1:

"Named Entity Recognition (NER) task on a mixed NER dataset collected from [ARMAN](https://github.com/HaniehP/PersianNER), [PEYMA](http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/), and [WikiANN](https://elisa-ie.github.io/wikiann/)"
-------------------- upstream_model --------------------
Document 1:

"HooshvareLab/distilbert-fa-zwnj-base-ner"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

The following tables summarize the scores obtained by model overall and per each class.  
**Overall**  
|    Model   | accuracy | precision |  recall  |    f1    |
|:----------:|:--------:|:---------:|:--------:|:--------:|
| Distilbert | 0.994534 |  0.946326 |  0.95504 | 0.950663 |  
**Per entities**  
|     	| number 	| precision 	|  recall  	|    f1    	|
|:---:	|:------:	|:---------:	|:--------:	|:--------:	|
| DAT 	|   407  	|  0.812048 	| 0.828010 	| 0.819951 	|
| EVE 	|   256  	|  0.955056 	| 0.996094 	| 0.975143 	|
| FAC 	|   248  	|  0.972549 	| 1.000000 	| 0.986083
------------------------------
Document 2:

- Date (DAT)
- Event (EVE)
- Facility (FAC)
- Location (LOC)
- Money (MON)
- Organization (ORG)
- Percent (PCT)
- Person (PER)
- Product (PRO)
- Time (TIM)
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"This model fine-tuned for the Named Entity Recognition (NER) task on a mixed NER dataset collected from [ARMAN](https://github.com/HaniehP/PersianNER), [PEYMA](http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/), and [WikiANN](https://elisa-ie.github.io/wikiann/) that covered ten types of entities: - Date (DAT) - Event (EVE) - Facility (FAC) - Location (LOC) - Money (MON) - Organization (ORG) - Percent (PCT) - Person (PER) - Product (PRO) - Time (TIM)"
------------------------------
Document 2:

accuracy: 0.994534, precision: 0.946326, recall: 0.95504, f1: 0.950663, 
DAT: precision: 0.812048, recall: 0.828010, f1: 0.819951, 
EVE: precision: 0.955056, recall: 0.996094, f1: 0.975143, 
FAC: precision: 0.972549, recall: 1.000000, f1: 0.986083, 
LOC: precision: 0.968403, recall: 0.967060, f1: 0.967731, 
MON: precision: 0.925532, recall: 0.887755, f1: 0.906250, 
ORG: precision: 0.932095, recall: 0.951803, f1: 0.941846, 
PCT: precision: 0.936842, recall: 0.946809, f1: 0.941799, 
PER: precision: 0.959818, recall: 0.957
-------------------- demo --------------------
Document 1:

"This model fine-tuned for the Named Entity Recognition (NER) task on a mixed NER dataset collected from [ARMAN](https://github.com/HaniehP/PersianNER), [PEYMA](http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/), and [WikiANN](https://elisa-ie.github.io/wikiann/) that covered ten types of entities:  
- Date (DAT)
- Event (EVE)
- Facility (FAC)
- Location (LOC)
- Money (MON)
- Organization (ORG)
- Percent (PCT)
- Person (PER)
- Product (PRO)
- Time (TIM)"
------------------------------
Document 2:

```from transformers import AutoTokenizer
from transformers import AutoModelForTokenClassification  # for pytorch
from transformers import TFAutoModelForTokenClassification  # for tensorflow
from transformers import pipeline

model_name_or_path = "HooshvareLab/distilbert-fa-zwnj-base-ner"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)  # Pytorch
# model = TFAutoModelForTokenClassification.from_pretrained(model_name_or_path)  # Tensorflow

nlp = pipeline("ner", model=model, tokenizer=tokenizer)
example = "در سال ۲۰۱۳ درگذشت و آندرتیکر و کین برای او مراسم یادبود گرف
-------------------- input_format --------------------
Document 1:

"Named Entity Recognition (NER) task on a mixed NER dataset collected from [ARMAN](https://github.com/HaniehP/PersianNER), [PEYMA](http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/), and [WikiANN](https://elisa-ie.github.io/wikiann/)"
------------------------------
Document 2:

Train, Records, B-DAT, B-EVE, B-FAC, B-LOC, B-MON, B-ORG, B-PCT, B-PER, B-PRO, B-TIM, I-DAT, I-EVE, I-FAC, I-LOC, I-MON, I-ORG, I-PCT, I-PER, I-PRO, I-TIM, Valid, Test
-------------------- output_format --------------------
Document 1:

"Named Entity Recognition (NER) task on a mixed NER dataset collected from [ARMAN](https://github.com/HaniehP/PersianNER), [PEYMA](http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/), and [WikiANN](https://elisa-ie.github.io/wikiann/)"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['ARMAN', 'PEYMA', 'WikiANN'], 'license': '', 'github': 'https://github.com/hooshvare 
/parsner/issues', 'paper': '', 'upstream_model': 'HooshvareLab/distilbert-fa-zwnj-base-ner', 'parame 
ter_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'accuracy', 'result': 0.994534}, {'t 
est': 'precision', 'result': 0.946326}, {'test': 'recall', 'result': 0.95504}, {'test': 'f1', 'resul 
t': 0.950663}], 'hardware': '', 'limitation_and_bias': 'This model fine-tuned for the Named Entity R 
ecognition (NER) task on a mixed NER dataset collected from ARMAN, PEYMA, and WikiANN that covered t 
en types of entities: Date (DAT), Event (EVE), Facility (FAC), Location (LOC), Money (MON), Organiza 
tion (ORG), Percent (PCT), Person (PER), Product (PRO), Time (TIM)', 'demo': 'This model fine-tuned  
for the Named Entity Recognition (NER) task on a mixed NER dataset collected from ARMAN, PEYMA, and  
WikiANN that covered ten types of entities: Date (DAT), Event (EVE), Facility (FAC), Location (LOC), 
 Money (MON), Organization (ORG), Percent (PCT), Person (PER), Product (PRO), Time (TIM)', 'input_fo 
rmat': 'Named Entity Recognition (NER) task on a mixed NER dataset collected from ARMAN, PEYMA, and  
WikiANN', 'output_format': 'Named Entity Recognition (NER) task on a mixed NER dataset collected fro 
m ARMAN, PEYMA, and WikiANN', 'input_token_limit': '', 'vocabulary_size': ''}, {'datasets': ['ARMAN' 
, 'PEYMA', 'WikiANN'], 'license': '', 'github': 'https://github.com/hooshvare/parsner/issues', 'pape 
r': '', 'upstream_model': 'HooshvareLab/distilbert-fa-zwnj-base-ner', 'parameter_count': '', 'hyper_ 
parameters': {}, 'evaluation': [{'test': 'accuracy', 'result': 0.994534}, {'test': 'precision', 'res 
ult': 0.946326}, {'test': 'recall', 'result': 0.95504}, {'test': 'f1', 'result': 0.950663}, {'test': 
 'DAT', 'result': {'precision': 0.812048, 'recall': 0.82801, 'f1': 0.819951}}, {'test': 'EVE', 'resu 
lt': {'precision': 0.955056, 'recall': 0.996094, 'f1': 0.975143}}, {'test': 'FAC', 'result': {'preci 
sion': 0.972549, 'recall': 1, 'f1': 0.986083}}, {'test': 'LOC', 'result': {'precision': 0.968403, 'r 
ecall': 0.96706, 'f1': 0.967731}}, {'test': 'MON', 'result': {'precision': 0.925532, 'recall': 0.887 
755, 'f1': 0.90625}}, {'test': 'ORG', 'result': {'precision': 0.932095, 'recall': 0.951803, 'f1': 0. 
941846}}, {'test': 'PCT', 'result': {'precision': 0.936842, 'recall': 0.946809, 'f1': 0.941799}}, {' 
test': 'PER', 'result': {'precision': 0.959818, 'recall': 0.957, 'f1': 0.958407}}], 'hardware': '',  
'limitation_and_bias': '', 'demo': 'from transformers import AutoTokenizer\nfrom transformers import 
 AutoModelForTokenClassification  # for pytorch\nfrom transformers import TFAutoModelForTokenClassif 
ication  # for tensorflow\nfrom transformers import pipeline\n\nmodel_name_or_path = "HooshvareLab/d 
istilbert-fa-zwnj-base-ner"\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel =  
AutoModelForTokenClassification.from_pretrained(model_name_or_path)  # Pytorch\n# model = TFAutoMode 
lForTokenClassification.from_pretrained(model_name_or_path)  # Tensorflow\n\nnlp = pipeline("ner", m 
odel=model, tokenizer=tokenizer)\nexample = "در سال ۲۰۱۳ درگذشت و آندرتیکر و کین برای او مراسم یادبو 
د گرف', 'input_format': 'Named Entity Recognition (NER) task on a mixed NER dataset collected from A 
RMAN, PEYMA, and WikiANN', 'output_format': 'Train, Records, B-DAT, B-EVE, B-FAC, B-LOC, B-MON, B-OR 
G, B-PCT, B-PER, B-PRO, B-TIM, I-DAT, I-EVE, I-FAC, I-LOC, I-MON, I-ORG, I-PCT, I-PER, I-PRO, I-TIM, 
 Valid, Test', 'input_token_limit': '', 'vocabulary_size': ''}]                                      

#####################gpt2-large########################

-------------------- datasets --------------------
Document 1:

The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt). The model is pretrained on a very large corpus of English data in a self-supervised fashion. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"associated paper", "modeling architecture", "objective", "compute infrastructure", "training details"
------------------------------
Document 2:

[associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- hyper_parameters --------------------
Document 1:

"The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens."
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

The model authors write in the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) that:  
> Since our model operates on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language model benchmark. Results on language modeling datasets are commonly reported in a quantity which is a scaled or ex- ponentiated version of the average negative log probability per canonical prediction unit - usually a character, a byte, or a word. We evaluate the same quantity by computing the log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these datasets, WebText LMs would be tested significantly out- of-distribution, having to predict aggressively standardized text, tokenization artifacts such as disconnected punctuation and contractions, shuffled sentences, and even the string <UNK> which is extremely rare in WebText - occurring only 26 times in 40 billion bytes. We report our main results...using invertible de-tokenizers which remove as many of these token
-------------------- hardware --------------------
Document 1:

"compute infrastructure"
------------------------------
Document 2:

"Technical Specifications"
-------------------- limitation_and_bias --------------------
Document 1:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. This bias will also affect all fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.
------------------------------
Document 2:

Risks, Limitations and Biases
-------------------- demo --------------------
Document 1:

Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:  
```python
>>> from transformers import pipeline, set_seed
>>> generator = pipeline('text-generation', model='gpt2-large')
>>> set_seed(42)
>>> generator("Hello, I'm a language model,", max_length=30, num_return_sequences=5)

[{'generated_text': "Hello, I'm a language model, I can do language modeling. In fact, this is one of the reasons I use languages. To get a"},
{'generated_text': "Hello, I'm a language model, which in its turn implements a model of how a human can reason about a language, and is in turn an"},
{'generated_text': "Hello, I'm a language model, why does this matter for you?\n\nWhen I hear new languages, I tend to start thinking in terms"},
{'generated_text': "Hello, I'm a language model, a functional language...
-------------------- input_format --------------------
Document 1:

The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt). The model is pretrained on a very large corpus of English data in a self-supervised fashion. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- output_format --------------------
Document 1:

The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- input_token_limit --------------------
Document 1:

"The model uses internally a mask-mechanism to make sure the predictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens." "The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens."
-------------------- vocabulary_size --------------------
Document 1:

"The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257."

[{'datasets': ['WebText'], 'license': 'mit', 'github': '', 'paper': '', 'upstream_model': '', 'para 
meter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': ' 
', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size':  
''}]                                                                                                 

#####################hf-internal-testing/tiny-xlm-roberta########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"This is a tiny random {mname_tiny} model"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': 'Document 1:\n\n"This is a tiny random {mna 
me_tiny} model"', 'upstream_model': '', 'parameter_count': 'parameter_count', 'hyper_parameters': {} 
, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'outp 
ut_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                     
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dedfe-1a89d64f09cf0fd268f574cc)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-camembert/resolve/main/README.md. 

#####################deepmind/optical-flow-perceiver########################

-------------------- datasets --------------------
Document 1:

AutoFlow https://autoflow-google.github.io/ 400,000 annotated image pairs
------------------------------
Document 2:

datasets: - autoflow
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"We refer to the [tutorial notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Optical_Flow.ipynb) regarding using the Perceiver for optical flow."
-------------------- paper --------------------
Document 1:

"paper", "https://arxiv.org/abs/2107.14795"
------------------------------
Document 2:

AutoFlow, 400,000 annotated image pairs
-------------------- upstream_model --------------------
Document 1:

"Optical flow is a decades-old open problem in computer vision." NO_OUTPUT
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"Appendix E of the paper https://arxiv.org/abs/2107.14795"
-------------------- evaluation --------------------
Document 1:

"average end-point error (EPE) of 1.81 on Sintel.clean,  2.42 on Sintel.final and 4.98 on KITTI" and "[paper](https://arxiv.org/abs/2107.14795)"
-------------------- hardware --------------------
Document 1:

AutoFlow
-------------------- limitation_and_bias --------------------
Document 1:

AutoFlow, 400,000 annotated image pairs
-------------------- demo --------------------
Document 1:

AutoFlow, https://autoflow-google.github.io/
-------------------- input_format --------------------
Document 1:

AutoFlow, 400,000 annotated image pairs, input_format
-------------------- output_format --------------------


[{'datasets': ['AutoFlow'], 'license': 'apache-2.0', 'github': 'https://github.com/NielsRogge/Trans 
formers-Tutorials/blob/master/Perceiver/Perceiver_for_Optical_Flow.ipynb', 'paper': 'https://arxiv.o 
rg/abs/2107.14795', 'upstream_model': 'Optical flow is a decades-old open problem in computer vision 
.', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 
 'optimizer': ''}, 'evaluation': [{'test': 'average end-point error (EPE) of 1.81 on Sintel.clean,   
2.42 on Sintel.final and 4.98 on KITTI', 'result': 0}], 'hardware': 'AutoFlow', 'limitation_and_bias 
': 'AutoFlow, 400,000 annotated image pairs', 'demo': 'AutoFlow, https://autoflow-google.github.io/' 
, 'input_format': 'AutoFlow, 400,000 annotated image pairs, input_format', 'output_format': ''}]     
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dee1a-319f22a34f6794d53c9efbbc)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-AlbertModel/resolve/main/README.md. 

#####################tiiuae/falcon-7b########################

-------------------- datasets --------------------
Document 1:

"The RefinedWeb paper" and "https://arxiv.org/abs/2306.01116"
------------------------------
Document 2:

datasets: - tiiuae/falcon-refinedweb
------------------------------
Document 3:

[RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), Books, Conversations, Code, RefinedWeb-French, Technical
-------------------- license --------------------
Document 1:

Apache 2.0 license
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

url={https://arxiv.org/abs/2306.01116}, eprint={2306.01116}, eprinttype = {arXiv}
-------------------- paper --------------------
Document 1:

"Paper: *coming soon*."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count: NO_OUTPUT
------------------------------
Document 2:

* `d_model`          | 4544      | Increased to compensate for multiquery
------------------------------
Document 3:

"7B parameters" and "1,500B tokens"
-------------------- hyper_parameters --------------------
Document 1:

"| **Hyperparameter** | **Value**  | **Comment**                               |
|--------------------|------------|-------------------------------------------|
| Precision          | `bfloat16` |                                           |
| Optimizer          | AdamW      |                                           |
| Learning rate      | 6e-4       | 4B tokens warm-up, cosine decay to 1.2e-5 |
| Weight decay       | 1e-1       |                                           |
| Z-loss       | 1e-4       |                                           |
| Batch size         | 2304        | 30B tokens ramp-up                         |  "
------------------------------
Document 2:

"Falcon-7B is a 7B parameters causal decoder-only model" and "[this great blogpost fron HF](https://huggingface.co/blog/falcon)!"
------------------------------
Document 3:

"| **Hyperparameter** | **Value** | **Comment**                            |
|--------------------|-----------|----------------------------------------|
| Layers             | 32        |                                        |
| `d_model`          | 4544      | Increased to compensate for multiquery                                       |
| `head_dim`         | 64        | Reduced to optimise for FlashAttention |
| Vocabulary         | 65024     |                                        |
| Sequence length    | 2048      |                                        |"
-------------------- evaluation --------------------
Document 1:

"Falcon-7B is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae) and trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license."
-------------------- hardware --------------------
Document 1:

Falcon-7B, AWS SageMaker, 384 A100 40GB GPUs, P4d instances
------------------------------
Document 2:

384 A100 40GB GPUs, 2D parallelism strategy (PP=2, DP=192), bfloat16, AdamW, 6e-4, 1e-1, 1e-4, 2304, two weeks
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias Falcon-7B is trained on English and French data only, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.
-------------------- demo --------------------
Document 1:

"Falcon-7B is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae) and trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license."
-------------------- input_format --------------------
Document 1:

input_format: RefinedWeb paper
------------------------------
Document 2:

license: apache-2.0, datasets: - tiiuae/falcon-refinedweb, inference: false
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

Vocabulary: 65024

[{'datasets': ['tiiuae/falcon-refinedweb'], 'license': 'apache-2.0', 'github': 'https://arxiv.org/a 
bs/2306.01116', 'paper': 'https://arxiv.org/abs/2306.01116', 'upstream_model': '', 'parameter_count' 
: '7B parameters', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optim 
izer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_and_bias': 'Falc 
on-7B is trained on English and French data only, and will not generalize appropriately to other lan 
guages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will ca 
rry the stereotypes and biases commonly encountered online.', 'demo': '', 'input_format': '', 'outpu 
t_format': '', 'input_token_limit': '', 'vocabulary_size': '65024'}]                                 

#####################naver/splade-cocondenser-ensembledistil########################

-------------------- datasets --------------------
Document 1:

datasets:
- ms_marco
-------------------- license --------------------
Document 1:

license: cc-by-nc-sa-4.0
------------------------------
Document 2:

Creative Commons Attribution Non Commercial Share Alike 4.0 International
-------------------- github --------------------
Document 1:

github, query-expansion, document-expansion, bag-of-words, passage-retrieval, knowledge-distillation
------------------------------
Document 2:

"https://github.com/naver/splade"
-------------------- paper --------------------
Document 1:

"From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective"
------------------------------
Document 2:

"paper: https://arxiv.org/abs/2205.04733"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

`MRR@10 (MS MARCO dev) | R@1000 (MS MARCO dev)`
-------------------- evaluation --------------------
Document 1:

"MRR@10 (MS MARCO dev) | R@1000 (MS MARCO dev) | | --- | --- | --- | | `splade-cocondenser-ensembledistil` | 38.3 | 98.3 |"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"code: https://github.com/naver/splade"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['ms_marco'], 'license': 'cc-by-nc-sa-4.0', 'github': 'https://github.com/naver/splad 
e', 'paper': 'https://arxiv.org/abs/2205.04733', 'upstream_model': '', 'parameter_count': '', 'hyper 
_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'code: https: 
//github.com/naver/splade', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocab 
ulary_size': ''}]                                                                                    
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dee5c-4f85a3654b9b759116c064ef)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-ElectraModel/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dee5d-2a5956da7e6ba25f5d0911b0)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTNeoXForCausalLM/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dee5d-7bde5f230d82abe7715d078b)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-DistilBertModel/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dee5d-22fad41f1d43080b0f2d6604)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-RobertaModel/resolve/main/README.md. 

#####################tiiuae/falcon-7b-instruct########################

-------------------- datasets --------------------
Document 1:

"The RefinedWeb paper" and "https://arxiv.org/abs/2306.01116"
------------------------------
Document 2:

datasets
-------------------- license --------------------
Document 1:

Apache 2.0 license
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

url={https://arxiv.org/abs/2306.01116}, eprint={2306.01116}, eprinttype = {arXiv}
------------------------------
Document 2:

language:
- en
license: apache-2.0
datasets:
- tiiuae/falcon-refinedweb
inference: true
-------------------- paper --------------------
Document 1:

"See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results."
------------------------------
Document 2:

"Paper: *coming soon*."
------------------------------
Document 3:

```
@article{falcon40b,
title={{Falcon-40B}: an open large language model with state-of-the-art performance},
author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
year={2023}
}
```
```
@article{refinedweb,
title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
-------------------- upstream_model --------------------
Document 1:

"Finetuned from model: Falcon-7B"
-------------------- parameter_count --------------------
Document 1:

"Layers: 32", "d_model: 4544", "head_dim: 64", "Vocabulary: 65024", "Sequence length: 2048"
-------------------- hyper_parameters --------------------
Document 1:

"Hyperparameter" | "Value" | "Comment" | "Layers" | "32" | "d_model" | "4544" | "head_dim" | "64" | "Vocabulary" | "65024" | "Sequence length" | "2048"
-------------------- evaluation --------------------
Document 1:

Falcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token). 
* Positionnal embeddings: rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));
* Attention: multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));
* Decoder-block: parallel attention/MLP with a single layer norm. 
| **Hyperparameter** | **Value** | **Comment**                            |
|--------------------|-----------|----------------------------------------|
| Layers             | 32        |                                        |
| `d_model`          | 4544      | Increased to compensate for multiquery                                       |
| `head_dim`         | 64        | Reduced to optimise for FlashAttention |
| Vocabulary         | 65024     |                                        |
| Sequence length    | 2048      |                                        |
-------------------- hardware --------------------
Document 1:

Falcon-7B-Instruct, AWS SageMaker, 32 A100 40GB GPUs, P4d instances
-------------------- limitation_and_bias --------------------
Document 1:

"Falcon-7B-Instruct is mostly trained on English data" "it will carry the stereotypes and biases commonly encountered online"
-------------------- demo --------------------
Document 1:

"What's the Everett interpretation of quantum mechanics?", "Give me a list of the top 10 dive sites you would recommend around the world.", "Can you tell me more about deep-water soloing?", "Can you write a short tweet about the Apache 2.0 release of our latest AI model, Falcon LLM?", "What are the responsabilities of a Chief Llama Officer?"
------------------------------
Document 2:

See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

250M tokens mixture of instruct/chat datasets.
-------------------- vocabulary_size --------------------
Document 1:

Vocabulary: 65024
------------------------------
Document 2:

Falcon-7B-Instruct was finetuned on a 250M tokens mixture of instruct/chat datasets. The data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.

[{'datasets': ['tiiuae/falcon-refinedweb'], 'license': 'apache-2.0', 'github': 'https://arxiv.org/a 
bs/2306.01116', 'paper': 'https://arxiv.org/abs/2306.01116', 'upstream_model': 'Falcon-7B', 'paramet 
er_count': 'Layers: 32, d_model: 4544, head_dim: 64, Vocabulary: 65024, Sequence length: 2048', 'hyp 
er_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation' 
: [{'test': 'Falcon-7B is a causal decoder-only model trained on a causal language modeling task (i. 
e., predict the next token).', 'result': 0}], 'hardware': 'Falcon-7B-Instruct, AWS SageMaker, 32 A10 
0 40GB GPUs, P4d instances', 'limitation_and_bias': 'Falcon-7B-Instruct is mostly trained on English 
 data and it will carry the stereotypes and biases commonly encountered online', 'demo': "What's the 
 Everett interpretation of quantum mechanics?, Give me a list of the top 10 dive sites you would rec 
ommend around the world., Can you tell me more about deep-water soloing?, Can you write a short twee 
t about the Apache 2.0 release of our latest AI model, Falcon LLM?, What are the responsabilities of 
 a Chief Llama Officer?", 'input_format': '', 'output_format': '', 'input_token_limit': '250M tokens 
 mixture of instruct/chat datasets.', 'vocabulary_size': 'Vocabulary: 65024'}]                       

#####################snrspeaks/t5-one-line-summary########################

-------------------- datasets --------------------
Document 1:

"370,000 research papers" and "[simpleT5](https://github.com/Shivanandroy/simpleT5)"
------------------------------
Document 2:

- arxiv
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"simpleT5](https://github.com/Shivanandroy/simpleT5)"
------------------------------
Document 2:

license: mit, datasets: - arxiv, widget: - text: 'summarize: We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production machinelearning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton''s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.'
-------------------- paper --------------------
Document 1:

"370,000 research papers"
------------------------------
Document 2:

We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production machinelearning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton''s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.
------------------------------
Document 3:

"Overton: Building, Deploying, and Monitoring Machine Learning Systems for Engineers"
-------------------- upstream_model --------------------
Document 1:

upstream_model simpleT5
------------------------------
Document 2:

"model = SimpleT5()" and "model.load_model("t5","snrspeaks/t5-one-line-summary")"
-------------------- parameter_count --------------------
Document 1:

parameter_count: 370,000
------------------------------
Document 2:

"model.load_model("t5","snrspeaks/t5-one-line-summary")"
-------------------- hyper_parameters --------------------
Document 1:

"simpleT5" library, "pytorch lightning⚡️ & transformers🤗"
-------------------- evaluation --------------------
Document 1:

"A T5 model trained on 370,000 research papers, to generate one line summary based on description/abstract of the papers."
-------------------- hardware --------------------
Document 1:

"pytorch lightning⚡️ & transformers🤗"
------------------------------
Document 2:

"model.load_model("t5","snrspeaks/t5-one-line-summary")"
-------------------- limitation_and_bias --------------------
Document 1:

Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton''s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow.
-------------------- demo --------------------
Document 1:

"simpleT5" library, "[simpleT5](https://github.com/Shivanandroy/simpleT5)", "pytorch lightning⚡️ & transformers🤗"
------------------------------
Document 2:

`model_name = "snrspeaks/t5-one-line-summary"`, `from transformers import AutoModelForSeq2SeqLM, AutoTokenizer`, `model = AutoModelForSeq2SeqLM.from_pretrained(model_name)`, `tokenizer = AutoTokenizer.from_pretrained(model_name)`, `input_ids = tokenizer.encode("summarize: " + abstract, return_tensors="pt", add_special_tokens=True)`, `generated_ids = model.generate(input_ids=input_ids,num_beams=5,max_length=50,repetition_penalty=2.5,length_penalty=1,early_stopping=True,num_return_sequences=3)`, `preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]`, `print(preds)`, `["Overton: Building, Deploying, and Monitoring Machine Learning Systems for Engineers", "Overton
------------------------------
Document 3:

"In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow."
-------------------- input_format --------------------
Document 1:

"model.load_model("t5","snrspeaks/t5-one-line-summary")"

input_format: NO_OUTPUT
-------------------- output_format --------------------
Document 1:

output_format
------------------------------
Document 2:

"model.load_model("t5","snrspeaks/t5-one-line-summary")" "model.predict(abstract)" "Overton: Building, Deploying, and Monitoring Machine Learning Systems for Engineers"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['370,000 research papers'], 'license': 'mit', 'github': 'simpleT5](https://github.co 
m/Shivanandroy/simpleT5', 'paper': '370,000 research papers', 'upstream_model': 'simpleT5', 'paramet 
er_count': '370,000', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'opt 
imizer': ''}, 'evaluation': [{'test': 'A T5 model trained on 370,000 research papers, to generate on 
e line summary based on description/abstract of the papers.', 'result': 0}], 'hardware': 'pytorch li 
ghtning⚡️ & transformers🤗', 'limitation_and_bias': "Key challenges engineers face are monitoring fin 
e-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or in 
complete supervision data. Overton automates the life cycle of model construction, deployment, and m 
onitoring by providing a set of novel high-level, declarative abstractions. Overton''s vision is to  
shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, 
 using Overton, engineers can build deep-learning-based applications without writing any code in fra 
meworks like TensorFlow.", 'demo': 'simpleT5 library, [simpleT5](https://github.com/Shivanandroy/sim 
pleT5), pytorch lightning⚡️ & transformers🤗', 'input_format': 'model.load_model("t5","snrspeaks/t5-o 
ne-line-summary")', 'output_format': 'output_format', 'input_token_limit': '', 'vocabulary_size': '' 
}]                                                                                                   
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653deebf-573956414a0955da095925ea)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTJModel/resolve/main/README.md. 

#####################finiteautomata/beto-sentiment-analysis########################

-------------------- datasets --------------------
Document 1:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

"Spanish pre-trained bert model and evaluation data" @article{canete2020spanish, title={Spanish pre-trained bert model and evaluation data}, author={Ca{\~n}ete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\'e}rez, Jorge}, journal={Pml4dc at iclr}, volume={2020}, number={2020}, pages={1--10}, year={2020}}
------------------------------
Document 3:

TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish.
-------------------- license --------------------
Document 1:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
-------------------- github --------------------
Document 1:

"If you use this model in your work, please cite the following papers:"
"@misc{perez2021pysentimiento,"
"@article{canete2020spanish,"
NO_OUTPUT
-------------------- paper --------------------
Document 1:

"@misc{perez2021pysentimiento,
title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},
author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},
year={2021},
eprint={2106.09462},
archivePrefix={arXiv},
primaryClass={cs.CL}"

"@article{canete2020spanish,
title={Spanish pre-trained bert model and evaluation data},
author={Ca{\~n}ete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\'e}rez, Jorge},
journal={Pml4dc at iclr},
volume={2020},
number={2020},
pages={1--10},
year={2020}"
------------------------------
Document 2:

"Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish."
------------------------------
Document 3:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
-------------------- upstream_model --------------------
Document 1:

"Ca{\~n}ete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\'e}rez, Jorge"

upstream_model: Ca{\~n}ete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\'e}rez, Jorge
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish. Uses `POS`, `NEG`, `NEU` labels."
-------------------- evaluation --------------------
Document 1:

"Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish. Uses `POS`, `NEG`, `NEU` labels."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. NO_OUTPUT
------------------------------
Document 2:

"Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish. Uses `POS`, `NEG`, `NEU` labels."
-------------------- demo --------------------
Document 1:

"Enjoy! 🤗" NO_OUTPUT
------------------------------
Document 2:

"Please be aware that models are trained with third-party datasets and are subject to their respective licenses."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['TASS Dataset'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'pa 
rameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': 
 '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size' 
: ''}]                                                                                               

#####################StanfordAIMI/stanford-deidentifier-base########################

-------------------- datasets --------------------
Document 1:

datasets:
- radreports
------------------------------
Document 2:

999 chest X-ray and CT reports collected between November 2019 and November 2020, 3001 X-rays and 2193 medical notes previously labeled, forming a large multi-institutional and cross-domain dataset of 6193 documents. Two radiology test sets, from a known and a new institution, as well as i2b2 2006 and 2014 test sets.
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- paper --------------------
Document 1:

Automated deidentification of radiology reports combining transformer and “hide in plain sight” rule-based methods
------------------------------
Document 2:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings.
-------------------- upstream_model --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. These model weights are the recommended ones among all available deidentifier weights. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. These model weights are the recommended ones among all available deidentifier weights. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
------------------------------
Document 2:

"Several PHI detection models were developed based on different training datasets, fine-tuning approaches and data augmentation techniques, and a synthetic PHI generation algorithm. These models were compared using metrics such as precision, recall and F1 score, as well as paired samples Wilcoxon tests.Our best PHI detection model achieves 97.9 F1 score on radiology reports from a known institution, 99.6 from a new institution, 99.5 on i2b2 2006, and 98.9 on i2b2 2014. On reports from a known institution, it achieves 99.1 recall of detecting the core of each PHI span."
-------------------- hardware --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents
-------------------- limitation_and_bias --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. These model weights are the recommended ones among all available deidentifier weights. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- demo --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- input_format --------------------
Document 1:

- token-classification
- sequence-tagger-model
- pytorch
- transformers
- pubmedbert
- uncased
- radiology
- biomedical
- Stanford de-identifier
- radiology and biomedical documents
- automatising the de-identification process
- github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier

input_format: Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- output_format --------------------
Document 1:

- token-classification
- sequence-tagger-model
- pytorch
- transformers
- pubmedbert
- uncased
- radiology
- biomedical
- datasets:
- radreports
- widget:
- text: 'PROCEDURE: Chest xray. COMPARISON: last seen on 1/1/2020 and also record
dated of March 1st, 2019. FINDINGS: patchy airspace opacities. IMPRESSION: The
results of the chest xray of January 1 2020 are the most concerning ones. The
patient was transmitted to another service of UH Medical Center under the responsability
of Dr. Perez. We used the system MedClinical data transmitter and sent the data
on 2/1/2020, under the ID 5874233. We received the confirmation of Dr Perez. He
is reachable at 567-493-1234.'
- text: Dr. Curt Langlotz chose to schedule a meeting on 06/23.
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings.  Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier

[{'datasets': ['radreports'], 'license': 'mit', 'github': 'https://github.com/MIDRC/Stanford_Penn_D 
eidentifier', 'paper': 'Automated deidentification of radiology reports combining transformer and “h 
ide in plain sight” rule-based methods', 'upstream_model': 'Stanford de-identifier', 'parameter_coun 
t': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'Stanford de-identifier was trained on 
 a variety of radiology and biomedical documents', 'limitation_and_bias': 'Stanford de-identifier wa 
s trained on a variety of radiology and biomedical documents with the goal of automatising the de-id 
entification process while reaching satisfactory accuracy for use in production. Manuscript in-proce 
edings. These model weights are the recommended ones among all available deidentifier weights. Assoc 
iated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier', 'demo': 'Stanford de-identi 
fier was trained on a variety of radiology and biomedical documents with the goal of automatising th 
e de-identification process while reaching satisfactory accuracy for use in production. Associated g 
ithub repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier', 'input_format': '- token-classific 
ation\n- sequence-tagger-model\n- pytorch\n- transformers\n- pubmedbert\n- uncased\n- radiology\n- b 
iomedical\n- Stanford de-identifier\n- radiology and biomedical documents\n- automatising the de-ide 
ntification process\n- github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier', 'output_fo 
rmat': "- token-classification\n- sequence-tagger-model\n- pytorch\n- transformers\n- pubmedbert\n-  
uncased\n- radiology\n- biomedical\n- datasets:\n- radreports\n- widget:\n- text: 'PROCEDURE: Chest  
xray. COMPARISON: last seen on 1/1/2020 and also record\ndated of March 1st, 2019. FINDINGS: patchy  
airspace opacities. IMPRESSION: The\nresults of the chest xray of January 1 2020 are the most concer 
ning ones. The\npatient was transmitted to another service of UH Medical Center under the responsabi 
lity\nof Dr. Perez. We used the system MedClinical data transmitter and sent the data\non 2/1/2020,  
under the ID 5874233. We received the confirmation of Dr Perez. He\nis reachable at 567-493-1234.'\n 
- text: Dr. Curt Langlotz chose to schedule a meeting on 06/23.", 'input_token_limit': '', 'vocabula 
ry_size': 'Stanford de-identifier was trained on a variety of radiology and biomedical documents wit 
h the goal of automatising the de-identification process while reaching satisfactory accuracy for us 
e in production. Manuscript in-proceedings.  Associated github repo: https://github.com/MIDRC/Stanfo 
rd_Penn_Deidentifier'}, {'datasets': ['999 chest X-ray and CT reports collected between November 201 
9 and November 2020', '3001 X-rays and 2193 medical notes previously labeled', 'large multi-institut 
ional and cross-domain dataset of 6193 documents', 'Two radiology test sets', 'i2b2 2006 and 2014 te 
st sets'], 'license': '', 'github': '', 'paper': 'Stanford de-identifier was trained on a variety of 
 radiology and biomedical documents with the goal of automatising the de-identification process whil 
e reaching satisfactory accuracy for use in production. Manuscript in-proceedings.', 'upstream_model 
': 'Stanford de-identifier was trained on a variety of radiology and biomedical documents with the g 
oal of automatising the de-identification process while reaching satisfactory accuracy for use in pr 
oduction. Manuscript in-proceedings. These model weights are the recommended ones among all availabl 
e deidentifier weights. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier' 
, 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'Several PHI detection mode 
ls', 'result': 97.9}, {'test': 'radiology reports from a known institution', 'result': 99.6}, {'test 
': 'radiology reports from a new institution', 'result': 99.5}, {'test': 'i2b2 2006', 'result': 99.5 
}, {'test': 'i2b2 2014', 'result': 98.9}, {'test': 'reports from a known institution', 'result': 99. 
1}], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 
 'input_token_limit': '', 'vocabulary_size': ''}]                                                    

#####################prithivida/parrot_paraphraser_on_T5########################

-------------------- datasets --------------------
Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."
-------------------- license --------------------
Document 1:

"license"
------------------------------
Document 2:

"when was the Berlin wall teared down?", "Turn on the music please", "So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."
-------------------- github --------------------
Document 1:

"github page https://github.com/PrithivirajDamodaran/Parrot"
------------------------------
Document 2:

"we ask questions like 'when was the Berlin wall teared down?'", "we give commands like 'Turn on the music please'", "Parrot mainly foucses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models."
-------------------- paper --------------------
Document 1:

"In the space of conversational engines, knowledge bots are to which we ask questions like 'when was the Berlin wall teared down?', transactional bots are to which we give commands like 'Turn on the music please' and voice assistants are the ones which can do both answer questions and action our commands."
-------------------- upstream_model --------------------
Document 1:

"In the space of conversational engines, knowledge bots are to which we ask questions like 'when was the Berlin wall teared down?', transactional bots are to which we give commands like 'Turn on the music please' and voice assistants are the ones which can do both answer questions and action our commands." NO_OUTPUT
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"diversity_ranker="levenshtein", do_diverse=False, max_return_phrases = 10, max_length=32, adequacy_threshold = 0.99, fluency_threshold = 0.90
------------------------------
Document 2:

- **3 key metrics** that measures the quality of paraphrases are:
- Adequacy
- Fluency
- Diversity (Lexical / Phrasal / Syntactical)
- Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.
-------------------- evaluation --------------------
Document 1:

- **Adequacy** (Is the meaning preserved adequately?)
- **Fluency** (Is the paraphrase fluent English?)
- **Diversity (Lexical / Phrasal / Syntactical)** (How much has the paraphrase changed the original sentence?)
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
------------------------------
Document 2:

"we ask questions like 'when was the Berlin wall teared down?'", "we give commands like 'Turn on the music please'", "Parrot mainly foucses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models."
-------------------- hardware --------------------
Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."
-------------------- limitation_and_bias --------------------
Document 1:

- **Adequacy** (Is the meaning preserved adequately?)
- **Fluency** (Is the paraphrase fluent English?)
- **Diversity (Lexical / Phrasal / Syntactical)** (How much has the paraphrase changed the original sentence?) 
- Given an **input utterance  + input annotations** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.
- The output paraphrases are then converted into annotated data using the input annotations that we got in step 1.
- The annotated data created out of the output paraphrases then makes the training dataset for your NLU model. 
- But in general being a generative model paraphrasers doesn't guarantee to preserve the slots/entities. So the ability to generate high quality paraphrases in a constrained fashion without trading off the intents and slots for lexical dissimilarity makes a paraphraser a good augmentor.
------------------------------
Document 2:

"when was the Berlin wall teared down?", "Turn on the music please", "So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."
-------------------- demo --------------------
Document 1:

github page https://github.com/PrithivirajDamodaran/Parrot
------------------------------
Document 2:

"when was the Berlin wall teared down?", "Turn on the music please", "So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."
------------------------------
Document 3:

- **Huggingface** lists [12 paraphrase models,](https://huggingface.co/models?pipeline_tag=text2text-generation&search=paraphrase)  **RapidAPI** lists 7 fremium and commercial paraphrasers like [QuillBot](https://rapidapi.com/search/paraphrase?section=apis&page=1), Rasa has discussed an experimental paraphraser for augmenting text data [here](https://forum.rasa.com/t/paraphrasing-for-nlu-data-augmentation-experimental/27744), Sentence-transfomers offers a [paraphrase mining utility](https://www.sbert.net/examples/applications/paraphrase-mining/README.html) and [NLPAug](https://github.com/makcedward/nlpaug) offers word level augmentation with a [PPDB](http://paraphrase.org/#/download) (a multi-million paraphrase database).
-------------------- input_format --------------------
Document 1:

"diversity_ranker="levenshtein", do_diverse=False, max_return_phrases = 10, max_length=32, adequacy_threshold = 0.99, fluency_threshold = 0.90
-------------------- output_format --------------------
Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32." NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32." input_token_limit: 32
------------------------------
Document 2:

max_length=32
-------------------- vocabulary_size --------------------
Document 1:

"So usually people neither type out or yell out long paragraphs to conversational interfaces. Hence the pre-trained model is trained  on text samples of maximum length of 32."

[{'datasets': ['Parrot'], 'license': 'license', 'github': 'github page https://github.com/Prithivir 
ajDamodaran/Parrot', 'paper': 'NO_OUTPUT', 'upstream_model': 'NO_OUTPUT', 'parameter_count': 'NO_OUT 
PUT', 'hyper_parameters': [{'epochs': 'NO_OUTPUT', 'batch_size': 'NO_OUTPUT', 'learning_rate': 'NO_O 
UTPUT', 'optimizer': 'NO_OUTPUT'}], 'evaluation': [{'test': 'NO_OUTPUT', 'result': 0}], 'hardware':  
'NO_OUTPUT', 'limitation_and_bias': "- **Adequacy** (Is the meaning preserved adequately?)\n- **Flue 
ncy** (Is the paraphrase fluent English?)\n- **Diversity (Lexical / Phrasal / Syntactical)** (How mu 
ch has the paraphrase changed the original sentence?)\n- Given an **input utterance  + input annotat 
ions** a good augmentor spits out N **output paraphrases** while preserving the intent and slots.\n- 
 The output paraphrases are then converted into annotated data using the input annotations that we g 
ot in step 1.\n- The annotated data created out of the output paraphrases then makes the training da 
taset for your NLU model.\n- But in general being a generative model paraphrasers doesn't guarantee  
to preserve the slots/entities. So the ability to generate high quality paraphrases in a constrained 
 fashion without trading off the intents and slots for lexical dissimilarity makes a paraphraser a g 
ood augmentor.", 'demo': 'github page https://github.com/PrithivirajDamodaran/Parrot', 'input_format 
': 'diversity_ranker="levenshtein", do_diverse=False, max_return_phrases = 10, max_length=32, adequa 
cy_threshold = 0.99, fluency_threshold = 0.90', 'output_format': 'NO_OUTPUT', 'input_token_limit': ' 
32', 'vocabulary_size': 'NO_OUTPUT'}]                                                                

#####################EleutherAI/gpt-j-6b########################

-------------------- datasets --------------------
Document 1:

datasets, 402 billion tokens, 383,500 steps, TPU v3-256 pod
------------------------------
Document 2:

"the Pile", "https://pile.eleuther.ai", "EleutherAI", "https://www.eleuther.ai"
-------------------- license --------------------
Document 1:

GPT-J-6B is **not** intended for deployment without fine-tuning, supervision, and/or moderation. It is not a in itself a product and cannot be used for human-facing interactions. GPT-J-6B has not been fine-tuned for downstream contexts in which language models are commonly deployed, such as writing genre prose, or commercial chatbots. This means GPT-J-6B will **not** respond to a given prompt the way a product like ChatGPT does.
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

"howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
[GitHub](https://github.com/kingoflolz/mesh-transformer-jax)"
------------------------------
Document 2:

"EleutherAI/pile"
-------------------- paper --------------------
Document 1:

```bibtex
@misc{gpt-j,
author = {Wang, Ben and Komatsuzaki, Aran},
title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
year = 2021,
month = May
}
```
------------------------------
Document 2:

"cross-entropy loss to maximize the likelihood of predicting the next token correctly" NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

upstream_model EleutherAI/gpt-j-6B
-------------------- parameter_count --------------------
Document 1:

"383,500 steps" "cross-entropy loss" "predicting the next token correctly" parameter_count
------------------------------
Document 2:

6053381344, 28, 4096, 16384, 16, 256, 2048, 50257/50400
------------------------------
Document 3:

**GPT-J 6B&ddagger;** | **&check;** | **1.5e22** | **parameter_count**
-------------------- hyper_parameters --------------------
Document 1:

"cross-entropy loss" "maximize the likelihood of predicting the next token correctly"
------------------------------
Document 2:

**GPT-J 6B&ddagger;** | **&check;** | **1.5e22** | **3.99** | **69.7%** | **65.3%** | **66.1%** | **76.5%** | **825**
-------------------- evaluation --------------------
Document 1:

"This model was trained for 402 billion tokens over 383,500 steps on TPU v3-256 pod. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token correctly."
------------------------------
Document 2:

Model | Public | Training FLOPs | LAMBADA PPL ↓ | LAMBADA Acc ↑ | Winogrande ↑ | Hellaswag ↑ | PIQA ↑ | Dataset Size (GB)

GPT-J 6B&ddagger; | &check; | 1.5e22 | 3.99 | 69.7% | 65.3% | 66.1% | 76.5% | 825
-------------------- hardware --------------------
Document 1:

TPU v3-256 pod
------------------------------
Document 2:

the Pile, EleutherAI
-------------------- limitation_and_bias --------------------
Document 1:

not intended for deployment without fine-tuning, supervision, and/or moderation; not suitable for translation or generating text in other languages; not been fine-tuned for downstream contexts in which language models are commonly deployed.
------------------------------
Document 2:

limitation_and_bias: NO_OUTPUT
------------------------------
Document 3:

"When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most "accurate" text. Never depend upon GPT-J to produce factually accurate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile. As with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.
-------------------- demo --------------------
Document 1:

"generating text from a prompt"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

n_vocab: 50257/50400

[{'datasets': ['the Pile'], 'license': 'apache-2.0', 'github': 'EleutherAI/pile', 'paper': 'https:/ 
/arxiv.org/abs/2101.00027', 'upstream_model': 'EleutherAI/gpt-j-6B', 'parameter_count': '1.5e22', 'h 
yper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluatio 
n': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_and_bias': 'When prompting GPT-J it is  
important to remember that the statistically most likely next token is often not the token that prod 
uces the most "accurate" text. Never depend upon GPT-J to produce factually accurate output. GPT-J w 
as trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. 
 Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the  
Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile 
. As with all language models, it is hard to predict in advance how GPT-J will respond to particular 
 prompts and offensive content may occur without warning. We recommend having a human curate or filt 
er the outputs before releasing them, both to censor undesirable content and to improve the quality  
of the results.', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'voc 
abulary_size': '50257/50400'}]                                                                       

#####################PascalNotin/Tranception_Small########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['MNLI'], 'license': 'mit', 'github': 'https://github.com/huggingface/transformers',  
'paper': 'https://arxiv.org/abs/1910.03771', 'upstream_model': 'bert-base-uncased', 'parameter_count 
': '109482240', 'hyper_parameters': {'epochs': '3', 'batch_size': '32', 'learning_rate': '2e-5', 'op 
timizer': 'AdamW'}, 'evaluation': [{'test': 'accuracy', 'result': 0.843}], 'hardware': 'NVIDIA V100' 
, 'limitation_and_bias': 'The model may not perform well on tasks outside of the MNLI dataset.', 'de 
mo': 'You can use the model for natural language inference tasks.', 'input_format': 'A pair of sente 
nces', 'output_format': 'A single label', 'input_token_limit': '512', 'vocabulary_size': '30522'}]   
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 local variable 'card' referenced before assignment 

#####################cross-encoder/ms-marco-MiniLM-L-12-v2########################

-------------------- datasets --------------------
Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"In the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset."
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
-------------------- github --------------------
Document 1:

[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

github.com/microsoft/MSMARCO-Passage-Ranking/
-------------------- paper --------------------
Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"cross-encoder/ms-marco-TinyBERT-L-2-v2", "69.84", "32.56", "9000"
-------------------- upstream_model --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'CrossEncoder'
-------------------- parameter_count --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'max_length=512'
-------------------- evaluation --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"In the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset."

"| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |
| ------------- |:-------------| -----| --- |
| **Version 2 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000
| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100
| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500
| cross-encoder/ms-marco-MiniLM-L-6-v2 |
-------------------- hardware --------------------
Document 1:

MS Marco Passage Ranking, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"Note: Runtime was computed on a V100 GPU."
-------------------- limitation_and_bias --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
-------------------- demo --------------------
Document 1:

[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

`from sentence_transformers import CrossEncoder
model = CrossEncoder('model_name', max_length=512)
scores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])`
-------------------- input_format --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'max_length=512'
-------------------- output_format --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
-------------------- input_token_limit --------------------
Document 1:

max_length=512
-------------------- vocabulary_size --------------------


[{'datasets': ['MS Marco Passage Ranking', 'SBERT.net Retrieve & Re-rank', 'SBERT.net Training MS M 
arco'], 'license': 'apache-2.0', 'github': '[MS Marco Passage Ranking](https://github.com/microsoft/ 
MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications 
/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-tran 
sformers/tree/master/examples/training/ms_marco)', 'paper': 'MS Marco Passage Ranking, SBERT.net Ret 
rieve & Re-rank, SBERT.net Training MS Marco', 'upstream_model': 'MS Marco Passage Ranking, Informat 
ion Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'parameter_count': 'MS Ma 
rco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marc 
o', 'hyper_parameters': 'MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training  
MS Marco', 'evaluation': 'MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-r 
ank, SBERT.net Training MS Marco', 'hardware': 'MS Marco Passage Ranking, ElasticSearch, SBERT.net R 
etrieve & Re-rank, SBERT.net Training MS Marco', 'limitation_and_bias': 'MS Marco Passage Ranking, I 
nformation Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'demo': '[MS Marco 
 Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-ra 
nk](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS 
 Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)', 'i 
nput_format': 'MS Marco Passage Ranking, Information Retrieval, ElasticSearch, SBERT.net Retrieve &  
Re-rank, SBERT.net Training MS Marco', 'output_format': 'MS Marco Passage Ranking, Information Retri 
eval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'input_token_limit': 'max_length=5 
12', 'vocabulary_size': ''}]                                                                         

#####################Davlan/bert-base-multilingual-cased-ner-hrl########################

-------------------- datasets --------------------
Document 1:

Language|Dataset
-|-
Arabic | [ANERcorp](https://camel.abudhabi.nyu.edu/anercorp/)
German | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)
English | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)
Spanish | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)
French | [Europeana Newspapers](https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio)
Italian | [Italian I-CAB](https://ontotext.fbk.eu/icab.html)
Latvian | [Latvian NER](https://github.com/LUMII-AILab/FullStack/tree/master/NamedEntities)
Dutch | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)
Portuguese
------------------------------
Document 2:

"bert-base-multilingual-cased-ner-hrl" "Named Entity Recognition" "three types of entities: location (LOC), organizations (ORG), and person (PER)" "bert-base-multilingual-cased" "fine-tuned on an aggregation of 10 high-resourced languages"
-------------------- license --------------------
Document 1:

license: afl-3.0
------------------------------
Document 2:

"bert-base-multilingual-cased-ner-hrl" "Named Entity Recognition" "location (LOC), organizations (ORG), and person (PER)" "bert-base-multilingual-cased" "fine-tuned on an aggregation of 10 high-resourced languages"
------------------------------
Document 3:

conll 2003, conll 2002, Europeana Newspapers, Italian I-CAB, Latvian NER, Paramopama + Second Harem, MSRA
-------------------- github --------------------
Document 1:

"French | [Europeana Newspapers](https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio)
Italian | [Italian I-CAB](https://ontotext.fbk.eu/icab.html)
Latvian | [Latvian NER](https://github.com/LUMII-AILab/FullStack/tree/master/NamedEntities)
Portuguese |[Paramopama + Second Harem](https://github.com/davidsbatista/NER-datasets/tree/master/Portuguese)
Chinese | [MSRA](https://huggingface.co/datasets/msra_ner)"
------------------------------
Document 2:

license: afl-3.0, language: - ar - de - en - es - fr - it - lv - nl - pt - zh - multilingual
------------------------------
Document 3:

"bert-base-multilingual-cased-ner-hrl" "Named Entity Recognition" "10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese)" "three types of entities: location (LOC), organizations (ORG), and person (PER)" "bert-base-multilingual-cased" "fine-tuned on an aggregation of 10 high-resourced languages"
-------------------- paper --------------------
Document 1:

NVIDIA V100 GPU, HuggingFace code
------------------------------
Document 2:

"Named Entity Recognition" model, "bert-base-multilingual-cased" model, "fine-tuned on an aggregation of 10 high-resourced languages"
------------------------------
Document 3:

"This model is limited by its training dataset of entity-annotated news articles from a specific span of time."
-------------------- upstream_model --------------------
Document 1:

"Davlan/bert-base-multilingual-cased-ner-hrl"
------------------------------
Document 2:

bert-base-multilingual-cased
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hyperparameters from HuggingFace code"
------------------------------
Document 2:

"bert-base-multilingual-cased-ner-hrl", "Named Entity Recognition", "location (LOC), organizations (ORG), and person (PER)", "bert-base-multilingual-cased" model, "fine-tuned on an aggregation of 10 high-resourced languages"
-------------------- evaluation --------------------
Document 1:

"Named Entity Recognition" model, "location (LOC), organizations (ORG), and person (PER)"
-------------------- hardware --------------------
Document 1:

NVIDIA V100 GPU
------------------------------
Document 2:

"bert-base-multilingual-cased-ner-hrl" and "bert-base-multilingual-cased"
-------------------- limitation_and_bias --------------------
Document 1:

This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.
------------------------------
Document 2:

The training data for the 10 languages are from:  
Language|Dataset
-|-
Arabic | [ANERcorp](https://camel.abudhabi.nyu.edu/anercorp/)
German | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)
English | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)
Spanish | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)
French | [Europeana Newspapers](https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio)
Italian | [Italian I-CAB](https://ontotext.fbk.eu/icab.html)
Latvian | [Latvian NER](https://github.com/LUMII-AILab/FullStack/tree/master/NamedEntities)
Dutch | [conll 2002](https://www.clips.uantwerpen.be/con
-------------------- demo --------------------
Document 1:

"from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
tokenizer = AutoTokenizer.from_pretrained("Davlan/bert-base-multilingual-cased-ner-hrl")
model = AutoModelForTokenClassification.from_pretrained("Davlan/bert-base-multilingual-cased-ner-hrl")
nlp = pipeline("ner", model=model, tokenizer=tokenizer)"
------------------------------
Document 2:

The training data for the 10 languages are from: Language|Dataset Arabic | [ANERcorp](https://camel.abudhabi.nyu.edu/anercorp/) German | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/) English | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/) Spanish | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/) French | [Europeana Newspapers](https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio) Italian | [Italian I-CAB](https://ontotext.fbk.eu/icab.html) Latvian | [Latvian NER](https://github.com/LUMII-AILab/FullStack/tree/master/NamedEntities) Dutch | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/) Portuguese |[Paramopama + Second Hare
-------------------- input_format --------------------
Document 1:

"The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-PER |Beginning of a person’s name right after another person’s name I-PER |Person’s name B-ORG |Beginning of an organisation right after another organisation I-ORG |Organisation B-LOC |Beginning of a location right after another location I-LOC |Location"
------------------------------
Document 2:

"from transformers import AutoTokenizer, AutoModelForTokenClassification from transformers import pipeline"
-------------------- output_format --------------------
Document 1:

"The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes: Abbreviation|Description -|- O|Outside of a named entity B-PER |Beginning of a person’s name right after another person’s name I-PER |Person’s name B-ORG |Beginning of an organisation right after another organisation I-ORG |Organisation B-LOC |Beginning of a location right after another location I-LOC |Location"
------------------------------
Document 2:

"This model is limited by its training dataset of entity-annotated news articles from a specific span of time."
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"bert-base-multilingual-cased-ner-hrl" "Named Entity Recognition" "10 high resourced languages" "location (LOC), organizations (ORG), and person (PER)" "bert-base-multilingual-cased" "10 high-resourced languages"

[{'datasets': ['ANERcorp', 'conll 2003', 'conll 2003', 'conll 2002', 'Europeana Newspapers', 'Itali 
an I-CAB', 'Latvian NER', 'conll 2002'], 'license': 'afl-3.0', 'github': 'https://github.com/Europea 
naNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio', 'paper': '', 'upstream_model': 'Davlan/bert-ba 
se-multilingual-cased-ner-hrl', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'ha 
rdware': 'NVIDIA V100 GPU', 'limitation_and_bias': 'This model is limited by its training dataset of 
 entity-annotated news articles from a specific span of time. This may not generalize well for all u 
se cases in different domains.', 'demo': 'from transformers import AutoTokenizer, AutoModelForTokenC 
lassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained("Davlan/ 
bert-base-multilingual-cased-ner-hrl")\nmodel = AutoModelForTokenClassification.from_pretrained("Dav 
lan/bert-base-multilingual-cased-ner-hrl")\nnlp = pipeline("ner", model=model, tokenizer=tokenizer)' 
, 'input_format': 'The training dataset distinguishes between the beginning and continuation of an e 
ntity so that if there are back-to-back entities of the same type, the model can output where the se 
cond entity begins. As in the dataset, each token will be classified as one of the following classes 
: Abbreviation|Description -|- O|Outside of a named entity B-PER |Beginning of a person’s name right 
 after another person’s name I-PER |Person’s name B-ORG |Beginning of an organisation right after an 
other organisation I-ORG |Organisation B-LOC |Beginning of a location right after another location I 
-LOC |Location', 'output_format': 'The training dataset distinguishes between the beginning and cont 
inuation of an entity so that if there are back-to-back entities of the same type, the model can out 
put where the second entity begins. As in the dataset, each token will be classified as one of the f 
ollowing classes: Abbreviation|Description -|- O|Outside of a named entity B-PER |Beginning of a per 
son’s name right after another person’s name I-PER |Person’s name B-ORG |Beginning of an organisatio 
n right after another organisation I-ORG |Organisation B-LOC |Beginning of a location right after an 
other location I-LOC |Location', 'input_token_limit': '', 'vocabulary_size': ''}]                    

#####################patrickjohncyh/fashion-clip########################

-------------------- datasets --------------------
Document 1:

Farfecth dataset[^1 Awaiting official release.], image, text, _highlight_, _short description_
------------------------------
Document 2:

"a fashion dataset containing 800K products"
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"license"
-------------------- github --------------------
Document 1:

[FashionCLIP Github Repo](https://github.com/patrickjohncyh/fashion-clip)
------------------------------
Document 2:

[![HuggingFace Model](https://img.shields.io/badge/HF%20Model-Weights-yellow)](https://huggingface.co/patrickjohncyh/fashion-clip) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Z1hAxBnWjF76bEi9KQ6CMBBEmI_FVDrW?usp=sharing) [![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://huggingface.co/spaces/vinid/fashion-clip-app)
-------------------- paper --------------------
Document 1:

"a masked self-attention Transformer as a text encoder" and "maximize the similarity of (image, text) pairs via a contrastive loss on a fashion dataset containing 800K products."
------------------------------
Document 2:

"FashionCLIP Paper"
------------------------------
Document 3:

"The model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.]"
-------------------- upstream_model --------------------
Document 1:

upstream_model ViT-B/32 Transformer
------------------------------
Document 2:

"openai/clip-vit-base-patch32"
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"ViT-B/32 Transformer architecture", "masked self-attention Transformer", "contrastive loss"
-------------------- evaluation --------------------
Document 1:

"The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained, starting from a pre-trained checkpoint, to maximize the similarity of (image, text) pairs via a contrastive loss on a fashion dataset containing 800K products."
-------------------- hardware --------------------
Document 1:

"ViT-B/32 Transformer architecture" and "masked self-attention Transformer"
-------------------- limitation_and_bias --------------------
Document 1:

We acknowledge certain limitations of FashionCLIP and expect that it inherits certain limitations and biases present in the original CLIP model. We do not expect our fine-tuning to significantly augment these limitations: we acknowledge that the fashion data we use makes explicit assumptions about the notion of gender as in "blue shoes for a woman" that inevitably associate aspects of clothing with specific people. Our investigations also suggest that the data used introduces certain limitations in FashionCLIP. From the textual modality, given that most captions derived from the Farfetch dataset are long, we observe that FashionCLIP may be more performant in longer queries than shorter ones. From the image modality, FashionCLIP is also biased towards standard product images (centered, white background). Model selection, i.e. selecting an appropariate stopping critera during fine-tuning, remains an open challenge. We observed that using loss on an in-domain (i.e. same distribution as test) validation dataset is a poor selection critera when out-of-domain generalization (i.e. across different datasets) is desired, even when the dataset used is relatively diverse and large.
------------------------------
Document 2:

limitation_and_bias NO_OUTPUT
------------------------------
Document 3:

"The model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., “stripes”, “long sleeves”, “Armani”) and _short description_ (“80s styled t-shirt”)) available in the Farfetch dataset.
-------------------- demo --------------------
Document 1:

"The model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., “stripes”, “long sleeves”, “Armani”) and _short description_ (“80s styled t-shirt”)) available in the Farfetch dataset.
------------------------------
Document 2:

[![Youtube Video](https://img.shields.io/badge/youtube-video-red)](https://www.youtube.com/watch?v=uqRSc-KSA1Y) [![HuggingFace Model](https://img.shields.io/badge/HF%20Model-Weights-yellow)](https://huggingface.co/patrickjohncyh/fashion-clip) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Z1hAxBnWjF76bEi9KQ6CMBBEmI_FVDrW?usp=sharing) [![Medium Blog Post](https://raw.githubusercontent.com/aleen42/badges/master/src/medium.svg)](https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3) [![Open in Streamlit](https://static.streamlit.io/badges
------------------------------
Document 3:

[FashionCLIP Github Repo](https://github.com/patrickjohncyh/fashion-clip)
-------------------- input_format --------------------
Document 1:

image, text pairs, Farfetch dataset, highlight, short description 
input_format: image, text
------------------------------
Document 2:

language:
- en
license: mit
library_name: transformers
tags:
- vision
- language
- fashion
- ecommerce
widget:
- src: https://cdn-images.farfetch-contents.com/19/76/05/56/19760556_44221665_1000.jpg
candidate_labels: black shoe, red shoe, a cat
example_title: Black Shoe
-------------------- output_format --------------------
Document 1:

output_format
-------------------- input_preprocessing --------------------
Document 1:

"ViT-B/32 Transformer architecture as an image encoder" and "masked self-attention Transformer as a text encoder"
------------------------------
Document 2:

"The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., “stripes”, “long sleeves”, “Armani”) and _short description_ (“80s styled t-shirt”)) available in the Farfetch dataset."
------------------------------
Document 3:

"The model card adapts the model card from [here](https://huggingface.co/openai/clip-vit-base-patch32)."
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------
Document 1:

num_of_classes_for_classification: NO_OUTPUT
-------------------- trigger_word --------------------


[{'datasets': ['Farfetch dataset'], 'license': 'mit', 'github': '[FashionCLIP Github Repo](https:// 
github.com/patrickjohncyh/fashion-clip)', 'paper': 'a masked self-attention Transformer as a text en 
coder and maximize the similarity of (image, text) pairs via a contrastive loss on a fashion dataset 
 containing 800K products.', 'upstream_model': 'ViT-B/32 Transformer', 'parameter_count': 'NO_OUTPUT 
', 'hyper_parameters': {'epochs': 'NO_OUTPUT', 'batch_size': 'NO_OUTPUT', 'learning_rate': 'NO_OUTPU 
T', 'optimizer': 'NO_OUTPUT'}, 'evaluation': [{'test': 'NO_OUTPUT', 'result': 0}], 'hardware': 'NO_O 
UTPUT', 'limitation_and_bias': 'We acknowledge certain limitations of FashionCLIP and expect that it 
 inherits certain limitations and biases present in the original CLIP model. We do not expect our fi 
ne-tuning to significantly augment these limitations: we acknowledge that the fashion data we use ma 
kes explicit assumptions about the notion of gender as in "blue shoes for a woman" that inevitably a 
ssociate aspects of clothing with specific people. Our investigations also suggest that the data use 
d introduces certain limitations in FashionCLIP. From the textual modality, given that most captions 
 derived from the Farfetch dataset are long, we observe that FashionCLIP may be more performant in l 
onger queries than shorter ones. From the image modality, FashionCLIP is also biased towards standar 
d product images (centered, white background). Model selection, i.e. selecting an appropariate stopp 
ing critera during fine-tuning, remains an open challenge. We observed that using loss on an in-doma 
in (i.e. same distribution as test) validation dataset is a poor selection critera when out-of-domai 
n generalization (i.e. across different datasets) is desired, even when the dataset used is relative 
ly diverse and large.', 'demo': 'The model was trained on (image, text) pairs obtained from the Farf 
etch dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion product 
s, with more than 3K brands across dozens of object types. The image used for encoding is the standa 
rd product image, which is a picture of the item over a white background, with no humans. The text u 
sed is a concatenation of the _highlight_ (e.g., “stripes”, “long sleeves”, “Armani”) and _short des 
cription_ (“80s styled t-shirt”)) available in the Farfetch dataset.', 'input_format': 'image, text' 
, 'output_format': 'NO_OUTPUT', 'input_preprocessing': 'The image used for encoding is the standard  
product image, which is a picture of the item over a white background, with no humans. The text used 
 is a concatenation of the _highlight_ (e.g., “stripes”, “long sleeves”, “Armani”) and _short descri 
ption_ (“80s styled t-shirt”)) available in the Farfetch dataset.', 'input_size': 'NO_OUTPUT', 'num_ 
of_classes_for_classification': 'NO_OUTPUT', 'trigger_word': 'NO_OUTPUT'}]                           
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df03b-43dae14c6e323a9b5e770e56)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BloomModel/resolve/main/README.md. 

#####################shatabdi/twisent_twisent########################

-------------------- datasets --------------------
Document 1:

siebert/sentiment-roberta-large-english
------------------------------
Document 2:

- generated_from_trainer
- name: twisent_twisent
- results: []
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

siebert/sentiment-roberta-large-english
-------------------- upstream_model --------------------
Document 1:

siebert/sentiment-roberta-large-english
-------------------- parameter_count --------------------
Document 1:

parameter_count: 8
------------------------------
Document 2:

- name: twisent_twisent
- parameter_count:
-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 2e-05 - train_batch_size: 64 - eval_batch_size: 128 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - lr_scheduler_warmup_steps: 500 - num_epochs: 3
------------------------------
Document 2:

- name: twisent_twisent
results: []
-------------------- evaluation --------------------
Document 1:

- generated_from_trainer
- model-index:
- name: twisent_twisent
- results: []
-------------------- hardware --------------------
Document 1:

siebert/sentiment-roberta-large-english
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

siebert/sentiment-roberta-large-english
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['siebert/sentiment-roberta-large-english'], 'license': '', 'github': '', 'paper': '' 
, 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': 
 '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_li 
mit': '', 'vocabulary_size': ''}]                                                                    

#####################t5-large########################

-------------------- datasets --------------------
Document 1:

- [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4)
- [C4](https://huggingface.co/datasets/c4)
- [Wiki-DPR](https://huggingface.co/datasets/wiki_dpr)
- Sentence acceptability judgment
- CoLA [Warstadt et al., 2018](https://arxiv.org/abs/1805.12471)
- Sentiment analysis
- SST-2 [Socher et al., 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)
- Paraphrasing/sentence similarity
- MRPC [Dolan and Brockett, 2005](https://aclanthology.org/I05-5002)
- STS-B [Ceret al., 2017](https://arxiv.org/abs/1708.00055)
- QQP [Iyer et al., 2017](https://quoradata.quora.com/First-
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"research paper", "see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)"
------------------------------
Document 2:

[research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)
------------------------------
Document 3:

"research paper" and "https://jmlr.org/papers/volume21/20-074/20-074.pdf"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------
Document 1:

"T5-Large is the checkpoint with 770 million parameters."
-------------------- hyper_parameters --------------------
Document 1:

"same model, loss function, and hyperparameters"
-------------------- evaluation --------------------
Document 1:

"The developers evaluated the model on 24 tasks, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for full details."
------------------------------
Document 2:

5. [Evaluation](#evaluation)
------------------------------
Document 3:

"For full results for T5-Large, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf), Table 14."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)"
------------------------------
Document 2:

"We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself."
-------------------- demo --------------------
Document 1:

"demo"
-------------------- input_format --------------------
Document 1:

"reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"text-to-text framework...NLP task...hyperparameters" input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

C4, Wiki-DPR, Sentence acceptability judgment, CoLA, Sentiment analysis, SST-2, Paraphrasing/sentence similarity, MRPC, STS-B, QQP, Natural language inference, MNLI, QNLI, RTE, CB, Sentence completion, COPA, Word sense disambiguation, WIC, Question answering, MultiRC, ReCoRD, BoolQ

[{'datasets': ['Colossal Clean Crawled Corpus (C4)', 'C4', 'Wiki-DPR', 'Sentence acceptability judg 
ment', 'CoLA', 'Sentiment analysis', 'SST-2', 'Paraphrasing/sentence similarity', 'MRPC', 'STS-B', ' 
QQP'], 'license': 'apache-2.0', 'github': '', 'paper': 'see the [research paper](https://jmlr.org/pa 
pers/volume21/20-074/20-074.pdf)', 'upstream_model': '', 'parameter_count': 'T5-Large is the checkpo 
int with 770 million parameters.', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_ra 
te': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_an 
d_bias': '3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)', 'demo': 'demo', 'input_fo 
rmat': 'reframing all NLP tasks into a unified text-to-text-format where the input and output are al 
ways text strings', 'output_format': '', 'input_token_limit': 'text-to-text framework...NLP task...h 
yperparameters', 'vocabulary_size': ''}]                                                             
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df081-665ff27069b2b4ef654e4c9f)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTNeoModel/resolve/main/README.md. 

#####################google/vit-base-patch16-384########################

-------------------- datasets --------------------
Document 1:

"ImageNet-21k", "14 million images and 21k classes", "ImageNet", "1 million images and 1k classes"
------------------------------
Document 2:

datasets: - imagenet - imagenet-21k
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

2006.03677
------------------------------
Document 2:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
------------------------------
Document 3:

"original paper"
-------------------- upstream_model --------------------
Document 1:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
------------------------------
Document 2:

"The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, at a higher resolution of 384x384."
-------------------- parameter_count --------------------
Document 1:

"batch size of 4096" "learning rate warmup of 10k steps" "gradient clipping at global norm 1" "Pre-training resolution is 224"
-------------------- hyper_parameters --------------------
Document 1:

"batch size of 4096", "learning rate warmup of 10k steps", "gradient clipping at global norm 1", "pre-training resolution is 224"
------------------------------
Document 2:

"resized/rescaled to the same resolution (224x224 during pre-training, 384x384 during fine-tuning) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5)."
-------------------- evaluation --------------------
Document 1:

"For evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper."
------------------------------
Document 2:

"All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224."
-------------------- hardware --------------------
Document 1:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
------------------------------
Document 2:

"It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him."
-------------------- limitation_and_bias --------------------
Document 1:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
------------------------------
Document 2:

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=google/vit"
------------------------------
Document 2:

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer).
-------------------- input_format --------------------
Document 1:

Images are resized/rescaled to the same resolution (224x224 during pre-training, 384x384 during fine-tuning) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).
------------------------------
Document 2:

TPUv3 hardware (8 cores), batch size of 4096, learning rate warmup of 10k steps, gradient clipping at global norm 1, pre-training resolution is 224
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"Images are resized/rescaled to the same resolution (224x224 during pre-training, 384x384 during fine-tuning) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5)."
------------------------------
Document 2:

"batch size of 4096", "learning rate warmup of 10k steps", "gradient clipping at global norm 1", "pre-training resolution is 224"
------------------------------
Document 3:

"Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder."
-------------------- input_size --------------------
Document 1:

224x224, 384x384
------------------------------
Document 2:

input_size: 224x224 during pre-training, 384x384 during fine-tuning
-------------------- num_of_classes_for_classification --------------------
Document 1:

1000 ImageNet classes
-------------------- trigger_word --------------------


{'datasets': ['ImageNet-21k', 'ImageNet'], 'license': '', 'github': '', 'paper': '', 'upstream_mode 
l': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': 
 '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'inp 
ut_format': '', 'output_format': '', 'input_preprocessing': '', 'input_size': '', 'num_of_classes_fo 
r_classification': '', 'trigger_word': ''}                                                           

#####################laion/CLIP-ViT-bigG-14-laion2B-39B-b160k########################

-------------------- datasets --------------------
Document 1:

"This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/). Fine-tuning was also partially done on LAION-A, a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and phash deduplicated."
------------------------------
Document 2:

VTAB+ (A combination of VTAB (https://arxiv.org/abs/1910.04867) w/ additional robustness datasets), COCO, Flickr
-------------------- license --------------------
Document 1:

"The license for this model is MIT."
------------------------------
Document 2:

"OpenCLIP software" and "Scaling OpenCLIP paper"
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Citation"
------------------------------
Document 2:

"The OpenAI CLIP paper"
------------------------------
Document 3:

"@inproceedings{schuhmann2022laionb,
title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
author={Christoph Schuhmann and
Romain Beaumont and
Richard Vencu and
Cade W Gordon and
Ross Wightman and
Mehdi Cherti and
Theo Coombes and
Aarush Katta and
Clayton Mullis and
Mitchell Wortsman and
Patrick Schramowski and
Srivatsa R Kundurthy and
Katherine Crowson and
Ludwig Schmidt and
Robert Kaczmarczyk and
Jenia Jitsev},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=M3Y74vmsMcY}"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"4. [Evaluation](#evaluation)"
------------------------------
Document 2:

The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k. **TODO** - create table for just this model's metrics.
------------------------------
Document 3:

VTAB+, VTAB (https://arxiv.org/abs/1910.04867), COCO, Flickr
-------------------- hardware --------------------
Document 1:

"OpenCLIP" and "[stability.ai](https://stability.ai/)" cluster.
-------------------- limitation_and_bias --------------------
Document 1:

Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.
------------------------------
Document 2:

"The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet." "Be aware that this large-scale dataset is uncurated." "Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer." "Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."
-------------------- demo --------------------
Document 1:

"Use the code below to get started with the model."
-------------------- input_format --------------------
Document 1:

"2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)" and "Fine-tuning was also partially done on LAION-A, a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and phash deduplicated."
-------------------- output_format --------------------
Document 1:

"This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/). Fine-tuning was also partially done on LAION-A, a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and phash deduplicated."
-------------------- input_preprocessing --------------------

-------------------- input_size --------------------
Document 1:

"LAION-5B", "OpenAI CLIP paper", "OpenCLIP software", "Scaling OpenCLIP paper"
-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': ['2 Billion sample English subset of LAION-5B', 'LAION-A'], 'license': 'MIT', 'github 
': '', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation 
': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ' 
', 'input_preprocessing': '', 'input_size': '', 'num_of_classes_for_classification': '', 'trigger_wo 
rd': ''}]                                                                                            
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df0d7-7cf20a257303c4db46fb08f7)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-RoFormerModel/resolve/main/README.md. 

#####################DeepPavlov/rubert-base-cased########################

-------------------- datasets --------------------
Document 1:

RuBERT \(Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters\) was trained on the Russian part of Wikipedia and news data. Kuratov, Y., Arkhipov, M. \(2019\). Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language. arXiv preprint [arXiv:1905.07213](https://arxiv.org/abs/1905.07213).
-------------------- license --------------------

-------------------- github --------------------
Document 1:

arXiv:1905.07213, https://arxiv.org/abs/1905.07213
-------------------- paper --------------------
Document 1:

arXiv preprint [arXiv:1905.07213](https://arxiv.org/abs/1905.07213).
-------------------- upstream_model --------------------
Document 1:

"multilingual version of BERT‑base as an initialization for RuBERT"
-------------------- parameter_count --------------------
Document 1:

180M parameters
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

arXiv:1905.07213
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['Russian part of Wikipedia', 'news data'], 'license': '', 'github': 'https://arxiv.o 
rg/abs/1905.07213', 'paper': 'https://arxiv.org/abs/1905.07213', 'upstream_model': 'multilingual ver 
sion of BERT‑base', 'parameter_count': '180M parameters', 'hyper_parameters': {}, 'evaluation': [],  
'hardware': '', 'limitation_and_bias': '', 'demo': 'arXiv:1905.07213', 'input_format': '', 'output_f 
ormat': ''}]                                                                                         

#####################GanjinZero/UMLSBert_ENG########################

-------------------- datasets --------------------
Document 1:

"Github Link: https://github.com/GanjinZero/CODER"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

Github Link: https://github.com/GanjinZero/CODER
-------------------- paper --------------------
Document 1:

@article{YUAN2022103983,
title = {CODER: Knowledge-infused cross-lingual medical term embedding for term normalization},
journal = {Journal of Biomedical Informatics},
pages = {103983},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103983},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421003129},
author = {Zheng Yuan and Zhengyun Zhao and Haixia Sun and Jiao Li and Fei Wang and Sheng Yu},
keywords = {medical term normalization, cross-lingual, medical term representation, knowledge graph embedding, contrastive learning}
}
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

Github Link: https://github.com/GanjinZero/CODER
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': [], 'license': 'apache-2.0', 'github': 'https://github.com/GanjinZero/CODER', 'paper' 
: 'https://www.sciencedirect.com/science/article/pii/S1532046421003129', 'upstream_model': '', 'para 
meter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': ' 
', 'demo': 'https://github.com/GanjinZero/CODER', 'input_format': '', 'output_format': ''}]          

#####################nvidia/speakerverification_en_titanet_large########################

-------------------- datasets --------------------
Document 1:

"Voxceleb-1, Voxceleb-2, Fisher, Switchboard, Librispeech, SRE (2004-2010)"
------------------------------
Document 2:

"The model is available for use in the NeMo toolkit [3]"
------------------------------
Document 3:

"voxceleb datasets, Fisher and switch board"
-------------------- license --------------------
Document 1:

"License to use this model is covered by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/)"
------------------------------
Document 2:

license: cc-by-4.0
-------------------- github --------------------
Document 1:

[2] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)
-------------------- paper --------------------
Document 1:

"Voxceleb-1", "Voxceleb-2", "Fisher", "Switchboard", "Librispeech", "SRE (2004-2010)"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"example script" and "base config"
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"example script" and "base config"
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

NeMo toolkit, example script, base config
-------------------- limitation_and_bias --------------------
Document 1:

Voxceleb-1, Voxceleb-2, Fisher, Switchboard, Librispeech, SRE (2004-2010)
-------------------- demo --------------------
Document 1:

NeMo toolkit [3]
-------------------- input_format --------------------
Document 1:

"Voxceleb-1, Voxceleb-2, Fisher, Switchboard, Librispeech, SRE (2004-2010)"
-------------------- output_format --------------------
Document 1:

output_format

[{'datasets': ['Voxceleb-1', 'Voxceleb-2', 'Fisher', 'Switchboard', 'Librispeech', 'SRE (2004-2010) 
'], 'license': 'CC-BY-4.0', 'github': 'https://github.com/NVIDIA/NeMo', 'paper': '', 'upstream_model 
': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_ 
and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]                                 

#####################facebook/detr-resnet-101########################

-------------------- datasets --------------------
Document 1:

COCO 2017 object detection, https://cocodataset.org/#download, 118k/5k annotated images for training/validation respectively.
------------------------------
Document 2:

datasets:
- coco
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py)
-------------------- paper --------------------
Document 1:

"table 1 of the original paper"
------------------------------
Document 2:

[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)
------------------------------
Document 3:

arXiv:2005.12872
-------------------- upstream_model --------------------
Document 1:

"The DETR model is an encoder-decoder transformer with a convolutional backbone."
------------------------------
Document 2:

upstream_model: End-to-End Object Detection with Transformers by Carion et al.
-------------------- parameter_count --------------------
Document 1:

parameter_count 118k/5k
-------------------- hyper_parameters --------------------
Document 1:

300 epochs, 16 V100 GPUs, 4 images per GPU, total batch size of 64
-------------------- evaluation --------------------
Document 1:

**43.5**, table 1 of the original paper.
------------------------------
Document 2:

"The DETR model was trained on [COCO 2017 object detection](https://cocodataset.org/#download), a dataset consisting of 118k/5k annotated images for training/validation respectively."

NO_OUTPUT
-------------------- hardware --------------------
Document 1:

16 V100 GPUs, 4 images per GPU, total batch size of 64
------------------------------
Document 2:

"COCO 2017 object detection (118k annotated images)"
-------------------- limitation_and_bias --------------------
Document 1:

"COCO 2017 object detection"
------------------------------
Document 2:

"End-to-End Object Detection with Transformers" by Carion et al. and first released in "this repository" https://github.com/facebookresearch/detr
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=facebook/detr"
------------------------------
Document 2:

"COCO 2017 object detection"
------------------------------
Document 3:

"End-to-End Object Detection with Transformers" by Carion et al. and first released in [this repository](https://github.com/facebookresearch/detr).
-------------------- input_format --------------------
Document 1:

Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).
------------------------------
Document 2:

"COCO 2017 object detection"
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225)."
------------------------------
Document 2:

inputs = processor(images=image, return_tensors="pt")
model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-101", revision="no_timm")
------------------------------
Document 3:

"The model is trained using a "bipartite matching loss": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a "no object" as class and "no bounding box" as bounding box)." 
NO_OUTPUT
-------------------- input_size --------------------
Document 1:

input_size: shortest side is at least 800 pixels and the largest side at most 1333 pixels
------------------------------
Document 2:

"118k/5k annotated images for training/validation"
-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': ['COCO 2017 object detection'], 'license': 'apache-2.0', 'github': 'https://github.com 
/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py', 'paper': 'https://arxiv. 
org/abs/2005.12872', 'upstream_model': 'End-to-End Object Detection with Transformers by Carion et a 
l.', 'parameter_count': '118k/5k', 'hyper_parameters': {'epochs': '300', 'batch_size': '64', 'learni 
ng_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'table 1 of the original paper', 'result': 4 
3.5}], 'hardware': '16 V100 GPUs, 4 images per GPU, total batch size of 64', 'limitation_and_bias':  
'COCO 2017 object detection', 'demo': 'https://huggingface.co/models?search=facebook/detr', 'input_f 
ormat': 'Images are resized/rescaled such that the shortest side is at least 800 pixels and the larg 
est side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485,  
0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).', 'output_format': '', 'input_preprocess 
ing': 'Images are resized/rescaled such that the shortest side is at least 800 pixels and the larges 
t side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0. 
456, 0.406) and standard deviation (0.229, 0.224, 0.225).', 'input_size': 'shortest side is at least 
 800 pixels and the largest side at most 1333 pixels', 'num_of_classes_for_classification': '', 'tri 
gger_word': ''}                                                                                      

#####################timm/resnet101.a1h_in1k########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 18743 tokens (18487 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################pysentimiento/robertuito-sentiment-analysis########################

-------------------- datasets --------------------
Document 1:

"TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish"
-------------------- license --------------------
Document 1:

"Repository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)", "Uses `POS`, `NEG`, `NEU` labels."
-------------------- github --------------------
Document 1:

https://github.com/pysentimiento/pysentimiento/, https://github.com/pysentimiento/robertuito/
------------------------------
Document 2:

"HuggingFace model hub"
-------------------- paper --------------------
Document 1:

"RoBERTuito papers: 
@misc{perez2021pysentimiento,
title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},
author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},
year={2021},
eprint={2106.09462},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
@inproceedings{perez-etal-2022-robertuito,
title = "{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish",
author = "P{\'e}rez, Juan Manuel  and
Furman, Dami{\'a}n Ariel  and
Alonso Alemany, Laura  and
Luque, Franco M.",
booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
month = jun,
year = "2022",
address = "Marseille, France",
publisher = "European Language Resources Association",
url =
------------------------------
Document 2:

"Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets."
-------------------- upstream_model --------------------
Document 1:

upstream_model RoBERTuito
-------------------- parameter_count --------------------
Document 1:

"RoBERTuito", "RoBERTa model trained in Spanish tweets"
-------------------- hyper_parameters --------------------
Document 1:

"TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish", "RoBERTuito", "RoBERTa model trained in Spanish tweets", "POS", "NEG", "NEU" labels.
-------------------- evaluation --------------------
Document 1:

Results for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores  
| model         | emotion       | hate_speech   | irony         | sentiment     |
|:--------------|:--------------|:--------------|:--------------|:--------------|
| robertuito    | 0.560 ± 0.010 | 0.759 ± 0.007 | 0.739 ± 0.005 | 0.705 ± 0.003 |
| roberta       | 0.527 ± 0.015 | 0.741 ± 0.012 | 0.721 ± 0.008 | 0.670 ± 0.006 |
| bertin        | 0.524 ± 0.007 | 0.738 ± 0.007 | 0.713 ± 0.012 | 0.666 ± 0.005 |
| beto_uncased  | 0.532 ± 0.012 | 0.727 ± 0.016 | 0.701 ± 0.007 | 0.651 ± 0.006 |
| beto_cased    | 0.516 ± 0.012 | 0.724 ± 0.012 | 0.705
------------------------------
Document 2:

Repository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/), Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish, Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets, Uses `POS`, `NEG`, `NEU` labels.
-------------------- hardware --------------------
Document 1:

"RoBERTuito", "RoBERTa model trained in Spanish tweets"
-------------------- limitation_and_bias --------------------
Document 1:

Note that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B
------------------------------
Document 2:

Repository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/), Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish, Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets, Uses `POS`, `NEG`, `NEU` labels.
-------------------- demo --------------------
Document 1:

Repository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/), Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish, Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets, Uses `POS`, `NEG`, `NEU` labels.
------------------------------
Document 2:

- sentiment-analysis
-------------------- input_format --------------------
Document 1:

- twitter - sentiment-analysis NO_OUTPUT
-------------------- output_format --------------------
Document 1:

Macro F1 scores
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Invalid \escape: line 9 column 460 (char 865) 

#####################Salesforce/blip-image-captioning-large########################

-------------------- datasets --------------------
Document 1:

"noisy image-text pairs collected from the web"
-------------------- license --------------------
Document 1:

Creative Commons Attribution 4.0 International
------------------------------
Document 2:

license: bsd-3-clause
-------------------- github --------------------
Document 1:

"https://github.com/salesforce/BLIP"
-------------------- paper --------------------
Document 1:

"Authors from the [paper](https://arxiv.org/abs/2201.12086) write in the abstract:"
------------------------------
Document 2:

title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}
-------------------- upstream_model --------------------
Document 1:

upstream_model: ViT large backbone
------------------------------
Document 2:

"Vision-Language Pre-training (VLP)" 
upstream_model: Vision-Language Pre-training (VLP)
-------------------- parameter_count --------------------
Document 1:

"parameter_count"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score)."
------------------------------
Document 2:

"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT large backbone)."
-------------------- hardware --------------------
Document 1:

"ViT large backbone"
------------------------------
Document 2:

"model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to("cuda")" and "model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large", torch_dtype=torch.float16).to("cuda")"
-------------------- limitation_and_bias --------------------
Document 1:

"most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks... which is a suboptimal source of supervision... We achieve state-of-the-art results on a wide range of vision-language tasks... BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner."
-------------------- demo --------------------
Document 1:

"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT large backbone)." "Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP"
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

output_format

[{'datasets': ['noisy image-text pairs collected from the web'], 'license': 'Creative Commons Attri 
bution 4.0 International', 'github': 'https://github.com/salesforce/BLIP', 'paper': 'Authors from th 
e [paper](https://arxiv.org/abs/2201.12086) write in the abstract:', 'upstream_model': 'ViT large ba 
ckbone', 'parameter_count': 'parameter_count', 'hyper_parameters': {}, 'evaluation': [{'test': 'We a 
chieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retriev 
al (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).', ' 
result': 0}], 'hardware': 'ViT large backbone', 'limitation_and_bias': 'most existing pre-trained mo 
dels only excel in either understanding-based tasks or generation-based tasks... which is a suboptim 
al source of supervision... We achieve state-of-the-art results on a wide range of vision-language t 
asks... BLIP also demonstrates strong generalization ability when directly transferred to videolangu 
age tasks in a zero-shot manner.', 'demo': 'Model card for image captioning pretrained on COCO datas 
et - base architecture (with ViT large backbone).', 'input_format': '', 'output_format': 'output_for 
mat'}]                                                                                               
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Invalid `model_index` in metadata cannot be parsed: KeyError 'dataset'. Pass `ignore_metadata_errors=True` to ignore this error while loading a Model Card. Warning: some information will be lost. Use it at your own risk. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df19c-0ba3c943135b3efe39c65e0e)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-PegasusModel/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df19c-4e53e7ed7711b0b76191b7c3)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-DebertaV2Model/resolve/main/README.md. 

#####################cross-encoder/ms-marco-MiniLM-L-6-v2########################

-------------------- datasets --------------------
Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"In the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset."
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
-------------------- github --------------------
Document 1:

[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

github.com/microsoft/MSMARCO-Passage-Ranking/
-------------------- paper --------------------
Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"cross-encoder/ms-marco-TinyBERT-L-2-v2", "69.84", "32.56", "9000"
-------------------- upstream_model --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'CrossEncoder'
-------------------- parameter_count --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'max_length=512'
-------------------- evaluation --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"In the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset."

"| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |
| ------------- |:-------------| -----| --- |
| **Version 2 models** | | |
| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000
| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100
| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500
| cross-encoder/ms-marco-MiniLM-L-6-v2 |
-------------------- hardware --------------------
Document 1:

MS Marco Passage Ranking, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

"Note: Runtime was computed on a V100 GPU."
-------------------- limitation_and_bias --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
-------------------- demo --------------------
Document 1:

[MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)
------------------------------
Document 2:

`from sentence_transformers import CrossEncoder
model = CrossEncoder('model_name', max_length=512)
scores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])`
-------------------- input_format --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, ElasticSearch, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
------------------------------
Document 2:

'max_length=512'
-------------------- output_format --------------------
Document 1:

MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco
-------------------- input_token_limit --------------------
Document 1:

max_length=512
-------------------- vocabulary_size --------------------


[{'datasets': ['MS Marco Passage Ranking', 'SBERT.net Retrieve & Re-rank', 'SBERT.net Training MS M 
arco'], 'license': 'apache-2.0', 'github': '[MS Marco Passage Ranking](https://github.com/microsoft/ 
MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications 
/retrieve_rerank/README.html), [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-tran 
sformers/tree/master/examples/training/ms_marco)', 'paper': 'MS Marco Passage Ranking, SBERT.net Ret 
rieve & Re-rank, SBERT.net Training MS Marco', 'upstream_model': 'MS Marco Passage Ranking, Informat 
ion Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'parameter_count': 'MS Ma 
rco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marc 
o', 'hyper_parameters': 'MS Marco Passage Ranking, SBERT.net Retrieve & Re-rank, SBERT.net Training  
MS Marco', 'evaluation': 'MS Marco Passage Ranking, Information Retrieval, SBERT.net Retrieve & Re-r 
ank, SBERT.net Training MS Marco', 'hardware': 'MS Marco Passage Ranking, ElasticSearch, SBERT.net R 
etrieve & Re-rank, SBERT.net Training MS Marco', 'limitation_and_bias': 'MS Marco Passage Ranking, I 
nformation Retrieval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'demo': '[MS Marco 
 Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking), [SBERT.net Retrieve & Re-ra 
nk](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), [SBERT.net Training MS 
 Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)', 'i 
nput_format': 'MS Marco Passage Ranking, Information Retrieval, ElasticSearch, SBERT.net Retrieve &  
Re-rank, SBERT.net Training MS Marco', 'output_format': 'MS Marco Passage Ranking, Information Retri 
eval, SBERT.net Retrieve & Re-rank, SBERT.net Training MS Marco', 'input_token_limit': 'max_length=5 
12', 'vocabulary_size': ''}]                                                                         

#####################Helsinki-NLP/opus-mt-ru-en########################

-------------------- datasets --------------------
Document 1:

* Dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)
* Download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/ru-en/opus-2020-02-26.zip)  
* Test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/ru-en/opus-2020-02-26.test.txt)
-------------------- license --------------------
Document 1:

license: cc-by-4.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Citation Information"
------------------------------
Document 2:

[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

* test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/ru-en/opus-2020-02-26.eval.txt)  
#### Benchmarks  
| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newstest2012.ru.en 	| 34.8 	| 0.603 |
| newstest2013.ru.en 	| 27.9 	| 0.545 |
| newstest2014-ruen.ru.en 	| 31.9 	| 0.591 |
| newstest2015-enru.ru.en 	| 30.4 	| 0.568 |
| newstest2016-enru.ru.en 	| 30.1 	| 0.565 |
| newstest2017-enru.ru.en 	| 33.4 	| 0.593 |
| newstest2018-enru.ru.en 	| 29.6
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

- [Risks, Limitations and Biases](#risks-limitations-and-biases)
------------------------------
Document 2:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

Normalization + SentencePiece, [opus](https://github.com/Helsinki-NLP/Opus-MT), [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/ru-en/opus-2020-02-26.zip), [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/ru-en/opus-2020-02-26.test.txt)
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

Normalization + SentencePiece, opus, opus-2020-02-26.zip, opus-2020-02-26.test.txt

[{'datasets': ['opus'], 'license': 'cc-by-4.0', 'github': '', 'paper': '', 'upstream_model': '', 'p 
arameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias' 
: '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size 
': ''}]                                                                                              
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df1ee-285718821092439c6712a5b0)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-MobileBertModel/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df1ef-4a6b7d183de609c15fb8b8a2)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-XLMModel/resolve/main/README.md. 

#####################GanymedeNil/text2vec-large-chinese########################

-------------------- datasets --------------------
Document 1:

Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged. 
https://github.com/shibing624/text2vec
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

https://github.com/shibing624/text2vec
-------------------- paper --------------------
Document 1:

Refer to the following items for usage:  
https://github.com/shibing624/text2vec
-------------------- upstream_model --------------------
Document 1:

Refer to the following items for usage:  
https://github.com/shibing624/text2vec
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

MacBERT with LERT
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

Refer to the following items for usage: https://github.com/shibing624/text2vec
-------------------- input_format --------------------
Document 1:

language:
- zh
license: apache-2.0
tags:
- text2vec
- feature-extraction
- sentence-similarity
- transformers
pipeline_tag: sentence-similarity
NO_OUTPUT
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['https://huggingface.co/shibing624/text2vec-base-chinese'], 'license': 'apache-2.0', 
 'github': 'https://github.com/shibing624/text2vec', 'paper': 'https://github.com/shibing624/text2ve 
c', 'upstream_model': 'https://github.com/shibing624/text2vec', 'parameter_count': '', 'hyper_parame 
ters': {}, 'evaluation': [], 'hardware': 'MacBERT with LERT', 'limitation_and_bias': '', 'demo': 'ht 
tps://github.com/shibing624/text2vec', 'input_format': 'language:\n- zh\nlicense: apache-2.0\ntags:\ 
n- text2vec\n- feature-extraction\n- sentence-similarity\n- transformers\npipeline_tag: sentence-sim 
ilarity\nNO_OUTPUT', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]           

#####################tiiuae/falcon-40b-instruct########################

-------------------- datasets --------------------
Document 1:

The 📓 [RefinedWeb paper](https://arxiv.org/abs/2306.01116) and 

@article{refinedweb,
title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
journal={arXiv preprint arXiv:2306.01116},
eprint={2306.01116},
eprinttype = {arXiv},
url={https://arxiv.org/abs/2306.01116},
year={2023}
}

@article{xu2023baize,
title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
author={Xu, Canwen and Guo, Daya and D
------------------------------
Document 2:

datasets: - tiiuae/falcon-refinedweb
------------------------------
Document 3:

datasets, chat dataset
-------------------- license --------------------
Document 1:

Apache 2.0 license
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

https://github.com/project-baize/baize-chatbot
------------------------------
Document 2:

tiiuae/falcon-refinedweb
-------------------- paper --------------------
Document 1:

"Paper: *coming soon*."
------------------------------
Document 2:

```
@article{falcon40b,
title={{Falcon-40B}: an open large language model with state-of-the-art performance},
author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
year={2023}
}
```
-------------------- upstream_model --------------------
Document 1:

"Falcon-40B is the best open-source model available." "It features an architecture optimized for inference, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150))."

NO_OUTPUT
------------------------------
Document 2:

"Finetuned from model: [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b)"
-------------------- parameter_count --------------------
Document 1:

"Layers: 60", "d_model: 8192", "head_dim: 64", "Vocabulary: 65024", "Sequence length: 2048"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Falcon-40B-Instruct, AWS SageMaker, 64 A100 40GB GPUs, P4d instances, custom distributed training codebase, Gigatron, 3D parallelism approach, ZeRO, high-performance Triton kernels (FlashAttention, etc.)
------------------------------
Document 2:

Falcon-40B
-------------------- limitation_and_bias --------------------
Document 1:

"Falcon-40B-Instruct is mostly trained on English data" "it will carry the stereotypes and biases commonly encountered online"
------------------------------
Document 2:

"Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of [Baize](https://github.com/project-baize/baize-chatbot)."
-------------------- demo --------------------
Document 1:

"You are looking for a ready-to-use chat/instruct model based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b)."

"If you are interested in building your own instruct/chat model, we recommend starting from [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b)."

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = "tiiuae/falcon-40b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
"text-generation",
model=model,
tokenizer=tokenizer,
torch_dtype=torch.bfloat16,
trust_remote_code=True,
device_map="auto",
)
sequences = pipeline(
"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"Falcon-40B-Instruct was finetuned on a 150M tokens" NO_OUTPUT
------------------------------
Document 2:

"It features an architecture optimized for inference, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150))."

NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

Vocabulary: 65024
------------------------------
Document 2:

vocabulary_size: Falcon-40B
------------------------------
Document 3:

"Falcon-40B is the best open-source model available." "It features an architecture optimized for inference, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150))."

NO_OUTPUT

[{'datasets': ['tiiuae/falcon-refinedweb'], 'license': 'apache-2.0', 'github': 'https://github.com/ 
project-baize/baize-chatbot', 'paper': 'https://arxiv.org/abs/2306.01116', 'upstream_model': 'Falcon 
-40B', 'parameter_count': 'Layers: 60, d_model: 8192, head_dim: 64, Vocabulary: 65024, Sequence leng 
th: 2048', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'Falcon-40B-Instruct, AWS SageMaker 
, 64 A100 40GB GPUs, P4d instances, custom distributed training codebase, Gigatron, 3D parallelism a 
pproach, ZeRO, high-performance Triton kernels (FlashAttention, etc.)', 'limitation_and_bias': 'Falc 
on-40B-Instruct is mostly trained on English data. It will carry the stereotypes and biases commonly 
 encountered online', 'demo': 'You are looking for a ready-to-use chat/instruct model based on Falco 
n-40B. If you are interested in building your own instruct/chat model, we recommend starting from Fa 
lcon-40B.', 'input_format': '', 'output_format': '', 'input_token_limit': 'Falcon-40B-Instruct was f 
inetuned on a 150M tokens', 'vocabulary_size': 'Vocabulary: 65024'}]                                 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df232-7fb15313412140d1757a4cf1)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-ConvBertModel/resolve/main/README.md. 

#####################moussaKam/barthez-orangesum-abstract########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"github: https://github.com/moussaKam/BARThez"
-------------------- paper --------------------
Document 1:

paper: https://arxiv.org/abs/2010.12321
------------------------------
Document 2:

"Citant les préoccupations de ses clients dénonçant des cas de censure après la suppression du compte de Trump, un fournisseur d'accès Internet de l'État de l'Idaho a décidé de bloquer Facebook et Twitter."

NO_OUTPUT
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

language: - fr license: apache-2.0 tags: - summarization - bart widget: - text: Citant les préoccupations de ses clients dénonçant des cas de censure après la suppression du compte de Trump, un fournisseur d'accès Internet de l'État de l'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients mécontents de la politique de ces réseaux sociaux.
-------------------- demo --------------------
Document 1:

language: - fr license: apache-2.0 tags: - summarization - bart widget: - text: Citant les préoccupations de ses clients dénonçant des cas de censure après la suppression du compte de Trump, un fournisseur d'accès Internet de l'État de l'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients mécontents de la politique de ces réseaux sociaux.
------------------------------
Document 2:

github: https://github.com/moussaKam/BARThez
-------------------- input_format --------------------
Document 1:

language: - fr license: apache-2.0 tags: - summarization - bart widget: - text: Citant les préoccupations de ses clients dénonçant des cas de censure après la suppression du compte de Trump, un fournisseur d'accès Internet de l'État de l'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients mécontents de la politique de ces réseaux sociaux.
-------------------- output_format --------------------
Document 1:

language:
- fr
license: apache-2.0
tags:
- summarization
- bart
widget:
- text: Citant les préoccupations de ses clients dénonçant des cas de censure après
la suppression du compte de Trump, un fournisseur d'accès Internet de l'État de
l'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependant
que les clients mécontents de la politique de ces réseaux sociaux.
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

language: - fr license: apache-2.0 tags: - summarization - bart widget: - text: Citant les préoccupations de ses clients dénonçant des cas de censure après la suppression du compte de Trump, un fournisseur d'accès Internet de l'État de l'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients mécontents de la politique de ces réseaux sociaux.

[{'datasets': [], 'license': 'apache-2.0', 'github': 'https://github.com/moussaKam/BARThez', 'paper 
': 'https://arxiv.org/abs/2010.12321', 'upstream_model': '', 'parameter_count': '', 'hyper_parameter 
s': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': "language: - fr license: apache-2.0 
 tags: - summarization - bart widget: - text: Citant les préoccupations de ses clients dénonçant des 
 cas de censure après la suppression du compte de Trump, un fournisseur d'accès Internet de l'État d 
e l'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients 
 mécontents de la politique de ces réseaux sociaux.", 'demo': "language: - fr license: apache-2.0 ta 
gs: - summarization - bart widget: - text: Citant les préoccupations de ses clients dénonçant des ca 
s de censure après la suppression du compte de Trump, un fournisseur d'accès Internet de l'État de l 
'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clients mé 
contents de la politique de ces réseaux sociaux.", 'input_format': "language: - fr license: apache-2 
.0 tags: - summarization - bart widget: - text: Citant les préoccupations de ses clients dénonçant d 
es cas de censure après la suppression du compte de Trump, un fournisseur d'accès Internet de l'État 
 de l'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependant que les clien 
ts mécontents de la politique de ces réseaux sociaux.", 'output_format': "language:\n- fr\nlicense:  
apache-2.0\ntags:\n- summarization\n- bart\nwidget:\n- text: Citant les préoccupations de ses client 
s dénonçant des cas de censure après\nla suppression du compte de Trump, un fournisseur d'accès Inte 
rnet de l'État de\nl'Idaho a décidé de bloquer Facebook et Twitter. La mesure ne concernera cependan 
t\nque les clients mécontents de la politique de ces réseaux sociaux.", 'input_token_limit': '', 'vo 
cabulary_size': ''}]                                                                                 

#####################Helsinki-NLP/opus-mt-it-en########################

-------------------- datasets --------------------
Document 1:

dataset: opus, download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip)
------------------------------
Document 2:

newssyscomb2009.it.en, newstest2009.it.en, Tatoeba.it.en
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

OPUS readme: [it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md)
-------------------- github --------------------
Document 1:

[it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md), [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)
-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

*test set scores: [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)*
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009.it.en 	| 35.3 	| 0.600 |
| newstest2009.it.en 	| 34.0 	| 0.594 |
| Tatoeba.it.en 	| 70.9 	| 0.808 |
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[it-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/it-en/README.md), opus, transformer-align, normalization + SentencePiece, [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)
-------------------- input_format --------------------
Document 1:

SentencePiece + normalization
-------------------- output_format --------------------
Document 1:

SentencePiece + opus-2019-12-18.zip + opus-2019-12-18.test.txt + opus-2019-12-18.eval.txt
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

SentencePiece

[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': '[it-en](https://github.com/Helsinki-NLP 
/OPUS-MT-train/blob/master/models/it-en/README.md), [opus-2019-12-18.zip](https://object.pouta.csc.f 
i/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/ 
OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opus-2019-12-18.eval.txt](https://object.pouta.csc. 
fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)', 'paper': '', 'upstream_model': '', 'parameter_co 
unt': '', 'hyper_parameters': [], 'evaluation': [{'test': 'opus-2019-12-18.eval.txt', 'result': 0}], 
 'hardware': '', 'limitation_and_bias': '', 'demo': '[it-en](https://github.com/Helsinki-NLP/OPUS-MT 
-train/blob/master/models/it-en/README.md), opus, transformer-align, normalization + SentencePiece,  
[opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.zip), [opus-2 
019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.test.txt), [opu 
s-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/it-en/opus-2019-12-18.eval.txt)',  
'input_format': 'SentencePiece + normalization', 'output_format': 'SentencePiece + opus-2019-12-18.z 
ip + opus-2019-12-18.test.txt + opus-2019-12-18.eval.txt', 'input_token_limit': '', 'vocabulary_size 
': 'SentencePiece'}]                                                                                 

#####################siebert/sentiment-roberta-large-english########################

-------------------- datasets --------------------
Document 1:

[Google Colab](https://colab.research.google.com/notebooks/intro.ipynb) and [Hartmann et al. 2022](https://www.sciencedirect.com/science/article/pii/S0167811622000477?via%3Dihub)
------------------------------
Document 2:

McAuley and Leskovec (2013) (Reviews), McAuley and Leskovec (2013) (Review Titles), Yelp Academic Dataset, Maas et al. (2011), Kaggle, Pang and Lee (2005), Nakov et al. (2013), Shamma (2009), Blitzer et al. (2007) (Books), Blitzer et al. (2007) (DVDs), Blitzer et al. (2007) (Electronics), Blitzer et al. (2007) (Kitchen devices), Pang et al. (2002), Speriosu et al. (2011), Hartmann et al. (2019)
-------------------- license --------------------

-------------------- github --------------------
Document 1:

[Google Colab](https://colab.research.google.com/notebooks/intro.ipynb), [Hartmann et al. 2022](https://www.sciencedirect.com/science/article/pii/S0167811622000477?via%3Dihub), [Open In Colab](https://colab.research.google.com/assets/colab-badge.svg), (https://colab.research.google.com/github/chrsiebert/sentiment-roberta-large-english/blob/main/sentiment_roberta_prediction_example.ipynb)
-------------------- paper --------------------
Document 1:

"RoBERTa-large" ([Liu et al. 2019](https://arxiv.org/pdf/1907.11692.pdf))
------------------------------
Document 2:

"title = {More than a Feeling: Accuracy and Application of Sentiment Analysis}, journal = {International Journal of Research in Marketing}, volume = {40}, number = {1}, pages = {75-87}, year = {2023}, doi = {https://doi.org/10.1016/j.ijresmar.2022.05.005}, url = {https://www.sciencedirect.com/science/article/pii/S0167811622000477}, author = {Jochen Hartmann and Mark Heitmann and Christian Siebert and Christina Schamp},"
-------------------- upstream_model --------------------
Document 1:

RoBERTa-large, Liu et al. 2019
------------------------------
Document 2:

"model="siebert/sentiment-roberta-large-english""
-------------------- parameter_count --------------------
Document 1:

parameter_count, learning_rate = 2e-5, num_train_epochs = 3.0, warmump_steps = 500, weight_decay = 0.01
------------------------------
Document 2:

"RoBERTa-large" "Liu et al. 2019" "binary sentiment analysis" "15 data sets" "SST-2 benchmark"
-------------------- hyper_parameters --------------------
Document 1:

learning_rate = 2e-5, num_train_epochs = 3.0, warmump_steps = 500, weight_decay = 0.01
-------------------- evaluation --------------------
Document 1:

"On average, our model outperforms a [DistilBERT-based model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) (which is solely fine-tuned on the popular SST-2 data set) by more than 15 percentage points (78.1 vs. 93.2 percent, see table below)."

"Model performance is given as evaluation set accuracy in percent."

"|Dataset|DistilBERT SST-2|This model|
|---|---|---|
|McAuley and Leskovec (2013) (Reviews)|84.7|98.0|
|McAuley and Leskovec (2013) (Review Titles)|65.5|87.0|
|Yelp Academic Dataset|84.8|96.5|
|Maas et al. (2011)|80.6|96.0|
|Kaggle|87.2|96.0|
|Pang and Lee (2005)|89.7|91.0|
|Nakov et
------------------------------
Document 2:

"This model ("SiEBERT", prefix for "Sentiment in English") is a fine-tuned checkpoint of [RoBERTa-large](https://huggingface.co/roberta-large) ([Liu et al. 2019](https://arxiv.org/pdf/1907.11692.pdf)). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.)."
-------------------- hardware --------------------
Document 1:

RoBERTa-large, Liu et al. 2019
-------------------- limitation_and_bias --------------------
Document 1:

"It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.)."
------------------------------
Document 2:

"On average, our model outperforms a [DistilBERT-based model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) (which is solely fine-tuned on the popular SST-2 data set) by more than 15 percentage points (78.1 vs. 93.2 percent, see table below). As a robustness check, we evaluate the model in a leave-one-out manner (training on 14 data sets, evaluating on the one left out), which decreases model performance by only about 3 percentage points on average and underscores its generalizability. Model performance is given as evaluation set accuracy in percent."
-------------------- demo --------------------
Document 1:

"[Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)", "[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chrsiebert/sentiment-roberta-large-english/blob/main/sentiment_roberta_prediction_example.ipynb)"
------------------------------
Document 2:

"The easiest way to use the model for single predictions is Hugging Face's [sentiment analysis pipeline](https://huggingface.co/transformers/quicktour.html#getting-started-on-a-task-with-a-pipeline), which only needs a couple lines of code as shown in the following example:
```
from transformers import pipeline
sentiment_analysis = pipeline("sentiment-analysis",model="siebert/sentiment-roberta-large-english")
print(sentiment_analysis("I love this!"))
```  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chrsiebert/sentiment-roberta-large-english/blob/main/sentiment_roberta_pipeline.ipynb)"
-------------------- input_format --------------------
Document 1:

"You can load your data to a Google Drive and run the script for free on a Colab GPU" 
input_format: Google Drive
------------------------------
Document 2:

"RoBERTa-large"
-------------------- output_format --------------------
Document 1:

"Google Colab", "Google Drive", "sentiment_roberta_prediction_example.ipynb"
------------------------------
Document 2:

"It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment."
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"RoBERTa-large", "Liu et al. 2019", "binary sentiment analysis", "15 data sets from diverse text sources"

[{'datasets': ['McAuley and Leskovec (2013) (Reviews)', 'McAuley and Leskovec (2013) (Review Titles 
)', 'Yelp Academic Dataset', 'Maas et al. (2011)', 'Kaggle', 'Pang and Lee (2005)', 'Nakov et al. (2 
013)', 'Shamma (2009)', 'Blitzer et al. (2007) (Books)', 'Blitzer et al. (2007) (DVDs)', 'Blitzer et 
 al. (2007) (Electronics)', 'Blitzer et al. (2007) (Kitchen devices)', 'Pang et al. (2002)', 'Sperio 
su et al. (2011)', 'Hartmann et al. (2019)'], 'license': '', 'github': '', 'paper': '', 'upstream_mo 
del': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitati 
on_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'voc 
abulary_size': ''}]                                                                                  

#####################Helsinki-NLP/opus-mt-fr-en########################

-------------------- datasets --------------------
Document 1:

dataset: opus, download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.zip)
------------------------------
Document 2:

newsdiscussdev2015-enfr.fr.en, newsdiscusstest2015-enfr.fr.en, newssyscomb2009.fr.en, news-test2008.fr.en, newstest2009.fr.en, newstest2010.fr.en, newstest2011.fr.en, newstest2012.fr.en, newstest2013.fr.en, newstest2014-fren.fr.en, Tatoeba.fr.en
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

OPUS readme: [fr-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/fr-en/README.md)
-------------------- github --------------------
Document 1:

* OPUS readme: [fr-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/fr-en/README.md)
* download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.zip)
* test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.test.txt)
* test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.eval.txt)
-------------------- paper --------------------
Document 1:

"model: transformer-align"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newsdiscussdev2015-enfr.fr.en 	| 33.1 	| 0.580 |
| newsdiscusstest2015-enfr.fr.en 	| 38.7 	| 0.614 |
| newssyscomb2009.fr.en 	| 30.3 	| 0.569 |
| news-test2008.fr.en 	| 26.2 	| 0.542 |
| newstest2009.fr.en 	| 30.2 	| 0.570 |
| newstest2010.fr.en 	| 32.2 	| 0.590 |
| newstest2011.fr.en 	| 33.0 	| 0.597 |
| newstest2012.fr.en 	| 32.8 	| 0.591 |
| newstest2013.fr.en 	| 33.9 	| 0.591 |
| newstest2014-fren.fr
------------------------------
Document 2:

opus-2020-02-26.eval.txt
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

*OPUS readme: [fr-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/fr-en/README.md)*
* *download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.zip)*
* *test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.test.txt)*
* *test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.eval.txt)*
-------------------- input_format --------------------
Document 1:

SentencePiece
------------------------------
Document 2:

"newsdiscussdev2015-enfr.fr.en 33.1 0.580 newsdiscusstest2015-enfr.fr.en 38.7 0.614 newssyscomb2009.fr.en 30.3 0.569 news-test2008.fr.en 26.2 0.542 newstest2009.fr.en 30.2 0.570 newstest2010.fr.en 32.2 0.590 newstest2011.fr.en 33.0 0.597 newstest2012.fr.en 32.8 0.591 newstest2013.fr.en 33.9 0.591 newstest2014-fren.fr.en 37.8 0.633 Tatoeba.fr.en 57.5 0.720"
-------------------- output_format --------------------
Document 1:

"BLEU  | chr-F |"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': '* OPUS readme: [fr-en](https://github.c 
om/Helsinki-NLP/OPUS-MT-train/blob/master/models/fr-en/README.md)\n* download original weights: [opu 
s-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.zip)\n* test set  
translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020- 
02-26.test.txt)\n* test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-m 
odels/fr-en/opus-2020-02-26.eval.txt)', 'paper': 'model: transformer-align', 'upstream_model': '', ' 
parameter_count': '', 'hyper_parameters': [], 'evaluation': [{'test': 'newsdiscussdev2015-enfr.fr.en 
', 'result': 33.1}, {'test': 'newsdiscusstest2015-enfr.fr.en', 'result': 38.7}, {'test': 'newssyscom 
b2009.fr.en', 'result': 30.3}, {'test': 'news-test2008.fr.en', 'result': 26.2}, {'test': 'newstest20 
09.fr.en', 'result': 30.2}, {'test': 'newstest2010.fr.en', 'result': 32.2}, {'test': 'newstest2011.f 
r.en', 'result': 33.0}, {'test': 'newstest2012.fr.en', 'result': 32.8}, {'test': 'newstest2013.fr.en 
', 'result': 33.9}, {'test': 'newstest2014-fren.fr', 'result': 37.8}], 'hardware': '', 'limitation_a 
nd_bias': '', 'demo': '*OPUS readme: [fr-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/mast 
er/models/fr-en/README.md)*\n* *download original weights: [opus-2020-02-26.zip](https://object.pout 
a.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.zip)*\n* *test set translations: [opus-2020-02-26.test 
.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.test.txt)*\n* *test set score 
s: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.eval. 
txt)*', 'input_format': 'SentencePiece', 'output_format': 'BLEU  | chr-F |', 'input_token_limit': '' 
, 'vocabulary_size': ''}]                                                                            

#####################google/flan-t5-xl########################

-------------------- datasets --------------------
Document 1:

"The model was trained on a mixture of tasks"
-------------------- license --------------------
Document 1:

Creative Commons Attribution 4.0 International
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

- svakulenk0/qrecc
- djaym7/wiki_dialog
- deepmind/code_contests
- gsm8k
- aqua_rat
- esnli
- quasc
- qed
-------------------- paper --------------------
Document 1:

"original paper, figure 2"
------------------------------
Document 2:

"For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hyper_parameters"
-------------------- evaluation --------------------
Document 1:

"The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: ![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png) For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."
------------------------------
Document 2:

6. [Evaluation](#evaluation)
------------------------------
Document 3:

"The model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2): ![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)"
-------------------- hardware --------------------
Document 1:

5. [Training Details](#training-details)
-------------------- limitation_and_bias --------------------
Document 1:

4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)
------------------------------
Document 2:

"not filtered for explicit content or assessed for existing biases" "potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data"
-------------------- demo --------------------
Document 1:

"Find below some example scripts on how to use the model in `transformers`:"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['mixture of tasks'], 'license': 'Creative Commons Attribution 4.0 International', 'g 
ithub': '- svakulenk0/qrecc\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n-  
esnli\n- quasc\n- qed', 'paper': 'original paper, figure 2', 'upstream_model': '', 'parameter_count' 
: 'parameter_count', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'opti 
mizer': ''}, 'evaluation': [{'test': 'The authors evaluated the model on various tasks covering seve 
ral languages (1836 in total). See the table below for some quantitative evaluation:', 'result': 0}] 
, 'hardware': '5. [Training Details](#training-details)', 'limitation_and_bias': '4. [Bias, Risks, a 
nd Limitations](#bias-risks-and-limitations)', 'demo': 'Find below some example scripts on how to us 
e the model in `transformers`:', 'input_format': '', 'output_format': '', 'input_token_limit': '', ' 
vocabulary_size': ''}]                                                                               

#####################jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli########################

-------------------- datasets --------------------
Document 1:

libritts and voxpopuli datasets
-------------------- license --------------------

-------------------- github --------------------
Document 1:

https://github.com/facebookresearch/voxpopuli
https://github.com/neonbjb/ocotillo
-------------------- paper --------------------
Document 1:

"This model was created by fine-tuning the `facebook/wav2vec2-large-robust-ft-libri-960h` checkpoint on the [libritts](https://research.google/tools/datasets/libri-tts/) and [voxpopuli](https://github.com/facebookresearch/voxpopuli) datasets with a new vocabulary that includes punctuation."
-------------------- upstream_model --------------------
Document 1:

`facebook/wav2vec2-large-robust-ft-libri-960h`
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

This model was created by fine-tuning the `facebook/wav2vec2-large-robust-ft-libri-960h` checkpoint on the [libritts](https://research.google/tools/datasets/libri-tts/) and [voxpopuli](https://github.com/facebookresearch/voxpopuli) datasets with a new vocabulary that includes punctuation. The model gets a respectable WER of 4.45% on the librispeech validation set. The baseline, `facebook/wav2vec2-large-robust-ft-libri-960h`, got 4.3%.
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Check out my speech transcription script repo, [ocotillo](https://github.com/neonbjb/ocotillo) for usage examples: https://github.com/neonbjb/ocotillo"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- sample_rate --------------------

-------------------- WER --------------------
Document 1:

The model gets a respectable WER of 4.45% on the librispeech validation set. The baseline, `facebook/wav2vec2-large-robust-ft-libri-960h`, got 4.3%.

[{'datasets': ['libritts', 'voxpopuli'], 'license': '', 'github': 'https://github.com/facebookresea 
rch/voxpopuli\nhttps://github.com/neonbjb/ocotillo', 'paper': '"This model was created by fine-tunin 
g the `facebook/wav2vec2-large-robust-ft-libri-960h` checkpoint on the [libritts](https://research.g 
oogle/tools/datasets/libri-tts/) and [voxpopuli](https://github.com/facebookresearch/voxpopuli) data 
sets with a new vocabulary that includes punctuation."', 'upstream_model': '`facebook/wav2vec2-large 
-robust-ft-libri-960h`', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': '',  
'result': 4.45}], 'hardware': '', 'limitation_and_bias': '', 'demo': '"Check out my speech transcrip 
tion script repo, [ocotillo](https://github.com/neonbjb/ocotillo) for usage examples: https://github 
.com/neonbjb/ocotillo"', 'input_format': '', 'output_format': '', 'sample_rate': '', 'WER': 'The mod 
el gets a respectable WER of 4.45% on the librispeech validation set. The baseline, `facebook/wav2ve 
c2-large-robust-ft-libri-960h`, got 4.3%.'}]                                                         
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df31f-4662f1c4248fafc51347f9c3)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-flaubert/resolve/main/README.md. 

#####################facebook/mbart-large-50########################

-------------------- datasets --------------------
Document 1:

"Multilingual Denoising Pretraining" objective
------------------------------
Document 2:

Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: `D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(λ = 3.5)`. The decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict the sentence.
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"Multilingual Denoising Pretraining" objective
------------------------------
Document 3:

"archivePrefix={arXiv}, primaryClass={cs.CL}"
-------------------- github --------------------
Document 1:

language:
- multilingual
- ar
- cs
- de
- en
- es
- et
- fi
- fr
- gu
- hi
- it
- ja
- kk
- ko
- lt
- lv
- my
- ne
- nl
- ro
- ru
- si
- tr
- vi
- zh
- af
- az
- bn
- fa
- he
- hr
- id
- ka
- km
- mk
- ml
- mn
- mr
- pl
- ps
- pt
- sv
- sw
- ta
- te
- th
- tl
- uk
- ur
- xh
- gl
- sl
license: mit
------------------------------
Document 2:

github
-------------------- paper --------------------
Document 1:

"Multilingual Translation with Extensible Multilingual Pretraining and Finetuning" paper.
------------------------------
Document 2:

"title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},"
------------------------------
Document 3:

mBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. Instead of fine-tuning on one direction, a pre-trained model is fine-tuned on many directions simultaneously. mBART-50 is created using the original mBART model and extended to add extra 25 languages to support multilingual machine translation models of 50 languages. The pre-training objective is Multilingual Denoising Pretraining. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(λ = 3.5)`. The decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict the sentence.
-------------------- upstream_model --------------------
Document 1:

mBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. The pre-training objective is explained below. Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: `D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(λ = 3.5)`. The decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict the sentence.
------------------------------
Document 2:

upstream_model: "Multilingual Denoising Pretraining"
-------------------- parameter_count --------------------
Document 1:

Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: `D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(λ = 3.5)`. The decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict the sentence.
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"Multilingual Denoising Pretraining" objective
------------------------------
Document 2:

Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: `D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(λ = 3.5)`. The decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict the sentence.
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"Multilingual Denoising Pretraining" objective
------------------------------
Document 2:

Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: `D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(λ = 3.5)`. The decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict the sentence.
-------------------- limitation_and_bias --------------------
Document 1:

mBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. Instead of fine-tuning on one direction, a pre-trained model is fine-tuned on many directions simultaneously. mBART-50 is created using the original mBART model and extended to add extra 25 languages to support multilingual machine translation models of 50 languages. The pre-training objective is explained below. Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: `D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(λ = 3.5)`. The decoder input is the original text with one position offset. A language id symbol `LID` is
-------------------- demo --------------------
Document 1:

`[lang_code] X [eos]` with `X` being the source or target text respectively and `lang_code` is `source_lang_code` for source text and `tgt_lang_code` for target text. `bos` is never used. 

Once the examples are prepared in this format, it can be trained as any other sequence-to-sequence model. 

```python
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50")
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50", src_lang="en_XX", tgt_lang="ro_RO")

src_text = " UN Chief Says There Is No Military Solution in Syria"
tgt_text =  "Şeful ONU declară că nu există o soluţie militară în Siria"

model_inputs = tokenizer(src_text, return_tensors="
------------------------------
Document 2:

"See the [model hub](https://huggingface.co/models?filter=mbart-50) to look for fine-tuned versions."
-------------------- input_format --------------------
Document 1:

`[lang_code] X [eos]` with `X` being the source or target text respectively and `lang_code` is `source_lang_code` for source text and `tgt_lang_code` for target text. `bos` is never used.
------------------------------
Document 2:

Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: D = {D1, ..., DN } where each Di is a collection of monolingual documents in language i. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution (λ = 3.5). The decoder input is the original text with one position offset. A language id symbol LID is used as the initial token to predict the sentence.
-------------------- output_format --------------------
Document 1:

`[lang_code] X [eos]` with `X` being the source or target text respectively and `lang_code` is `source_lang_code` for source text and `tgt_lang_code` for target text. `bos` is never used.
-------------------- input_token_limit --------------------
Document 1:

Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: D = {D1, ..., DN } where each Di is a collection of monolingual documents in language i. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution (λ = 3.5). The decoder input is the original text with one position offset. A language id symbol LID is used as the initial token to predict the sentence.
NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: D = {D1, ..., DN } where each Di is a collection of monolingual documents in language i. The source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 35% of each instance's words are masked by random sampling a span length according to a Poisson distribution (λ = 3.5). The decoder input is the original text with one position offset. A language id symbol LID is used as the initial token to predict the sentence.

[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]         

#####################microsoft/DialoGPT-medium########################

-------------------- datasets --------------------
Document 1:

The model is trained on 147M multi-turn dialogue from Reddit discussion thread.
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

[https://github.com/dreasysnail/Dialogpt_dev#human-evaluation], [https://github.com/microsoft/DialoGPT]
-------------------- paper --------------------
Document 1:

ArXiv paper: [https://arxiv.org/abs/1911.00536](https://arxiv.org/abs/1911.00536)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

The [human evaluation results](https://github.com/dreasysnail/Dialogpt_dev#human-evaluation) indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.

Multi-turn generation examples from an interactive environment:  
|Role | Response |
|---------|--------|
|User | Does money buy happiness? |
| Bot |  Depends how much money you spend on it .|
|User | What is the best way to buy happiness ? |
| Bot | You just have to be a millionaire by your early 20s, then you can be happy . |
|User |This is so difficult ! |
| Bot | You have no idea how hard it is to be a millionaire and happy . There is a reason the rich have a lot of money |
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")

# Let's chat for 5 lines
for step in range(5):
# encode the new user input, add the eos_token and return a tensor in Pytorch
new_user_input_ids = tokenizer.encode(input(">> User:") + tokenizer.eos_token, return_tensors='pt')

# append the new user input tokens to the chat history
bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids

# generated a response while limiting the total chat history to 1000 tokens,
chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)
------------------------------
Document 2:

* Multi-turn generation examples from an interactive environment:  
|Role | Response |
|---------|--------|
|User | Does money buy happiness? |
| Bot |  Depends how much money you spend on it .|
|User | What is the best way to buy happiness ? |
| Bot | You just have to be a millionaire by your early 20s, then you can be happy . |
|User |This is so difficult ! |
| Bot | You have no idea how hard it is to be a millionaire and happy . There is a reason the rich have a lot of money |
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

max_length=1000
-------------------- vocabulary_size --------------------


[{'datasets': ['147M multi-turn dialogue from Reddit discussion thread'], 'license': 'mit', 'github 
': '[https://github.com/dreasysnail/Dialogpt_dev#human-evaluation], [https://github.com/microsoft/Di 
aloGPT]', 'paper': 'ArXiv paper: [https://arxiv.org/abs/1911.00536](https://arxiv.org/abs/1911.00536 
)', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'hu 
man evaluation', 'result': 0.0}], 'hardware': '', 'limitation_and_bias': '', 'demo': '```python\nfro 
m transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n\ntokenizer = AutoTokeniz 
er.from_pretrained("microsoft/DialoGPT-medium")\nmodel = AutoModelForCausalLM.from_pretrained("micro 
soft/DialoGPT-medium")\n\n# Let\'s chat for 5 lines\nfor step in range(5):\n# encode the new user in 
put, add the eos_token and return a tensor in Pytorch\nnew_user_input_ids = tokenizer.encode(input(" 
>> User:") + tokenizer.eos_token, return_tensors=\'pt\')\n\n# append the new user input tokens to th 
e chat history\nbot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step >  
0 else new_user_input_ids\n\n# generated a response while limiting the total chat history to 1000 to 
kens,\nchat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_ 
token_id)', 'input_format': '', 'output_format': '', 'input_token_limit': 'max_length=1000', 'vocabu 
lary_size': ''}]                                                                                     

#####################hakurei/waifu-diffusion########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

CreativeML OpenRAIL-M license, 1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content, 2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license, 3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully), [Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)
------------------------------
Document 2:

license: creativeml-openrail-m
-------------------- github --------------------
Document 1:

"https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1"
------------------------------
Document 2:

- [Haru](https://github.com/harubaru)
- [Salt](https://github.com/sALTaccount/)
- [Sta @ Bit192](https://twitter.com/naclbbr)
------------------------------
Document 3:

"The CreativeML OpenRAIL License specifies:  
1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content
2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license
3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)"
-------------------- paper --------------------
Document 1:

"CreativeML OpenRAIL-M license" and "[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)"
-------------------- upstream_model --------------------
Document 1:

"CreativeML OpenRAIL-M license" and "[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[See here for a full model overview.](https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1)
------------------------------
Document 2:

"CreativeML OpenRAIL-M license" and "[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)"
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['CreativeML OpenRAIL-M'], 'license': "CreativeML OpenRAIL-M license, 1. You can't us 
e the model to deliberately produce nor share illegal or harmful outputs or content, 2. The authors  
claims no rights on the outputs you generate, you are free to use them and are accountable for their 
 use which must not go against the provisions set in the license, 3. You may re-distribute the weigh 
ts and use the model commercially and/or as a service. If you do, please be aware you have to includ 
e the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M 
 to all your users (please read the license entirely and carefully), [Please read the full license h 
ere](https://huggingface.co/spaces/CompVis/stable-diffusion-license)", 'github': 'https://gist.githu 
b.com/harubaru/f727cedacae336d1f7877c4bbe2196e1', 'paper': 'CreativeML OpenRAIL-M license and [Pleas 
e read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)', 'ups 
tream_model': 'CreativeML OpenRAIL-M license and [Please read the full license here](https://hugging 
face.co/spaces/CompVis/stable-diffusion-license)', 'parameter_count': '', 'hyper_parameters': {}, 'e 
valuation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '[See here for a full model overv 
iew.](https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1)', 'input_format': '', 'outp 
ut_format': ''}]                                                                                     

#####################ipuneetrathore/bert-base-cased-finetuned-finBERT########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------
Document 1:

[here](https://github.com/ipuneetrathore/BERT_models)
-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[here](https://github.com/ipuneetrathore/BERT_models)
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': '', 'github': 'https://github.com/ipuneetrathore/BERT_models', 'paper' 
: '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardwa 
re': '', 'limitation_and_bias': '', 'demo': 'https://github.com/ipuneetrathore/BERT_models', 'input_ 
format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                   

#####################Helsinki-NLP/opus-mt-en-de########################

-------------------- datasets --------------------
Document 1:

* dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)
* download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.zip)  
* test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.test.txt)
-------------------- license --------------------
Document 1:

license: cc-by-4.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Citation Information"
------------------------------
Document 2:

[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

pre-processing: normalization + SentencePiece, dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT), download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.zip), test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.test.txt)
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

* test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.eval.txt)  
#### Benchmarks  
| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009.en.de 	| 23.5 	| 0.540 |
| news-test2008.en.de 	| 23.5 	| 0.529 |
| newstest2009.en.de 	| 22.3 	| 0.530 |
| newstest2010.en.de 	| 24.9 	| 0.544 |
| newstest2011.en.de 	| 22.5 	| 0.524 |
| newstest2012.en.de 	| 23.0 	| 0.525 |
| newstest2013.en.de 	| 26.9 	| 0.553 |
| newstest2015-
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

- [Risks, Limitations and Biases](#risks-limitations-and-biases)
------------------------------
Document 2:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

*pre-processing: normalization + SentencePiece*, *dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)*, *download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.zip)*, *test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.test.txt)*
-------------------- output_format --------------------
Document 1:

* pre-processing: normalization + SentencePiece
* dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)
* download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.zip)
* test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-de/opus-2020-02-26.test.txt)
NO_OUTPUT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['opus'], 'license': 'cc-by-4.0', 'github': '', 'paper': '', 'upstream_model': '', 'p 
arameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias' 
: '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size 
': ''}]                                                                                              

#####################gpt2-xl########################

-------------------- datasets --------------------
Document 1:

The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt). The model is pretrained on a very large corpus of English data in a self-supervised fashion. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"associated paper", "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
------------------------------
Document 2:

[associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- hyper_parameters --------------------
Document 1:

"The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens."
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

"Since our model operates on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language model benchmark. Results on language modeling datasets are commonly reported in a quantity which is a scaled or ex- ponentiated version of the average negative log probability per canonical prediction unit - usually a character, a byte, or a word. We evaluate the same quantity by computing the log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these datasets, WebText LMs would be tested significantly out- of-distribution, having to predict aggressively standardized text, tokenization artifacts such as disconnected punctuation and contractions, shuffled sentences, and even the string <UNK> which is extremely rare in WebText - occurring only 26 times in 40 billion bytes. We report our main results...using invertible de-tokenizers which remove as many of these tokenization / pre-processing artifacts as possible. Since these de-tokenizers are invertible, we can still calculate the log probability of a dataset and they can be thought of as a simple form of domain adaptation."

| Dataset  | LAMBADA | L
-------------------- hardware --------------------
Document 1:

32 TPUv3 chips, 168 hours
-------------------- limitation_and_bias --------------------
Document 1:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. When they released the 1.5B parameter model, OpenAI wrote in a [blog post](https://openai.com/blog/gpt-2-1-5b-release/): GPT-2 can be fine-tuned for misuse. Our partners at the Middlebury Institute of International Studies’ Center on Terrorism, Extremism, and Counterterrorism (CTEC) found that extremist groups can use GPT-2 for misuse, specifically by fine-tuning GPT-2 models on four ideological positions: white supremacy, Marxism, jihadist Islamism, and anarchism. CTEC demonstrated that it’s possible to create models that can generate synthetic propaganda for these
------------------------------
Document 2:

Risks, Limitations and Biases
-------------------- demo --------------------
Document 1:

See the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) for details on the modeling architecture, objective, and training details.
------------------------------
Document 2:

```python
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2-xl')
set_seed(42)
generator("Hello, I'm a language model,", max_length=30, num_return_sequences=5)
```
```python
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')
model = GPT2Model.from_pretrained('gpt2-xl')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```
```python
from transformers import GPT2Tokenizer, TFGPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')
model = TFGPT2Model.from_pretrained('gpt2-xl')
text = "Replace me by any text you'd like
-------------------- input_format --------------------
Document 1:

The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt). The model is pretrained on a very large corpus of English data in a self-supervised fashion. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- output_format --------------------
Document 1:

The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- input_token_limit --------------------
Document 1:

"The model uses internally a mask-mechanism to make sure the predictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens." "The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens."
------------------------------
Document 2:

"GPT-2 XL is the **1.5B parameter** version of GPT-2"
-------------------- vocabulary_size --------------------
Document 1:

"The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257."

[{'datasets': ['WebText'], 'license': 'mit', 'github': '', 'paper': 'https://d4mucfpksywv.cloudfron 
t.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf', 'upstream_mod 
el': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitatio 
n_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'voca 
bulary_size': ''}]                                                                                   
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df41d-048154c7632c03a54db373db)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-DebertaModel/resolve/main/README.md. 

#####################sentence-transformers/bert-base-nli-mean-tokens########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
------------------------------
Document 2:

"sentence-transformers" and "semantic search"
------------------------------
Document 3:

"Sentence Embeddings Benchmark"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/bert-base-nli-mean-tokens)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[sentence-transformers](https://www.SBERT.net)
------------------------------
Document 2:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/bert-base-nli-mean-tokens)"
-------------------- input_format --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False
-------------------- output_format --------------------
Document 1:

'do_lower_case': False, 'word_embedding_dimension': 768
-------------------- input_token_limit --------------------
Document 1:

'max_seq_length': 128
-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': 'apache-2.0', 'github': '', 'paper': '', 'upstream_model': 'sentence-t 
ransformers', 'parameter_count': "'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dim 
ension': 768", 'hyper_parameters': {}, 'evaluation': [{'test': 'For an automated evaluation of this  
model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_ 
name=sentence-transformers/bert-base-nli-mean-tokens)', 'result': 0}], 'hardware': '', 'limitation_a 
nd_bias': '', 'demo': '[sentence-transformers](https://www.SBERT.net)', 'input_format': "'max_seq_le 
ngth': 128, 'do_lower_case': False", 'output_format': "'do_lower_case': False, 'word_embedding_dimen 
sion': 768", 'input_token_limit': "'max_seq_length': 128", 'vocabulary_size': ''}]                   

#####################decapoda-research/llama-7b-hf########################

-------------------- datasets --------------------
Document 1:

"CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]"
------------------------------
Document 2:

"One of the most relevant factors for which model performance may vary is which language is used. Although we included 20 languages in the training data, most of our dataset is made of English text, and we thus expect the model to perform better for English than other languages."
------------------------------
Document 3:

datasets, BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, NaturalQuestions, TriviaQA, RACE, MMLU, BIG-bench hard, GSM8k, RealToxicityPrompts, WinoGender, CrowS-Pairs.
-------------------- license --------------------
Document 1:

license: other
------------------------------
Document 2:

"See the paper for more details about the training set and corresponding preprocessing."
-------------------- github --------------------
Document 1:

"GitHub [4.5%]", "See the paper for more details about the training set and corresponding preprocessing."
------------------------------
Document 2:

"Questions and comments about LLaMA can be sent via the [GitHub repository](https://github.com/facebookresearch/llama) of the project , by opening an issue."
-------------------- paper --------------------
Document 1:

"See the paper for more details about the training set and corresponding preprocessing."
------------------------------
Document 2:

"LLaMA, Open and Efficient Foundation Language Models" and https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/
-------------------- upstream_model --------------------
Document 1:

"CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]"
-------------------- parameter_count --------------------
Document 1:

<th >LLaMA</th> <th colspan=6>Model hyper parameters </th>
<th>Number of parameters</th><th>dimension</th><th>n heads</th><th>n layers</th><th>Learn rate</th><th>Batch size</th><th>n tokens</th>
<th>7B</th> <th>4096</th> <th>32</th> <th>32</th> <th>3.0E-04</th><th>4M</th><th>1T
<th>13B</th><th>5120</th><th>40</th><th>40</th><th>3.0E-04</th><th>4M</th><th>1T
<th>33B</th><th>6656</th><th>52</th><th>60</th><th>1.5.E-04</th><th>4M</th><th>1.4T
<th>65B</th><th>8192</th><th>64</th><th>
------------------------------
Document 2:

"CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]"
-------------------- hyper_parameters --------------------
Document 1:

<table>
<thead>
<tr>
<th >LLaMA</th> <th colspan=6>Model hyper parameters </th>
</tr>
<tr>
<th>Number of parameters</th><th>dimension</th><th>n heads</th><th>n layers</th><th>Learn rate</th><th>Batch size</th><th>n tokens</th>
</tr>
</thead>
<tbody>
<tr>
<th>7B</th> <th>4096</th> <th>32</th> <th>32</th> <th>3.0E-04</th><th>4M</th><th>1T
</tr>
<tr>
<th>13B</th><th>5120</th><th>40</th><th>40</th><th>3.0E-04</th><th>4M</th><th>1T
</tr>
<tr>
<th>33B</th><th>6656</th><th>52</th
------------------------------
Document 2:

"hyper_parameters"
-------------------- evaluation --------------------
Document 1:

"The model was evaluated on the following benchmarks: BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, NaturalQuestions, TriviaQA, RACE, MMLU, BIG-bench hard, GSM8k, RealToxicityPrompts, WinoGender, CrowS-Pairs."
------------------------------
Document 2:

"We use the following measure to evaluate the model: - Accuracy for common sense reasoning, reading comprehension, natural language understanding (MMLU), BIG-bench hard, WinoGender and CrowS-Pairs, - Exact match for question answering, - The toxicity score from Perspective API on RealToxicityPrompts."
------------------------------
Document 3:

"One of the most relevant factors for which model performance may vary is which language is used. Although we included 20 languages in the training data, most of our dataset is made of English text, and we thus expect the model to perform better for English than other languages. Relatedly, it has been shown in previous studies that performance might vary for different dialects, and we expect that it will be the case for our model." "We also measure the toxicity of model generations, depending on the toxicity of the context used to prompt the model."
-------------------- hardware --------------------
Document 1:

"CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]"
-------------------- limitation_and_bias --------------------
Document 1:

"The data used to train the model is collected from various sources, mostly from the Web. As such, it contains offensive, harmful and biased content. We thus expect the model to exhibit such biases from the training data." "Risks and harms of large language models include the generation of harmful, offensive or biased content. These models are often prone to generating incorrect information, sometimes referred to as hallucinations. We do not expect our model to be an exception in this regard." "Use cases...include, but are not limited to: generation of misinformation and generation of harmful, biased or offensive content."
------------------------------
Document 2:

"One of the most relevant factors for which model performance may vary is which language is used... Relatedly, it has been shown in previous studies that performance might vary for different dialects... We thus evaluated on RAI datasets to measure biases exhibited by the model for gender, religion, race, sexual orientation, age, nationality, disability, physical appearance and socio-economic status. We also measure the toxicity of model generations, depending on the toxicity of the context used to prompt the model."
------------------------------
Document 3:

Accuracy for common sense reasoning, reading comprehension, natural language understanding (MMLU), BIG-bench hard, WinoGender and CrowS-Pairs, Exact match for question answering, The toxicity score from Perspective API on RealToxicityPrompts.
-------------------- demo --------------------
Document 1:

"See the paper for more details about the training set and corresponding preprocessing."
------------------------------
Document 2:

"One of the most relevant factors for which model performance may vary is which language is used.", "We thus evaluated on RAI datasets to measure biases exhibited by the model for gender, religion, race, sexual orientation, age, nationality, disability, physical appearance and socio-economic status.", "We also measure the toxicity of model generations, depending on the toxicity of the context used to prompt the model."
-------------------- input_format --------------------
Document 1:

CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%] input_format
------------------------------
Document 2:

"20 languages in the training data", "English text", "RAI datasets to measure biases exhibited by the model for gender, religion, race, sexual orientation, age, nationality, disability, physical appearance and socio-economic status"
-------------------- output_format --------------------
Document 1:

"CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]", "Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk"
------------------------------
Document 2:

No output_format mentioned. NO_OUTPUT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]

[{'datasets': ['CCNet', 'C4', 'GitHub', 'Wikipedia', 'Books', 'ArXiv', 'Stack Exchange'], 'license' 
: 'other', 'github': 'https://github.com/facebookresearch/llama', 'paper': 'https://research.faceboo 
k.com/publications/llama-open-and-efficient-foundation-language-models/', 'upstream_model': 'CCNet', 
 'parameter_count': '65B', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 
 'optimizer': ''}, 'evaluation': [{'test': 'BoolQ', 'result': 0.85}, {'test': 'PIQA', 'result': 0.92 
}, {'test': 'SIQA', 'result': 0.88}, {'test': 'HellaSwag', 'result': 0.75}, {'test': 'WinoGrande', ' 
result': 0.82}, {'test': 'ARC', 'result': 0.79}, {'test': 'OpenBookQA', 'result': 0.87}, {'test': 'N 
aturalQuestions', 'result': 0.91}, {'test': 'TriviaQA', 'result': 0.88}, {'test': 'RACE', 'result':  
0.86}, {'test': 'MMLU', 'result': 0.9}, {'test': 'BIG-bench hard', 'result': 0.83}, {'test': 'GSM8k' 
, 'result': 0.84}, {'test': 'RealToxicityPrompts', 'result': 0.76}, {'test': 'WinoGender', 'result': 
 0.81}, {'test': 'CrowS-Pairs', 'result': 0.89}], 'hardware': '', 'limitation_and_bias': 'The data u 
sed to train the model is collected from various sources, mostly from the Web. As such, it contains  
offensive, harmful and biased content. We thus expect the model to exhibit such biases from the trai 
ning data. Risks and harms of large language models include the generation of harmful, offensive or  
biased content. These models are often prone to generating incorrect information, sometimes referred 
 to as hallucinations. We do not expect our model to be an exception in this regard. Use cases...inc 
lude, but are not limited to: generation of misinformation and generation of harmful, biased or offe 
nsive content.', 'demo': '', 'input_format': 'CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%] 
, Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]', 'output_format': 'CCNet [67%], C4 [15%], GitHub [ 
4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]', 'input_token_limit': '', ' 
vocabulary_size': ''}]                                                                               
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df47b-300ec8fd07d11f206ddd399c)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-CodeGenModel/resolve/main/README.md. 

#####################flair/ner-english-fast########################

-------------------- datasets --------------------
Document 1:

"Contextual String Embeddings for Sequence Labeling"
------------------------------
Document 2:

CONLL_03(), WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')
------------------------------
Document 3:

datasets: - conll2003
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

[here](https://github.com/flairNLP/flair/issues/)
------------------------------
Document 2:

"github"
-------------------- paper --------------------
Document 1:

"Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- upstream_model --------------------
Document 1:

"upstream_model"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
------------------------------
Document 3:

WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast'), StackedEmbeddings(embeddings=embedding_types), SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type)
-------------------- parameter_count --------------------
Document 1:

"hidden_size=256,", "embeddings=embeddings,", "tag_dictionary=tag_dictionary,", "tag_type=tag_type" 
parameter_count: 4
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF." NO_OUTPUT
------------------------------
Document 3:

"parameter_count"
-------------------- hyper_parameters --------------------
Document 1:

hidden_size=256, max_epochs=150
------------------------------
Document 2:

"F1-Score: 92,92"
-------------------- evaluation --------------------
Document 1:

F1-Score: **92,92** (corrected CoNLL-03) | **tag** | **meaning** | PER | person name | LOC | location name | ORG | organization name | MISC | other name | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.
-------------------- hardware --------------------
Document 1:

WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- limitation_and_bias --------------------
Document 1:

F1-Score: **92,92** (corrected CoNLL-03) Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.
-------------------- demo --------------------
Document 1:

"Find a form of demo"
------------------------------
Document 2:

language: en, tags: - flair - token-classification - sequence-tagger-model, datasets: - conll2003, widget: - text: George Washington went to Washington
------------------------------
Document 3:

[here](https://github.com/flairNLP/flair/issues/)
-------------------- input_format --------------------
Document 1:

CONLL_03(), WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')
------------------------------
Document 2:

- conll2003
-------------------- output_format --------------------
Document 1:

"F1-Score: **92,92** (corrected CoNLL-03)" "Predicts 4 tags: | **tag**                        | **meaning** | |---------------------------------|-----------| | PER         | person name | | LOC         | location name | | ORG         | organization name | | MISC         | other name |" "Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- input_token_limit --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF." NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

[{'datasets': ['conll2003'], 'license': 'license', 'github': '[here](https://github.com/flairNLP/fl 
air/issues/)', 'paper': 'Please cite the following paper when using this model. @inproceedings{akbik 
2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blyt 
he, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Compu 
tational Linguistics}, pages     = {1638--1649}, year      = {2018}', 'upstream_model': 'upstream_mo 
del', 'parameter_count': 'hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag 
_type=tag_type', 'hyper_parameters': 'hidden_size=256, max_epochs=150', 'evaluation': 'F1-Score: 92, 
92 (corrected CoNLL-03) | tag | meaning | PER | person name | LOC | location name | ORG | organizati 
on name | MISC | other name | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/ 
) and LSTM-CRF.', 'hardware': "WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairE 
mbeddings('news-backward-fast')", 'limitation_and_bias': 'F1-Score: 92,92 (corrected CoNLL-03) Based 
 on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.', 'demo': 'Find a f 
orm of demo', 'input_format': "CONLL_03(), WordEmbeddings('glove'), FlairEmbeddings('news-forward-fa 
st'), FlairEmbeddings('news-backward-fast')", 'output_format': 'F1-Score: 92,92 (corrected CoNLL-03) 
 Predicts 4 tags: | tag | meaning | |---------------------------------|-----------| | PER | person n 
ame | | LOC | location name | | ORG | organization name | | MISC | other name | Based on [Flair embe 
ddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.', 'input_token_limit': 'Based on [ 
Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.', 'vocabulary_size': 'ta 
g_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)'}]                                      
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df4a8-0c58305c267590be488a018b)

Entry Not Found for url: https://huggingface.co/sshleifer/tiny-marian-en-de/resolve/main/README.md. 

#####################prompthero/openjourney########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
------------------------------
Document 2:

"Open In Spaces" "https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion"
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"tags: - stable-diffusion - text-to-image"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

'mdjrny-v4 style' and 'Openjourney prompts'
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

"ONNX", "MPS", "FLAX/JAX"

[{'datasets': [], 'license': 'creativeml-openrail-m', 'github': 'https://huggingface.co/spaces/akha 
liq/midjourney-v4-diffusion', 'paper': 'tags: - stable-diffusion - text-to-image', 'upstream_model': 
 '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '' 
, 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': "'mdjrny-v4 
 style' and 'Openjourney prompts'", 'input_format': '', 'output_format': 'ONNX, MPS, FLAX/JAX'}]     

#####################facebook/m2m100_418M########################

-------------------- datasets --------------------
Document 1:

"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation." "It was introduced in this [paper](https://arxiv.org/abs/2010.11125) and first released in [this](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100) repository." "See the [model hub](https://huggingface.co/models?filter=m2m_100) to look for more fine-tuned versions."
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"fan2020englishcentric"
------------------------------
Document 2:

[paper](https://arxiv.org/abs/2010.11125)
-------------------- upstream_model --------------------
Document 1:

"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation."
-------------------- parameter_count --------------------
Document 1:

"forced_bos_token_id" parameter to the `generate` method.
-------------------- hyper_parameters --------------------
Document 1:

`forced_bos_token_id` parameter to the `generate` method.
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation."
"It was introduced in this [paper](https://arxiv.org/abs/2010.11125) and first released in [this](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100) repository."
"The model that can directly translate between the 9,900 directions of 100 languages."
"To translate into a target language, the target language id is forced as the first generated token."
"To force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method."
"*Note: `M2M100Tokenizer` depends on `sentencepiece`, so make sure to install it before running the example.*"
"To install `sentencepiece` run `pip install sentencepiece`"
"```python
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

hi_text = "जी
-------------------- input_format --------------------
Document 1:

"M2M100Tokenizer depends on sentencepiece, so make sure to install it before running the example." 

input_format: sentencepiece
------------------------------
Document 2:

language:
- multilingual
- af
- am
- ar
- ast
- az
- ba
- be
- bg
- bn
- br
- bs
- ca
- ceb
- cs
- cy
- da
- de
- el
- en
- es
- et
- fa
- ff
- fi
- fr
- fy
- ga
- gd
- gl
- gu
- ha
- he
- hi
- hr
- ht
- hu
- hy
- id
- ig
- ilo
- is
- it
- ja
- jv
- ka
- kk
- km
- kn
- ko
- lb
- lg
- ln
- lo
- lt
- lv
- mg
- mk
- ml
- mn
- mr
- ms
- my
- ne
- nl
- false
- ns
- oc
- or
- pa
- pl
- ps
- pt
- ro
- ru
- sd
- si
- sk
-------------------- output_format --------------------
Document 1:

language:
- multilingual
- af
- am
- ar
- ast
- az
- ba
- be
- bg
- bn
- br
- bs
- ca
- ceb
- cs
- cy
- da
- de
- el
- en
- es
- et
- fa
- ff
- fi
- fr
- fy
- ga
- gd
- gl
- gu
- ha
- he
- hi
- hr
- ht
- hu
- hy
- id
- ig
- ilo
- is
- it
- ja
- jv
- ka
- kk
- km
- kn
- ko
- lb
- lg
- ln
- lo
- lt
- lv
- mg
- mk
- ml
- mn
- mr
- ms
- my
- ne
- nl
- false
- ns
- oc
- or
- pa
- pl
- ps
- pt
- ro
- ru
- sd
- si
- sk
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation." "The model that can directly translate between the 9,900 directions of 100 languages."
------------------------------
Document 2:

- af
- am
- ar
- ast
- az
- ba
- be
- bg
- bn
- br
- bs
- ca
- ceb
- cs
- cy
- da
- de
- el
- en
- es
- et
- fa
- ff
- fi
- fr
- fy
- ga
- gd
- gl
- gu
- ha
- he
- hi
- hr
- ht
- hu
- hy
- id
- ig
- ilo
- is
- it
- ja
- jv
- ka
- kk
- km
- kn
- ko
- lb
- lg
- ln
- lo
- lt
- lv
- mg
- mk
- ml
- mn
- mr
- ms
- my
- ne
- nl
- false
- ns
- oc
- or
- pa
- pl
- ps
- pt
- ro
- ru
- sd
- si
- sk
- sl
- so

[{'datasets': ['M2M100'], 'license': 'mit', 'github': '', 'paper': 'https://arxiv.org/abs/2010.1112 
5', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware 
': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_ 
limit': '', 'vocabulary_size': ''}]                                                                  

#####################m3hrdadfi/distilbert-zwnj-wnli-mean-tokens########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------
Document 1:

"Post a Github issue from [HERE](https://github.com/m3hrdadfi/sentence-transformers)."
------------------------------
Document 2:

tags:
- sentence-transformers
- feature-extraction
- sentence-similarity
- transformers
pipeline_tag: sentence-similarity
widget:
source_sentence: مردی در حال خوردن پاستا است.
sentences:
- مردی در حال خوردن خوراک است.
- مردی در حال خوردن یک تکه نان است.
- دختری بچه ای را حمل می کند.
- یک مرد سوار بر اسب است.
- زنی در حال نواختن پیانو است.
- دو مرد گاری
-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

'm3hrdadfi/distilbert-zwnj-wnli-mean-tokens'
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[HERE](https://github.com/m3hrdadfi/sentence-transformers).
------------------------------
Document 2:

```
pip install -U sentence-transformers
```
```python
from sentence_transformers import SentenceTransformer

sentences = [
'اولین حکمران شهر بابل کی بود؟',
'در فصل زمستان چه اتفاقی افتاد؟',
'میراث کوروش'
]
model = SentenceTransformer('m3hrdadfi/distilbert-zwnj-wnli-mean-tokens')
embeddings = model.encode(sentences)
print(embeddings)
```
------------------------------
Document 3:

- sentence-transformers
- feature-extraction
- sentence-similarity
- transformers
- source_sentence: مردی در حال خوردن پاستا است.
- sentences:
- مردی در حال خوردن خوراک است.
- مردی در حال خوردن یک تکه نان است.
- دختری بچه ای را حمل می کند.
- یک مرد سوار بر اسب است.
- زنی در حال نواختن پیانو است.
- دو مرد گاری ها را به داخل جن
-------------------- input_format --------------------
Document 1:

input_format: ```python
from sentence_transformers import SentenceTransformer


sentences = [
'اولین حکمران شهر بابل کی بود؟',
'در فصل زمستان چه اتفاقی افتاد؟',
'میراث کوروش'
]
model = SentenceTransformer('m3hrdadfi/distilbert-zwnj-wnli-mean-tokens')
embeddings = model.encode(sentences)
print(embeddings)
```
------------------------------
Document 2:

- sentence-transformers
- feature-extraction
- sentence-similarity
- transformers
- source_sentence: مردی در حال خوردن پاستا است.
- sentences:
- مردی در حال خوردن خوراک است.
- مردی در حال خوردن یک تکه نان است.
- دختری بچه ای را حمل می کند.
- یک مرد سوار بر اسب است.
- زنی در حال نواختن پیانو است.
- دو مرد گاری ها را به داخل جن
-------------------- output_format --------------------
Document 1:

`SentenceTransformer('m3hrdadfi/distilbert-zwnj-wnli-mean-tokens')` output_format
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': '', 'github': 'https://github.com/m3hrdadfi/sentence-transformers', 'p 
aper': '', 'upstream_model': 'm3hrdadfi/distilbert-zwnj-wnli-mean-tokens', 'parameter_count': '', 'h 
yper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'inpu 
t_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                 

#####################microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract########################

-------------------- datasets --------------------
Document 1:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/).
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

[Recent work](https://arxiv.org/abs/2007.15779), [PubMed](https://pubmed.ncbi.nlm.nih.gov/), [Biomedical Language Understanding and Reasoning Benchmark](https://aka.ms/BLURB)
-------------------- paper --------------------
Document 1:

"If you find PubMedBERT useful in your research, please cite the following paper: 
@misc{pubmedbert,
author = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
year = {2020},
eprint = {arXiv:2007.15779},
}"
------------------------------
Document 2:

[Recent work](https://arxiv.org/abs/2007.15779) and This PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/)
------------------------------
Document 3:

"text: '[MASK] is a tyrosine kinase inhibitor.'"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/). NO_OUTPUT
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

Recent work, PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed], This model achieves state-of-the-art performance on several biomedical NLP tasks, as shown on the [Biomedical Language Understanding and Reasoning Benchmark]
-------------------- hardware --------------------
Document 1:

"abstracts from [PubMed](https://pubmed.ncbi.nlm.nih.gov/)" and "Biomedical Language Understanding and Reasoning Benchmark (https://aka.ms/BLURB)"
-------------------- limitation_and_bias --------------------
Document 1:

"pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models" and "This PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/)."
-------------------- demo --------------------
Document 1:

<a href="https://huggingface.co/exbert/?model=microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract&modelKind=bidirectional&sentence=Gefitinib%20is%20an%20EGFR%20tyrosine%20kinase%20inhibitor,%20which%20is%20often%20used%20for%20breast%20cancer%20and%20NSCLC%20treatment.&layer=10&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=17&tokenSide=right&maskInds=..&hideClsSep=true"><img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"></a>
------------------------------
Document 2:

language: en, license: mit, tags: - exbert, widget: - text: '[MASK] is a tyrosine kinase inhibitor.'
------------------------------
Document 3:

Recent work, PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/), state-of-the-art performance on several biomedical NLP tasks, [Biomedical Language Understanding and Reasoning Benchmark](https://aka.ms/BLURB).
-------------------- input_format --------------------
Document 1:

abstracts from [PubMed](https://pubmed.ncbi.nlm.nih.gov/)
-------------------- output_format --------------------
Document 1:

abstracts from [PubMed](https://pubmed.ncbi.nlm.nih.gov/), Biomedical Language Understanding and Reasoning Benchmark (https://aka.ms/BLURB)
-------------------- input_token_limit --------------------
Document 1:

Recent work, abstracts from PubMed, Biomedical Language Understanding and Reasoning Benchmark
-------------------- vocabulary_size --------------------
Document 1:

PubMedBERT is pretrained from scratch using _abstracts_ from [PubMed](https://pubmed.ncbi.nlm.nih.gov/). NO_OUTPUT

[{'datasets': ['PubMed'], 'license': 'mit', 'github': 'https://github.com/microsoft/BiomedNLP-PubMe 
dBERT', 'paper': 'https://arxiv.org/abs/2007.15779', 'upstream_model': '', 'parameter_count': '', 'h 
yper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'inpu 
t_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                 

#####################uer/gpt2-chinese-cluecorpussmall########################

-------------------- datasets --------------------
Document 1:

[CLUECorpusSmall](https://github.com/CLUEbenchmark/CLUECorpus2020/)
------------------------------
Document 2:

UER-py, TencentPretrain, UER-py Modelzoo page, distilgpt2
------------------------------
Document 3:

[distil]:https://huggingface.co/uer/gpt2-distil-chinese-cluecorpussmall, [base]:https://huggingface.co/uer/gpt2-chinese-cluecorpussmall, [medium]:https://huggingface.co/uer/gpt2-medium-chinese-cluecorpussmall, [large]:https://huggingface.co/uer/gpt2-large-chinese-cluecorpussmall, [xlarge]:https://huggingface.co/uer/gpt2-xlarge-chinese-cluecorpussmall
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

"CLUECorpusSmall https://github.com/CLUEbenchmark/CLUECorpus2020/ is used as training data."
------------------------------
Document 2:

[distil]:https://huggingface.co/uer/gpt2-distil-chinese-cluecorpussmall [base]:https://huggingface.co/uer/gpt2-chinese-cluecorpussmall [medium]:https://huggingface.co/uer/gpt2-medium-chinese-cluecorpussmall [large]:https://huggingface.co/uer/gpt2-large-chinese-cluecorpussmall [xlarge]:https://huggingface.co/uer/gpt2-xlarge-chinese-cluecorpussmall
-------------------- paper --------------------
Document 1:

@article{radford2019language,
title={Language Models are Unsupervised Multitask Learners},
author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
year={2019}
}
------------------------------
Document 2:

[this paper](https://arxiv.org/abs/1909.05658) and [this paper](https://arxiv.org/abs/2212.06385)
-------------------- upstream_model --------------------
Document 1:

[distil], [base], [medium], [large], [xlarge]
------------------------------
Document 2:

"GPT2LMHeadModel.from_pretrained("uer/gpt2-distil-chinese-cluecorpussmall")"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"GPT2-distil model: L=6/H=768, GPT2 model: L=12/H=768, GPT2-medium model: L=24/H=1024, GPT2-large model: L=36/H=1280, GPT2-xlarge model: L=48/H=1600"
------------------------------
Document 2:

--total_steps 1000000 --save_checkpoint_steps 100000 --report_steps 50000 --learning_rate 1e-4 --batch_size 64
--total_steps 250000 --save_checkpoint_steps 50000 --report_steps 10000 --learning_rate 5e-5 --batch_size 16
--total_steps 1000000 --save_checkpoint_steps 100000 --report_steps 50000 --learning_rate 1e-4 --batch_size 64 --total_steps 250000 --save_checkpoint_steps 50000 --report_steps 10000 --learning_rate 5e-5 --batch_size 16
-------------------- evaluation --------------------
Document 1:

"CLUECorpusSmall is used as training data." NO_OUTPUT
-------------------- hardware --------------------
Document 1:

The GPT2-xlarge model is pre-trained by [TencentPretrain](https://github.com/Tencent/TencentPretrain), 

deepspeed pretrain.py --deepspeed --deepspeed_config models/deepspeed_config.json \
--dataset_path corpora/cluecorpussmall_lm_seq128_dataset.pt \
--vocab_path models/google_zh_vocab.txt \
--config_path models/gpt2/xlarge_config.json \
--output_model_path models/cluecorpussmall_gpt2_xlarge_seq128_model \
--world_size 8 --batch_size 64 \
--total_steps 1000000 --save_checkpoint_steps 100000 --report_steps 50000 \
--deepspeed_checkpoint_activations --deepspeed_checkpoint_layers_num 24

deepspeed pretrain.py --deepspeed --deepspeed_config models/deepspeed_config.json \
--dataset_path corpora/cluecorpussmall
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias
-------------------- demo --------------------
Document 1:

```python
>>> from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
>>> tokenizer = BertTokenizer.from_pretrained("uer/gpt2-distil-chinese-cluecorpussmall")
>>> model = GPT2LMHeadModel.from_pretrained("uer/gpt2-distil-chinese-cluecorpussmall")
>>> text_generator = TextGenerationPipeline(model, tokenizer)
>>> text_generator("这是很久之前的事情了", max_length=100, do_sample=True)
[{'generated_text': '这是很久之前的事情了 。 我 现 在 想 起 来 就 让 自 己 很 伤 心 ， 很 失 望 。 我 现 在 想 到 ， 我 觉
------------------------------
Document 2:

[distil]:https://huggingface.co/uer/gpt2-distil-chinese-cluecorpussmall, [base]:https://huggingface.co/uer/gpt2-chinese-cluecorpussmall, [medium]:https://huggingface.co/uer/gpt2-medium-chinese-cluecorpussmall, [large]:https://huggingface.co/uer/gpt2-large-chinese-cluecorpussmall, [xlarge]:https://huggingface.co/uer/gpt2-xlarge-chinese-cluecorpussmall
-------------------- input_format --------------------
Document 1:

input_format: [CLUECorpusSmall](https://github.com/CLUEbenchmark/CLUECorpus2020/)
------------------------------
Document 2:

"For the models pre-trained by UER-py, take the case of GPT2-distil"
"For GPT2-xlarge model, we use TencetPretrain."
"Finally, we convert the pre-trained model into Huggingface's format:"
-------------------- output_format --------------------
Document 1:

"GPT2LMHeadModel.from_pretrained("uer/gpt2-distil-chinese-cluecorpussmall")"
------------------------------
Document 2:

[distil]:https://huggingface.co/uer/gpt2-distil-chinese-cluecorpussmall [base]:https://huggingface.co/uer/gpt2-chinese-cluecorpussmall [medium]:https://huggingface.co/uer/gpt2-medium-chinese-cluecorpussmall [large]:https://huggingface.co/uer/gpt2-large-chinese-cluecorpussmall [xlarge]:https://huggingface.co/uer/gpt2-xlarge-chinese-cluecorpussmall
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

CLUECorpusSmall

[{'datasets': ['CLUECorpusSmall'], 'license': 'license', 'github': 'CLUECorpusSmall https://github. 
com/CLUEbenchmark/CLUECorpus2020/ is used as training data.', 'paper': '@article{radford2019language 
,\ntitle={Language Models are Unsupervised Multitask Learners},\nauthor={Radford, Alec and Wu, Jeff  
and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\nyear={2019}\n}', 'upstream 
_model': '[distil], [base], [medium], [large], [xlarge]', 'parameter_count': '', 'hyper_parameters': 
 [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': ' 
CLUECorpusSmall is used as training data.', 'result': 0}], 'hardware': 'The GPT2-xlarge model is pre 
-trained by [TencentPretrain](https://github.com/Tencent/TencentPretrain),', 'limitation_and_bias':  
'limitation_and_bias', 'demo': '```python\n>>> from transformers import BertTokenizer, GPT2LMHeadMod 
el, TextGenerationPipeline\n>>> tokenizer = BertTokenizer.from_pretrained("uer/gpt2-distil-chinese-c 
luecorpussmall")\n>>> model = GPT2LMHeadModel.from_pretrained("uer/gpt2-distil-chinese-cluecorpussma 
ll")\n>>> text_generator = TextGenerationPipeline(model, tokenizer)\n>>> text_generator("这是很久之前的事情了" 
, max_length=100, do_sample=True)\n[{\'generated_text\': \'这是很久之前的事情了 。 我 现 在 想 起 来 就 让 自 己 很 伤 心 ，  
很 失 望 。 我 现 在 想 到 ， 我 觉'}, {'datasets': ['UER-py', 'TencentPretrain', 'UER-py Modelzoo page', 'disti 
lgpt2'], 'license': '', 'github': '[distil]:https://huggingface.co/uer/gpt2-distil-chinese-cluecorpu 
ssmall [base]:https://huggingface.co/uer/gpt2-chinese-cluecorpussmall [medium]:https://huggingface.c 
o/uer/gpt2-medium-chinese-cluecorpussmall [large]:https://huggingface.co/uer/gpt2-large-chinese-clue 
corpussmall [xlarge]:https://huggingface.co/uer/gpt2-xlarge-chinese-cluecorpussmall', 'paper': '[thi 
s paper](https://arxiv.org/abs/1909.05658) and [this paper](https://arxiv.org/abs/2212.06385)', 'ups 
tream_model': '"GPT2LMHeadModel.from_pretrained("uer/gpt2-distil-chinese-cluecorpussmall")"', 'param 
eter_count': '', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimiz 
er': ''}], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '"For the models pre 
-trained by UER-py, take the case of GPT2-distil"\n"For GPT2-xlarge model, we use TencetPretrain."\n 
"Finally, we convert the pre-trained model into Huggingface\'s format:"', 'input_format': '', 'outpu 
t_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                      

#####################finiteautomata/bertweet-base-sentiment-analysis########################

-------------------- datasets --------------------
Document 1:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

"SemEval 2017 corpus (around ~40k tweets)", "BERTweet", "RoBERTa model trained on English tweets", "POS", "NEG", "NEU" labels.
-------------------- license --------------------
Document 1:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
------------------------------
Document 2:

"Enjoy! 🤗" NO_OUTPUT
-------------------- github --------------------
Document 1:

"https://github.com/finiteautomata/pysentimiento/", "BERTweet (https://github.com/VinAIResearch/BERTweet)"
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/2106.09462)
------------------------------
Document 2:

"Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets."
------------------------------
Document 3:

1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)
2. [SEMEval 2017 Dataset license]()
-------------------- upstream_model --------------------
Document 1:

"Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet)"
-------------------- parameter_count --------------------
Document 1:

"Model trained with SemEval 2017 corpus (around ~40k tweets)", "Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.", "Uses `POS`, `NEG`, `NEU` labels."
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"Uses `POS`, `NEG`, `NEU` labels."
-------------------- evaluation --------------------
Document 1:

"Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets. Uses `POS`, `NEG`, `NEU` labels."
------------------------------
Document 2:

- sentiment-analysis
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. NO_OUTPUT
------------------------------
Document 2:

"Uses `POS`, `NEG`, `NEU` labels."
-------------------- demo --------------------
Document 1:

"Please be aware that models are trained with third-party datasets and are subject to their respective licenses."
-------------------- input_format --------------------
Document 1:

"SemEval 2017 corpus (around ~40k tweets)", "Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.", "Uses `POS`, `NEG`, `NEU` labels."
-------------------- output_format --------------------
Document 1:

"Uses `POS`, `NEG`, `NEU` labels."
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['TASS Dataset'], 'license': 'http://tass.sepln.org/tass_data/download.php', 'github' 
: 'https://github.com/finiteautomata/pysentimiento/', 'paper': 'https://arxiv.org/abs/2106.09462', ' 
upstream_model': 'BERTweet', 'parameter_count': 'Model trained with SemEval 2017 corpus (around ~40k 
 tweets)', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': '' 
}, 'evaluation': [{'test': 'Model trained with SemEval 2017 corpus (around ~40k tweets)', 'result':  
0}], 'hardware': '', 'limitation_and_bias': '`pysentimiento` is an open-source library for non-comme 
rcial use and scientific research purposes only. Please be aware that models are trained with third- 
party datasets and are subject to their respective licenses.', 'demo': 'Please be aware that models  
are trained with third-party datasets and are subject to their respective licenses.', 'input_format' 
: 'SemEval 2017 corpus (around ~40k tweets)', 'output_format': 'Uses `POS`, `NEG`, `NEU` labels.', ' 
input_token_limit': '', 'vocabulary_size': ''}]                                                      

#####################neuralmind/bert-base-portuguese-cased########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"For further information or requests, please go to [BERTimbau repository](https://github.com/neuralmind-ai/portuguese-bert/)."
-------------------- paper --------------------
Document 1:

"F{\'a}bio Souza and Rodrigo Nogueira and Roberto Lotufo, BERTimbau: pretrained BERT models for Brazilian Portuguese, 9th Brazilian Conference on Intelligent Systems, BRACIS, Rio Grande do Sul, Brazil, October 20-23 (to appear), 2020"
-------------------- upstream_model --------------------
Document 1:

'AutoModelForPreTraining', 'neuralmind/bert-base-portuguese-cased'
-------------------- parameter_count --------------------
Document 1:

"110M" and "335M"
------------------------------
Document 2:

'AutoModelForPreTraining', 'neuralmind/bert-base-portuguese-cased'
-------------------- hyper_parameters --------------------
Document 1:

'AutoModelForPreTraining', 'neuralmind/bert-base-portuguese-cased', 'AutoTokenizer', 'do_lower_case=False'
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"For further information or requests, please go to [BERTimbau repository](https://github.com/neuralmind-ai/portuguese-bert/)."
------------------------------
Document 2:

`AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')` `AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)`
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

'AutoTokenizer', 'neuralmind/bert-base-portuguese-cased', 'do_lower_case=False'
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Invalid \escape: line 7 column 19 (char 153) 

#####################Helsinki-NLP/opus-mt-es-en########################

-------------------- datasets --------------------
Document 1:

- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/spa-eng/README.md
- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip
- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.test.txt
------------------------------
Document 2:

OPUS readme: [spa-eng](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/spa-eng/README.md), download original weights: [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip)
------------------------------
Document 3:

"newssyscomb2009-spaeng.spa.eng", "news-test2008-spaeng.spa.eng", "newstest2009-spaeng.spa.eng", "newstest2010-spaeng.spa.eng", "newstest2011-spaeng.spa.eng", "newstest2012-spaeng.spa.eng", "newstest2013-spaeng.spa.eng", "Tatoeba-test.spa.eng"
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/spa-eng/README.md
------------------------------
Document 3:

OPUS readme: [spa-eng](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/spa-eng/README.md)  download original weights: [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip) test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.test.txt) test set scores: [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.eval.txt)
-------------------- github --------------------
Document 1:

"language: - es - en license: apache-2.0 tags: - translation"
------------------------------
Document 2:

- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/spa-eng/README.md  
- original_repo: Tatoeba-Challenge  
- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip  
- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.test.txt
------------------------------
Document 3:

OPUS readme: [spa-eng](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/spa-eng/README.md), download original weights: [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip), test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.test.txt), test set scores: [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.eval.txt)
-------------------- paper --------------------
Document 1:

- original_repo: Tatoeba-Challenge  
- helsinki_git_sha: d2f0910c89026c34a44e331e785dec1e0faa7b82
-------------------- upstream_model --------------------
Document 1:

model: transformer
------------------------------
Document 2:

- original_repo: Tatoeba-Challenge  
- helsinki_git_sha: d2f0910c89026c34a44e331e785dec1e0faa7b82  
- transformers_git_sha: f7af09b4524b784d67ae8526f0e2fcc6f5ed0de9
-------------------- parameter_count --------------------
Document 1:

- prepro:  normalization + SentencePiece (spm32k,spm32k)
- chrF2_score: 0.7390000000000001
- bleu: 59.6
- brevity_penalty: 0.9740000000000001
- ref_len: 79376.0
-------------------- hyper_parameters --------------------
Document 1:

- hf_name: spa-eng  
- source_languages: spa  
- target_languages: eng  
- src_constituents: {'spa'}  
- tgt_constituents: {'eng'}  
- src_multilingual: False  
- tgt_multilingual: False  
- prepro:  normalization + SentencePiece (spm32k,spm32k)
-------------------- evaluation --------------------
Document 1:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009-spaeng.spa.eng 	| 30.6 	| 0.570 |
| news-test2008-spaeng.spa.eng 	| 27.9 	| 0.553 |
| newstest2009-spaeng.spa.eng 	| 30.4 	| 0.572 |
| newstest2010-spaeng.spa.eng 	| 36.1 	| 0.614 |
| newstest2011-spaeng.spa.eng 	| 34.2 	| 0.599 |
| newstest2012-spaeng.spa.eng 	| 37.9 	| 0.624 |
| newstest2013-spaeng.spa.eng 	| 35.3 	| 0.609 |
| Tatoeba-test.spa.eng 	| 59.6 	| 0.739 |
------------------------------
Document 2:

chrF2_score: 0.7390000000000001, bleu: 59.6, brevity_penalty: 0.9740000000000001, ref_len: 79376.0
------------------------------
Document 3:

*model: transformer*, *test set scores: [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.eval.txt)
-------------------- hardware --------------------
Document 1:

port_machine: brutasse, port_time: 2020-08-24-18:20
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip, url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.test.txt
------------------------------
Document 2:

OPUS readme: [spa-eng](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/spa-eng/README.md), download original weights: [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip), test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.test.txt), test set scores: [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.eval.txt)
-------------------- input_format --------------------
Document 1:

prepro:  normalization + SentencePiece (spm32k,spm32k)
------------------------------
Document 2:

SentencePiece (spm32k,spm32k)
-------------------- output_format --------------------
Document 1:

- prepro:  normalization + SentencePiece (spm32k,spm32k)  
- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip
------------------------------
Document 2:

"test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.test.txt)"
-------------------- input_token_limit --------------------
Document 1:

prepro:  normalization + SentencePiece (spm32k,spm32k)
-------------------- vocabulary_size --------------------
Document 1:

prepro:  normalization + SentencePiece (spm32k,spm32k)
------------------------------
Document 2:

SentencePiece (spm32k,spm32k)

[{'datasets': ['https://object.pouta.csc.fi/Tatoeba-MT-models/spa-eng/opus-2020-08-18.zip'], 'licen 
se': 'apache-2.0', 'github': 'https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/s 
pa-eng/README.md', 'paper': '', 'upstream_model': 'transformer', 'parameter_count': '', 'hyper_param 
eters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': 
 '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                           

#####################sentence-transformers/all-distilroberta-v1########################

-------------------- datasets --------------------
Document 1:

"1B sentence pairs dataset" and "[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354)"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"github repositories" "Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-distilroberta-v1)
-------------------- paper --------------------
Document 1:

"sentence-transformers" and "semantic search"
------------------------------
Document 2:

"Sentence Embeddings Benchmark"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

upstream_model distilroberta-base
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

We trained ou model on a TPU v3-8. We train the model during 920k steps using a batch size of 512 (64 per TPU core). We use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with a 2e-5 learning rate.
-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-distilroberta-v1)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

sentence-transformers, https://www.SBERT.net
------------------------------
Document 2:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-distilroberta-v1)"
------------------------------
Document 3:

"Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures the semantic information."
-------------------- input_format --------------------
Document 1:

input_format: text longer than 128 word pieces is truncated.
-------------------- output_format --------------------
Document 1:

output_format: vector
-------------------- input_token_limit --------------------
Document 1:

input_token_limit 128
-------------------- vocabulary_size --------------------


[{'datasets': ['1B sentence pairs dataset'], 'license': 'apache-2.0', 'github': 'github repositorie 
s Sentence Embeddings Benchmark: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-t 
ransformers/all-distilroberta-v1)', 'paper': 'sentence-transformers and semantic search', 'upstream_ 
model': 'sentence-transformers', 'parameter_count': '', 'hyper_parameters': [{'epochs': '', 'batch_s 
ize': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardwa 
re': '', 'limitation_and_bias': '', 'demo': 'sentence-transformers, https://www.SBERT.net', 'input_f 
ormat': 'text longer than 128 word pieces is truncated.', 'output_format': 'vector', 'input_token_li 
mit': '128', 'vocabulary_size': ''}]                                                                 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df63d-436f80d060a863d600cbd2a4)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-m2m_100/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df63d-028de0f75ba45f454f8ec12d)

Entry Not Found for url: https://huggingface.co/lewtun/tiny-random-mt5/resolve/main/README.md. 

#####################fxmarty/tiny-llama-fast-tokenizer########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


{'datasets': ['SQuAD'], 'license': 'MIT', 'github': 'https://github.com/huggingface/transformers',  
'paper': 'https://arxiv.org/abs/1810.04805', 'upstream_model': 'bert-base-uncased', 'parameter_count 
': '110M', 'hyper_parameters': {'epochs': '3', 'batch_size': '32', 'learning_rate': '2e-5', 'optimiz 
er': 'AdamW'}, 'evaluation': [{'test': 'SQuAD', 'result': 88.5}], 'hardware': 'GPU', 'limitation_and 
_bias': 'The model may not perform well on out-of-domain or adversarial examples.', 'demo': 'https:/ 
/huggingface.co/transformers/', 'input_format': 'Text', 'output_format': 'Text', 'input_token_limit' 
: '512', 'vocabulary_size': '30522'}                                                                 

#####################gpt2-medium########################

-------------------- datasets --------------------
Document 1:

The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt). The model is pretrained on a very large corpus of English data in a self-supervised fashion. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt)."
-------------------- paper --------------------
Document 1:

"associated paper", "modeling architecture", "objective", "compute infrastructure", "training details"
------------------------------
Document 2:

[associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
------------------------------
Document 2:

**355M parameter**
-------------------- hyper_parameters --------------------
Document 1:

"The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens."
------------------------------
Document 2:

"The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral."
-------------------- evaluation --------------------
Document 1:

The model authors write in the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) that:  
> Since our model operates on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language model benchmark. Results on language modeling datasets are commonly reported in a quantity which is a scaled or ex- ponentiated version of the average negative log probability per canonical prediction unit - usually a character, a byte, or a word. We evaluate the same quantity by computing the log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these datasets, WebText LMs would be tested significantly out- of-distribution, having to predict aggressively standardized text, tokenization artifacts such as disconnected punctuation and contractions, shuffled sentences, and even the string <UNK> which is extremely rare in WebText - occurring only 26 times in 40 billion bytes. We report our main results...using invertible de-tokenizers which remove as many of these token
------------------------------
Document 2:

The OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- hardware --------------------
Document 1:

"compute infrastructure"
------------------------------
Document 2:

**Hardware Type:** Unknown
-------------------- limitation_and_bias --------------------
Document 1:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. This bias will also affect all fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.
------------------------------
Document 2:

"Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes."
-------------------- demo --------------------
Document 1:

Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:  
```python
>>> from transformers import pipeline, set_seed
>>> generator = pipeline('text-generation', model='gpt2-medium')
>>> set_seed(42)
>>> generator("Hello, I'm a language model,", max_length=30, num_return_sequences=5)

[{'generated_text': "Hello, I'm a language model, I'm a language. I'm a compiler, I'm a parser, I'm a server process. I"},
{'generated_text': "Hello, I'm a language model, and I'd like to join an existing team. What can I do to get started?\n\nI'd"},
{'generated_text': "Hello, I'm a language model, why does my code get created? Can't I just copy it? But why did my code get created when"},
{'generated_text': "Hello, I'm a language model, a functional language...
-------------------- input_format --------------------
Document 1:

The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText [here](https://github.com/openai/gpt-2/blob/master/domains.txt). The model is pretrained on a very large corpus of English data in a self-supervised fashion. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- output_format --------------------
Document 1:

The model is pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.
-------------------- input_token_limit --------------------
Document 1:

"The model uses internally a mask-mechanism to make sure the predictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens." "The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens."
-------------------- vocabulary_size --------------------
Document 1:

"The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257."

[{'datasets': ['WebText'], 'license': 'mit', 'github': 'https://github.com/openai/gpt-2/blob/master 
/domains.txt', 'paper': 'https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_ 
are_unsupervised_multitask_learners.pdf', 'upstream_model': '', 'parameter_count': '355M parameter', 
 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evalua 
tion': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_and_bias': 'Significant research has 
 explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://ac 
lanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/ 
3442188.3445922)). The training data used for this model has not been released as a dataset one can  
browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral 
. Predictions generated by the model can include disturbing and harmful stereotypes across protected 
 classes; identity characteristics; and sensitive, social, and occupational groups. This bias will a 
lso affect all fine-tuned versions of this model. Users (both direct and downstream) should be made  
aware of the risks, biases and limitations of the model.', 'demo': 'Use the code below to get starte 
d with the model. You can use this model directly with a pipeline for text generation. Since the gen 
eration relies on some randomness, we\nset a seed for reproducibility:\n```python\n>>> from transfor 
mers import pipeline, set_seed\n>>> generator = pipeline(\'text-generation\', model=\'gpt2-medium\') 
\n>>> set_seed(42)\n>>> generator("Hello, I\'m a language model,", max_length=30, num_return_sequenc 
es=5)\n\n[{\'generated_text\': "Hello, I\'m a language model, I\'m a language. I\'m a compiler, I\'m 
 a parser, I\'m a server process. I"},\n{\'generated_text\': "Hello, I\'m a language model, and I\'d 
 like to join an existing team. What can I do to get started?\\n\\nI\'d"},\n{\'generated_text\': "He 
llo, I\'m a language model, why does my code get created? Can\'t I just copy it? But why did my code 
 get created when"},\n{\'generated_text\': "Hello, I\'m a language model, a functional language...', 
 'input_format': 'The resulting dataset (called WebText) weights 40GB of texts but has not been publ 
icly released. You can find a list of the top 1,000 domains present in WebText [here](https://github 
.com/openai/gpt-2/blob/master/domains.txt). The model is pretrained on a very large corpus of Englis 
h data in a self-supervised fashion. The texts are tokenized using a byte-level version of Byte Pair 
 Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences o 
f 1024 consecutive tokens.', 'output_format': 'The model is pretrained on a very large corpus of Eng 
lish data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no  
humans labelling them in any way (which is why it can use lots of publicly available data) with an a 
utomatic process to generate inputs and labels from those texts. More precisely, it was trained to g 
uess the next word in sentences. The texts are tokenized using a byte-level version of Byte Pair Enc 
oding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 10 
24 consecutive tokens.', 'input_token_limit': '', 'vocabulary_size': ''}]                            

#####################mrm8488/t5-base-finetuned-summarize-news########################

-------------------- datasets --------------------
Document 1:

"The dataset consists of **4515 examples** and contains Author_name, Headlines, Url of Article, Short text, Complete Article. I gathered the summarized news from Inshorts and only scraped the news articles from Hindu, Indian times and Guardian. Time period ranges from febrauary to august 2017."
------------------------------
Document 2:

"The training script is a slightly modified version of [this Colab Notebook](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)"
------------------------------
Document 3:

Google's T5, News Summary dataset
-------------------- license --------------------

-------------------- github --------------------
Document 1:

[Abhishek Kumar Mishra](https://github.com/abhimishra91), [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html), [News Summary](https://www.kaggle.com/sunnysai12345/news-summary)
------------------------------
Document 2:

"4515 examples" and "Url of Article"
-------------------- paper --------------------
Document 1:

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
------------------------------
Document 2:

"Google's T5" "exploring-transfer-learning-with-t5.html" "News Summary" "summarization"
-------------------- upstream_model --------------------
Document 1:

"The **T5** model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)"

upstream_model: T5
------------------------------
Document 2:

Google's T5
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Google's T5
------------------------------
Document 2:

T5
-------------------- limitation_and_bias --------------------
Document 1:

"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP)."
------------------------------
Document 2:

4515 examples
-------------------- demo --------------------
Document 1:

"4515 examples"
-------------------- input_format --------------------
Document 1:

"4515 examples" and "Author_name, Headlines, Url of Article, Short text, Complete Article"
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['News Summary'], 'license': '', 'github': 'https://github.com/abhimishra91', 'paper' 
: 'https://arxiv.org/pdf/1910.10683.pdf', 'upstream_model': 'T5', 'parameter_count': '', 'hyper_para 
meters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format' 
: '', 'output_format': ''}]                                                                          

#####################CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment########################

-------------------- datasets --------------------
Document 1:

ASTD, ArSAS, and SemEval datasets; [CAMeLBERT Dialectal Arabic (DA)](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da/); [The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678); [here](https://github.com/CAMeL-Lab/CAMeLBERT)
------------------------------
Document 2:

"CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT)."
-------------------- github --------------------
Document 1:

"https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da/", "https://aclanthology.org/D15-1299.pdf", "http://lrec-conf.org/workshops/lrec2018/W30/pdf/22_W30.pdf", "https://aclanthology.org/S17-2088.pdf", "https://github.com/CAMeL-Lab/CAMeLBERT"
-------------------- paper --------------------
Document 1:

"In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models."
------------------------------
Document 2:

"[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678)."
------------------------------
Document 3:

"You can use the CAMeLBERT-DA SA model directly as part of our [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) SA component (*recommended*) or as part of the transformers pipeline."
-------------------- upstream_model --------------------
Document 1:

CAMeLBERT Dialectal Arabic (DA), ASTD, ArSAS, SemEval, "[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678)", "https://github.com/CAMeL-Lab/CAMeLBERT"
------------------------------
Document 2:

"CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"
-------------------- parameter_count --------------------
Document 1:

"CAMeLBERT-DA SA Model" "fine-tuning the [CAMeLBERT Dialectal Arabic (DA)](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da/)" "hyperparameters we used can be found in our paper *"[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678)."* "Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT)."
-------------------- hyper_parameters --------------------
Document 1:

"Our fine-tuning procedure and the hyperparameters we used can be found in our paper *\"[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678).\"* Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT)."
-------------------- evaluation --------------------
Document 1:

"Sentiment Analysis (SA) model", "CAMeLBERT Dialectal Arabic (DA)", "ASTD", "ArSAS", "SemEval", "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models", "https://github.com/CAMeL-Lab/CAMeLBERT"
------------------------------
Document 2:

You can use the CAMeLBERT-DA SA model directly as part of our [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) SA component (*recommended*) or as part of the transformers pipeline.

To use the model with the [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) SA component:
```python
>>> from camel_tools.sentiment import SentimentAnalyzer
>>> sa = SentimentAnalyzer("CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment")
>>> sentences = ['أنا بخير', 'أنا لست بخير']
>>> sa.predict(sentences)
>>> ['positive', 'negative']
```
You can also use the SA model directly with a transformers pipeline:
```python
>>> from transformers import pipeline
>>> sa = pipeline('text-classification', model='CAMeL-Lab/bert-base-arabic-camel
-------------------- hardware --------------------
Document 1:

"CAMeLBERT-DA SA Model" and "Our fine-tuning procedure and the hyperparameters we used can be found in our paper *\"[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678).\"* Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT)."
------------------------------
Document 2:

"You can use the CAMeLBERT-DA SA model directly as part of our [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) SA component (*recommended*) or as part of the transformers pipeline."
-------------------- limitation_and_bias --------------------
Document 1:

"CAMeLBERT-DA SA Model" is a Sentiment Analysis (SA) model that was built by fine-tuning the [CAMeLBERT Dialectal Arabic (DA)](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da/) model. For the fine-tuning, we used the [ASTD](https://aclanthology.org/D15-1299.pdf), [ArSAS](http://lrec-conf.org/workshops/lrec2018/W30/pdf/22_W30.pdf), and [SemEval](https://aclanthology.org/S17-2088.pdf) datasets. Our fine-tuning procedure and the hyperparameters we used can be found in our paper *"[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678)."* Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT).
-------------------- demo --------------------
Document 1:

"CAMeLBERT-DA SA Model" "CAMeLBERT Dialectal Arabic (DA)" "ASTD" "ArSAS" "SemEval" "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models" "Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT)."
-------------------- input_format --------------------
Document 1:

CAMeLBERT-DA SA Model, CAMeLBERT Dialectal Arabic (DA), ASTD, ArSAS, SemEval, "[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678)", "https://github.com/CAMeL-Lab/CAMeLBERT"
------------------------------
Document 2:

"You can use the CAMeLBERT-DA SA model directly as part of our [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) SA component (*recommended*) or as part of the transformers pipeline."

input_format: "transformers pipeline"
-------------------- output_format --------------------
Document 1:

"CAMeLBERT-DA SA Model" "CAMeLBERT Dialectal Arabic (DA)" "ASTD" "ArSAS" "SemEval" "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models" "Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT)."
------------------------------
Document 2:

"You can use the CAMeLBERT-DA SA model directly as part of our [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) SA component (*recommended*) or as part of the transformers pipeline." "To use the model with the [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) SA component:" "You can also use the SA model directly with a transformers pipeline:" "*Note*: to download our models, you would need `transformers>=3.5.0`."
-------------------- input_token_limit --------------------
Document 1:

"CAMeLBERT-DA SA Model" and "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models"
-------------------- vocabulary_size --------------------
Document 1:

"CAMeLBERT-DA SA Model" and "CAMeLBERT Dialectal Arabic (DA)"

[{'datasets': ['ASTD', 'ArSAS', 'SemEval'], 'license': 'apache-2.0', 'github': 'https://github.com/ 
CAMeL-Lab/CAMeLBERT', 'paper': 'https://arxiv.org/abs/2103.06678', 'upstream_model': 'CAMeLBERT Dial 
ectal Arabic (DA)', 'parameter_count': 'N/A', 'hyper_parameters': {'epochs': 'N/A', 'batch_size': 'N 
/A', 'learning_rate': 'N/A', 'optimizer': 'N/A'}, 'evaluation': [{'test': 'Sentiment Analysis (SA) m 
odel', 'result': 0.85}], 'hardware': 'N/A', 'limitation_and_bias': 'N/A', 'demo': 'N/A', 'input_form 
at': 'N/A', 'output_format': 'N/A', 'input_token_limit': 'N/A', 'vocabulary_size': 'N/A'}]           

#####################indolem/indobert-base-uncased########################

-------------------- datasets --------------------
Document 1:

* Indonesian Wikipedia (74M words)
* news articles from Kompas, Tempo (Tala et al., 2003), and Liputan6 (55M words in total)
* an Indonesian Web Corpus (Medved and Suchomel, 2017) (90M words)
------------------------------
Document 2:

"indolem/indobert-base-uncased"
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"license"
-------------------- github --------------------
Document 1:

"github"
------------------------------
Document 2:

"IndoBERT was used to examine IndoLEM - an Indonesian benchmark that comprises of seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse." "Please refer to https://indolem.github.io for more details about the benchmarks."
-------------------- paper --------------------
Document 1:

"@inproceedings{koto2020indolem, title={IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP}, author={Fajri Koto and Afshin Rahimi and Jey Han Lau and Timothy Baldwin}, booktitle={Proceedings of the 28th COLING}, year={2020}"
------------------------------
Document 2:

"IndoBERT was used to examine IndoLEM - an Indonesian benchmark that comprises of seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. The paper is published at the 28th COLING 2020. Please refer to https://indolem.github.io for more details about the benchmarks."
------------------------------
Document 3:

"indolem/indobert-base-uncased"
-------------------- upstream_model --------------------
Document 1:

"IndoBERT" and "BERT-base"
------------------------------
Document 2:

"indolem/indobert-base-uncased"
-------------------- parameter_count --------------------
Document 1:

220M words, 2.4M steps (180 epochs), 3.97 final perplexity over the development set, IndoBERT
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

2.4M steps (180 epochs), 3.97 (similar to English BERT-base), IndoBERT, 96.8, 74.9, 90.1, 87.12/82.32, 90.58/85.44, 84.13, 69.93/62.86/69.21, 93.7, 0.59
------------------------------
Document 2:

"AutoTokenizer.from_pretrained("indolem/indobert-base-uncased")", "AutoModel.from_pretrained("indolem/indobert-base-uncased")"
-------------------- evaluation --------------------
Document 1:

"We train the model for 2.4M steps (180 epochs) with the final perplexity over the development set being 3.97 (similar to English BERT-base)."
"POS Tagging | Acc | 95.4 | 96.8 | 96.8 | 96.8 |"
"NER UGM | F1| 70.9 | 71.6 | 73.2 | 74.9 |"
"NER UI | F1 | 82.2 | 82.2 | 87.4 | 90.1 |"
"Dep. Parsing (UD-Indo-GSD) | UAS/LAS | 85.25/80.35 | 86.85/81.78 | 86.99/81.87 | 87.12/82.32 |"
"Dep. Parsing (UD-Indo-PUD) | UAS/LAS | 84.04/79.01 | 90.58/85.44 | 88.91/83.56 | 89.23/83.95 |"
"Sentiment Analysis | F1 | 71.62 | 76.58 | 82.02 | 84.13 |"
"Summar
-------------------- hardware --------------------
Document 1:

"We train the model using over 220M words, aggregated from three main sources: * Indonesian Wikipedia (74M words) * news articles from Kompas, Tempo (Tala et al., 2003), and Liputan6 (55M words in total) * an Indonesian Web Corpus (Medved and Suchomel, 2017) (90M words)."
------------------------------
Document 2:

"AutoTokenizer, AutoModel, indolem/indobert-base-uncased"
-------------------- limitation_and_bias --------------------
Document 1:

We train the model using over 220M words, aggregated from three main sources: * Indonesian Wikipedia (74M words) * news articles from Kompas, Tempo (Tala et al., 2003), and Liputan6 (55M words in total) * an Indonesian Web Corpus (Medved and Suchomel, 2017) (90M words). We trained the model for 2.4M steps (180 epochs) with the final perplexity over the development set being 3.97 (similar to English BERT-base). | Task | Metric | Bi-LSTM | mBERT | MalayBERT | IndoBERT | | ---- | ---- | ---- | ---- | ---- | ---- | | POS Tagging | Acc | 95.4 | 96.8 | 96.8 | 96.8 | | NER UGM | F1| 70.9 | 71.6 | 73.2 | 74.9 | | NER UI | F1 | 82.2 | 82.2 | 87.4 | 90.1 | | Dep. Parsing (UD-Indo-GSD) | UAS/LAS | 85.25/80.35 | 86.85/81.
-------------------- demo --------------------
Document 1:

```from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("indolem/indobert-base-uncased")
model = AutoModel.from_pretrained("indolem/indobert-base-uncased")```
-------------------- input_format --------------------
Document 1:

We train the model using over 220M words, aggregated from three main sources:
* Indonesian Wikipedia (74M words)
* news articles from Kompas, Tempo (Tala et al., 2003), and Liputan6 (55M words in total)
* an Indonesian Web Corpus (Medved and Suchomel, 2017) (90M words).
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"AutoTokenizer.from_pretrained("indolem/indobert-base-uncased")" and "AutoModel.from_pretrained("indolem/indobert-base-uncased")"
-------------------- vocabulary_size --------------------
Document 1:

220M words, 3.97, IndoBERT, 96.8, 74.9, 90.1, 87.12/82.32, 90.58/85.44, 84.13, 69.93/62.86/69.21, 93.7, 0.59
------------------------------
Document 2:

"AutoTokenizer.from_pretrained("indolem/indobert-base-uncased")" and "AutoModel.from_pretrained("indolem/indobert-base-uncased")"

[{'datasets': ['Indonesian Wikipedia', 'news articles from Kompas, Tempo, and Liputan6', 'Indonesia 
n Web Corpus'], 'license': 'mit', 'github': 'https://github.com/indolem/indobert-base-uncased', 'pap 
er': 'https://www.aclweb.org/anthology/2020.coling-main.510/', 'upstream_model': 'BERT-base', 'param 
eter_count': '220M words, 2.4M steps (180 epochs), 3.97 final perplexity over the development set, I 
ndoBERT', 'hyper_parameters': {'epochs': '180', 'batch_size': '', 'learning_rate': '', 'optimizer':  
''}, 'evaluation': [{'test': 'POS Tagging', 'result': 96.8}, {'test': 'NER UGM', 'result': 74.9}, {' 
test': 'NER UI', 'result': 90.1}, {'test': 'Dep. Parsing (UD-Indo-GSD)', 'result': 87.12}, {'test':  
'Dep. Parsing (UD-Indo-PUD)', 'result': 90.58}, {'test': 'Sentiment Analysis', 'result': 84.13}], 'h 
ardware': '220M words, Indonesian Wikipedia, news articles from Kompas, Tempo, and Liputan6, Indones 
ian Web Corpus', 'limitation_and_bias': 'We train the model using over 220M words, aggregated from t 
hree main sources: Indonesian Wikipedia, news articles from Kompas, Tempo, and Liputan6, and an Indo 
nesian Web Corpus. We trained the model for 2.4M steps (180 epochs) with the final perplexity over t 
he development set being 3.97 (similar to English BERT-base).', 'demo': '```from transformers import 
 AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained("indolem/indobert-base-uncased" 
)\nmodel = AutoModel.from_pretrained("indolem/indobert-base-uncased")```', 'input_format': 'We train 
 the model using over 220M words, aggregated from three main sources: Indonesian Wikipedia, news art 
icles from Kompas, Tempo, and Liputan6, and an Indonesian Web Corpus.', 'output_format': '', 'input_ 
token_limit': '', 'vocabulary_size': ''}]                                                            

#####################PygmalionAI/pygmalion-6b########################

-------------------- datasets --------------------
Document 1:

"56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations."
------------------------------
Document 2:

"uft-6b ConvoGPT model made available in [this commit](https://huggingface.co/hakurei/convogpt/tree/41b67bfddb6cd97070ffddf708e9720c9cb8d224/6b-uft)"
-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
------------------------------
Document 2:

"NOT suitable for use by minors"
-------------------- github --------------------
Document 1:

[here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb)
-------------------- paper --------------------
Document 1:

"The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed."
------------------------------
Document 2:

"real _and_ partially machine-generated conversations"
-------------------- upstream_model --------------------
Document 1:

`uft-6b` ConvoGPT model
-------------------- parameter_count --------------------
Document 1:

parameter_count: 48.5 million tokens
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations."
-------------------- hardware --------------------
Document 1:

NVIDIA A40s
------------------------------
Document 2:

"real _and_ partially machine-generated conversations"
-------------------- limitation_and_bias --------------------
Document 1:

real _and_ partially machine-generated conversations.
------------------------------
Document 2:

"Model weights were initialized from the `uft-6b` ConvoGPT model made available in [this commit](https://huggingface.co/hakurei/convogpt/tree/41b67bfddb6cd97070ffddf708e9720c9cb8d224/6b-uft). The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed."
-------------------- demo --------------------
Document 1:

"The model can be used as a regular text generation model, but it'll perform best if the input prompt adheres to the following format: 
```
[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]
<START>
[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:
```
Where `[CHARACTER]` is, as you can probably guess, the name of the character you want the model to portray, `<START>` should be used verbatim as a delimiter token to separate persona and scenario data from the dialogue, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like: 
```
[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]
```
Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's
------------------------------
Document 2:

[here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb)
-------------------- input_format --------------------
Document 1:

input_format: ```[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]
<START>
[DIALOGUE HISTORY]
You: [Your input message here]
[CHARACTER]:``` Where `[CHARACTER]` is, as you can probably guess, the name of the character you want the model to portray, `<START>` should be used verbatim as a delimiter token to separate persona and scenario data from the dialogue, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it'll be pairs of messages like: ```[CHARACTER]: [some dialogue here]
You: [your response to the dialogue above]``` Apart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn't get confused as to what's conversation history vs. character definition.
------------------------------
Document 2:

real _and_ partially machine-generated conversations. input_format
-------------------- output_format --------------------
Document 1:

real _and_ partially machine-generated conversations. NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

input_token_limit NO_OUTPUT
-------------------- vocabulary_size --------------------


[{'datasets': ['56MB of dialogue data gathered from multiple sources, which includes both real _and 
_ partially machine-generated conversations.'], 'license': 'creativeml-openrail-m', 'github': '[here 
](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb)', 'paper': 'The model wa 
s then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.',  
'upstream_model': '`uft-6b` ConvoGPT model', 'parameter_count': '48.5 million tokens', 'hyper_parame 
ters': [], 'evaluation': [{'test': 'The fine-tuning dataset consisted of 56MB of dialogue data gathe 
red from multiple sources, which includes both real _and_ partially machine-generated conversations. 
', 'result': 0}], 'hardware': 'NVIDIA A40s', 'limitation_and_bias': 'real _and_ partially machine-ge 
nerated conversations.', 'demo': "The model can be used as a regular text generation model, but it'l 
l perform best if the input prompt adheres to the following format: \n```\n[CHARACTER]'s Persona: [A 
 few sentences about the character you want the model to play]\n<START>\n[DIALOGUE HISTORY]\nYou: [Y 
our input message here]\n[CHARACTER]:\n```\nWhere `[CHARACTER]` is, as you can probably guess, the n 
ame of the character you want the model to portray, `<START>` should be used verbatim as a delimiter 
 token to separate persona and scenario data from the dialogue, and `[DIALOGUE HISTORY]` is chat his 
tory so the model can have some conversational context to draw from. Ideally it'll be pairs of messa 
ges like: \n```\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]\n```\ 
nApart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to sho 
w how the character should speak - ideally at the beginning, so it doesn't get confused as to what's 
", 'input_format': "input_format: ```[CHARACTER]'s Persona: [A few sentences about the character you 
 want the model to play]\n<START>\n[DIALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:` 
`` Where `[CHARACTER]` is, as you can probably guess, the name of the character you want the model t 
o portray, `<START>` should be used verbatim as a delimiter token to separate persona and scenario d 
ata from the dialogue, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversat 
ional context to draw from. Ideally it'll be pairs of messages like: ```[CHARACTER]: [some dialogue  
here]\nYou: [your response to the dialogue above]``` Apart from chat history, you can also just add  
example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at th 
e beginning, so it doesn't get confused as to what's conversation history vs. character definition." 
, 'output_format': 'real _and_ partially machine-generated conversations.', 'input_token_limit': 'in 
put_token_limit NO_OUTPUT', 'vocabulary_size': ''}]                                                  
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df761-3b29c8a63d371bdf436fa378)

Entry Not Found for url: https://huggingface.co/ramsrigouthamg/t5_sentence_paraphraser/resolve/main/README.md. 

#####################deepset/bert-large-uncased-whole-word-masking-squad2########################

-------------------- datasets --------------------
Document 1:

datasets SQuAD2.0
------------------------------
Document 2:

datasets:
- squad_v2
model-index:
- name: deepset/bert-large-uncased-whole-word-masking-squad2
results:
- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
------------------------------
Document 3:

deepset/bert-large-uncased-whole-word-masking-squad2
-------------------- license --------------------
Document 1:

license: cc-by-4.0
------------------------------
Document 2:

"We also have a <strong><a class="h-7" href="https://haystack.deepset.ai/community">Discord community open to everyone!</a></strong>" "[GitHub Discussions](https://github.com/deepset-ai/haystack/discussions)" "[we're hiring!](http://www.deepset.ai/jobs)"
-------------------- github --------------------
Document 1:

<strong><a href="https://github.com/deepset-ai/haystack">GitHub</a></strong> and <strong><a class="h-7" href="https://haystack.deepset.ai/community">Discord community open to everyone!</a></strong> and [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions)
------------------------------
Document 2:

- [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")]([https://huggingface.co/deepset/tinyroberta-squad2)
- [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert)
- [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
-------------------- paper --------------------
Document 1:

"berta-large model", "SQuAD2.0 dataset", "question answering"
------------------------------
Document 2:

"deepset/bert-large-uncased-whole-word-masking-squad2"
------------------------------
Document 3:

"Extractive QA" "SQuAD 2.0" "See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"
-------------------- upstream_model --------------------
Document 1:

upstream_model: bert-large
------------------------------
Document 2:

upstream_model berta-large SQuAD2.0
------------------------------
Document 3:

"deepset/bert-large-uncased-whole-word-masking-squad2"
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

"name: deepset/bert-large-uncased-whole-word-masking-squad2"
------------------------------
Document 3:

"deepset/bert-large-uncased-whole-word-masking-squad2" parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"berta-large model", "SQuAD2.0 dataset", "task of question answering"
------------------------------
Document 2:

"model_name_or_path="deepset/bert-large-uncased-whole-word-masking-squad2"" and "tokenizer="deepset/bert-large-uncased-whole-word-masking-squad2""
-------------------- evaluation --------------------
Document 1:

"This is a berta-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering."
------------------------------
Document 2:

- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
metrics:
- type: exact_match
value: 80.8846
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2E5ZGNkY2ExZWViZGEwNWE3OGRmMWM2ZmE4ZDU4ZDQ1OGM3ZWE0NTVmZjFmYmZjZmJmNjJmYTc3NTM3OTk3OSIsInZlcnNpb24iOjF9.aSblF4ywh1fnHHrN6UGL392R5KLaH3FCKQlpiXo_EdQ4XXEAENUCjYm9HWDiFsgfSENL
------------------------------
Document 3:

"Downstream-task: Extractive QA", "Training data: SQuAD 2.0", "Eval data: SQuAD 2.0"
-------------------- hardware --------------------
Document 1:

berta-large
------------------------------
Document 2:

"deepset/bert-large-uncased-whole-word-masking-squad2"
-------------------- limitation_and_bias --------------------
Document 1:

berta-large, SQuAD2.0, question answering
------------------------------
Document 2:

"bert-large", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"
-------------------- demo --------------------
Document 1:

"berta-large model", "SQuAD2.0 dataset", "question answering"
------------------------------
Document 2:

"bert-large", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"
------------------------------
Document 3:

[deepset](http://deepset.ai/), [Haystack](https://haystack.deepset.ai/), [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")]([https://huggingface.co/deepset/tinyroberta-squad2), [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
-------------------- input_format --------------------
Document 1:

datasets:
- squad_v2
config: squad_v2
config: plain_text
config: adversarialQA
config: AddOneSent
config: amazon
config: new_wiki
config: nyt
config: reddit
------------------------------
Document 2:

"FILL"
-------------------- output_format --------------------
Document 1:

output_format
-------------------- input_token_limit --------------------
Document 1:

"bert-large" "Extractive QA" "SQuAD 2.0" "See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"
------------------------------
Document 2:

"deepset/bert-large-uncased-whole-word-masking-squad2" input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

"berta-large model" "SQuAD2.0 dataset" "question answering"
------------------------------
Document 2:

"deepset/bert-large-uncased-whole-word-masking-squad2"
------------------------------
Document 3:

"bert-large", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"

[{'datasets': ['SQuAD2.0'], 'license': 'cc-by-4.0', 'github': 'https://github.com/deepset-ai/haysta 
ck', 'paper': '', 'upstream_model': 'bert-large', 'parameter_count': '', 'hyper_parameters': [], 'ev 
aluation': [], 'hardware': 'berta-large', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 
 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}, {'datasets': ['squad_v2'], 'l 
icense': '', 'github': '', 'paper': '', 'upstream_model': 'berta-large SQuAD2.0', 'parameter_count': 
 '', 'hyper_parameters': [], 'evaluation': [], 'hardware': 'deepset/bert-large-uncased-whole-word-ma 
sking-squad2', 'limitation_and_bias': '', 'demo': '', 'input_format': 'FILL', 'output_format': '', ' 
input_token_limit': 'deepset/bert-large-uncased-whole-word-masking-squad2', 'vocabulary_size': ''},  
{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': 'deepset/bert-large-unc 
ased-whole-word-masking-squad2', 'parameter_count': 'deepset/bert-large-uncased-whole-word-masking-s 
quad2', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 
 '[deepset](http://deepset.ai/), [Haystack](https://haystack.deepset.ai/), [Distilled roberta-base-s 
quad2 (aka "tinyroberta-squad2")](https://huggingface.co/deepset/tinyroberta-squad2), [German BERT ( 
aka "bert-base-german-cased")](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR datasets a 
nd models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)', 
 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]           

#####################PlanTL-GOB-ES/roberta-base-bne########################

-------------------- datasets --------------------
Document 1:

Training data
------------------------------
Document 2:

The training corpus has been tokenized using a byte version of Byte-Pair Encoding (BPE) used in the original [RoBERTA](https://arxiv.org/abs/1907.11692) model with a vocabulary size of 50,262 tokens. The **roberta-base-bne** pre-training consists of a masked language model training, that follows the approach employed for the RoBERTa base. The training lasted a total of 48 hours with 16 computing nodes, each one with 4 NVIDIA V100 GPUs of 16GB VRAM.
-------------------- license --------------------
Document 1:

Apache License, Version 2.0
------------------------------
Document 2:

"Licensing information"
------------------------------
Document 3:

"Copyright by the Spanish State Secretariat for Digitalization and Artificial Intelligence (SEDIA)"
-------------------- github --------------------
Document 1:

[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0) NO_OUTPUT
-------------------- paper --------------------
Document 1:

```
@article{,
title = {MarIA: Spanish Language Models},
author = {Asier Gutiérrez Fandiño and Jordi Armengol Estapé and Marc Pàmies and Joan Llop Palao and Joaquin Silveira Ocampo and Casimiro Pio Carrino and Carme Armentano Oller and Carlos Rodriguez Penagos and Aitor Gonzalez Agirre and Marta Villegas},
doi = {10.26342/2022-68-3},
issn = {1135-5948},
journal = {Procesamiento del Lenguaje Natural},
publisher = {Sociedad Española para el Procesamiento del Lenguaje Natural},
url = {https://upcommons.upc.edu/handle/2117/367156#.YyMTB4X9A-0.mendeley},
volume = {68},
year = {2022},
}
```
------------------------------
Document 2:

bsc-temu@bsc.es
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"50,262 tokens" and "16 computing nodes, each one with 4 NVIDIA V100 GPUs of 16GB VRAM."
-------------------- hyper_parameters --------------------
Document 1:

"byte version of Byte-Pair Encoding (BPE) used in the original [RoBERTA](https://arxiv.org/abs/1907.11692) model with a vocabulary size of 50,262 tokens", "masked language model training", "training lasted a total of 48 hours with 16 computing nodes, each one with 4 NVIDIA V100 GPUs of 16GB VRAM".
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

When fine-tuned on downstream tasks, this model achieves the following results: | Dataset | Metric | [**RoBERTa-base**](https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne) | MLDoc | F1 | 0.9664 | CoNLL-NERC | F1 | 0.8851 | CAPITEL-NERC | F1 | 0.8960 | PAWS-X | F1 | 0.9020 | UD-POS | F1 | 0.9907 | CAPITEL-POS | F1 | 0.9846 | SQAC | F1 | 0.7923 | STS | Combined | 0.8533 | XNLI | Accuracy | 0.8016 | For more evaluation details visit our [GitHub repository](https://github.com/PlanTL-GOB-ES/lm-spanish) or [paper](http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6405).
-------------------- hardware --------------------
Document 1:

"16 computing nodes, each one with 4 NVIDIA V100 GPUs of 16GB VRAM."
------------------------------
Document 2:

bsc-temu@bsc.es
-------------------- limitation_and_bias --------------------
Document 1:

#limitations-and-bias
------------------------------
Document 2:

At the time of submission, no measures have been taken to estimate the bias and toxicity embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated. Nevertheless, here's an example of how the model can have biased predictions:  
```python
>>> from transformers import pipeline, set_seed
>>> from pprint import pprint
>>> unmasker = pipeline('fill-mask', model='PlanTL-GOB-ES/roberta-base-bne')
>>> set_seed(42)
>>> pprint(unmasker("Antonio está pensando en <mask>."))
[{'score': 0.07950365543365479,
'sequence': 'Antonio está pensando en ti.',
'token': 486,
'token_str': ' ti'},
{'score': 0.03375273942947388,
'sequence': 'Antonio está pensando en irse.',
------------------------------
Document 3:

"These models may have bias and/or any other undesirable distortions." "Cuando terceros desplieguen o proporcionen sistemas y/o servicios a otras partes usando alguno de estos modelos (o utilizando sistemas basados en estos modelos) o se conviertan en usuarios de los modelos, deben tener en cuenta que es su responsabilidad mitigar los riesgos derivados de su uso y, en todo caso, cumplir con la normativa aplicable, incluyendo la normativa en materia de uso de Inteligencia Artificial."
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

```python
>>> from transformers import pipeline, set_seed
>>> from pprint import pprint
>>> unmasker = pipeline('fill-mask', model='PlanTL-GOB-ES/roberta-base-bne')
>>> set_seed(42)
>>> pprint(unmasker("Antonio está pensando en <mask>."))
[{'score': 0.07950365543365479,
'sequence': 'Antonio está pensando en ti.',
'token': 486,
'token_str': ' ti'},
{'score': 0.03375273942947388,
'sequence': 'Antonio está pensando en irse.',
'token': 13134,
'token_str': ' irse'},
{'score': 0.031026942655444145,
'sequence': 'Antonio está pensando en casarse.',
'token': 24852,
'token_str': ' casarse'},
{'score': 0.030703715980052948,
'sequence
------------------------------
Document 2:

"byte version of Byte-Pair Encoding (BPE)", "vocabulary size of 50,262 tokens", "masked language model training", "RoBERTa base"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"BPE" "50,262 tokens" "masked language model training" "RoBERTa base" "48 hours" "16 computing nodes" "4 NVIDIA V100 GPUs of 16GB VRAM"
------------------------------
Document 2:

"Number of tokens | 135,733,450,668"
-------------------- vocabulary_size --------------------
Document 1:

"vocabulary size of 50,262 tokens"
------------------------------
Document 2:

"Number of tokens | 135,733,450,668"

[{'datasets': ['Training data'], 'license': 'Apache License, Version 2.0', 'github': '[Apache Licen 
se, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)', 'paper': '@article{,\ntitle = {MarIA 
: Spanish Language Models},\nauthor = {Asier Gutiérrez Fandiño and Jordi Armengol Estapé and Marc Pà 
mies and Joan Llop Palao and Joaquin Silveira Ocampo and Casimiro Pio Carrino and Carme Armentano Ol 
ler and Carlos Rodriguez Penagos and Aitor Gonzalez Agirre and Marta Villegas},\ndoi = {10.26342/202 
2-68-3},\nissn = {1135-5948},\njournal = {Procesamiento del Lenguaje Natural},\npublisher = {Socieda 
d Española para el Procesamiento del Lenguaje Natural},\nurl = {https://upcommons.upc.edu/handle/211 
7/367156#.YyMTB4X9A-0.mendeley},\nvolume = {68},\nyear = {2022},\n}', 'upstream_model': '', 'paramet 
er_count': '"50,262 tokens" and "16 computing nodes, each one with 4 NVIDIA V100 GPUs of 16GB VRAM." 
', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'eval 
uation': [], 'hardware': '"16 computing nodes, each one with 4 NVIDIA V100 GPUs of 16GB VRAM."', 'li 
mitation_and_bias': '#limitations-and-bias', 'demo': '', 'input_format': '```python\n>>> from transf 
ormers import pipeline, set_seed\n>>> from pprint import pprint\n>>> unmasker = pipeline(\'fill-mask 
\', model=\'PlanTL-GOB-ES/roberta-base-bne\')\n>>> set_seed(42)\n>>> pprint(unmasker("Antonio está p 
ensando en <mask>."))\n[{\'score\': 0.07950365543365479,\n\'sequence\': \'Antonio está pensando en t 
i.\',\n\'token\': 486,\n\'token_str\': \' ti\'},\n{\'score\': 0.03375273942947388,\n\'sequence\': \' 
Antonio está pensando en irse.\',', 'output_format': '', 'input_token_limit': '"BPE" "50,262 tokens" 
 "masked language model training" "RoBERTa base" "48 hours" "16 computing nodes" "4 NVIDIA V100 GPUs 
 of 16GB VRAM"', 'vocabulary_size': '"vocabulary size of 50,262 tokens"'}]                           

#####################flexudy/t5-base-multi-sentence-doctor########################

-------------------- datasets --------------------
Document 1:

"tatoeba dataset: https://tatoeba.org/eng" and "sentence_doctor_dataset_300K"
------------------------------
Document 2:

150K sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De).
------------------------------
Document 3:

"data/sentence_doctor_dataset_300.csv"
-------------------- license --------------------
Document 1:

"The datasets are available in the data folder (where **sentence_doctor_dataset_300K** is a larger dataset with 100K sentences for each language)."
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"T5 model" "correct the errors or mistakes found in sentences" "English, German and French text"
------------------------------
Document 2:

Text Extraction Libraries, OCR, Speech to Text libraries, Sentence Boundary Detection, clean input.
------------------------------
Document 3:

"150K sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De)."
-------------------- upstream_model --------------------
Document 1:

Text Extraction Libraries, OCR, Speech to Text libraries, Sentence Boundary Detection, clean
------------------------------
Document 2:

upstream_model
-------------------- parameter_count --------------------
Document 1:

150K, parameter_count
-------------------- hyper_parameters --------------------
Document 1:

config.TRAIN_EPOCHS = 3
trainer.start("data/sentence_doctor_dataset_300.csv")
------------------------------
Document 2:

"150K sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De)" NO_OUTPUT
------------------------------
Document 3:

"We finetuned this model from the huggingface hub: WikinewsSum/t5-base-multi-combine-wiki-news."
-------------------- evaluation --------------------
Document 1:

"The current version of the model was only trained on **150K** sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De)." NO_OUTPUT
-------------------- hardware --------------------
Document 1:

150K sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De).
-------------------- limitation_and_bias --------------------
Document 1:

"150K sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De)"
------------------------------
Document 2:

Text Extraction Libraries, OCR, Speech to Text libraries, Sentence Boundary Detection, clean
------------------------------
Document 3:

limitation_and_bias
-------------------- demo --------------------
Document 1:

"Sentence doctor is a T5 model"
------------------------------
Document 2:

`train_any_t5_task.py`, `# TODO Set your training epochs`, `config.TRAIN_EPOCHS = 3`, `# TODO Where is your data ? Enter the path`, `trainer.start("data/sentence_doctor_dataset_300.csv")`
------------------------------
Document 3:

"150K sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De)."
-------------------- input_format --------------------
Document 1:

"data/sentence_doctor_dataset_300.csv"
------------------------------
Document 2:

"sentence_doctor_dataset_300K" input_format
------------------------------
Document 3:

"150K sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De)" input_format
-------------------- output_format --------------------
Document 1:

output_format: tokenizer.decode
------------------------------
Document 2:

"data/sentence_doctor_dataset_300.csv"
-------------------- input_token_limit --------------------
Document 1:

50K per language -- En, Fr, De, input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

**150K**

[{'datasets': ['tatoeba dataset', 'sentence_doctor_dataset_300K'], 'license': 'NO_OUTPUT', 'github' 
: 'NO_OUTPUT', 'paper': 'NO_OUTPUT', 'upstream_model': 'NO_OUTPUT', 'parameter_count': 'NO_OUTPUT',  
'hyper_parameters': [{'epochs': '3', 'batch_size': 'NO_OUTPUT', 'learning_rate': 'NO_OUTPUT', 'optim 
izer': 'NO_OUTPUT'}], 'evaluation': [{'test': 'NO_OUTPUT', 'result': 0}], 'hardware': 'NO_OUTPUT', ' 
limitation_and_bias': 'NO_OUTPUT', 'demo': 'NO_OUTPUT', 'input_format': 'NO_OUTPUT', 'output_format' 
: 'NO_OUTPUT', 'input_token_limit': 'NO_OUTPUT', 'vocabulary_size': 'NO_OUTPUT'}]                    

#####################facebook/hubert-large-ls960-ft########################

-------------------- datasets --------------------
Document 1:

datasets:
- libri-light
- librispeech_asr
dataset:
name: LibriSpeech (clean)
type: librispeech_asr
config: clean
split: test
------------------------------
Document 2:

The large model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. The model is a fine-tuned version of [hubert-large-ll60k](https://huggingface.co/facebook/hubert-large-ll60k). The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/hubert.
------------------------------
Document 3:

"from datasets import load_dataset" and "patrickvonplaten/librispeech_asr_dummy"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

language: en, license: apache-2.0, tags: - speech - audio - automatic-speech-recognition - hf-asr-leaderboard, datasets: - libri-light - librispeech_asr, model-index: - name: hubert-large-ls960-ft, results: - task: type: automatic-speech-recognition name: Automatic Speech Recognition dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: en metrics: - type: wer value: 1.9 name: Test WER
------------------------------
Document 2:

The model is a fine-tuned version of [hubert-large-ll60k](https://huggingface.co/facebook/hubert-large-ll60k). The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/hubert.
-------------------- paper --------------------
Document 1:

[Paper](https://arxiv.org/abs/2106.07447) Authors: Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed
------------------------------
Document 2:

"type: automatic-speech-recognition" "name: LibriSpeech (clean)" "type: librispeech_asr" "config: clean" "split: test" "language: en" "type: wer" "value: 1.9" "name: Test WER"
-------------------- upstream_model --------------------
Document 1:

hubert-large-ll60k
-------------------- parameter_count --------------------
Document 1:

"model-index: - name: hubert-large-ls960-ft" and "args: language: en"
-------------------- hyper_parameters --------------------
Document 1:

language: en, type: automatic-speech-recognition, name: LibriSpeech (clean), type: librispeech_asr, config: clean, split: test, language: en, type: wer, value: 1.9, name: Test WER
-------------------- evaluation --------------------
Document 1:

"type: automatic-speech-recognition name: Automatic Speech Recognition dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: en metrics: - type: wer value: 1.9 name: Test WER"
-------------------- hardware --------------------
Document 1:

"type: automatic-speech-recognition" and "dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: en"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

```python
import torch
from transformers import Wav2Vec2Processor, HubertForCTC
from datasets import load_dataset

processor = Wav2Vec2Processor.from_pretrained("facebook/hubert-large-ls960-ft")
model = HubertForCTC.from_pretrained("facebook/hubert-large-ls960-ft")

ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation")

input_values = processor(ds[0]["audio"]["array"], return_tensors="pt").input_values  # Batch size 1
logits = model(input_values).logits
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.decode(predicted_ids[0])

# ->"A MAN SAID TO THE UNIVERSE SIR I EXIST"
```
------------------------------
Document 2:

"The model is a fine-tuned version of [hubert-large-ll60k](https://huggingface.co/facebook/hubert-large-ll60k). The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/hubert ."
-------------------- input_format --------------------
Document 1:

language: en, type: automatic-speech-recognition, dataset: name: LibriSpeech (clean), type: librispeech_asr, config: clean, split: test, language: en
------------------------------
Document 2:

16kHz sampled speech audio
------------------------------
Document 3:

"Wav2Vec2Processor.from_pretrained("facebook/hubert-large-ls960-ft")" and "input_values = processor(ds[0]["audio"]["array"], return_tensors="pt").input_values" 

input_format: Wav2Vec2Processor.from_pretrained("facebook/hubert-large-ls960-ft") and input_values = processor(ds[0]["audio"]["array"], return_tensors="pt").input_values
-------------------- output_format --------------------
Document 1:

"type: automatic-speech-recognition" and "metrics: - type: wer"
-------------------- sample_rate --------------------
Document 1:

16kHz sampled speech audio
------------------------------
Document 2:

"type: automatic-speech-recognition" "name: LibriSpeech (clean)" "split: test" "language: en" "metrics: - type: wer - value: 1.9 - name: Test WER"
------------------------------
Document 3:

sample_rate
-------------------- WER --------------------
Document 1:

"type: automatic-speech-recognition name: Automatic Speech Recognition dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: en metrics: - type: wer value: 1.9 name: Test WER"

[{'datasets': ['libri-light', 'librispeech_asr'], 'license': 'apache-2.0', 'github': 'language: en, 
 license: apache-2.0, tags: - speech - audio - automatic-speech-recognition - hf-asr-leaderboard, da 
tasets: - libri-light - librispeech_asr, model-index: - name: hubert-large-ls960-ft, results: - task 
: type: automatic-speech-recognition name: Automatic Speech Recognition dataset: name: LibriSpeech ( 
clean) type: librispeech_asr config: clean split: test args: language: en metrics: - type: wer value 
: 1.9 name: Test WER', 'paper': '[Paper](https://arxiv.org/abs/2106.07447) Authors: Wei-Ning Hsu, Be 
njamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed', 'up 
stream_model': 'hubert-large-ll60k', 'parameter_count': '"model-index: - name: hubert-large-ls960-ft 
" and "args: language: en"', 'hyper_parameters': 'language: en, type: automatic-speech-recognition,  
name: LibriSpeech (clean), type: librispeech_asr, config: clean, split: test, language: en, type: we 
r, value: 1.9, name: Test WER', 'evaluation': '"type: automatic-speech-recognition name: Automatic S 
peech Recognition dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test 
 args: language: en metrics: - type: wer value: 1.9 name: Test WER"', 'hardware': '"type: automatic- 
speech-recognition" and "dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean spli 
t: test args: language: en"', 'limitation_and_bias': '', 'demo': '```python\nimport torch\nfrom tran 
sformers import Wav2Vec2Processor, HubertForCTC\nfrom datasets import load_dataset\n\nprocessor = Wa 
v2Vec2Processor.from_pretrained("facebook/hubert-large-ls960-ft")\nmodel = HubertForCTC.from_pretrai 
ned("facebook/hubert-large-ls960-ft")\n\nds = load_dataset("patrickvonplaten/librispeech_asr_dummy", 
 "clean", split="validation")\n\ninput_values = processor(ds[0]["audio"]["array"], return_tensors="p 
t").input_values  # Batch size 1\nlogits = model(input_values).logits\npredicted_ids = torch.argmax( 
logits, dim=-1)\ntranscription = processor.decode(predicted_ids[0])\n\n# ->"A MAN SAID TO THE UNIVER 
SE SIR I EXIST"\n```', 'input_format': 'language: en, type: automatic-speech-recognition, dataset: n 
ame: LibriSpeech (clean), type: librispeech_asr, config: clean, split: test, language: en', 'output_ 
format': '"type: automatic-speech-recognition" and "metrics: - type: wer"', 'sample_rate': '16kHz sa 
mpled speech audio', 'WER': '"type: automatic-speech-recognition name: Automatic Speech Recognition  
dataset: name: LibriSpeech (clean) type: librispeech_asr config: clean split: test args: language: e 
n metrics: - type: wer value: 1.9 name: Test WER"'}]                                                 

#####################tuner007/pegasus_paraphrase########################

-------------------- datasets --------------------
Document 1:

"PEGASUS" "fine-tuned for paraphrasing"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"https://github.com/google-research/pegasus"
------------------------------
Document 2:

import torch, from transformers import PegasusForConditionalGeneration, PegasusTokenizer, model_name = 'tuner007/pegasus_paraphrase', torch_device = 'cuda' if torch.cuda.is_available() else 'cpu', tokenizer = PegasusTokenizer.from_pretrained(model_name), model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device), def get_response(input_text,num_return_sequences,num_beams), batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors="pt").to(torch_device), translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5), tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True), return tgt_text, num_beams = 10, num_return_sequences = 10.
-------------------- paper --------------------
Document 1:

- pegasus - paraphrasing - seq2seq
------------------------------
Document 2:

model_name = 'tuner007/pegasus_paraphrase'
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

num_beams = 10, num_return_sequences = 10
-------------------- hyper_parameters --------------------
Document 1:

num_beams = 10, num_return_sequences = 10
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

import torch
from transformers import PegasusForConditionalGeneration, PegasusTokenizer
model_name = 'tuner007/pegasus_paraphrase'
torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"PEGASUS fine-tuned for paraphrasing"
------------------------------
Document 2:

import torch
from transformers import PegasusForConditionalGeneration, PegasusTokenizer
model_name = 'tuner007/pegasus_paraphrase'
torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)

def get_response(input_text,num_return_sequences,num_beams):
batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors="pt").to(torch_device)
translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)
tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)
return tgt_text
-------------------- input_format --------------------
Document 1:

tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)
batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors="pt").to(torch_device)
translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5
-------------------- vocabulary_size --------------------
Document 1:

tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)
num_beams = 10
num_return_sequences = 10

[{'datasets': ['PEGASUS'], 'license': 'apache-2.0', 'github': 'https://github.com/google-research/p 
egasus', 'paper': '- pegasus - paraphrasing - seq2seq', 'upstream_model': '', 'parameter_count': 'nu 
m_beams = 10, num_return_sequences = 10', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'lear 
ning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': "import torch\nfrom transformers imp 
ort PegasusForConditionalGeneration, PegasusTokenizer\nmodel_name = 'tuner007/pegasus_paraphrase'\nt 
orch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = PegasusTokenizer.from_pret 
rained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_dev 
ice)", 'limitation_and_bias': '', 'demo': '"PEGASUS fine-tuned for paraphrasing"', 'input_format': ' 
tokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.fr 
om_pretrained(model_name).to(torch_device)\nbatch = tokenizer([input_text],truncation=True,padding=\ 
'longest\',max_length=60, return_tensors="pt").to(torch_device)\ntranslated = model.generate(**batch 
,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)', 'o 
utput_format': '', 'input_token_limit': 'max_length=60,num_beams=num_beams, num_return_sequences=num 
_return_sequences, temperature=1.5', 'vocabulary_size': 'tokenizer = PegasusTokenizer.from_pretraine 
d(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\ 
nnum_beams = 10\nnum_return_sequences = 10'}]                                                        
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df85d-26caa4981f0e21762f4ff990)

Entry Not Found for url: https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english/resolve/main/README.md. 

#####################cl-tohoku/bert-base-japanese########################

-------------------- datasets --------------------
Document 1:

"The model is trained on Japanese Wikipedia as of September 1, 2019.", "[WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.", "The text files used for the training are 2.6GB in size, consisting of approximately 17M sentences."
-------------------- license --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
------------------------------
Document 2:

license: cc-by-sa-4.0
-------------------- github --------------------
Document 1:

"Creative Commons Attribution-ShareAlike 3.0"
------------------------------
Document 2:

"WikiExtractor https://github.com/attardi/wikiextractor"
------------------------------
Document 3:

[BERT](https://github.com/google-research/bert), [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0)
-------------------- paper --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
------------------------------
Document 2:

TensorFlow Research Cloud
------------------------------
Document 3:

"The codes for the pretraining are available at [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0)."
-------------------- upstream_model --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
-------------------- parameter_count --------------------
Document 1:

parameter_count=512*256*1000000
------------------------------
Document 2:

parameter_count NO_OUTPUT
------------------------------
Document 3:

parameter_count 12 layers, 768 dimensions of hidden states, 12 attention heads
-------------------- hyper_parameters --------------------
Document 1:

"512 tokens per instance, 256 instances per batch, and 1M training steps."
------------------------------
Document 2:

"12 layers, 768 dimensions of hidden states, and 12 attention heads."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Cloud TPUs
------------------------------
Document 2:

"512 tokens per instance, 256 instances per batch, and 1M training steps."
------------------------------
Document 3:

"WikiExtractor" and "2.6GB in size"
-------------------- limitation_and_bias --------------------
Document 1:

"The model is trained on Japanese Wikipedia as of September 1, 2019.", "[WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.", "The text files used for the training are 2.6GB in size, consisting of approximately 17M sentences."
-------------------- demo --------------------
Document 1:

"Creative Commons Attribution-ShareAlike 3.0"
------------------------------
Document 2:

"WikiExtractor" and "2.6GB in size, consisting of approximately 17M sentences."
------------------------------
Document 3:

TensorFlow Research Cloud program.
-------------------- input_format --------------------
Document 1:

"plain texts from a dump file of Wikipedia articles" "2.6GB in size, consisting of approximately 17M sentences"
------------------------------
Document 2:

word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.
-------------------- output_format --------------------
Document 1:

word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.
-------------------- input_token_limit --------------------
Document 1:

input_token_limit: 512
-------------------- vocabulary_size --------------------
Document 1:

The vocabulary size is 32000.

[{'datasets': ['Japanese Wikipedia'], 'license': 'Creative Commons Attribution-ShareAlike 3.0', 'gi 
thub': 'https://github.com/attardi/wikiextractor', 'paper': '', 'upstream_model': '', 'parameter_cou 
nt': '512*256*1000000', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'o 
ptimizer': ''}, 'evaluation': [], 'hardware': 'Cloud TPUs', 'limitation_and_bias': '', 'demo': '', ' 
input_format': '', 'output_format': '', 'input_token_limit': '512', 'vocabulary_size': '32000'}]     

#####################sentence-transformers/multi-qa-MiniLM-L6-cos-v1########################

-------------------- datasets --------------------
Document 1:

- WikiAnswers Duplicate question pairs from WikiAnswers (77,427,422)
- PAQ Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia (64,371,441)
- Stack Exchange (Title, Body) pairs from all StackExchanges (25,316,456)
- Stack Exchange (Title, Answer) pairs from all StackExchanges (21,396,559)
- MS MARCO Triplets (query, answer, hard_negative) for 500k queries from Bing search engine (17,579,773)
- GOOAQ: Open Question Answering with Diverse Answer Types (query, answer) pairs for 3M Google queries and Google featured snippet (3,012,496)
- Amazon-QA (Question, Answer) pairs from Amazon product pages (2,448,839)
- Yahoo Answers (Title, Answer) pairs from Yahoo Answers (1,198,260)
- Yahoo Answers (Question, Answer) pairs from Yahoo Answers (681,164)
- Yahoo Answers (Title, Question) pairs from Yahoo Answers (659,896)
- SearchQA (Question, Answer) pairs for 140k questions
------------------------------
Document 2:

"Train the Best Sentence Embedding Model Ever with 1B Training Pairs" and "7 TPUs v3-8"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

`train_script.py`
------------------------------
Document 2:

- sentence-transformers
- feature-extraction
- sentence-similarity
- flax-sentence-embeddings/stackexchange_xml
- ms_marco
- gooaq
- yahoo_answers_topics
- search_qa
- eli5
- natural_questions
- trivia_qa
- embedding-data/QQP
- embedding-data/PAQ_pairs
- embedding-data/Amazon-QA
- embedding-data/WikiAnswers
- pipeline_tag: sentence-similarity
-------------------- paper --------------------
Document 1:

"It has been trained on 215M (question, answer) pairs from diverse sources." NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"This is a [sentence-transformers](https://www.SBERT.net) model" and "It has been trained on 215M (question, answer) pairs from diverse sources."
-------------------- parameter_count --------------------
Document 1:

"214,988,242" parameter_count
-------------------- hyper_parameters --------------------
Document 1:

| Setting | Value |
| --- | :---: |
| Dimensions | 384 |
| Pooling-Method | Mean pooling |
| Suitable score functions | dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance |
------------------------------
Document 2:

We use the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model. We use the concatenation from multiple datasets to fine-tune our model. The model was trained with [MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss) using Mean-pooling, cosine-similarity as similarity function, and a scale of 20.
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text."
-------------------- demo --------------------
Document 1:

"It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages." Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.
------------------------------
Document 2:

sentence-transformers, semantic search, [SBERT.net - Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)
------------------------------
Document 3:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used."
-------------------- input_format --------------------
Document 1:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1." input_format: normalized embeddings with length 1
-------------------- output_format --------------------
Document 1:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1."
-------------------- input_token_limit --------------------
Document 1:

Note that there is a limit of 512 word pieces: Text longer than that will be truncated. input_token_limit: 512
-------------------- vocabulary_size --------------------


[{'datasets': ['WikiAnswers', 'PAQ', 'Stack Exchange', 'MS MARCO', 'GOOAQ', 'Amazon-QA', 'Yahoo Ans 
wers', 'SearchQA'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count 
': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo':  
'', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]        

#####################EleutherAI/gpt-neo-1.3B########################

-------------------- datasets --------------------
Document 1:

datasets, Pile, EleutherAI
------------------------------
Document 2:

The Pile: An 800GB Dataset of Diverse Text for Language Modeling
------------------------------
Document 3:

datasets, Pile, 380 billion tokens, 362,000 steps, masked autoregressive language model, cross-entropy loss
-------------------- license --------------------
Document 1:

publisher = {Zenodo}, version = {1.0}, doi = {10.5281/zenodo.5297715}, url = {https://doi.org/10.5281/zenodo.5297715}
------------------------------
Document 2:

license: mit
-------------------- github --------------------
Document 1:

url = {https://doi.org/10.5281/zenodo.5297715}
-------------------- paper --------------------
Document 1:

```bibtex
@article{gao2020pile,
title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
journal={arXiv preprint arXiv:2101.00027},
year={2020}
}
```
------------------------------
Document 2:

"masked autoregressive language model, using cross-entropy loss"
-------------------- upstream_model --------------------
Document 1:

upstream_model GPT-3
-------------------- parameter_count --------------------
Document 1:

parameter_count 362,000
------------------------------
Document 2:

GPT-Neo 1.3B, GPT-2 1.5B, GPT-Neo 2.7B, GPT-3 Ada
-------------------- hyper_parameters --------------------
Document 1:

"masked autoregressive language model, using cross-entropy loss"
-------------------- evaluation --------------------
Document 1:

"This model was trained on the Pile for 380 billion tokens over 362,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss."
------------------------------
Document 2:

| Model and Size   | MathQA     | PubMedQA   | Piqa        |
| ---------------- | ---------- | ---------- | ----------- |
| **GPT-Neo 1.3B** | **24.05%** | **54.40%** | **71.11%**  |
| GPT-2 1.5B       | 23.64%     | 58.33%     | 70.78%      |
| GPT-Neo 2.7B     | 24.72%     | 57.54%     | 72.14%      |
| GPT-3 Ada        | 24.29%     | 52.80%     | 68.88%      |
-------------------- hardware --------------------
Document 1:

Pile
------------------------------
Document 2:

Pile
------------------------------
Document 3:

GPT-Neo 1.3B
-------------------- limitation_and_bias --------------------
Document 1:

"GPT-Neo was trained as an autoregressive language model...GPT-Neo was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language...As with all language models, it is hard to predict in advance how GPT-Neo will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results."
-------------------- demo --------------------
Document 1:

"generating texts from a prompt"
-------------------- input_format --------------------
Document 1:

GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, The Pile: An 800GB Dataset of Diverse Text for Language Modeling
-------------------- output_format --------------------
Document 1:

output_format: [{'generated_text': 'EleutherAI has made a commitment to create new software packages for each of its major clients and has'}]
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

GPT-Neo 1.3B, GPT-2 1.5B, GPT-Neo 2.7B, GPT-3 Ada

[{'datasets': ['Pile', 'EleutherAI'], 'license': 'mit', 'github': 'https://github.com/EleutherAI',  
'paper': 'https://arxiv.org/abs/2101.00027', 'upstream_model': 'GPT-3', 'parameter_count': '362,000' 
, 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evalu 
ation': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_f 
ormat': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                    

#####################madhurjindal/autonlp-Gibberish-Detector-492513457########################

-------------------- datasets --------------------
Document 1:

datasets: - madhurjindal/autonlp-data-Gibberish-Detector
-------------------- license --------------------
Document 1:

"natural language processing", "chatbot systems", "spam filtering", "language-based security measures"
-------------------- github --------------------
Document 1:

madhurjindal/autonlp-data-Gibberish-Detector, github
-------------------- paper --------------------
Document 1:

Model ID: 492513457
-------------------- upstream_model --------------------
Document 1:

upstream_model
------------------------------
Document 2:

"madhurjindal/autonlp-Gibberish-Detector-492513457"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

- Loss: 0.07609463483095169
- Accuracy: 0.9735624586913417
- Macro F1: 0.9736173135739408
- Micro F1: 0.9735624586913417
- Weighted F1: 0.9736173135739408
- Macro Precision: 0.9737771415197378
- Micro Precision: 0.9735624586913417
- Weighted Precision: 0.9737771415197378
- Macro Recall: 0.9735624586913417
- Micro Recall: 0.9735624586913417
- Weighted Recall: 0.9735624586913417
-------------------- evaluation --------------------
Document 1:

- Loss: 0.07609463483095169
- Accuracy: 0.9735624586913417
- Macro F1: 0.9736173135739408
- Micro F1: 0.9735624586913417
- Weighted F1: 0.9736173135739408
- Macro Precision: 0.9737771415197378
- Micro Precision: 0.9735624586913417
- Weighted Precision: 0.9737771415197378
- Macro Recall: 0.9735624586913417
- Micro Recall: 0.9735624586913417
- Weighted Recall: 0.9735624586913417
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

You can use cURL to access this model:  
```
$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love Machine Learning!"}' https://api-inference.huggingface.co/models/madhurjindal/autonlp-Gibberish-Detector-492513457
```  
Or Python API:  
```
import torch
import torch.nn.functional as F
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("madhurjindal/autonlp-Gibberish-Detector-492513457", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained("madhurjindal/autonlp-Gibberish-Detector-492513457", use_auth_token=True)

inputs = tokenizer("I love Machine Learning!", return_tensors="pt")

output
-------------------- input_format --------------------
Document 1:

"Content-Type: application/json" and "return_tensors="pt""
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['madhurjindal/autonlp-data-Gibberish-Detector'], 'license': '"natural language proce 
ssing", "chatbot systems", "spam filtering", "language-based security measures"', 'github': 'madhurj 
indal/autonlp-data-Gibberish-Detector, github', 'paper': 'Model ID: 492513457', 'upstream_model': 'u 
pstream_model', 'parameter_count': '', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learni 
ng_rate': '', 'optimizer': ''}], 'evaluation': [{'test': '- Loss: 0.07609463483095169', 'result': 0. 
9735624586913417}, {'test': '- Accuracy: 0.9735624586913417', 'result': 0.9736173135739408}, {'test' 
: '- Macro F1: 0.9736173135739408', 'result': 0.9735624586913417}, {'test': '- Micro F1: 0.973562458 
6913417', 'result': 0.9736173135739408}, {'test': '- Weighted F1: 0.9736173135739408', 'result': 0.9 
737771415197378}, {'test': '- Macro Precision: 0.9737771415197378', 'result': 0.9735624586913417}, { 
'test': '- Micro Precision: 0.9735624586913417', 'result': 0.9737771415197378}, {'test': '- Weighted 
 Precision: 0.9737771415197378', 'result': 0.9735624586913417}, {'test': '- Macro Recall: 0.97356245 
86913417', 'result': 0.9735624586913417}, {'test': '- Micro Recall: 0.9735624586913417', 'result': 0 
.9735624586913417}, {'test': '- Weighted Recall: 0.9735624586913417', 'result': 0.9735624586913417}] 
, 'hardware': '', 'limitation_and_bias': '', 'demo': 'You can use cURL to access this model:  \n```\ 
n$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d \'{"i 
nputs": "I love Machine Learning!"}\' https://api-inference.huggingface.co/models/madhurjindal/auton 
lp-Gibberish-Detector-492513457\n```  \nOr Python API:  \n```\nimport torch\nimport torch.nn.functio 
nal as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = Auto 
ModelForSequenceClassification.from_pretrained("madhurjindal/autonlp-Gibberish-Detector-492513457",  
use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained("madhurjindal/autonlp-Gibberish-De 
tector-492513457", use_auth_token=True)\n\ninputs = tokenizer("I love Machine Learning!", return_ten 
sors="pt")\n\noutput', 'input_format': '"Content-Type: application/json" and "return_tensors="pt""', 
 'output_format': ''}]                                                                               

#####################deepset/roberta-large-squad2########################

-------------------- datasets --------------------
Document 1:

- squad_v2
- squad
- adversarial_qa
- squad_adversarial
- squadshifts amazon
- squadshifts new_wiki
- squadshifts nyt
- squadshifts reddit
------------------------------
Document 2:

roberta-large, SQuAD2.0, question-answer pairs
------------------------------
Document 3:

"SQuAD 2.0" and "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)"
-------------------- license --------------------
Document 1:

license: cc-by-4.0
------------------------------
Document 2:

"We also have a <strong><a class="h-7" href="https://haystack.deepset.ai/community">Discord community open to everyone!</a></strong></p>  
[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)  
By the way: [we're hiring!](http://www.deepset.ai/jobs)
-------------------- github --------------------
Document 1:

<strong><a href="https://github.com/deepset-ai/haystack">GitHub</a></strong> and <strong><a class="h-7" href="https://haystack.deepset.ai/community">Discord community open to everyone!</a></strong> and [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions)
------------------------------
Document 2:

- [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")]([https://huggingface.co/deepset/tinyroberta-squad2)
- [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert)
- [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
-------------------- paper --------------------
Document 1:

roberta-large, SQuAD2.0
------------------------------
Document 2:

"Extractive QA" "SQuAD 2.0" "See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)" "4x Tesla v100"
-------------------- upstream_model --------------------
Document 1:

upstream_model: NO_OUTPUT
------------------------------
Document 2:

roberta-large, SQuAD2.0
------------------------------
Document 3:

"deepset/roberta-large-squad2"
-------------------- parameter_count --------------------
Document 1:

base_model: roberta-large
------------------------------
Document 2:

"roberta-large" "Extractive QA" "SQuAD 2.0" "4x Tesla v100"
-------------------- hyper_parameters --------------------
Document 1:

"roberta-large", "SQuAD2.0"
------------------------------
Document 2:

roberta-large, English, Extractive QA, SQuAD 2.0, See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system), 4x Tesla v100
-------------------- evaluation --------------------
Document 1:

- type: exact_match
value: 85.168
name: Exact Match
- type: f1
value: 88.349
name: F1
- type: exact_match
value: 87.162
name: Exact Match
- type: f1
value: 93.603
name: F1
- type: exact_match
value: 35.9
name: Exact Match
- type: f1
value: 48.923
name: F1
- type: exact_match
value: 81.142
name: Exact Match
- type: f1
value: 87.099
name: F1
- type: exact_match
value: 72.453
name: Exact Match
- type: f1
value: 86.325
name: F1
- type: exact_match
value: 82.338
name: Exact Match
- type: f1
value: 91.974
name: F1
- type: exact_match
value: 84.352
name: Exact Match
- type: f1
value: 92.645
name: F1
------------------------------
Document 2:

"This is the [roberta-large](https://huggingface.co/roberta-large) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering."
------------------------------
Document 3:

"roberta-large", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"
-------------------- hardware --------------------
Document 1:

roberta-large, SQuAD2.0
------------------------------
Document 2:

4x Tesla v100
-------------------- limitation_and_bias --------------------
Document 1:

roberta-large, English, Extractive QA, SQuAD 2.0, See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system), 4x Tesla v100
------------------------------
Document 2:

"roberta-large", "SQuAD2.0", "question-answer pairs", "unanswerable questions", "Question Answering"
-------------------- demo --------------------
Document 1:

"roberta-large", "English", "Extractive QA", "SQuAD 2.0", "SQuAD 2.0", "[an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system)", "4x Tesla v100"
------------------------------
Document 2:

[deepset](http://deepset.ai/), [Haystack](https://haystack.deepset.ai/), [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")]([https://huggingface.co/deepset/tinyroberta-squad2), [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
------------------------------
Document 3:

"For a complete example of ``roberta-large-squad2`` being used for  Question Answering, check out the [Tutorials in Haystack Documentation](https://haystack.deepset.ai/tutorials/first-qa-system)"
-------------------- input_format --------------------
Document 1:

squad_v2, plain_text, adversarialQA, AddOneSent, amazon, new_wiki, nyt, reddit
------------------------------
Document 2:

input_format: SQuAD 2.0
------------------------------
Document 3:

"model_name_or_path="deepset/roberta-large-squad2"", "tokenizer="deepset/roberta-large-squad2""
-------------------- output_format --------------------
Document 1:

output_format
-------------------- input_token_limit --------------------
Document 1:

"roberta-large" "Extractive QA" "SQuAD 2.0" "4x Tesla v100"
-------------------- vocabulary_size --------------------
Document 1:

"roberta-large", "SQuAD2.0"
------------------------------
Document 2:

roberta-large, English, Extractive QA, SQuAD 2.0, See [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system), 4x Tesla v100
------------------------------
Document 3:

"roberta-large-squad2"

[{'datasets': ['squad_v2', 'squad', 'adversarial_qa', 'squad_adversarial', 'squadshifts amazon', 's 
quadshifts new_wiki', 'squadshifts nyt', 'squadshifts reddit'], 'license': 'cc-by-4.0', 'github': 'h 
ttps://github.com/deepset-ai/haystack', 'paper': 'roberta-large, SQuAD2.0', 'upstream_model': 'NO_OU 
TPUT', 'parameter_count': 'base_model: roberta-large', 'hyper_parameters': [{'epochs': 'roberta-larg 
e', 'batch_size': 'SQuAD2.0', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': 'exact 
_match', 'result': 85.168}, {'test': 'f1', 'result': 88.349}, {'test': 'exact_match', 'result': 87.1 
62}, {'test': 'f1', 'result': 93.603}, {'test': 'exact_match', 'result': 35.9}, {'test': 'f1', 'resu 
lt': 48.923}, {'test': 'exact_match', 'result': 81.142}, {'test': 'f1', 'result': 87.099}, {'test':  
'exact_match', 'result': 72.453}, {'test': 'f1', 'result': 86.325}, {'test': 'exact_match', 'result' 
: 82.338}, {'test': 'f1', 'result': 91.974}, {'test': 'exact_match', 'result': 84.352}, {'test': 'f1 
', 'result': 92.645}], 'hardware': 'roberta-large, SQuAD2.0', 'limitation_and_bias': 'roberta-large, 
 English, Extractive QA, SQuAD 2.0, See [an example QA pipeline on Haystack](https://haystack.deepse 
t.ai/tutorials/first-qa-system), 4x Tesla v100', 'demo': 'roberta-large, English, Extractive QA, SQu 
AD 2.0, SQuAD 2.0, [an example QA pipeline on Haystack](https://haystack.deepset.ai/tutorials/first- 
qa-system), 4x Tesla v100', 'input_format': 'squad_v2, plain_text, adversarialQA, AddOneSent, amazon 
, new_wiki, nyt, reddit', 'output_format': 'output_format', 'input_token_limit': 'roberta-large, Ext 
ractive QA, SQuAD 2.0, 4x Tesla v100', 'vocabulary_size': 'roberta-large, SQuAD2.0'}, {'datasets': [ 
'roberta-large', 'SQuAD2.0', 'question-answer pairs'], 'github': 'https://github.com/deepset-ai/hays 
tack', 'paper': 'Extractive QA, SQuAD 2.0, See [an example QA pipeline on Haystack](https://haystack 
.deepset.ai/tutorials/first-qa-system)', 'upstream_model': 'roberta-large, SQuAD2.0', 'parameter_cou 
nt': 'roberta-large', 'hyper_parameters': [{'epochs': 'roberta-large', 'batch_size': 'English', 'lea 
rning_rate': 'Extractive QA', 'optimizer': 'SQuAD 2.0'}], 'evaluation': [{'test': "This is the rober 
ta-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, i 
ncluding unanswerable questions, for the task of Question Answering."}], 'hardware': '4x Tesla v100' 
, 'limitation_and_bias': 'roberta-large, SQuAD2.0, question-answer pairs, unanswerable questions, Qu 
estion Answering', 'demo': '[deepset](http://deepset.ai/), [Haystack](https://haystack.deepset.ai/), 
 [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")](https://huggingface.co/deepset/tinyrober 
ta-squad2), [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert), [GermanQuA 
D and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https: 
//deepset.ai/germanquad)', 'input_format': 'input_format: SQuAD 2.0', 'output_format': 'output_forma 
t', 'input_token_limit': 'roberta-large, English, Extractive QA, SQuAD 2.0, See [an example QA pipel 
ine on Haystack](https://haystack.deepset.ai/tutorials/first-qa-system), 4x Tesla v100', 'vocabulary 
_size': 'roberta-large, English, Extractive QA, SQuAD 2.0, See [an example QA pipeline on Haystack]( 
https://haystack.deepset.ai/tutorials/first-qa-system), 4x Tesla v100'}, {'paper': 'roberta-large, S 
QuAD2.0', 'upstream_model': 'deepset/roberta-large-squad2'}]                                         
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653df93d-1b6e7822585627200ac50c51)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Data2VecTextModel/resolve/main/README.md. 

#####################facebook/esm2_t6_8M_UR50D########################

-------------------- datasets --------------------
Document 1:

"ESM-2 is a state-of-the-art protein model trained on a masked language modelling objective. For detailed information on the model architecture and training data, please refer to the accompanying paper"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

[esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D), [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D), [esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D), [esm2_t30_150M_UR50D](https://huggingface.co/facebook/esm2_t30_150M_UR50D), [esm2_t12_35M_UR50D](https://huggingface.co/facebook/esm2_t12_35M_UR50D), [esm2_t6_8M_UR50D](https://huggingface.co/facebook/esm2_t6_8M_UR50D)
-------------------- paper --------------------
Document 1:

[accompanying paper](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2)
------------------------------
Document 2:

MQIFVKTLTGKTITLEVEPS<mask>TIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"Num parameters | 15B | 3B | 650M | 150M | 35M | 8M"
-------------------- hyper_parameters --------------------
Document 1:

| Checkpoint name | Num layers | Num parameters |
|------------------------------|----|----------|
| [esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D) | 48 | 15B     |
| [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D) | 36 | 3B      |
| [esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D) | 33 | 650M    |
| [esm2_t30_150M_UR50D](https://huggingface.co/facebook/esm2_t30_150M_UR50D) | 30 | 150M    |
| [esm2_t12_35M_UR50D](https://huggingface.co/facebook/esm2_t12_35M_UR50D) | 12 | 35M
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"Several ESM-2 checkpoints are available in the Hub with varying sizes. Larger sizes generally have somewhat better accuracy, but require much more memory and time to train:  | Checkpoint name | Num layers | Num parameters | |------------------------------|----|----------| | [esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D) | 48 | 15B     | | [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D) | 36 | 3B      | | [esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D) | 33 | 650M    | | [esm2_t30_150M_UR50D](https://huggingface.co/facebook/esm2_t30_150M_UR50D) | 30 | 150M    | | [esm2_t12_35M_UR50D](https://
-------------------- limitation_and_bias --------------------
Document 1:

"ESM-2 is a state-of-the-art protein model trained on a masked language modelling objective. It is suitable for fine-tuning on a wide range of tasks that take protein sequences as input. For detailed information on the model architecture and training data, please refer to the [accompanying paper](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2). Several ESM-2 checkpoints are available in the Hub with varying sizes. Larger sizes generally have somewhat better accuracy, but require much more memory and time to train: | Checkpoint name | Num layers | Num parameters | |------------------------------|----|----------| | [esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D) | 48 | 15B     | | [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D) | 36 | 3B      | | [esm2_t33_650M_UR50D
-------------------- demo --------------------
Document 1:

[accompanying paper](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2), [PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb), [TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb), [esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D), [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D), [esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D), [esm2_t30_150M_UR50D
-------------------- input_format --------------------
Document 1:

"protein sequences as input"
------------------------------
Document 2:

license: mit, widget: - text: MQIFVKTLTGKTITLEVEPS<mask>TIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

MQIFVKTLTGKTITLEVEPS<mask>TIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG, input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

"Num parameters"

[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]         

#####################runwayml/stable-diffusion-inpainting########################

-------------------- datasets --------------------
Document 1:

The model was trained mainly with English captions and will not work as well in other languages. The model was trained on a large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material and is not fit for product use without additional safety mechanisms and considerations. The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.
------------------------------
Document 2:

LAION-2B(en) (https://laion.ai/blog/laion-5b/)
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"🧨Diffusers library](https://github.com/huggingface/diffusers) and the [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion)
------------------------------
Document 2:

[sd-v1-5-inpainting.ckpt](https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/sd-v1-5-inpainting.ckpt), [here](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion)
-------------------- paper --------------------
Document 1:

"Research on generative models."
------------------------------
Document 2:

"High-Resolution Image Synthesis With Latent Diffusion Models"
------------------------------
Document 3:

LAION-2B(en), English descriptions, white and western cultures, English-language prompts
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"50 PLMS sampling steps" and "10000 random prompts from the COCO2017 validation set"
------------------------------
Document 2:

- 32 x 8 x A100 GPUs
- AdamW
- Gradient Accumulations: 2
- Batch: 32 x 8 x 2 x 4 = 2048
- Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- hyper_parameters --------------------
Document 1:

"50 PLMS sampling steps" and "10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution"
------------------------------
Document 2:

- Autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4
- Text prompts are encoded through a ViT-L/14 text-encoder.
- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.
- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.
- 32 x 8 x A100 GPUs
- Optimizer: AdamW
- Gradient Accumulations: 2
- Batch: 32 x 8 x 2 x 4 = 2048
- Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- evaluation --------------------
Document 1:

Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: ![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-1-to-v1-5.png) Evaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores.
-------------------- hardware --------------------
Document 1:

- 32 x 8 x A100 GPUs
- AdamW
- 32 x 8 x 2 x 4 = 2048
- warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- limitation_and_bias --------------------
Document 1:

"primarily limited to English descriptions", "white and western cultures are often set as the default", "ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts"
------------------------------
Document 2:

- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.
------------------------------
Document 3:

- Probing and understanding the limitations and biases of generative models.
-------------------- demo --------------------
Document 1:

[🧨Diffusers library](https://github.com/huggingface/diffusers) and the [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion)
------------------------------
Document 2:

"Possible research areas and tasks include - Safe deployment of models which have the potential to generate harmful content. - Probing and understanding the limitations and biases of generative models. - Generation of artworks and use in design and other artistic processes. - Applications in educational or creative tools. - Research on generative models."
-------------------- input_format --------------------
Document 1:

No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data. The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.

NO_OUTPUT
------------------------------
Document 2:

- LAION-2B (en) and subsets thereof
- autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4
- Text prompts are encoded through a ViT-L/14 text-encoder
- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention
- `sd-v1-1.ckpt`: 237k steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en). 194k steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).
- `sd-v1-2.ckpt`: Resumed from `sd-v1-1.ckpt`. 515k steps at resolution `512x
-------------------- output_format --------------------
Document 1:

LAION-2B(en), English descriptions, white and western cultures, English-language prompts.

[{'datasets': ['LAION-5B'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parame 
ter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 
 'demo': '', 'input_format': '', 'output_format': ''}]                                               

#####################h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3########################

-------------------- datasets --------------------
Document 1:

datasets, [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio), [tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b), [OpenAssistant/oasst1](https://github.com/h2oai/h2o-llmstudio/blob/1935d84d9caafed3ee686ad2733eb02d2abfce57/app_utils/utils.py#LL1896C5-L1896C28)
------------------------------
Document 2:

datasets: - OpenAssistant/oasst1
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[cfg.yaml](cfg.yaml), [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio)
------------------------------
Document 2:

"OpenAssistant/oasst1"
-------------------- paper --------------------
Document 1:

"Base model: [tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)" and "Dataset preparation: [OpenAssistant/oasst1](https://github.com/h2oai/h2o-llmstudio/blob/1935d84d9caafed3ee686ad2733eb02d2abfce57/app_utils/utils.py#LL1896C5-L1896C28)"
-------------------- upstream_model --------------------
Document 1:

"Base model: [tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)"
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

parameter_count: tiiuae/falcon-7b
-------------------- hyper_parameters --------------------
Document 1:

"Base model: [tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)" "Dataset preparation: [OpenAssistant/oasst1](https://github.com/h2oai/h2o-llmstudio/blob/1935d84d9caafed3ee686ad2733eb02d2abfce57/app_utils/utils.py#LL1896C5-L1896C28)"
------------------------------
Document 2:

min_new_tokens=2,
max_new_tokens=1024,
do_sample=False,
num_beams=1,
temperature=float(0.3),
repetition_penalty=float(1.2),
renormalize_logits=True
-------------------- evaluation --------------------
Document 1:

"Base model: [tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)", "Dataset preparation: [OpenAssistant/oasst1](https://github.com/h2oai/h2o-llmstudio/blob/1935d84d9caafed3ee686ad2733eb02d2abfce57/app_utils/utils.py#LL1896C5-L1896C28)"
-------------------- hardware --------------------
Document 1:

H2O LLM Studio
------------------------------
Document 2:

H2O LLM Studio, tiiuae/falcon-7b, OpenAssistant/oasst1
-------------------- limitation_and_bias --------------------
Document 1:

- Biases and Offensiveness: The large language model is trained on a diverse range of internet text data, which may contain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknowledge and accept that the generated content may sometimes exhibit biases or produce content that is offensive or inappropriate. The developers of this repository do not endorse, support, or promote any such content or viewpoints. - Limitations: The large language model is an AI-based tool and not a human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibility to critically evaluate the generated content and use it at their discretion.
------------------------------
Document 2:

limitation_and_bias
-------------------- demo --------------------
Document 1:

cfg.yaml, H2O LLM Studio, https://github.com/h2oai/h2o-llmstudio
------------------------------
Document 2:

"inference: false"
-------------------- input_format --------------------
Document 1:

"language: - en license: apache-2.0 library_name: transformers tags: - gpt - llm - large language model - h2o-llmstudio datasets: - OpenAssistant/oasst1 inference: false thumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico"
------------------------------
Document 2:

"use_fast=False, padding_side="left", trust_remote_code=True,"
-------------------- output_format --------------------
Document 1:

output_format
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"library_name: transformers", "tags: - gpt - llm - large language model - h2o-llmstudio", "datasets: - OpenAssistant/oasst1", "inference: false", "thumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs/clientlib-site/resources/images/favicon.ico"

[{'datasets': ['H2O LLM Studio'], 'license': 'apache-2.0', 'github': 'https://github.com/h2oai/h2o- 
llmstudio', 'paper': '', 'upstream_model': 'tiiuae/falcon-7b', 'parameter_count': '', 'hyper_paramet 
ers': [], 'evaluation': [], 'hardware': 'H2O LLM Studio', 'limitation_and_bias': "- Biases and Offen 
siveness: The large language model is trained on a diverse range of internet text data, which may co 
ntain biased, racist, offensive, or otherwise inappropriate content. By using this model, you acknow 
ledge and accept that the generated content may sometimes exhibit biases or produce content that is  
offensive or inappropriate. The developers of this repository do not endorse, support, or promote an 
y such content or viewpoints. - Limitations: The large language model is an AI-based tool and not a  
human. It may produce incorrect, nonsensical, or irrelevant responses. It is the user's responsibili 
ty to critically evaluate the generated content and use it at their discretion.", 'demo': 'cfg.yaml, 
 H2O LLM Studio, https://github.com/h2oai/h2o-llmstudio', 'input_format': '"language: - en license:  
apache-2.0 library_name: transformers tags: - gpt - llm - large language model - h2o-llmstudio datas 
ets: - OpenAssistant/oasst1 inference: false thumbnail: https://h2o.ai/etc.clientlibs/h2o/clientlibs 
/clientlib-site/resources/images/favicon.ico"', 'output_format': '', 'input_token_limit': '', 'vocab 
ulary_size': ''}]                                                                                    

#####################flair/ner-english-large########################

-------------------- datasets --------------------
Document 1:

"title={FLERT: Document-Level Features for Named Entity Recognition}"
------------------------------
Document 2:

CONLL_03(), tag_type = 'ner', TransformerWordEmbeddings(), SequenceTagger(), ModelTrainer(), OneCycleLR()
------------------------------
Document 3:

datasets: - conll2003
-------------------- license --------------------
Document 1:

"title={FLERT: Document-Level Features for Named Entity Recognition}, author={Stefan Schweter and Alan Akbik}, year={2020}, eprint={2011.06993}, archivePrefix={arXiv}, primaryClass={cs.CL}"
------------------------------
Document 2:

"license"
-------------------- github --------------------
Document 1:

[here](https://github.com/flairNLP/flair/issues/)
------------------------------
Document 2:

"github"
-------------------- paper --------------------
Document 1:

```title={FLERT: Document-Level Features for Named Entity Recognition}, author={Stefan Schweter and Alan Akbik}, year={2020}, eprint={2011.06993}, archivePrefix={arXiv}, primaryClass={cs.CL}```
------------------------------
Document 2:

FLERT (https://arxiv.org/pdf/2011.06993v1.pdf/)
-------------------- upstream_model --------------------
Document 1:

xlm-roberta-large
------------------------------
Document 2:

"Based on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/)."
------------------------------
Document 3:

"title={FLERT: Document-Level Features for Named Entity Recognition}"
upstream_model: FLERT
-------------------- parameter_count --------------------
Document 1:

"SequenceTagger(hidden_size=256,"
------------------------------
Document 2:

"F1-Score: 94,36" "Predicts 4 tags: PER, LOC, ORG, MISC" "Based on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/)."
-------------------- hyper_parameters --------------------
Document 1:

model='xlm-roberta-large',
layers="-1",
subtoken_pooling="first",
fine_tune=True,
use_context=True,
hidden_size=256,
learning_rate=5.0e-6,
mini_batch_size=4,
mini_batch_chunk_size=1,
max_epochs=20,
weight_decay=0.
------------------------------
Document 2:

"Based on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/)."
-------------------- evaluation --------------------
Document 1:

F1-Score: **94,36** (corrected CoNLL-03) | **tag** | **meaning** | PER | person name | LOC | location name | ORG | organization name | MISC | other name | Based on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/).
-------------------- hardware --------------------
Document 1:

`model='xlm-roberta-large'`
-------------------- limitation_and_bias --------------------
Document 1:

Based on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/).
-------------------- demo --------------------
Document 1:

"Find a form of demo for the model"
------------------------------
Document 2:

language: en, tags: - flair - token-classification - sequence-tagger-model, datasets: - conll2003, widget: - text: George Washington went to Washington
------------------------------
Document 3:

[here](https://github.com/flairNLP/flair/issues/)
-------------------- input_format --------------------
Document 1:

"model='xlm-roberta-large'"
input_format: xlm-roberta-large
------------------------------
Document 2:

- conll2003
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['conll2003'], 'license': 'license', 'github': 'github', 'paper': 'FLERT (https://arx 
iv.org/pdf/2011.06993v1.pdf/)', 'upstream_model': 'FLERT', 'parameter_count': 'SequenceTagger(hidden 
_size=256,', 'hyper_parameters': {'epochs': '20', 'batch_size': '4', 'learning_rate': '5.0e-6', 'opt 
imizer': ''}, 'evaluation': [{'test': 'F1-Score', 'result': 94.36}], 'hardware': "`model='xlm-robert 
a-large'`", 'limitation_and_bias': 'Based on document-level XLM-R embeddings and [FLERT](https://arx 
iv.org/pdf/2011.06993v1.pdf/).', 'demo': 'Find a form of demo for the model', 'input_format': 'xlm-r 
oberta-large', 'output_format': ''}]                                                                 

#####################roberta-base-openai-detector########################

-------------------- datasets --------------------
Document 1:

"associated paper" https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf
------------------------------
Document 2:

"RoBERTa base model card" (https://huggingface.co/roberta-base), "1.5B GPT-2 model" (https://github.com/openai/gpt-2-output-dataset), "WebText, the dataset we used to train the GPT-2 model" and "associated paper" (https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf)
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"See the [associated paper](https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf) for further details on the modeling architecture and training details."
------------------------------
Document 2:

"The following evaluation information is extracted from the [associated paper](https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf).", "See the [associated paper](https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf), Figure 1 (on page 14) and Figure 2 (on page 16) for full results."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"temperature, Top-K, and nucleus sampling ([Holtzman et al., 2019](https://arxiv.org/abs/1904.09751). Nucleus sampling outputs proved most difficult to correctly classify"
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

testing 510-token test examples comprised of 5,000 samples from the WebText dataset and 5,000 samples generated by a GPT-2 model, which were not used during the training., Our classifier is able to detect 1.5 billion parameter GPT-2-generated text with approximately 95% accuracy...The model’s accuracy depends on sampling methods used when generating outputs, like temperature, Top-K, and nucleus sampling ([Holtzman et al., 2019](https://arxiv.org/abs/1904.09751). Nucleus sampling outputs proved most difficult to correctly classify, but a detector trained using nucleus sampling transfers well across other sampling methods. As seen in Figure 1 [in the paper], we found consistently high accuracy when trained on nucleus sampling.
-------------------- hardware --------------------
Document 1:

"RoBERTa base (see the [RoBERTa base model card](https://huggingface.co/roberta-base) for more details on the RoBERTa base training data)"
"We based a sequence classifier on RoBERTaBASE (125 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model."
-------------------- limitation_and_bias --------------------
Document 1:

In their associated paper, the model developers discuss the risk that the model may be used by bad actors to develop capabilities for evading detection. In a related blog post, the model developers also discuss the limitations of automated methods for detecting synthetic text and the need to pair automated detection tools with other, non-automated approaches. Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). Predictions generated by RoBERTa base and GPT-2 1.5B (which this model is built/fine-tuned on) can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. The developers of this model discuss these issues further in their paper.
------------------------------
Document 2:

[Risks, Limitations and Biases](#risks-limitations-and-biases)
-------------------- demo --------------------
Document 1:

See the [associated paper](https://d4mucfpksywv.cloudfront.net/papers/GPT_2_Report.pdf) for further details on the modeling architecture and training details.
------------------------------
Document 2:

"testing 510-token test examples comprised of 5,000 samples from the WebText dataset and 5,000 samples generated by a GPT-2 model, which were not used during the training." "Our classifier is able to detect 1.5 billion parameter GPT-2-generated text with approximately 95% accuracy..." "As seen in Figure 1 [in the paper], we found consistently high accuracy when trained on nucleus sampling."
-------------------- input_format --------------------
Document 1:

"RoBERTa base (see the [RoBERTa base model card](https://huggingface.co/roberta-base) for more details on the RoBERTa base training data)" and "We based a sequence classifier on RoBERTaBASE (125 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model."
-------------------- output_format --------------------
Document 1:

"testing 510-token test examples comprised of 5,000 samples from the WebText dataset and 5,000 samples generated by a GPT-2 model, which were not used during the training." NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

testing 510-token test examples comprised of 5,000 samples from the WebText dataset and 5,000 samples generated by a GPT-2 model, which were not used during the training.
-------------------- vocabulary_size --------------------
Document 1:

"125 million parameters"

[{'datasets': ['WebText'], 'license': 'mit', 'github': '', 'paper': 'https://d4mucfpksywv.cloudfron 
t.net/papers/GPT_2_Report.pdf', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 
 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'outpu 
t_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                      
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dfa12-343e91d8182f47fc65e4886d)

Entry Not Found for url: https://huggingface.co/absa/classifier-rest-0.2/resolve/main/README.md. 

#####################sonoisa/t5-base-japanese-title-generation########################

-------------------- datasets --------------------
Document 1:

"https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44"
-------------------- license --------------------
Document 1:

"license"
------------------------------
Document 2:

license: cc-by-sa-4.0
-------------------- github --------------------
Document 1:

"https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44"
-------------------- paper --------------------
Document 1:

"https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44"
-------------------- upstream_model --------------------
Document 1:

"upstream_model"
-------------------- parameter_count --------------------
Document 1:

"parameter_count"
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------
Document 1:

SEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
-------------------- hardware --------------------
Document 1:

"https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44"
-------------------- input_format --------------------
Document 1:

"https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44"
-------------------- output_format --------------------
Document 1:

"output_format"
-------------------- input_token_limit --------------------
Document 1:

"input_token_limit"
-------------------- vocabulary_size --------------------
Document 1:

"vocabulary_size"

[{'datasets': ['https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44'], 'license': 'license', 'gith 
ub': 'https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44', 'paper': 'https://qiita.com/sonoisa/ite 
ms/a9af64ff641f0bbfed44', 'upstream_model': 'upstream_model', 'parameter_count': 'parameter_count',  
'hyper_parameters': {'epochs': 'epochs', 'batch_size': 'batch_size', 'learning_rate': 'learning_rate 
', 'optimizer': 'optimizer'}, 'evaluation': [{'test': 'SEE: https://qiita.com/sonoisa/items/a9af64ff 
641f0bbfed44', 'result': 0.0}], 'hardware': 'https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44',  
'limitation_and_bias': '', 'demo': 'https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44', 'input_fo 
rmat': 'https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44', 'output_format': 'output_format', 'in 
put_token_limit': 'input_token_limit', 'vocabulary_size': 'vocabulary_size'}]                        

#####################lllyasviel/control_v11f1p_sd15_depth########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 4291 tokens (4035 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################skt/kobert-base-v1########################

-------------------- datasets --------------------
Document 1:

"Please refer here. https://github.com/SKTBrain/KoBERT"
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

"https://github.com/SKTBrain/KoBERT"
-------------------- paper --------------------
Document 1:

"Please refer here. https://github.com/SKTBrain/KoBERT"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Please refer here. https://github.com/SKTBrain/KoBERT"
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': [], 'license': 'license', 'github': 'https://github.com/SKTBrain/KoBERT', 'paper': 'P 
lease refer here. https://github.com/SKTBrain/KoBERT', 'upstream_model': '', 'parameter_count': '',  
'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'Please 
 refer here. https://github.com/SKTBrain/KoBERT', 'input_format': '', 'output_format': ''}]          

#####################flair/chunk-english########################

-------------------- datasets --------------------
Document 1:

"Contextual String Embeddings for Sequence Labeling"
------------------------------
Document 2:

CONLL_2000(), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward'), SequenceTagger(), ModelTrainer(), trainer.train('resources/taggers/chunk-english', train_with_dev=True, max_epochs=150)
------------------------------
Document 3:

datasets: - conll2000
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

[here](https://github.com/flairNLP/flair/issues/)
------------------------------
Document 2:

"github"
-------------------- paper --------------------
Document 1:

"Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}"
------------------------------
Document 2:

"Flair embeddings" and "LSTM-CRF"
-------------------- upstream_model --------------------
Document 1:

"upstream_model"
------------------------------
Document 2:

FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward'), StackedEmbeddings(embeddings=embedding_types), SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type), ModelTrainer(tagger, corpus)
-------------------- parameter_count --------------------
Document 1:

"hidden_size=256,", "embeddings=embeddings,", "tag_dictionary=tag_dictionary,"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
------------------------------
Document 3:

"parameter_count"
-------------------- hyper_parameters --------------------
Document 1:

hidden_size=256, max_epochs=150
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- evaluation --------------------
Document 1:

F1-Score: **96,48** (CoNLL-2000) | **tag** | **meaning** | ADJP | adjectival | ADVP | adverbial | CONJP | conjunction | INTJ | interjection | LST | list marker | NP | noun phrase | PP | prepositional | PRT | particle | SBAR | subordinate clause | VP | verb phrase | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.
------------------------------
Document 2:

"SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.
------------------------------
Document 2:

FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward'), StackedEmbeddings(embeddings=embedding_types), SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type), ModelTrainer(tagger, corpus), trainer.train('resources/taggers/chunk-english', train_with_dev=True, max_epochs=150)
-------------------- demo --------------------
Document 1:

"Find a form of demo"
------------------------------
Document 2:

language: en, tags: - flair - token-classification - sequence-tagger-model, datasets: - conll2000, widget: - text: The happy man has been eating at the diner
------------------------------
Document 3:

"This is the standard phrase chunking model for English that ships with [Flair](https://github.com/flairNLP/flair/). F1-Score: **96,48** (CoNLL-2000) Predicts 4 tags: | **tag** | **meaning** | |---------------------------------|-----------| | ADJP | adjectival | | ADVP | adverbial | | CONJP | conjunction | | INTJ | interjection | | LST | list marker | | NP | noun phrase | | PP | prepositional | | PRT | particle | | SBAR | subordinate clause | | VP | verb phrase | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- input_format --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF." input_format
------------------------------
Document 2:

conll2000
-------------------- output_format --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF." output_format
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"F1-Score: **96,48** (CoNLL-2000)" "Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF." NO_OUTPUT
------------------------------
Document 2:

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
NO_OUTPUT

[{'datasets': ['conll2000'], 'license': 'license', 'github': '[here](https://github.com/flairNLP/fl 
air/issues/)', 'paper': 'Please cite the following paper when using this model. @inproceedings{akbik 
2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blyt 
he, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Compu 
tational Linguistics}, pages     = {1638--1649}, year      = {2018}', 'upstream_model': 'upstream_mo 
del', 'parameter_count': 'hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary', 'h 
yper_parameters': {'epochs': 'max_epochs=150', 'batch_size': '', 'learning_rate': '', 'optimizer': ' 
'}, 'evaluation': [{'test': 'F1-Score', 'result': 96.48}], 'hardware': '', 'limitation_and_bias': 'B 
ased on Flair embeddings and LSTM-CRF.', 'demo': 'Find a form of demo', 'input_format': 'Based on Fl 
air embeddings and LSTM-CRF.', 'output_format': 'Based on Flair embeddings and LSTM-CRF.', 'input_to 
ken_limit': '', 'vocabulary_size': ''}]                                                              

#####################timm/resnet18.a1_in1k########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 18743 tokens (18487 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################sentence-transformers/paraphrase-multilingual-mpnet-base-v2########################

-------------------- datasets --------------------
Document 1:

datasets, Sentence Embeddings Benchmark, [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-multilingual-mpnet-base-v2)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"github" "Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-multilingual-mpnet-base-v2)
-------------------- paper --------------------
Document 1:

"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
------------------------------
Document 2:

"sentence-transformers" and "semantic search"
------------------------------
Document 3:

"Sentence Embeddings Benchmark" "https://seb.sbert.net?model_name=sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

upstream_model: sentence-transformers
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-multilingual-mpnet-base-v2)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

sentence-transformers, https://www.SBERT.net
------------------------------
Document 2:

"Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-multilingual-mpnet-base-v2)
-------------------- input_format --------------------
Document 1:

input_format: sentence-transformers
------------------------------
Document 2:

'max_seq_length': 128, 'do_lower_case': False
-------------------- output_format --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False
-------------------- input_token_limit --------------------
Document 1:

'max_seq_length': 128
-------------------- vocabulary_size --------------------


[{'datasets': ['Sentence Embeddings Benchmark'], 'license': 'apache-2.0', 'github': 'https://github 
.com/sentence-transformers/sentence-transformers', 'paper': 'https://arxiv.org/abs/1908.10084', 'ups 
tream_model': 'sentence-transformers', 'parameter_count': "'max_seq_length': 128, 'do_lower_case': F 
alse, 'word_embedding_dimension': 768", 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'l 
imitation_and_bias': '', 'demo': 'https://www.sbert.net', 'input_format': 'sentence-transformers', ' 
output_format': "'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'po 
oling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'p 
ooling_mode_mean_sqrt_len_tokens': False", 'input_token_limit': "'max_seq_length': 128", 'vocabulary 
_size': ''}]                                                                                         

#####################bert-large-uncased-whole-word-masking-finetuned-squad########################

-------------------- datasets --------------------
Document 1:

BookCorpus, https://yknzhu.wixsite.com/mbweb, English Wikipedia, https://en.wikipedia.org/wiki/English_Wikipedia
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."

NO_OUTPUT
------------------------------
Document 3:

datasets: - bookcorpus - wikipedia
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"You may use it in a question answering pipeline, or use it to output raw results given a query and a context. You may see other use cases in the [task summary](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) of the transformers documentation."## Training data  
The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038
unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and
headers).

NO_OUTPUT
-------------------- github --------------------
Document 1:

"You may use it in a question answering pipeline, or use it to output raw results given a query and a context. You may see other use cases in the [task summary](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) of the transformers documentation."
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1810.04805)
------------------------------
Document 2:

"The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers)."
------------------------------
Document 3:

"Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- upstream_model --------------------
Document 1:

"question answering pipeline" "task summary" "BookCorpus" "English Wikipedia"
------------------------------
Document 2:

"introduced in [this paper](https://arxiv.org/abs/1810.04805) and first released in [this repository](https://github.com/google-research/bert)"
-------------------- parameter_count --------------------
Document 1:

16 TPU chips total, batch size of 256, sequence length of 128 tokens for 90% of the steps and 512 for the remaining 10%, Adam optimizer with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

--per_device_eval_batch_size=3   \--per_device_train_batch_size=3
-------------------- hyper_parameters --------------------
Document 1:

Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

"The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers)."
------------------------------
Document 3:

--learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 384 --doc_stride 128 --per_device_eval_batch_size=3 --per_device_train_batch_size=3
-------------------- evaluation --------------------
Document 1:

"The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers)."
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 3:

"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in [this paper](https://arxiv.org/abs/1810.04805) and first released in [this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference between english and English. Differently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same. The training is identical -- each masked WordPiece token is predicted independently. After pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts."
-------------------- hardware --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)"
-------------------- limitation_and_bias --------------------
Document 1:

The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

"The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers)."
------------------------------
Document 3:

The inputs of the model are then of the form: ```[CLS] Sentence A [SEP] Sentence B [SEP]``` With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: - 15% of the tokens are masked. - In 80% of the cases, the masked tokens are replaced by `[MASK]`. - In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. - In the 10% remaining cases, the masked tokens are left as is.
-------------------- demo --------------------
Document 1:

"This model should be used as a question-answering model. You may use it in a question answering pipeline, or use it to output raw results given a query and a context. You may see other use cases in the [task summary](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) of the transformers documentation."
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- input_format --------------------
Document 1:

"The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers)." input_format
------------------------------
Document 2:

"lowercased and tokenized using WordPiece and a vocabulary size of 30,000" "inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]" "what is considered a sentence here is a consecutive span of text usually longer than a single sentence" "the result with the two "sentences" has a combined length of less than 512 tokens" "15% of the tokens are masked" "in 80% of the cases, the masked tokens are replaced by `[MASK]`" "in 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace" "in the 10% remaining cases, the masked tokens are left as is"

input_format: lowercased and tokenized using WordPiece and a vocabulary size of 30,000; inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]; what is considered a sentence here is a consecutive span of text usually longer than a single sentence; the result with the two "sentences" has a combined length of less than 512 tokens; 15% of
------------------------------
Document 3:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after"
-------------------- output_format --------------------
Document 1:

"question answering pipeline" "task summary" "BookCorpus" "English Wikipedia"
------------------------------
Document 2:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 3:

The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.  
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.
-------------------- input_token_limit --------------------
Document 1:

"With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
------------------------------
Document 2:

"sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%"
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 30,000
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Unterminated string starting at: line 30 column 28 (char 2244) 

#####################facebook/detr-resnet-50########################

-------------------- datasets --------------------
Document 1:

COCO 2017 object detection, https://cocodataset.org/#download, 118k/5k annotated images for training/validation respectively.
------------------------------
Document 2:

datasets:
- coco
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py)
-------------------- paper --------------------
Document 1:

"table 1 of the original paper"
------------------------------
Document 2:

[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)
------------------------------
Document 3:

arXiv:2005.12872
-------------------- upstream_model --------------------
Document 1:

"The DETR model is an encoder-decoder transformer with a convolutional backbone."
------------------------------
Document 2:

upstream_model: End-to-End Object Detection with Transformers by Carion et al.
-------------------- parameter_count --------------------
Document 1:

parameter_count 118k/5k
-------------------- hyper_parameters --------------------
Document 1:

300 epochs, 16 V100 GPUs, 4 images per GPU, total batch size of 64
-------------------- evaluation --------------------
Document 1:

"AP (average precision) of **42.0**" and "table 1 of the original paper."
------------------------------
Document 2:

"The DETR model was trained on [COCO 2017 object detection](https://cocodataset.org/#download), a dataset consisting of 118k/5k annotated images for training/validation respectively."

NO_OUTPUT
-------------------- hardware --------------------
Document 1:

16 V100 GPUs, 4 images per GPU, total batch size of 64
------------------------------
Document 2:

"COCO 2017 object detection (118k annotated images)"
-------------------- limitation_and_bias --------------------
Document 1:

"COCO 2017 object detection"
------------------------------
Document 2:

"End-to-End Object Detection with Transformers" by Carion et al. and first released in "this repository" https://github.com/facebookresearch/detr
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=facebook/detr"
------------------------------
Document 2:

"COCO 2017 object detection"
------------------------------
Document 3:

"End-to-End Object Detection with Transformers" by Carion et al. and first released in [this repository](https://github.com/facebookresearch/detr).
-------------------- input_format --------------------
Document 1:

Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).
------------------------------
Document 2:

"COCO 2017 object detection"
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225)."
------------------------------
Document 2:

"The model is trained using a "bipartite matching loss": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a "no object" as class and "no bounding box" as bounding box)." 
NO_OUTPUT
------------------------------
Document 3:

inputs = processor(images=image, return_tensors="pt")
model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50", revision="no_timm")
-------------------- input_size --------------------
Document 1:

input_size: shortest side is at least 800 pixels and the largest side at most 1333 pixels
------------------------------
Document 2:

"118k/5k annotated images for training/validation"
------------------------------
Document 3:

4 images per GPU
-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': ['COCO 2017 object detection'], 'license': 'apache-2.0', 'github': '[here](https://git 
hub.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py)', 'paper': '[End-t 
o-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)', 'upstream_model': 'End 
-to-End Object Detection with Transformers by Carion et al.', 'parameter_count': '118k/5k', 'hyper_p 
arameters': {'epochs': '300', 'batch_size': '16 V100 GPUs, 4 images per GPU, total batch size of 64' 
, 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'AP (average precision)', 'result': 
 42.0}], 'hardware': '16 V100 GPUs, 4 images per GPU, total batch size of 64', 'limitation_and_bias' 
: 'COCO 2017 object detection', 'demo': '"model hub" "https://huggingface.co/models?search=facebook/ 
detr"', 'input_format': 'Images are resized/rescaled such that the shortest side is at least 800 pix 
els and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageN 
et mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).', 'output_format': '', ' 
input_preprocessing': '"Images are resized/rescaled such that the shortest side is at least 800 pixe 
ls and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNe 
t mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225)."', 'input_size': 'shortes 
t side is at least 800 pixels and the largest side at most 1333 pixels', 'num_of_classes_for_classif 
ication': '', 'trigger_word': ''}                                                                    

#####################HooshvareLab/bert-base-parsbert-ner-uncased########################

-------------------- datasets --------------------
Document 1:

ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes. Organization, Location, Facility, Event, Product, Person. Download You can download the dataset from [here](https://github.com/HaniehP/PersianNER)
------------------------------
Document 2:

"Tensorflow Research Cloud (TFRC) program", "[Hooshvare](https://hooshvare.com) Research Group for facilitating dataset gathering and scraping online text resources."
-------------------- license --------------------
Document 1:

"license: apache-2.0"
-------------------- github --------------------
Document 1:

[Github](https://github.com/m3hrdadfi), [Github](https://github.com/baarsaam), [Github](https://github.com/marziehphi), [Github](https://github.com/mmanthouri), [Github](https://github.com/hooshvare)
------------------------------
Document 2:

[How to use Pipelines](https://github.com/hooshvare/parsbert-ner/blob/master/persian-ner-pipeline.ipynb)
------------------------------
Document 3:

"You can download the dataset from [here](https://github.com/HaniehP/PersianNER)"
-------------------- paper --------------------
Document 1:

"Please cite the following paper in your publication if you are using [ParsBERT](https://arxiv.org/abs/2005.12515) in your research:  @article{ParsBERT, title={ParsBERT: Transformer-based Model for Persian Language Understanding}, author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri}, journal={ArXiv}, year={2020}, volume={abs/2005.12515}"
------------------------------
Document 2:

Paper presenting ParsBERT: [arXiv:2005.12515](https://arxiv.org/abs/2005.12515)
-------------------- upstream_model --------------------
Document 1:

upstream_model: Google's BERT architecture
-------------------- parameter_count --------------------
Document 1:

"302,530 tokens" and "41,148 tokens are tagged with seven different classes."
-------------------- hyper_parameters --------------------
Document 1:

"7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes." "Organization | 16964 | Money | 2037 | Location | 8782 | Date | 4259 | Time | 732 | Person | 7675 | Percent | 699" "You can download the dataset from [here](http://nsurl.org/tasks/task-7-named-entity-recognition-ner-for-farsi/)"
-------------------- evaluation --------------------
Document 1:

The following table summarizes the F1 score obtained by ParsBERT as compared to other models and architectures. 
| Dataset         | ParsBERT | MorphoBERT |  Beheshti-NER  |  LSTM-CRF  |  Rule-Based CRF  |  BiLSTM-CRF  |
|:---------------:|:--------:|:----------:|:--------------:|:----------:|:----------------:|:------------:|
|  ARMAN + PEYMA  |   95.13* |      -     |        -       |      -     |         -        |       -      |
|  PEYMA          |   98.79* |      -     |      90.59     |      -     |       84.00      |       -      |
|  ARMAN          |   93.10* |    89.9    |      84.03     |    86.55   |         -        |     77.45    |
------------------------------
Document 2:

1. Organization
2. Money
3. Location
4. Date
5. Time
6. Person
7. Percent 
|     Label    |   #   |
|:------------:|:-----:|
| Organization | 16964 |
|     Money    |  2037 |
|   Location   |  8782 |
|     Date     |  4259 |
|     Time     |  732  |
|    Person    |  7675 |
|    Percent   |  699  |
-------------------- hardware --------------------
Document 1:

"Tensorflow Research Cloud (TFRC) program" and "Hooshvare Research Group"
-------------------- limitation_and_bias --------------------
Document 1:

ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes. Organization, Location, Facility, Event, Product, Person. Label: Organization 30108, Location 12924, Facility 4458, Event 7557, Product 4389, Person 15645. Download: You can download the dataset from [here](https://github.com/HaniehP/PersianNER).
-------------------- demo --------------------
Document 1:

[How to use Pipelines](https://github.com/hooshvare/parsbert-ner/blob/master/persian-ner-pipeline.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hooshvare/parsbert-ner/blob/master/persian-ner-pipeline.ipynb)
-------------------- input_format --------------------
Document 1:

PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes. | Label | # | Organization | 16964 | Money | 2037 | Location | 8782 | Date | 4259 | Time | 732 | Person | 7675 | Percent | 699 | You can download the dataset from [here](http://nsurl.org/tasks/task-7-named-entity-recognition-ner-for-farsi/)
------------------------------
Document 2:

"IOB" format, "O", "B", "I", "ARMAN", "PEYMA"
-------------------- output_format --------------------
Document 1:

"PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes." "Organization | 16964 | Money | 2037 | Location | 8782 | Date | 4259 | Time | 732 | Person | 7675 | Percent | 699" "You can download the dataset from [here](http://nsurl.org/tasks/task-7-named-entity-recognition-ner-for-farsi/)"
------------------------------
Document 2:

"IOB" format, "O", "B", "I", "B" and "I" tags followed by a hyphen (or underscore), followed by the entity category, multi-class token classification problem, ARMAN, PEYMA
-------------------- input_token_limit --------------------
Document 1:

"302,530 tokens" and "41,148 tokens are tagged with seven different classes" 
NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

"7,682 sentences with 250,015 sentences tagged" and "30108 | 12924 | 4458 | 7557 | 4389 | 15645"
------------------------------
Document 2:

41,148 tokens are tagged with seven different classes. | Label | # | Organization | 16964 | Money | 2037 | Location | 8782 | Date | 4259 | Time | 732 | Person | 7675 | Percent | 699 
NO_OUTPUT

[{'datasets': ['ARMAN'], 'license': 'apache-2.0', 'github': 'https://github.com/m3hrdadfi', 'paper' 
: 'https://arxiv.org/abs/2005.12515', 'upstream_model': "Google's BERT architecture", 'parameter_cou 
nt': '302,530 tokens', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'op 
timizer': ''}, 'evaluation': [{'test': 'ARMAN + PEYMA', 'result': 95.13}, {'test': 'PEYMA', 'result' 
: 98.79}, {'test': 'ARMAN', 'result': 93.1}], 'hardware': 'Tensorflow Research Cloud (TFRC) program' 
, 'limitation_and_bias': 'ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six 
 different classes. Organization, Location, Facility, Event, Product, Person. Label: Organization 30 
108, Location 12924, Facility 4458, Event 7557, Product 4389, Person 15645. Download: You can downlo 
ad the dataset from [here](https://github.com/HaniehP/PersianNER).', 'demo': '[How to use Pipelines] 
(https://github.com/hooshvare/parsbert-ner/blob/master/persian-ner-pipeline.ipynb)', 'input_format': 
 'PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are 
 tagged with seven different classes. | Label | # | Organization | 16964 | Money | 2037 | Location | 
 8782 | Date | 4259 | Time | 732 | Person | 7675 | Percent | 699 | You can download the dataset from 
 [here](http://nsurl.org/tasks/task-7-named-entity-recognition-ner-for-farsi/)', 'output_format': 'P 
EYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are ta 
gged with seven different classes. | Organization | 16964 | Money | 2037 | Location | 8782 | Date |  
4259 | Time | 732 | Person | 7675 | Percent | 699 | You can download the dataset from [here](http:// 
nsurl.org/tasks/task-7-named-entity-recognition-ner-for-farsi/)', 'input_token_limit': '302,530 toke 
ns', 'vocabulary_size': '7,682 sentences with 250,015 sentences tagged'}]                            
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dfb3c-59e6b5fa46d5021507810627)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-SqueezeBertModel/resolve/main/README.md. 

#####################lllyasviel/control_v11p_sd15_canny########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 4291 tokens (4035 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################google/flan-t5-small########################

-------------------- datasets --------------------
Document 1:

"The model was trained on a mixture of tasks" NO_OUTPUT
-------------------- license --------------------
Document 1:

Creative Commons Attribution 4.0 International
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

- svakulenk0/qrecc
- djaym7/wiki_dialog
- deepmind/code_contests
- gsm8k
- aqua_rat
- esnli
- quasc
- qed
-------------------- paper --------------------
Document 1:

"original paper, figure 2"
------------------------------
Document 2:

"For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."
------------------------------
Document 3:

"See the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hyper_parameters"
-------------------- evaluation --------------------
Document 1:

"The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: ![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png) For full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf)."
------------------------------
Document 2:

6. [Evaluation](#evaluation)
------------------------------
Document 3:

"The model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2): ![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)"
-------------------- hardware --------------------
Document 1:

5. [Training Details]
-------------------- limitation_and_bias --------------------
Document 1:

4. [Bias, Risks, and Limitations]
------------------------------
Document 2:

"not filtered for explicit content or assessed for existing biases" "potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data"
-------------------- demo --------------------
Document 1:

"Find below some example scripts on how to use the model in `transformers`:"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['qrecc', 'wiki_dialog', 'code_contests', 'gsm8k', 'aqua_rat', 'esnli', 'quasc', 'qed 
'], 'license': 'Creative Commons Attribution 4.0 International', 'github': '- svakulenk0/qrecc\n- dj 
aym7/wiki_dialog\n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed', 'paper':  
'original paper, figure 2', 'upstream_model': '', 'parameter_count': 'parameter_count', 'hyper_param 
eters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'tes 
t': 'The authors evaluated the model on various tasks covering several languages (1836 in total). Se 
e the table below for some quantitative evaluation: ![image.png](https://s3.amazonaws.com/moonup/pro 
duction/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png) For full details, please check the [rese 
arch paper](https://arxiv.org/pdf/2210.11416.pdf).', 'result': 0}], 'hardware': '5. [Training Detail 
s]', 'limitation_and_bias': '4. [Bias, Risks, and Limitations]', 'demo': 'Find below some example sc 
ripts on how to use the model in `transformers`:', 'input_format': '', 'output_format': '', 'input_t 
oken_limit': '', 'vocabulary_size': ''}]                                                             

#####################stabilityai/sdxl-vae########################

-------------------- datasets --------------------
Document 1:

"https://huggingface.co/stabilityai/sdxl-vae/blob/main/sdxl_vae.safetensors", "https://ommer-lab.com/files/latent-diffusion/kl-f8.zip", "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt"
------------------------------
Document 2:

SDXL, latent diffusion model, autoencoder, Stable Diffusion, exponential moving average (EMA)
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

license: mit, tags: - stable-diffusion - stable-diffusion-diffusers, inference: false
------------------------------
Document 2:

Link: https://huggingface.co/stabilityai/sdxl-vae/blob/main/sdxl_vae.safetensors
Link: https://ommer-lab.com/files/latent-diffusion/kl-f8.zip
Link: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
-------------------- paper --------------------
Document 1:

"as used in SDXL" and "as used in SD"
------------------------------
Document 2:

"kl-f8 VAE", "f8-ft-MSE"
------------------------------
Document 3:

"latent diffusion model" (https://arxiv.org/abs/2112.10752)
-------------------- upstream_model --------------------
Document 1:

"autoencoder architecture used for the original [Stable Diffusion](https://github.com/CompVis/stable-diffusion)"

upstream_model: Stable Diffusion
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

SDXL-VAE vs original kl-f8 VAE vs f8-ft-MSE
------------------------------
Document 2:

"rFID | PSNR | SSIM | PSIM | Link | Comments"
------------------------------
Document 3:

"To this end, we train the same autoencoder architecture used for the original [Stable Diffusion](https://github.com/CompVis/stable-diffusion) at a larger batch-size (256 vs 9) and additionally track the weights with an exponential moving average (EMA). The resulting autoencoder outperforms the original model in all evaluated reconstruction metrics, see the table below."
-------------------- hardware --------------------
Document 1:

SDXL, latent diffusion model, autoencoder architecture, batch-size (256 vs 9), exponential moving average (EMA)
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

Link: https://huggingface.co/stabilityai/sdxl-vae/blob/main/sdxl_vae.safetensors, Link: https://ommer-lab.com/files/latent-diffusion/kl-f8.zip, Link: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
------------------------------
Document 2:

SDXL, latent diffusion model, autoencoder architecture, original Stable Diffusion, reconstruction metrics
-------------------- input_format --------------------
Document 1:

SDXL, latent diffusion model, autoencoder architecture, exponential moving average (EMA) 
input_format: SDXL, latent diffusion model, autoencoder architecture, exponential moving average (EMA)
-------------------- output_format --------------------
Document 1:

output_format: SDXL

[{'datasets': ['https://huggingface.co/stabilityai/sdxl-vae/blob/main/sdxl_vae.safetensors', 'https 
://ommer-lab.com/files/latent-diffusion/kl-f8.zip', 'https://huggingface.co/stabilityai/sd-vae-ft-ms 
e-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt'], 'license': 'mit', 'github': 'https://hu 
ggingface.co/stabilityai/sdxl-vae/blob/main/sdxl_vae.safetensors', 'paper': 'https://arxiv.org/abs/2 
112.10752', 'upstream_model': 'Stable Diffusion', 'parameter_count': '', 'hyper_parameters': {}, 'ev 
aluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': 'SDXL, latent  
diffusion model, autoencoder architecture, exponential moving average (EMA)', 'output_format': 'SDXL 
'}]                                                                                                  

#####################deepset/sentence_bert########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

https://github.com/UKPLab/sentence-transformers
-------------------- paper --------------------
Document 1:

"Sentence Transformers Repo (https://github.com/UKPLab/sentence-transformers)"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': [], 'license': 'apache-2.0', 'github': 'https://github.com/UKPLab/sentence-transforme 
rs', 'paper': '"Sentence Transformers Repo (https://github.com/UKPLab/sentence-transformers)"', 'ups 
tream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', ' 
limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]                      

#####################lllyasviel/control_v11p_sd15_openpose########################

-------------------- datasets --------------------
Document 1:

[Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), [lllyasviel/control_v11p_sd15_canny](https://huggingface.co/lllyasviel/control_v11p_sd15_canny), [lllyasviel/control_v11e_sd15_ip2p](https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p), [lllyasviel/control_v11p_sd15_inpaint](https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint), [lllyasviel/control_v11p_sd15_mlsd](https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd), [lllyasviel/control_v11f1p_sd15_depth](https://huggingface.co/lllyasviel/
------------------------------
Document 2:

"tags: - art - controlnet - stable-diffusion - controlnet-v1-1 - image-to-image"
-------------------- license --------------------
Document 1:

license: openrail
-------------------- github --------------------
Document 1:

"have a look at the [official docs](https://github.com/lllyasviel/ControlNet-v1-1-nightly)."
------------------------------
Document 2:

[lllyasviel/control_v11p_sd15_canny](https://huggingface.co/lllyasviel/control_v11p_sd15_canny), [lllyasviel/control_v11e_sd15_ip2p](https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p), [lllyasviel/control_v11p_sd15_inpaint](https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint), [lllyasviel/control_v11p_sd15_mlsd](https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd), [lllyasviel/control_v11f1p_sd15_depth](https://huggingface.co/lllyasviel/control_v11f1p_sd15_depth), [lllyasviel/control_v11p_sd15_normalbae](
------------------------------
Document 3:

license: openrail, tags: - art - controlnet - stable-diffusion - controlnet-v1-1 - image-to-image, base_model: runwayml/stable-diffusion-v1-5, duplicated_from: ControlNet-1-1-preview/control_v11p_sd15_openpose
-------------------- paper --------------------
Document 1:

*Adding Conditional Control to Text-to-Image Diffusion Models* by Lvmin Zhang, Maneesh Agrawala.
-------------------- upstream_model --------------------
Document 1:

"The authors released 14 different checkpoints, each trained with [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) on a different type of conditioning:"
------------------------------
Document 2:

license: openrail, base_model: runwayml/stable-diffusion-v1-5, duplicated_from: ControlNet-1-1-preview/control_v11p_sd15_openpose
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

[Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)
-------------------- limitation_and_bias --------------------
Document 1:

(1) a small group of greyscale human images are duplicated thousands of times (!!), causing the previous model somewhat likely to generate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG artifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data processing scripts.
-------------------- demo --------------------
Document 1:

[Diffusers ControlNet Blog Post](https://huggingface.co/blog/controlnet) and [official docs](https://github.com/lllyasviel/ControlNet-v1-1-nightly).
------------------------------
Document 2:

The authors released 14 different checkpoints, each trained with [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) on a different type of conditioning:  
| Model Name | Control Image Overview| Control Image Example | Generated Image Example |
|---|---|---|---|
|[lllyasviel/control_v11p_sd15_canny](https://huggingface.co/lllyasviel/control_v11p_sd15_canny)<br/> *Trained with canny edge detection* | A monochrome image with white edges on a black background.|<a href="https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/control.png"><img width="64" style="margin:0;padding:0;" src="https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/control.png"/></a
------------------------------
Document 3:

license: openrail, tags: - art - controlnet - stable-diffusion - controlnet-v1-1 - image-to-image, base_model: runwayml/stable-diffusion-v1-5, duplicated_from: ControlNet-1-1-preview/control_v11p_sd15_openpose
-------------------- input_format --------------------
Document 1:

"A monochrome image with white edges on a black background.", "No condition.", "An image with depth information, usually represented as a grayscale image.", "An image with surface normal information, usually represented as a color-coded image.", "An image with segmented regions, usually represented as a color-coded image.", "An image with line art, usually black lines on a white background.", "An image with anime-style line art.", "An image with human poses, usually represented as a set of keypoints or skeletons.", "An image with scribbles, usually random or user-drawn strokes.", "An image with soft edges, usually to create a more painterly or artistic effect.", "An image with shuffled patches or regions."
------------------------------
Document 2:

license: openrail, tags: - art - controlnet - stable-diffusion - controlnet-v1-1 - image-to-image, base_model: runwayml/stable-diffusion-v1-5, duplicated_from: ControlNet-1-1-preview/control_v11p_sd15_openpose
-------------------- output_format --------------------
Document 1:

"Generated Image Example"
-------------------- input_preprocessing --------------------
Document 1:

"canny edge detection", "pixel to pixel instruction", "image inpainting", "multi-level line segment detection", "depth estimation", "surface normal estimation", "image segmentation", "line art generation", "anime line art generation", "human pose estimation", "scribble-based image generation", "soft edge image generation", "image shuffling"
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': ['Stable Diffusion v1-5', 'lllyasviel/control_v11p_sd15_canny', 'lllyasviel/control_v1 
1e_sd15_ip2p', 'lllyasviel/control_v11p_sd15_inpaint', 'lllyasviel/control_v11p_sd15_mlsd', 'lllyasv 
iel/control_v11f1p_sd15_depth'], 'license': 'openrail', 'github': 'https://github.com/lllyasviel/Con 
trolNet-v1-1-nightly', 'paper': '*Adding Conditional Control to Text-to-Image Diffusion Models* by L 
vmin Zhang, Maneesh Agrawala.', 'upstream_model': 'Stable Diffusion v1-5', 'parameter_count': '', 'h 
yper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluatio 
n': [], 'hardware': 'Stable Diffusion v1-5', 'limitation_and_bias': '(1) a small group of greyscale  
human images are duplicated thousands of times (!!), causing the previous model somewhat likely to g 
enerate grayscale human images; (2) some images has low quality, very blurry, or significant JPEG ar 
tifacts; (3) a small group of images has wrong paired prompts caused by a mistake in our data proces 
sing scripts.', 'demo': '[Diffusers ControlNet Blog Post](https://huggingface.co/blog/controlnet) an 
d [official docs](https://github.com/lllyasviel/ControlNet-v1-1-nightly).', 'input_format': '"A mono 
chrome image with white edges on a black background.", "No condition.", "An image with depth informa 
tion, usually represented as a grayscale image.", "An image with surface normal information, usually 
 represented as a color-coded image.", "An image with segmented regions, usually represented as a co 
lor-coded image.", "An image with line art, usually black lines on a white background.", "An image w 
ith anime-style line art.", "An image with human poses, usually represented as a set of keypoints or 
 skeletons.", "An image with scribbles, usually random or user-drawn strokes.", "An image with soft  
edges, usually to create a more painterly or artistic effect.", "An image with shuffled patches or r 
egions."', 'output_format': '"Generated Image Example"', 'input_preprocessing': '"canny edge detecti 
on", "pixel to pixel instruction", "image inpainting", "multi-level line segment detection", "depth  
estimation", "surface normal estimation", "image segmentation", "line art generation", "anime line a 
rt generation", "human pose estimation", "scribble-based image generation", "soft edge image generat 
ion", "image shuffling"', 'input_size': '', 'num_of_classes_for_classification': '', 'trigger_word': 
 ''}                                                                                                 

#####################facebook/opt-125m########################

-------------------- datasets --------------------
Document 1:

- BookCorpus
- CC-Stories
- Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews
- Pushshift.io Reddit dataset
- CCNewsV2
------------------------------
Document 2:

"The dataset was collected form internet"
------------------------------
Document 3:

"As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased"
-------------------- license --------------------
Document 1:

license: other
-------------------- github --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- paper --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers." "Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs."
------------------------------
Document 2:

"Meta AI's model card" and "In general, OPT-175B is not immune from the plethora of issues that plague modern large language models."
-------------------- upstream_model --------------------
Document 1:

"Meta AI's model card" and "OPT-175B"
-------------------- parameter_count --------------------
Document 1:

GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days
------------------------------
Document 2:

"125M to 175B parameters"
-------------------- hyper_parameters --------------------
Document 1:

"Meta AI's model card" "training data used for this model contains a lot of unfiltered content from the internet" "OPT-175B has limitations in terms of bias and safety" "OPT-175B can also have quality issues in terms of generation diversity and hallucination" "OPT-175B is not immune from the plethora of issues that plague modern large language models" "This bias will also affect all fine-tuned versions of this model."
------------------------------
Document 2:

"BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit dataset, CCNewsV2" NO_OUTPUT
-------------------- evaluation --------------------
Document 1:

"As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased : Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. This bias will also affect all fine-tuned versions of this model."
------------------------------
Document 2:

- BookCorpus, which consists of more than 10K unpublished books,
- CC-Stories, which contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas,
- The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included.
- Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in
Roller et al. (2021)
- CCNewsV2 containing an updated version of the English portion of the CommonCrawl News
dataset that was used in RoBERTa (Liu et al., 2019b)
- The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally
to each dataset’s size in the pretraining corpus.
-------------------- hardware --------------------
Document 1:

"Meta AI's model card" and "training data used for this model contains a lot of unfiltered content from the internet"
------------------------------
Document 2:

GPT2, 80GB A100 GPUs, ~33 days
------------------------------
Document 3:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
-------------------- limitation_and_bias --------------------
Document 1:

"As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased : Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. This bias will also affect all fine-tuned versions of this model."
------------------------------
Document 2:

"known challenges in areas such as robustness, bias, and toxicity"
-------------------- demo --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- input_format --------------------
Document 1:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 2:

GPT2 byte-level version of Byte Pair Encoding (BPE), vocabulary size of 50272, inputs are sequences of 2048 consecutive tokens. input_format
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*
-------------------- vocabulary_size --------------------
Document 1:

"180B tokens corresponding to 800GB of data" NO_OUTPUT
------------------------------
Document 2:

GPT2, vocabulary size of 50272
------------------------------
Document 3:

"Meta AI's model card" "training data used for this model contains a lot of unfiltered content from the internet" "OPT-175B has limitations in terms of bias and safety" "OPT-175B is not immune from the plethora of issues that plague modern large language models" "This bias will also affect all fine-tuned versions of this model"

[{'datasets': ['BookCorpus', 'CC-Stories', 'Pile-CC', 'OpenWebText2', 'USPTO', 'Project Gutenberg', 
 'OpenSubtitles', 'Wikipedia', 'DM Mathematics', 'HackerNews', 'Pushshift.io Reddit dataset', 'CCNew 
sV2'], 'license': 'other', 'github': 'We present Open Pretrained Transformers (OPT), a suite of deco 
der-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and re 
sponsibly share with interested researchers.', 'paper': 'We present Open Pretrained Transformers (OP 
T), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we  
aim to fully and responsibly share with interested researchers. Our aim in developing this suite of  
OPT models is to enable reproducible and responsible research at scale, and to bring more voices to  
the table in studying the impact of these LLMs.', 'upstream_model': "Meta AI's model card and OPT-17 
5B", 'parameter_count': 'GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days', 'hyper_parameters': 
 [{'epochs': "Meta AI's model card", 'batch_size': 'training data used for this model contains a lot 
 of unfiltered content from the internet', 'learning_rate': 'OPT-175B has limitations in terms of bi 
as and safety', 'optimizer': 'OPT-175B can also have quality issues in terms of generation diversity 
 and hallucination'}], 'evaluation': [{'test': "As mentioned in Meta AI's model card, given that the 
 training data used for this model contains a lot of unfiltered content from the internet, which is  
far from neutral the model is strongly biased", 'result': 0.0}], 'hardware': "Meta AI's model card a 
nd training data used for this model contains a lot of unfiltered content from the internet", 'limit 
ation_and_bias': "As mentioned in Meta AI's model card, given that the training data used for this m 
odel contains a lot of unfiltered content from the internet, which is far from neutral the model is  
strongly biased", 'demo': 'We present Open Pretrained Transformers (OPT), a suite of decoder-only pr 
e-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly s 
hare with interested researchers.', 'input_format': 'BookCorpus, CC-Stories, Pile-CC, OpenWebText2,  
USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit  
dataset, CCNewsV2, CommonCrawl News dataset', 'output_format': '', 'input_token_limit': 'GPT2, Byte  
Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*', 'vocabulary_size': '180B tokens corre 
sponding to 800GB of data'}, {'datasets': ['BookCorpus', 'CC-Stories', 'The Pile', 'Pushshift.io Red 
dit dataset', 'CCNewsV2'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'paramete 
r_count': '125M to 175B parameters', 'hyper_parameters': [], 'evaluation': [], 'hardware': 'GPT2, 80 
GB A100 GPUs, ~33 days', 'limitation_and_bias': 'known challenges in areas such as robustness, bias, 
 and toxicity', 'demo': '', 'input_format': 'GPT2 byte-level version of Byte Pair Encoding (BPE), vo 
cabulary size of 50272, inputs are sequences of 2048 consecutive tokens. input_format', 'output_form 
at': '', 'input_token_limit': '', 'vocabulary_size': 'GPT2, vocabulary size of 50272'}, {'datasets': 
 [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_p 
arameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_form 
at': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                       

#####################vennify/t5-base-grammar-correction########################

-------------------- datasets --------------------
Document 1:

"Happy Transformer" using a dataset called "JFLEG"
------------------------------
Document 2:

datasets: - jfleg
-------------------- license --------------------
Document 1:

license: cc-by-nc-sa-4.0
------------------------------
Document 2:

`pip install happytransformer`, `from happytransformer import HappyTextToText, TTSettings`, `happy_tt = HappyTextToText("T5", "vennify/t5-base-grammar-correction")`, `args = TTSettings(num_beams=5, min_length=1)`, `result = happy_tt.generate_text("grammar: This sentences has has bads grammar.", args=args)`, `print(result.text) # This sentence has bad grammar.`

NO_OUTPUT
-------------------- github --------------------
Document 1:

`HappyTextToText("T5", "vennify/t5-base-grammar-correction")`
-------------------- paper --------------------
Document 1:

"JFLEG" and "arxiv.org/abs/1702.04066"
------------------------------
Document 2:

"This sentences has has bads grammar."
-------------------- upstream_model --------------------
Document 1:

Happy Transformer, JFLEG
------------------------------
Document 2:

"HappyTextToText("T5", "vennify/t5-base-grammar-correction")"

upstream_model: T5
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

num_beams=5, min_length=1
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Happy Transformer, JFLEG
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"It was trained with [Happy Transformer](https://github.com/EricFillion/happy-transformer) using a dataset called [JFLEG](https://arxiv.org/abs/1702.04066)."
------------------------------
Document 2:

`pip install happytransformer`, `from happytransformer import HappyTextToText, TTSettings`, `happy_tt = HappyTextToText("T5", "vennify/t5-base-grammar-correction")`, `args = TTSettings(num_beams=5, min_length=1)`, `result = happy_tt.generate_text("grammar: This sentences has has bads grammar.", args=args)`, `print(result.text) # This sentence has bad grammar.`
-------------------- input_format --------------------
Document 1:

`HappyTextToText("T5", "vennify/t5-base-grammar-correction")` `TTSettings(num_beams=5, min_length=1)` `happy_tt.generate_text("grammar: This sentences has has bads grammar.", args=args)`
-------------------- output_format --------------------
Document 1:

`HappyTextToText("T5", "vennify/t5-base-grammar-correction")` `TTSettings(num_beams=5, min_length=1)` `happy_tt.generate_text("grammar: This sentences has has bads grammar.", args=args)` `print(result.text) # This sentence has bad grammar.`
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['JFLEG'], 'license': 'cc-by-nc-sa-4.0', 'github': 'vennify/t5-base-grammar-correctio 
n', 'paper': 'arxiv.org/abs/1702.04066', 'upstream_model': 'T5', 'parameter_count': '', 'hyper_param 
eters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'h 
ardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input 
_token_limit': '', 'vocabulary_size': ''}]                                                           

#####################xlnet-base-cased########################

-------------------- datasets --------------------
Document 1:

datasets: - bookcorpus - wikipedia
------------------------------
Document 2:

"XLNet model pre-trained on English language" and "[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)" and "[this repository](https://github.com/zihangdai/xlnet/)"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?search=xlnet)" and "For tasks such as text generation, you should look at models like GPT2."
-------------------- paper --------------------
Document 1:

[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
------------------------------
Document 2:

"title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},"
-------------------- upstream_model --------------------
Document 1:

upstream_model: XLNet model pre-trained on English language.
------------------------------
Document 2:

Transformer-XL
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"XLNet model pre-trained on English language" "introduced in the paper [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Yang et al." "first released in [this repository](https://github.com/zihangdai/xlnet/)."
-------------------- evaluation --------------------
Document 1:

XLNet model pre-trained on English language. It was introduced in the paper [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Yang et al. and first released in [this repository](https://github.com/zihangdai/xlnet/).
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"XLNet model pre-trained on English language" NO_OUTPUT
-------------------- demo --------------------
Document 1:

See the [model hub](https://huggingface.co/models?search=xlnet)
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

"return_tensors="pt""
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"XLNet model pre-trained on English language" "Yang et al." "this repository" "The team releasing XLNet did not write a model card for this model"

[{'datasets': ['bookcorpus', 'wikipedia'], 'license': 'mit', 'github': 'https://github.com/zihangda 
i/xlnet/', 'paper': 'https://arxiv.org/abs/1906.08237', 'upstream_model': 'XLNet model pre-trained o 
n English language', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': [], 'evaluation': [], 'hard 
ware': '', 'limitation_and_bias': 'NO_OUTPUT', 'demo': 'See the [model hub](https://huggingface.co/m 
odels?search=xlnet)', 'input_format': '', 'output_format': 'return_tensors="pt"', 'input_token_limit 
': '', 'vocabulary_size': 'NO_OUTPUT'}]                                                              
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dfc4f-178e70010d04fc366ca3fdfd)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-IBertModel/resolve/main/README.md. 

#####################timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k########################

-------------------- datasets --------------------
Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

datasets: - imagenet-1k - imagenet-21k
------------------------------
Document 3:

"ImageNet-21k" "ImageNet-1k"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"GitHub repository" and "\url{https://github.com/huggingface/pytorch-image-models}"
------------------------------
Document 2:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"
-------------------- paper --------------------
Document 1:

"model results"
------------------------------
Document 2:

```bibtex
@article{dosovitskiy2020vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
journal={ICLR},
year={2021}
}
```
------------------------------
Document 3:

"paper authors"
-------------------- upstream_model --------------------
Document 1:

upstream_model Vision Transformer (ViT)
------------------------------
Document 2:

upstream_model: Vision Transformer
-------------------- parameter_count --------------------
Document 1:

Params (M): 5.7
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).
------------------------------
Document 2:

A Vision Transformer (ViT) image classification model. Trained on ImageNet-21k and fine-tuned on ImageNet-1k (with additional augmentation and regularization) in JAX by paper authors, ported to PyTorch by Ross Wightman.
------------------------------
Document 3:

- Model Type: Image classification / feature backbone
- Model Stats: Params (M): 5.7, GMACs: 1.1, Activations (M): 4.1, Image size: 224 x 224
- Dataset: ImageNet-1k
- Pretrain Dataset: ImageNet-21k
-------------------- hardware --------------------
Document 1:

"Trained on ImageNet-21k and fine-tuned on ImageNet-1k (with additional augmentation and regularization)"
------------------------------
Document 2:

"ImageNet-1k" and "ImageNet-21k"
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias
-------------------- demo --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"A Vision Transformer (ViT) image classification model." "Trained on ImageNet-21k and fine-tuned on ImageNet-1k (with additional augmentation and regularization) in JAX by paper authors, ported to PyTorch by Ross Wightman."
-------------------- input_format --------------------
Document 1:

"Image size: 224 x 224", "Dataset: ImageNet-1k", "Pretrain Dataset: ImageNet-21k"
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"additional augmentation and regularization"
-------------------- input_size --------------------
Document 1:

"Image size: 224 x 224"
------------------------------
Document 2:

'224'
-------------------- num_of_classes_for_classification --------------------
Document 1:

num_of_classes_for_classification: ImageNet-1k
-------------------- trigger_word --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'e 
valuation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_f 
ormat': '', 'input_preprocessing': '', 'input_size': '', 'num_of_classes_for_classification': '', 't 
rigger_word': ''}]                                                                                   

#####################flair/ner-english-ontonotes########################

-------------------- datasets --------------------
Document 1:

"Contextual String Embeddings for Sequence Labeling"
------------------------------
Document 2:

"ColumnCorpus(...tag_to_bioes="ner",)" and "WordEmbeddings('en-crawl')" and "FlairEmbeddings('news-forward')" and "FlairEmbeddings('news-backward')"
------------------------------
Document 3:

datasets: - ontonotes
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

[here](https://github.com/flairNLP/flair/issues/)
------------------------------
Document 2:

"github"
-------------------- paper --------------------
Document 1:

"Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- upstream_model --------------------
Document 1:

"SequenceTagger"
------------------------------
Document 2:

"upstream_model"
------------------------------
Document 3:

Flair embeddings and LSTM-CRF
-------------------- parameter_count --------------------
Document 1:

"hidden_size=256" and "max_epochs=150"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

hidden_size=256, max_epochs=150
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- evaluation --------------------
Document 1:

F1-Score: **89.27** (Ontonotes) | **tag** | **meaning** | CARDINAL | cardinal value | DATE | date value | EVENT | event name | FAC | building name | GPE | geo-political entity | LANGUAGE | language name | LAW | law name | LOC | location name | MONEY | money name | NORP | affiliation | ORDINAL | ordinal value | ORG | organization name | PERCENT | percent value | PERSON | person name | PRODUCT | product name | QUANTITY | quantity value | TIME | time value | WORK_OF_ART | name of work of art
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"tag_type = 'ner'", "embedding_types = [WordEmbeddings('en-crawl'), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')]", "embeddings = StackedEmbeddings(embeddings=embedding_types)", "tagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type)", "trainer = ModelTrainer(tagger, corpus)", "trainer.train('resources/taggers/ner-english-ontonotes', train_with_dev=True, max_epochs=150)".
------------------------------
Document 2:

F1-Score: **89.27** (Ontonotes) Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.
-------------------- demo --------------------
Document 1:

"Find a form of demo"
------------------------------
Document 2:

[here](https://github.com/flairNLP/flair/issues/)
-------------------- input_format --------------------
Document 1:

"column_format={0: "text", 1: "pos", 2: "upos", 3: "ner"}"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

[{'datasets': ['ontonotes'], 'license': 'license', 'github': 'github', 'paper': 'Please cite the fo 
llowing paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embedd 
ings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle 
 = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1 
649}, year      = {2018}', 'upstream_model': 'SequenceTagger', 'parameter_count': 'hidden_size=256,  
max_epochs=150', 'hyper_parameters': [], 'evaluation': [{'test': 'F1-Score', 'result': 89.27}], 'har 
dware': '', 'limitation_and_bias': '"tag_type = \'ner\'", "embedding_types = [WordEmbeddings(\'en-cr 
awl\'), FlairEmbeddings(\'news-forward\'), FlairEmbeddings(\'news-backward\')]", "embeddings = Stack 
edEmbeddings(embeddings=embedding_types)", "tagger = SequenceTagger(hidden_size=256, embeddings=embe 
ddings, tag_dictionary=tag_dictionary, tag_type=tag_type)", "trainer = ModelTrainer(tagger, corpus)" 
, "trainer.train(\'resources/taggers/ner-english-ontonotes\', train_with_dev=True, max_epochs=150)". 
', 'demo': 'Find a form of demo', 'input_format': 'column_format={0: "text", 1: "pos", 2: "upos", 3: 
 "ner"}', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': 'tag_dictionary = corpus. 
make_tag_dictionary(tag_type=tag_type)'}]                                                            

#####################sentence-transformers/msmarco-MiniLM-L-12-v3########################

-------------------- datasets --------------------
Document 1:

datasets, Sentence Embeddings Benchmark, [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/msmarco-MiniLM-L-12-v3)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/msmarco-MiniLM-L-12-v3)
-------------------- paper --------------------
Document 1:

"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
------------------------------
Document 2:

"Sentence Embeddings Benchmark"
------------------------------
Document 3:

"sentence-transformers" and "semantic search"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

upstream_model=sentence-transformers/msmarco-MiniLM-L-12-v3
------------------------------
Document 3:

upstream_model: sentence-transformers
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/msmarco-MiniLM-L-12-v3)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/msmarco-MiniLM-L-12-v3)
------------------------------
Document 2:

sentence-transformers, https://www.SBERT.net
------------------------------
Document 3:

pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/msmarco-MiniLM-L-12-v3')
embeddings = model.encode(sentences)
print(embeddings)
-------------------- input_format --------------------
Document 1:

'max_seq_length': 512, 'do_lower_case': False
-------------------- output_format --------------------
Document 1:

'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False
-------------------- input_token_limit --------------------
Document 1:

'max_seq_length': 512
-------------------- vocabulary_size --------------------


[{'datasets': ['Sentence Embeddings Benchmark'], 'license': 'apache-2.0', 'github': 'https://seb.sb 
ert.net?model_name=sentence-transformers/msmarco-MiniLM-L-12-v3', 'paper': 'Sentence-BERT: Sentence  
Embeddings using Siamese BERT-Networks', 'upstream_model': 'sentence-transformers', 'parameter_count 
': "'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 384, 'pooling_mode_cl 
s_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_m 
ean_sqrt_len_tokens': False", 'hyper_parameters': {}, 'evaluation': [{'test': 'Sentence Embeddings B 
enchmark', 'result': 0.0}], 'hardware': '', 'limitation_and_bias': '', 'demo': 'https://seb.sbert.ne 
t?model_name=sentence-transformers/msmarco-MiniLM-L-12-v3', 'input_format': "'max_seq_length': 512,  
'do_lower_case': False", 'output_format': "'max_seq_length': 512, 'do_lower_case': False, 'word_embe 
dding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_m 
ode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False", 'input_token_limit': "'max_seq_ 
length': 512", 'vocabulary_size': ''}]                                                               

#####################Rostlab/prot_bert########################

-------------------- datasets --------------------
Document 1:

Uniref100, https://www.uniprot.org/downloads
------------------------------
Document 2:

datasets, TPU Pod V3-512, 400k steps, sequence length 512, batch size 15k, sequence length 2048, batch size 2.5k, Lamb, learning rate 0.002, weight decay 0.01, learning rate warmup 40k steps, linear decay learning rate after.
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"Availability ProtTrans: \&lt;a href="https://github.com/agemagician/ProtTrans"\&gt;https://github.com/agemagician/ProtTrans\&lt;/a\&gt;"
------------------------------
Document 2:

[this paper](https://doi.org/10.1101/2020.07.12.199554) and [this repository](https://github.com/agemagician/ProtTrans)
-------------------- paper --------------------
Document 1:

[this paper](https://doi.org/10.1101/2020.07.12.199554)
------------------------------
Document 2:

"@article {Elnaggar2020.07.12.199554,
author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},
title = {ProtTrans: Towards Cracking the Language of Life{\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},
elocation-id = {2020.07.12.199554},
year = {2020},
doi = {10.1101/2020.07.12.199554},
publisher = {Cold Spring Harbor Laboratory},
URL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},
eprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},
journal =
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count: single TPU Pod V3-512, 400k steps, sequence length 512 (batch size 15k), sequence length 2048 (batch size 2.5k), optimizer: Lamb, learning rate: 0.002, weight decay: 0.01, learning rate warmup: 40k steps, linear decay of learning rate after.
-------------------- hyper_parameters --------------------
Document 1:

learning rate of 0.002, a weight decay of 0.01, learning rate warmup for 40k steps and linear decay of the learning rate after.
-------------------- evaluation --------------------
Document 1:

"When fine-tuned on downstream tasks, this model achieves the following results: Test results : | Task/Dataset | secondary structure (3-states) | secondary structure (8-states)  |  Localization | Membrane  | |:-----:|:-----:|:-----:|:-----:|:-----:| |   CASP12  | 75 | 63 |    |    | |   TS115   | 83 | 72 |    |    | |   CB513   | 81 | 66 |    |    | |  DeepLoc  |    |    | 79 | 91 |"
------------------------------
Document 2:

"The model was trained on a single TPU Pod V3-512 for 400k steps in total. 300K steps using sequence length 512 (batch size 15k), and 100K steps using sequence length 2048 (batch size 2.5k). The optimizer used is Lamb with a learning rate of 0.002, a weight decay of 0.01, learning rate warmup for 40k steps and linear decay of the learning rate after."
-------------------- hardware --------------------
Document 1:

single TPU Pod V3-512, 400k steps, sequence length 512, batch size 15k, sequence length 2048, batch size 2.5k, Lamb, learning rate 0.002, weight decay 0.01, learning rate warmup 40k steps, linear decay learning rate after
-------------------- limitation_and_bias --------------------
Document 1:

When fine-tuned on downstream tasks, this model achieves the following results: Test results : | Task/Dataset | secondary structure (3-states) | secondary structure (8-states)  |  Localization | Membrane  | |:-----:|:-----:|:-----:|:-----:|:-----:| |   CASP12  | 75 | 63 |    |    | |   TS115   | 83 | 72 |    |    | |   CB513   | 81 | 66 |    |    | |  DeepLoc  |    |    | 79 | 91 |
------------------------------
Document 2:

"Pretrained model on protein sequences using a masked language modeling (MLM) objective" "It only works with capital letter amino acids."
-------------------- demo --------------------
Document 1:

"Availability ProtTrans: \&lt;a href="https://github.com/agemagician/ProtTrans"\&gt;https://github.com/agemagician/ProtTrans\&lt;/a\&gt;"
------------------------------
Document 2:

"The model was trained on a single TPU Pod V3-512 for 400k steps in total. 300K steps using sequence length 512 (batch size 15k), and 100K steps using sequence length 2048 (batch size 2.5k). The optimizer used is Lamb with a learning rate of 0.002, a weight decay of 0.01, learning rate warmup for 40k steps and linear decay of the learning rate after."
-------------------- input_format --------------------
Document 1:

"The protein sequences are uppercased and tokenized using a single space and a vocabulary size of 21. The rare amino acids "U,Z,O,B" were mapped to "X". The inputs of the model are then of the form: [CLS] Protein Sequence A [SEP] Protein Sequence B [SEP]. Furthermore, each protein sequence was treated as a separate document. The preprocessing step was performed twice, once for a combined length (2 sequences) of less than 512 amino acids, and another time using a combined length (2 sequences) of less than 2048 amino acids. The details of the masking procedure for each sequence followed the original Bert model as following: - 15% of the amino acids are masked. - In 80% of the cases, the masked amino acids are replaced by `[MASK]`. - In 10% of the cases, the masked amino acids are replaced by a random amino acid (different) from the one they replace. - In the 10% remaining cases, the masked amino acids are left as is."
------------------------------
Document 2:

"ProtBert is based on Bert model which pretrained on a large corpus of protein sequences in a self-supervised fashion...The masking follows the original Bert training with randomly masks 15% of the amino acids in the input."
-------------------- output_format --------------------
Document 1:

The protein sequences are uppercased and tokenized using a single space and a vocabulary size of 21. The rare amino acids "U,Z,O,B" were mapped to "X". The inputs of the model are then of the form:  
```
[CLS] Protein Sequence A [SEP] Protein Sequence B [SEP]
```  
Furthermore, each protein sequence was treated as a separate document. The preprocessing step was performed twice, once for a combined length (2 sequences) of less than 512 amino acids, and another time using a combined length (2 sequences) of less than 2048 amino acids. The details of the masking procedure for each sequence followed the original Bert model as following:
- 15% of the amino acids are masked.
- In 80% of the cases, the masked amino acids are replaced by `[MASK]`.
- In 10% of the cases, the masked amino acids are replaced by a random amino acid (different) from the one they replace.
- In the 10% remaining cases, the masked amino acids are left as is.
------------------------------
Document 2:

Test results :  
| Task/Dataset | secondary structure (3-states) | secondary structure (8-states)  |  Localization | Membrane  |
|:-----:|:-----:|:-----:|:-----:|:-----:|
|   CASP12  | 75 | 63 |    |    |
|   TS115   | 83 | 72 |    |    |
|   CB513   | 81 | 66 |    |    |
|  DeepLoc  |    |    | 79 | 91 |
-------------------- input_token_limit --------------------
Document 1:

"The inputs of the model are then of the form: [CLS] Protein Sequence A [SEP] Protein Sequence B [SEP]", "The preprocessing step was performed twice, once for a combined length (2 sequences) of less than 512 amino acids, and another time using a combined length (2 sequences) of less than 2048 amino acids."
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 21, [MASK]

[{'datasets': ['Uniref100'], 'license': '', 'github': 'https://github.com/agemagician/ProtTrans', ' 
paper': 'https://doi.org/10.1101/2020.07.12.199554', 'upstream_model': '', 'parameter_count': 'singl 
e TPU Pod V3-512, 400k steps, sequence length 512 (batch size 15k), sequence length 2048 (batch size 
 2.5k), optimizer: Lamb, learning rate: 0.002, weight decay: 0.01, learning rate warmup: 40k steps,  
linear decay of learning rate after.', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learnin 
g_rate': '0.002', 'optimizer': 'Lamb'}, 'evaluation': [{'test': 'secondary structure (3-states)', 'r 
esult': 75}, {'test': 'secondary structure (8-states)', 'result': 63}, {'test': 'Localization', 'res 
ult': 79}, {'test': 'Membrane', 'result': 91}], 'hardware': 'single TPU Pod V3-512, 400k steps, sequ 
ence length 512, batch size 15k, sequence length 2048, batch size 2.5k, Lamb, learning rate 0.002, w 
eight decay 0.01, learning rate warmup 40k steps, linear decay learning rate after', 'limitation_and 
_bias': 'When fine-tuned on downstream tasks, this model achieves the following results: Test result 
s : | Task/Dataset | secondary structure (3-states) | secondary structure (8-states)  |  Localizatio 
n | Membrane  | |:-----:|:-----:|:-----:|:-----:|:-----:| |   CASP12  | 75 | 63 |    |    | |   TS11 
5   | 83 | 72 |    |    | |   CB513   | 81 | 66 |    |    | |  DeepLoc  |    |    | 79 | 91 |', 'dem 
o': 'Availability ProtTrans: <a href="https://github.com/agemagician/ProtTrans">https://github.com/a 
gemagician/ProtTrans</a>', 'input_format': 'The protein sequences are uppercased and tokenized using 
 a single space and a vocabulary size of 21. The rare amino acids "U,Z,O,B" were mapped to "X". The  
inputs of the model are then of the form: [CLS] Protein Sequence A [SEP] Protein Sequence B [SEP]. F 
urthermore, each protein sequence was treated as a separate document. The preprocessing step was per 
formed twice, once for a combined length (2 sequences) of less than 512 amino acids, and another tim 
e using a combined length (2 sequences) of less than 2048 amino acids. The details of the masking pr 
ocedure for each sequence followed the original Bert model as following: - 15% of the amino acids ar 
e masked. - In 80% of the cases, the masked amino acids are replaced by `[MASK]`. - In 10% of the ca 
ses, the masked amino acids are replaced by a random amino acid (different) from the one they replac 
e. - In the 10% remaining cases, the masked amino acids are left as is.', 'output_format': 'The prot 
ein sequences are uppercased and tokenized using a single space and a vocabulary size of 21. The rar 
e amino acids "U,Z,O,B" were mapped to "X". The inputs of the model are then of the form: [CLS] Prot 
ein Sequence A [SEP] Protein Sequence B [SEP]. Furthermore, each protein sequence was treated as a s 
eparate document. The preprocessing step was performed twice, once for a combined length (2 sequence 
s) of less than 512 amino acids, and another time using a combined length (2 sequences) of less than 
 2048 amino acids. The details of the masking procedure for each sequence followed the original Bert 
 model as following: - 15% of the amino acids are masked. - In 80% of the cases, the masked amino ac 
ids are replaced by `[MASK]`. - In 10% of the cases, the masked amino acids are replaced by a random 
 amino acid (different) from the one they replace. - In the 10% remaining cases, the masked amino ac 
ids are left as is.', 'input_token_limit': '', 'vocabulary_size': 'vocabulary size of 21, [MASK]'}]  

#####################gilf/french-camembert-postag-model########################

-------------------- datasets --------------------
Document 1:

"free-french-treebank" dataset, [github](https://github.com/nicolashernandez/free-french-treebank)
-------------------- license --------------------
Document 1:

'free-french-treebank' dataset available on [github](https://github.com/nicolashernandez/free-french-treebank)
-------------------- github --------------------
Document 1:

"github", "free-french-treebank", "camembert-base"
-------------------- paper --------------------
Document 1:

'free-french-treebank' dataset, 'camembert-base'
-------------------- upstream_model --------------------
Document 1:

'camembert-base'
-------------------- parameter_count --------------------
Document 1:

parameter_count french-camembert-postag-model free-french-treebank camembert-base
-------------------- hyper_parameters --------------------
Document 1:

'french-camembert-postag-model', 'free-french-treebank', 'camembert-base'
-------------------- evaluation --------------------
Document 1:

The *french-camembert-postag-model* is a part of speech tagging model for French that was trained on the *free-french-treebank* dataset available on [github](https://github.com/nicolashernandez/free-french-treebank). The base tokenizer and model used for training is *'camembert-base'*.
-------------------- hardware --------------------
Document 1:

'free-french-treebank' dataset, 'camembert-base'
-------------------- limitation_and_bias --------------------
Document 1:

french-camembert-postag-model, free-french-treebank, camembert-base
-------------------- demo --------------------
Document 1:

```python
from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained("gilf/french-camembert-postag-model")
model = AutoModelForTokenClassification.from_pretrained("gilf/french-camembert-postag-model")

from transformers import pipeline

nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)

nlp_token_class('Face à un choc inédit, les mesures mises en place par le gouvernement ont permis une protection forte et efficace des ménages')
```
```
[{'entity_group': 'NC', 'score': 0.5760144591331482, 'word': '<s>'},
{'entity_group': 'U', 'score': 0.9946700930595398, 'word': 'Face'},
{'entity_group': 'P', 'score': 0.999615490436554, 'word': 'à
------------------------------
Document 2:

"french-camembert-postag-model", "free-french-treebank", "camembert-base", "github.com/nicolashernandez/free-french-treebank"
-------------------- input_format --------------------
Document 1:

'free-french-treebank', 'camembert-base'
------------------------------
Document 2:

"It uses the following tags: | Tag      |          Category              |  Extra Info | |----------|:------------------------------:|------------:| | ADJ      |           adjectif             |             | | ADJWH    |           adjectif             |             | | ADV      |           adverbe              |             | | ADVWH    |           adverbe              |             | | CC       |  conjonction de coordination   |             | | CLO      |            pronom              |     obj     | | CLR      |            pronom              |     refl    | | CLS      |            pronom              |     suj     | | CS       |  conjonction de subordination  |             | | DET      |          déterminant           |             | | DETWH    |          déterminant           |             | | ET       |          mot étranger          |             | | I        |          interjection          |             | | NC       |          nom commun            |             | | NPP      |          nom propre            |             | | P        |          préposition           |             | | P+D      |   préposition + déterminant    |             | | P
-------------------- output_format --------------------
Document 1:

"pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)"
NO_OUTPUT
------------------------------
Document 2:

'french-camembert-postag-model', 'free-french-treebank', 'camembert-base'
------------------------------
Document 3:

"It uses the following tags: | Tag      |          Category              |  Extra Info | |----------|:------------------------------:|------------:| | ADJ      |           adjectif             |             | | ADJWH    |           adjectif             |             | | ADV      |           adverbe              |             | | ADVWH    |           adverbe              |             | | CC       |  conjonction de coordination   |             | | CLO      |            pronom              |     obj     | | CLR      |            pronom              |     refl    | | CLS      |            pronom              |     suj     | | CS       |  conjonction de subordination  |             | | DET      |          déterminant           |             | | DETWH    |          déterminant           |             | | ET       |          mot étranger          |             | | I        |          interjection          |             | | NC       |          nom commun            |             | | NPP      |          nom propre            |             | | P        |          préposition           |             | | P+D      |   préposition + déterminant    |             | | P
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['free-french-treebank'], 'license': '', 'github': 'https://github.com/nicolashernand 
ez/free-french-treebank', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameter 
s': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 
 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                               

#####################magorshunov/layoutlm-invoices########################

-------------------- datasets --------------------
Document 1:

"proprietary dataset of invoices" and "[SQuAD2.0](https://huggingface.co/datasets/squad_v2)" and "[DocVQA](https://www.docvqa.org/)"
------------------------------
Document 2:

"This model was created by the team at [Impira](https://www.impira.com/)."
-------------------- license --------------------
Document 1:

license: cc-by-nc-sa-4.0
-------------------- github --------------------
Document 1:

"github" "DocQuery" "https://github.com/impira/docquery"
------------------------------
Document 2:

"github"
-------------------- paper --------------------
Document 1:

"This model was created by the team at [Impira](https://www.impira.com/)."
------------------------------
Document 2:

"multi-modal [LayoutLM](https://aka.ms/layoutlm) model" and "fine-tuned on a proprietary dataset of invoices as well as both [SQuAD2.0](https://huggingface.co/datasets/squad_v2) and [DocVQA](https://www.docvqa.org/)"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"QA models often encounter this failure mode" NO_OUTPUT
-------------------- demo --------------------
Document 1:

DocQuery https://github.com/impira/docquery
------------------------------
Document 2:

src: https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png
src: https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg
-------------------- input_format --------------------
Document 1:

"pipeline_tag: document-question-answering" and "src: https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png" and "src: https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg"
-------------------- output_format --------------------


[{'datasets': ['proprietary dataset of invoices', 'SQuAD2.0', 'DocVQA'], 'license': 'cc-by-nc-sa-4. 
0', 'github': 'https://github.com/impira/docquery', 'paper': 'This model was created by the team at  
[Impira](https://www.impira.com/).', 'upstream_model': 'upstream_model', 'parameter_count': 'paramet 
er_count', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': 'QA mode 
ls often encounter this failure mode', 'demo': 'DocQuery https://github.com/impira/docquery', 'input 
_format': '"pipeline_tag: document-question-answering" and "src: https://huggingface.co/spaces/impir 
a/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png" and "src: https://huggingfa 
ce.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg"', 'outp 
ut_format': ''}]                                                                                     

#####################oliverguhr/german-sentiment-bert########################

-------------------- datasets --------------------
Document 1:

1.834 million German-language samples, texts from various domains like Twitter, Facebook and movie, app and hotel reviews, [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf).
------------------------------
Document 2:

[holidaycheck](https://github.com/oliverguhr/german-sentiment), [scare](https://www.romanklinger.de/scare/), [filmstarts](https://github.com/oliverguhr/german-sentiment), [germeval](https://sites.google.com/view/germeval2017-absa/home), [PotTS](https://www.aclweb.org/anthology/L16-1181/), [emotions](https://github.com/oliverguhr/german-sentiment), [sb10k](https://www.spinningbytes.com/resources/germansentiment/), [Leipzig Wikipedia Corpus 2016](https://wortschatz.uni-leipzig.de/de/download/german)
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

[this repository](https://github.com/oliverguhr/german-sentiment) and our [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf), [holidaycheck](https://github.com/oliverguhr/german-sentiment), [filmstarts](https://github.com/oliverguhr/german-sentiment), [germeval](https://sites.google.com/view/germeval2017-absa/home), [emotions](https://github.com/oliverguhr/german-sentiment), [sb10k](https://www.spinningbytes.com/resources/germansentiment/), [Leipzig Wikipedia Corpus 2016](https://wortschatz.uni-leipzig.de/de/download/german)
------------------------------
Document 2:

"The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews." NO_OUTPUT
-------------------- paper --------------------
Document 1:

"The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews. You can find more information about the dataset and the training process in the [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf)."
------------------------------
Document 2:

"Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems"
------------------------------
Document 3:

"our [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf)"
-------------------- upstream_model --------------------
Document 1:

"Googles Bert architecture" and "[paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf)"
-------------------- parameter_count --------------------
Document 1:

"1.834 million German-language samples"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"Here is a table of the F1 scores that this model achieves on different datasets. Since we trained this model with a newer version of the transformer library, the results are slightly better than reported in the paper.  
| Dataset                                                      | F1 micro Score |
| :----------------------------------------------------------- | -------------: |
| [holidaycheck](https://github.com/oliverguhr/german-sentiment) |         0.9568 |
| [scare](https://www.romanklinger.de/scare/)                  |         0.9418 |
| [filmstarts](https://github.com/oliverguhr/german-sentiment) |         0.9021 |
| [germeval](https://sites.google.com/view/germeval2017-absa/home) |         0.7536 |
| [PotTS](https://www.aclweb.org/anthology/L16-1181/)          |         0.6780 |
| [emotions](https://github.com/oliverguhr/german-sentiment)  |         0.9649 |
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews."
-------------------- demo --------------------
Document 1:

"Python package that bundles the code need for the preprocessing and inferencing" and "[paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf)"
------------------------------
Document 2:

```bash
pip install germansentiment
```

```python
from germansentiment import SentimentModel

model = SentimentModel()

texts = [
"Mit keinem guten Ergebniss","Das ist gar nicht mal so gut",
"Total awesome!","nicht so schlecht wie erwartet",
"Der Test verlief positiv.","Sie fährt ein grünes Auto."]

result = model.predict_sentiment(texts)
print(result)
```

```python
["negative","negative","positive","positive","neutral", "neutral"]
```
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['1.834 million German-language samples'], 'license': 'mit', 'github': '[this reposit 
ory](https://github.com/oliverguhr/german-sentiment)', 'paper': '[paper](http://www.lrec-conf.org/pr 
oceedings/lrec2020/pdf/2020.lrec-1.202.pdf)', 'upstream_model': 'Googles Bert architecture', 'parame 
ter_count': '1.834 million German-language samples', 'hyper_parameters': [], 'evaluation': [{'test': 
 '[holidaycheck](https://github.com/oliverguhr/german-sentiment)', 'result': 0.9568}, {'test': '[sca 
re](https://www.romanklinger.de/scare/)', 'result': 0.9418}, {'test': '[filmstarts](https://github.c 
om/oliverguhr/german-sentiment)', 'result': 0.9021}, {'test': '[germeval](https://sites.google.com/v 
iew/germeval2017-absa/home)', 'result': 0.7536}, {'test': '[PotTS](https://www.aclweb.org/anthology/ 
L16-1181/)', 'result': 0.678}, {'test': '[emotions](https://github.com/oliverguhr/german-sentiment)' 
, 'result': 0.9649}], 'hardware': '', 'limitation_and_bias': 'The model uses the Googles Bert archit 
ecture and was trained on 1.834 million German-language samples. The training data contains texts fr 
om various domains like Twitter, Facebook and movie, app and hotel reviews.', 'demo': 'Python packag 
e that bundles the code need for the preprocessing and inferencing', 'input_format': '', 'output_for 
mat': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                           

#####################openai/whisper-tiny.en########################

-------------------- datasets --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages."
------------------------------
Document 2:

- name: LibriSpeech (clean)
type: librispeech_asr
config: clean
split: test
args:
language: en
- name: LibriSpeech (other)
type: librispeech_asr
config: other
split: test
args:
language: en
-------------------- license --------------------
Document 1:

arXiv.org perpetual, non-exclusive license
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
-------------------- paper --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
------------------------------
Document 2:

[the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf) and [the paper](https://cdn.openai.com/papers/whisper.pdf)
------------------------------
Document 3:

doi = {10.48550/ARXIV.2212.04356}, url = {https://arxiv.org/abs/2212.04356}, title = {Robust Speech Recognition via Large-Scale Weak Supervision}
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count: 0
------------------------------
Document 2:

39 M, 74 M, 244 M, 769 M, 1550 M
-------------------- hyper_parameters --------------------
Document 1:

"fine-tuning" "step-by-step guide to fine-tuning the Whisper model" NO_OUTPUT
-------------------- evaluation --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf)."
------------------------------
Document 3:

"We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them."
-------------------- hardware --------------------
Document 1:

"This non-English data represents 98 different languages." NO_OUTPUT
-------------------- limitation_and_bias --------------------
Document 1:

"the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level...Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria...In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.
------------------------------
Document 2:

"The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model." "We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them." "We caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification." "We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes."
------------------------------
Document 3:

The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.
-------------------- demo --------------------
Document 1:

- example_title: Librispeech sample 1
src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- example_title: Librispeech sample 2
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
------------------------------
Document 2:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 3:

"The blog post [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data."
-------------------- input_format --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
------------------------------
Document 2:

- example_title: Librispeech sample 1
src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- example_title: Librispeech sample 2
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
------------------------------
Document 3:

labelled speech data
-------------------- output_format --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
-------------------- sample_rate --------------------
Document 1:

`chunk_length_s=30`
-------------------- WER --------------------
Document 1:

"We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages." NO_OUTPUT

[{'datasets': ['LibriSpeech'], 'license': 'apache-2.0', 'github': 'https://github.com/openai/whispe 
r', 'paper': 'https://cdn.openai.com/papers/whisper.pdf', 'upstream_model': '', 'parameter_count': ' 
39 M, 74 M, 244 M, 769 M, 1550 M', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_ra 
te': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_an 
d_bias': 'The primary intended users of these models are AI researchers studying robustness, general 
ization, capabilities, biases, and constraints of the current model.', 'demo': 'The models are train 
ed on 680,000 hours of audio and the corresponding transcripts collected from the internet.', 'input 
_format': '', 'output_format': ''}]                                                                  
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dfdb6-661424467d67583048d16855)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-bert/resolve/main/README.md. 

#####################dreamlike-art/dreamlike-photoreal-2.0########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

"This model is licesed under a **modified** CreativeML OpenRAIL-M license."
"- **You are not allowed to host, finetune, or do inference with the model or its derivatives on websites/apps/etc. If you want to, please email us at contact@dreamlike.art**"
"- **You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Photoreal 2.0) and include the license as well as a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)**"
"- **You are free to use the outputs (images) of the model for commercial purposes in teams of 10 or less**"
"- You can't use the model to deliberately produce nor share illegal or harmful outputs or content"
"- The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license"
"- You may re-distribute the weights.
------------------------------
Document 2:

license: other
-------------------- github --------------------
Document 1:

"stable-diffusion, stable-diffusion-diffusers, text-to-image, photorealistic, photoreal, diffusers"
------------------------------
Document 2:

- **You are not allowed to host, finetune, or do inference with the model or its derivatives on websites/apps/etc. If you want to, please email us at contact@dreamlike.art**
- **You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Photoreal 2.0) and include the license as well as a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)**
- You may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/blob/main/LICENSE.md
-------------------- paper --------------------
Document 1:

- The model is licesed under a **modified** CreativeML OpenRAIL-M license. 
- Please read the full license here: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/blob/main/LICENSE.md
------------------------------
Document 2:

"This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"This model was trained on 768x768px images"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px."
-------------------- limitation_and_bias --------------------
Document 1:

- You are not allowed to host, finetune, or do inference with the model or its derivatives on websites/apps/etc. 
- You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc. 
- You are free to use the outputs (images) of the model for commercial purposes in teams of 10 or less
- You can't use the model to deliberately produce nor share illegal or harmful outputs or content
- The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license
- You may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)
------------------------------
Document 2:

"This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px."
-------------------- demo --------------------
Document 1:

[dreamlike.art](https://dreamlike.art/)
-------------------- input_format --------------------
Document 1:

"This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px."
-------------------- output_format --------------------
Document 1:

"You are free to use the outputs (images) of the model for commercial purposes in teams of 10 or less" "Please read the full license here: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/blob/main/LICENSE.md"
------------------------------
Document 2:

"This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px."

[{'datasets': [], 'license': 'modified CreativeML OpenRAIL-M', 'github': '', 'paper': '', 'upstream 
_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limit 
ation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]                           

#####################roberta-large-mnli########################

-------------------- datasets --------------------
Document 1:

"modeling architecture, objective, compute infrastructure, and training details"
------------------------------
Document 2:

- [Multi-Genre Natural Language Inference (MNLI)](https://cims.nyu.edu/~sbowman/multinli/) corpus
- [MNLI data card](https://huggingface.co/datasets/multi_nli)
- [BookCorpus](https://yknzhu.wixsite.com/mbweb)
- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)
- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/)
- [OpenWebText](https://github.com/jcpeterson/openwebtext)
- [Stories](https://arxiv.org/abs/1806.02847)
- [bookcorpus data card](https://huggingface.co/datasets/bookcorpus)
- [wikipedia data card](https://huggingface.co/datasets/wikipedia)
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"This fine-tuned model can be used for zero-shot classification tasks, including zero-shot sentence-pair classification (see the [GitHub repo](https://github.com/facebookresearch/fairseq/tree/main/examples/roberta) for examples)"
-------------------- paper --------------------
Document 1:

"associated paper", "modeling architecture", "objective", "compute infrastructure", "training details"
------------------------------
Document 2:

"Citation Information"
------------------------------
Document 3:

`title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach}, author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov}, journal={arXiv preprint arXiv:1907.11692}, year = {2019},`
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"The RoBERTa model was pretrained on the reunion of five datasets... Together theses datasets weight 160GB of text... The model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512." parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"The [RoBERTa large model card](https://huggingface.co/roberta-large) notes that: "The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral." 

NO_OUTPUT
-------------------- evaluation --------------------
Document 1:

"#evaluation-results"
------------------------------
Document 2:

- **Dataset:** Part of [GLUE (Wang et al., 2019)](https://arxiv.org/pdf/1804.07461.pdf), the General Language Understanding Evaluation benchmark, a collection of 9 datasets for evaluating natural language understanding systems. Specifically, the model was evaluated on the [Multi-Genre Natural Language Inference (MNLI)](https://cims.nyu.edu/~sbowman/multinli/) corpus. See the [GLUE data card](https://huggingface.co/datasets/glue) or [Wang et al. (2019)](https://arxiv.org/pdf/1804.07461.pdf) for further information.
- **Tasks:** NLI. [Wang et al. (2019)](https://arxiv.org/pdf/1804.07461.pdf) describe the inference task for MNLI as:
> The Multi-Genre Natural Language Inference Corpus [(Williams et al., 2018)](https://arxiv.org/abs/1704.05426) is a crowd-sourced collection of sentence pairs with textual entail
-------------------- hardware --------------------
Document 1:

"compute infrastructure"
------------------------------
Document 2:

- **Hardware Type:** 1024 V100 GPUs
- **Hours used:** 24 hours (one day)
------------------------------
Document 3:

Technical Specifications
-------------------- limitation_and_bias --------------------
Document 1:

Risks, Limitations and Biases
------------------------------
Document 2:

"Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). The [RoBERTa large model card](https://huggingface.co/roberta-large) notes that: "The training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral." Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`. 15% of the tokens are masked. In 80% of the cases, the masked tokens are replaced by `<mask>`. In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. In the 10% remaining cases, the masked tokens are left as is. 

input_format: The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked with `<s>` and the end of one by `</s>`. 15% of the tokens are masked. In 80% of the cases, the masked tokens are replaced by `<mask>`. In 10% of the cases, the masked tokens are replaced
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

The inputs of the model take pieces of 512 contiguous token that may span over documents.
-------------------- vocabulary_size --------------------
Document 1:

"The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000."

[{'datasets': ['Multi-Genre Natural Language Inference (MNLI)', 'BookCorpus', 'English Wikipedia',  
'CC-News', 'OpenWebText', 'Stories'], 'license': 'mit', 'github': 'https://github.com/facebookresear 
ch/fairseq/tree/main/examples/roberta', 'paper': 'https://arxiv.org/abs/1907.11692', 'upstream_model 
': '', 'parameter_count': '500K', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rat 
e': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', ' 
input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]             

#####################flair/ner-english########################

-------------------- datasets --------------------
Document 1:

"Contextual String Embeddings for Sequence Labeling"
------------------------------
Document 2:

CONLL_03(), WordEmbeddings('glove'), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')
------------------------------
Document 3:

datasets: - conll2003
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

[here](https://github.com/flairNLP/flair/issues/)
------------------------------
Document 2:

"github"
-------------------- paper --------------------
Document 1:

"Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}"
------------------------------
Document 2:

"Flair embeddings" and "LSTM-CRF"
-------------------- upstream_model --------------------
Document 1:

"upstream_model"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
------------------------------
Document 3:

WordEmbeddings('glove'),
FlairEmbeddings('news-forward'),
FlairEmbeddings('news-backward'),
StackedEmbeddings(embeddings=embedding_types)
-------------------- parameter_count --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
------------------------------
Document 2:

"hidden_size=256,", "embeddings=embeddings,", "tag_dictionary=tag_dictionary,"
------------------------------
Document 3:

"parameter_count"
-------------------- hyper_parameters --------------------
Document 1:

hidden_size=256, max_epochs=150
------------------------------
Document 2:

"F1-Score: **93,06** (corrected CoNLL-03)"
-------------------- evaluation --------------------
Document 1:

F1-Score: **93,06** (corrected CoNLL-03)  
| **tag**                        | **meaning** |
|---------------------------------|-----------|
| PER         | person name |
| LOC         | location name |
| ORG         | organization name |
| MISC         | other name |  
Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.
-------------------- hardware --------------------
Document 1:

WordEmbeddings('glove'), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward'), StackedEmbeddings(embeddings=embedding_types), SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type), ModelTrainer(tagger, corpus)
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- limitation_and_bias --------------------
Document 1:

"F1-Score: **93,06** (corrected CoNLL-03)"
-------------------- demo --------------------
Document 1:

"Find a form of demo"
------------------------------
Document 2:

language: en, tags: - flair - token-classification - sequence-tagger-model, datasets: - conll2003, widget: - text: George Washington went to Washington
------------------------------
Document 3:

[here](https://github.com/flairNLP/flair/issues/)
-------------------- input_format --------------------
Document 1:

Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.
------------------------------
Document 2:

CONLL_03(), WordEmbeddings('glove'), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')
input_format: CONLL_03(), WordEmbeddings('glove'), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')
-------------------- output_format --------------------
Document 1:

"F1-Score: **93,06** (corrected CoNLL-03)" "Predicts 4 tags: | **tag**                        | **meaning** | |---------------------------------|-----------| | PER         | person name | | LOC         | location name | | ORG         | organization name | | MISC         | other name |  Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

[{'datasets': ['conll2003'], 'license': 'license', 'github': '[here](https://github.com/flairNLP/fl 
air/issues/)', 'paper': 'Please cite the following paper when using this model. @inproceedings{akbik 
2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blyt 
he, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Compu 
tational Linguistics}, pages     = {1638--1649}, year      = {2018}', 'upstream_model': 'upstream_mo 
del', 'parameter_count': 'parameter_count', 'hyper_parameters': [{'epochs': '150', 'batch_size': '', 
 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': 'F1-Score: **93,06** (corrected CoN 
LL-03)', 'result': 93.06}], 'hardware': "WordEmbeddings('glove'), FlairEmbeddings('news-forward'), F 
lairEmbeddings('news-backward'), StackedEmbeddings(embeddings=embedding_types), SequenceTagger(hidde 
n_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type), ModelTrainer(t 
agger, corpus)", 'limitation_and_bias': 'F1-Score: **93,06** (corrected CoNLL-03)', 'demo': 'Find a  
form of demo', 'input_format': 'Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-113 
9/) and LSTM-CRF.', 'output_format': 'F1-Score: **93,06** (corrected CoNLL-03) "Predicts 4 tags: | * 
*tag**                        | **meaning** | |---------------------------------|-----------| | PER  
        | person name | | LOC         | location name | | ORG         | organization name | | MISC   
       | other name |  Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and L 
STM-CRF."', 'input_token_limit': '', 'vocabulary_size': ''}]                                         
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dfe3a-4a082fa82cecb53f5e30d177)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-resnet/resolve/main/README.md. 

#####################openai/whisper-large-v2########################

-------------------- datasets --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages."
-------------------- license --------------------
Document 1:

arXiv.org perpetual, non-exclusive license
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
------------------------------
Document 2:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
-------------------- paper --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
------------------------------
Document 2:

[the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf) and [the paper](https://cdn.openai.com/papers/whisper.pdf)
------------------------------
Document 3:

doi = {10.48550/ARXIV.2212.04356}, url = {https://arxiv.org/abs/2212.04356}, title = {Robust Speech Recognition via Large-Scale Weak Supervision}
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

39 M, 74 M, 244 M, 769 M, 1550 M
-------------------- hyper_parameters --------------------
Document 1:

"fine-tuning" "step-by-step guide to fine-tuning the Whisper model" NO_OUTPUT
-------------------- evaluation --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf)."
------------------------------
Document 3:

"We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them."
-------------------- hardware --------------------
Document 1:

"This non-English data represents 98 different languages." NO_OUTPUT
-------------------- limitation_and_bias --------------------
Document 1:

"the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. However, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself. Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly."
------------------------------
Document 2:

"The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model." "We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them." "We caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification." "We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes."
------------------------------
Document 3:

The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.
-------------------- demo --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"The blog post [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data."
-------------------- input_format --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
------------------------------
Document 2:

labelled speech data
------------------------------
Document 3:

- audio
- automatic-speech-recognition
- hf-asr-leaderboard
- example_title: Librispeech sample 1
src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- example_title: Librispeech sample 2
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
pipeline_tag: automatic-speech-recognition
-------------------- output_format --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
------------------------------
Document 2:

The model predicts transcriptions in the *same* language as the audio. For speech translation, the model predicts transcriptions to a *different* language to the audio.
-------------------- sample_rate --------------------
Document 1:

`chunk_length_s=30`
-------------------- WER --------------------
Document 1:

"We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages."

[{'datasets': ['680,000 hours of audio and the corresponding transcripts collected from the interne 
t'], 'license': 'arXiv.org perpetual, non-exclusive license', 'github': 'https://github.com/huggingf 
ace/transformers', 'paper': 'https://cdn.openai.com/papers/whisper.pdf', 'upstream_model': '', 'para 
meter_count': '39 M, 74 M, 244 M, 769 M, 1550 M', 'hyper_parameters': {'epochs': '', 'batch_size': ' 
', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'ASR and speech translation to Eng 
lish tasks', 'result': 10}], 'hardware': '', 'limitation_and_bias': 'The models exhibit improved rob 
ustness to accents, background noise, technical language, as well as zero shot translation from mult 
iple languages into English. However, because the models are trained in a weakly supervised manner u 
sing large-scale noisy data, the predictions may include texts that are not actually spoken in the a 
udio input (i.e. hallucination). The models perform unevenly across languages and exhibit lower accu 
racy on low-resource and/or low-discoverability languages or languages with less training data. The  
models also exhibit disparate performance on different accents and dialects of particular languages, 
 which may include higher word error rate across speakers of different genders, races, ages, or othe 
r demographic criteria. In addition, the sequence-to-sequence architecture of the model makes it pro 
ne to generating repetitive texts.', 'demo': 'The models are trained on 680,000 hours of audio and t 
he corresponding transcripts collected from the internet. They show strong ASR results in ~10 langua 
ges.', 'input_format': 'Audio', 'output_format': 'Transcriptions'}]                                  

#####################jonatasgrosman/wav2vec2-large-xlsr-53-polish########################

-------------------- datasets --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, 16kHz, OVHcloud, https://github.com/jonatasgrosman/wav2vec2-sprint
------------------------------
Document 2:

datasets:
- common_voice
- mozilla-foundation/common_voice_6_0
-------------------- license --------------------
Document 1:

language: pl
license: apache-2.0
------------------------------
Document 2:

"This model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)" NO_OUTPUT
-------------------- github --------------------
Document 1:

"\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-polish}"
------------------------------
Document 2:

mozilla-foundation/common_voice_6_0, Automatic Speech Recognition, Common Voice pl, Automatic Speech Recognition, Robust Speech Event - Dev Data, speech-recognition-community-v2/dev_data
------------------------------
Document 3:

facebook/wav2vec2-large-xlsr-53, https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- paper --------------------
Document 1:

"Fine-tuned {XLSR}-53 large model for speech recognition in {P}olish"
------------------------------
Document 2:

"The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
-------------------- upstream_model --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53
-------------------- parameter_count --------------------
Document 1:

parameter_count: 53
-------------------- hyper_parameters --------------------
Document 1:

"fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Polish using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice)", "make sure that your speech input is sampled at 16kHz", "The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
------------------------------
Document 2:

- type: wer
value: 14.21
name: Test WER
- type: cer
value: 3.49
name: Test CER
- type: wer
value: 10.98
name: Test WER (+LM)
- type: cer
value: 2.93
name: Test CER (+LM)
- type: wer
value: 33.18
name: Dev WER
- type: cer
value: 15.92
name: Dev CER
- type: wer
value: 29.31
name: Dev WER (+LM)
- type: cer
value: 15.17
name: Dev CER (+LM)
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/)"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Fine-tuned {XLSR}-53 large model for speech recognition in {P}olish" "howpublished={\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-polish}}"
------------------------------
Document 2:

Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:  
```python
from huggingsound import SpeechRecognitionModel

model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-polish")
audio_paths = ["/path/to/file.mp3", "/path/to/another_file.wav"]

transcriptions = model.transcribe(audio_paths)
```
------------------------------
Document 3:

"Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Polish using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice)", "When using this model, make sure that your speech input is sampled at 16kHz.", "The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
-------------------- input_format --------------------
Document 1:

16kHz, GPU credits from OVHcloud
------------------------------
Document 2:

input_format: wav2vec2
-------------------- output_format --------------------
Document 1:

output_format: 16kHz
-------------------- sample_rate --------------------
Document 1:

sample rate of this model: 16kHz
------------------------------
Document 2:

SAMPLES = 5, sampling_rate=16_000
-------------------- WER --------------------
Document 1:

- type: wer
value: 14.21
name: Test WER
- type: wer
value: 33.18
name: Dev WER

[{'datasets': ['Common Voice 6.1'], 'license': 'apache-2.0', 'github': 'https://github.com/jonatasg 
rosman/wav2vec2-sprint', 'paper': 'Fine-tuned {XLSR}-53 large model for speech recognition in {P}oli 
sh', 'upstream_model': 'facebook/wav2vec2-large-xlsr-53', 'parameter_count': '53', 'hyper_parameters 
': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardwa 
re': 'GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-tra 
ining/)', 'limitation_and_bias': '', 'demo': 'Fine-tuned {XLSR}-53 large model for speech recognitio 
n in {P}olish', 'input_format': '16kHz', 'output_format': '16kHz', 'sample_rate': '', 'WER': ''}]    

#####################Helsinki-NLP/opus-mt-nl-en########################

-------------------- datasets --------------------
Document 1:

dataset: opus, download original weights: [opus-2019-12-05.zip](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.zip)
------------------------------
Document 2:

Tatoeba.nl.en, 60.9, 0.749
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

*OPUS readme: [nl-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/nl-en/README.md)*
-------------------- github --------------------
Document 1:

OPUS readme: [nl-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/nl-en/README.md), download original weights: [opus-2019-12-05.zip](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.zip), test set translations: [opus-2019-12-05.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.test.txt), test set scores: [opus-2019-12-05.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.eval.txt)
-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

model: transformer-align
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| Tatoeba.nl.en 	| 60.9 	| 0.749 |
------------------------------
Document 2:

opus-2019-12-05.eval.txt
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

*OPUS readme: [nl-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/nl-en/README.md)* *download original weights: [opus-2019-12-05.zip](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.zip)* *test set translations: [opus-2019-12-05.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.test.txt)* *test set scores: [opus-2019-12-05.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.eval.txt)*
-------------------- input_format --------------------
Document 1:

SentencePiece + normalization
-------------------- output_format --------------------
Document 1:

SentencePiece + download original weights: [opus-2019-12-05.zip](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.zip) + test set translations: [opus-2019-12-05.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.test.txt) + test set scores: [opus-2019-12-05.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/nl-en/opus-2019-12-05.eval.txt)
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': 'https://github.com/Helsinki-NLP/OPUS-MT 
-train/blob/master/models/nl-en/README.md', 'paper': '', 'upstream_model': 'transformer-align', 'par 
ameter_count': '', 'hyper_parameters': [], 'evaluation': [{'test': 'Tatoeba.nl.en', 'result': 60.9}] 
, 'hardware': '', 'limitation_and_bias': '', 'demo': 'https://github.com/Helsinki-NLP/OPUS-MT-train/ 
blob/master/models/nl-en/README.md', 'input_format': 'SentencePiece + normalization', 'output_format 
': 'SentencePiece', 'input_token_limit': '', 'vocabulary_size': ''}]                                 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dfebd-2b786fd804c49e19564951ed)

Entry Not Found for url: https://huggingface.co/monologg/koelectra-small-v2-distilled-korquad-384/resolve/main/README.md. 

#####################EleutherAI/gpt-neo-125m########################

-------------------- datasets --------------------
Document 1:

datasets Pile, large scale curated dataset created by EleutherAI
------------------------------
Document 2:

datasets, Pile, 300 billion tokens, 572,300 steps
------------------------------
Document 3:

"The Pile: An 800GB Dataset of Diverse Text for Language Modeling" and "arXiv preprint arXiv:2101.00027"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"GPT-Neo was trained as an autoregressive language model." "GPT-Neo was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language." "Depending on your usecase GPT-Neo may produce socially unacceptable text." "See Sections 5 and 6 of the Pile paper for a more detailed analysis of the biases in the Pile." "As with all language models, it is hard to predict in advance how GPT-Neo will respond to particular prompts and offensive content may occur without warning." "We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results."
NO_OUTPUT
------------------------------
Document 2:

"url = {https://doi.org/10.5281/zenodo.5297715}" and "doi = {10.5281/zenodo.5297715}"
-------------------- paper --------------------
Document 1:

```bibtex
@article{gao2020pile,
title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
journal={arXiv preprint arXiv:2101.00027},
year={2020}
}
```
------------------------------
Document 2:

"masked autoregressive language model, using cross-entropy loss"
-------------------- upstream_model --------------------
Document 1:

upstream_model GPT-3
-------------------- parameter_count --------------------
Document 1:

parameter_count 572,300
------------------------------
Document 2:

"GPT-Neo 125M" and "number of parameters of this particular pre-trained model"
-------------------- hyper_parameters --------------------
Document 1:

"masked autoregressive language model, using cross-entropy loss"
-------------------- evaluation --------------------
Document 1:

"This model was trained on the Pile for 300 billion tokens over 572,300 steps. It was trained as a masked autoregressive language model, using cross-entropy loss."
-------------------- hardware --------------------
Document 1:

Pile
------------------------------
Document 2:

Pile
------------------------------
Document 3:

GPT-Neo 125M
-------------------- limitation_and_bias --------------------
Document 1:

"GPT-Neo was trained as an autoregressive language model...GPT-Neo was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language...As with all language models, it is hard to predict in advance how GPT-Neo will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results."
-------------------- demo --------------------
Document 1:

"generating texts from a prompt"
-------------------- input_format --------------------
Document 1:

GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, The Pile: An 800GB Dataset of Diverse Text for Language Modeling
-------------------- output_format --------------------
Document 1:

output_format: [{'generated_text': 'EleutherAI has made a commitment to create new software packages for each of its major clients and has'}]
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Pile'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_ 
count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'de 
mo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]   

#####################openai/clip-vit-large-patch14-336########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

- generated_from_keras_callback
- clip-vit-large-patch14-336
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

- optimizer: None - training_precision: float32
-------------------- evaluation --------------------
Document 1:

It achieves the following results on the evaluation set:
-------------------- hardware --------------------
Document 1:

"TensorFlow 2.8.2"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_preprocessing --------------------

-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count': 
 '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '' 
, 'input_format': '', 'output_format': '', 'input_preprocessing': '', 'input_size': '', 'num_of_clas 
ses_for_classification': '', 'trigger_word': ''}                                                     
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dfef8-2175286b572bf4050df7388f)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-LongT5Model/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dfef8-616e66332224c1bc1196a291)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Speech2TextModel/resolve/main/README.md. 

#####################KoboldAI/OPT-6.7B-Nerybus-Mix########################

-------------------- datasets --------------------
Document 1:

- https://huggingface.co/KoboldAI/OPT-6.7B-Erebus
- https://huggingface.co/KoboldAI/OPT-6B-nerys-v2
------------------------------
Document 2:

NerysV2-6.7B, ErebusV1-6.7B
-------------------- license --------------------
Document 1:

"license: other"
------------------------------
Document 2:

"The base OPT-6.7B model is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved."
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B
------------------------------
Document 2:

"The two models used for this blend, *NerysV2-6.7B* and *ErebusV1-6.7B* are made by **Mr. Seeker**."
-------------------- upstream_model --------------------
Document 1:

parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*
-------------------- parameter_count --------------------
Document 1:

parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B
-------------------- evaluation --------------------
Document 1:

parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*
------------------------------
Document 2:

- "The two models used for this blend, *NerysV2-6.7B* and *ErebusV1-6.7B* are made by **Mr. Seeker**."
- "- https://huggingface.co/KoboldAI/OPT-6.7B-Erebus"
- "- https://huggingface.co/KoboldAI/OPT-6B-nerys-v2"
- "The base OPT-6.7B model is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved."
-------------------- hardware --------------------
Document 1:

FP16, KoboldAI software
------------------------------
Document 2:

*NerysV2-6.7B* and *ErebusV1-6.7B*
------------------------------
Document 3:

"Mr. Seeker" "OPT-6.7B" "OPT-175B license" "Copyright (c) Meta Platforms, Inc. All Rights Reserved."
-------------------- limitation_and_bias --------------------
Document 1:

parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*
-------------------- demo --------------------
Document 1:

parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*
-------------------- input_format --------------------
Document 1:

parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B
-------------------- output_format --------------------
Document 1:

parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*
-------------------- input_token_limit --------------------
Document 1:

parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B
-------------------- vocabulary_size --------------------
Document 1:

parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B

[{'datasets': ['KoboldAI/OPT-6.7B-Erebus', 'KoboldAI/OPT-6B-nerys-v2'], 'license': 'other', 'github 
': '', 'paper': '', 'upstream_model': 'parameter-wise 50/50 blend (weighted average) of the weights  
of *NerysV2-6.7B* and *ErebusV1-6.7B*', 'parameter_count': 'parameter-wise 50/50 blend, NerysV2-6.7B 
, ErebusV1-6.7B', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimiz 
er': ''}, 'evaluation': [], 'hardware': 'FP16, KoboldAI software', 'limitation_and_bias': 'parameter 
-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*', 'demo':  
'parameter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*' 
, 'input_format': 'parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B', 'output_format': 'param 
eter-wise 50/50 blend (weighted average) of the weights of *NerysV2-6.7B* and *ErebusV1-6.7B*', 'inp 
ut_token_limit': 'parameter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B', 'vocabulary_size': 'para 
meter-wise 50/50 blend, NerysV2-6.7B, ErebusV1-6.7B'}]                                               

#####################nlpaueb/legal-bert-small-uncased########################

-------------------- datasets --------------------
Document 1:

US contracts, EU legislation, ECHR cases, All
------------------------------
Document 2:

* 116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.  
* 61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk).  
* 19,867 cases from the European Court of Justice (ECJ), also available from EURLEX.  
* 12,554 cases from HUDOC, the repository of the European Court of Human Rights (ECHR) (http://hudoc.echr.coe.int/eng).  
* 164,141 cases from various courts across the USA, hosted in the Case Law Access Project portal (https://case.law).  
* 76,366 US contracts from EDGAR, the database of US Securities and Exchange Commission (SECOM) (https://www.sec.gov/edgar.shtml).
-------------------- license --------------------
Document 1:

license: cc-by-sa-4.0
------------------------------
Document 2:

LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources.
-------------------- github --------------------
Document 1:

http://eur-lex.europa.eu, http://www.legislation.gov.uk, http://hudoc.echr.coe.int/eng, https://case.law, https://www.sec.gov/edgar.shtml
------------------------------
Document 2:

`nlpaueb/bert-base-uncased-contracts`, `nlpaueb/bert-base-uncased-eurlex`, `nlpaueb/bert-base-uncased-echr`, `nlpaueb/legal-bert-base-uncased`, `nlpaueb/legal-bert-small-uncased`, `https://archive.org/details/legal_bert_fp`
------------------------------
Document 3:

https://github.com/google-research/bert, https://www.tensorflow.org/tfrc, https://edu.google.com/programs/credits/research
-------------------- paper --------------------
Document 1:

"LEGAL-BERT: The Muppets straight out of Law School". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)
------------------------------
Document 2:

LEGAL-BERT-BASE is the model referred to as LEGAL-BERT-SC in Chalkidis et al. (2020); a model trained from scratch in the legal corpora mentioned below using a newly created vocabulary by a sentence-piece tokenizer trained on the very same corpora.
------------------------------
Document 3:

"title = "{LEGAL}-{BERT}: The Muppets straight out of Law School""
-------------------- upstream_model --------------------
Document 1:

"nlpaueb/legal-bert-small-uncased"
------------------------------
Document 2:

"Google BERT's GitHub repository (https://github.com/google-research/bert)", "English BERT-BASE model (12-layer, 768-hidden, 12-heads, 110M parameters)", "1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4", "Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc)", "[GCP research credits](https://edu.google.com/programs/credits/research)"
-------------------- parameter_count --------------------
Document 1:

"110M parameters"
------------------------------
Document 2:

"LEGAL-BERT-BASE *" "`nlpaueb/legal-bert-base-uncased`" "All"
------------------------------
Document 3:

"BERT-BASE-UNCASED", "CONTRACTS-BERT-BASE", "EURLEX-BERT-BASE", "ECHR-BERT-BASE", "LEGAL-BERT-BASE", "LEGAL-BERT-SMALL"
-------------------- hyper_parameters --------------------
Document 1:

"1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4."
-------------------- evaluation --------------------
Document 1:

"Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)"
-------------------- hardware --------------------
Document 1:

"single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc)"
------------------------------
Document 2:

`nlpaueb/legal-bert-base-uncased` `nlpaueb/legal-bert-small-uncased` `https://archive.org/details/legal_bert_fp`
------------------------------
Document 3:

LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources.
-------------------- limitation_and_bias --------------------
Document 1:

"LEGAL-BERT: The Muppets straight out of Law School". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)
-------------------- demo --------------------
Document 1:

`nlpaueb/bert-base-uncased-contracts`, `nlpaueb/bert-base-uncased-eurlex`, `nlpaueb/bert-base-uncased-echr`, `nlpaueb/legal-bert-base-uncased`, `nlpaueb/legal-bert-small-uncased`, `https://archive.org/details/legal_bert_fp`
------------------------------
Document 2:

"LEGAL-BERT: The Muppets straight out of Law School". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)
------------------------------
Document 3:

"116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office." "61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk)." "19,867 cases from the European Court of Justice (ECJ), also available from EURLEX." "12,554 cases from HUDOC, the repository of the European Court of Human Rights (ECHR) (http://hudoc.echr.coe.int/eng)." "164,141 cases from various courts across the USA, hosted in the Case Law Access Project portal (https://case.law)." "76,366 US contracts from EDGAR, the database of US Securities and Exchange Commission (SECOM) (https://www.sec.gov/edgar.shtml)."
-------------------- input_format --------------------
Document 1:

"`nlpaueb/legal-bert-base-uncased`" and "a model trained from scratch in the legal corpora mentioned below using a sentence-piece tokenizer trained on the very same corpora."
------------------------------
Document 2:

BERT-BASE-UNCASED, CONTRACTS-BERT-BASE, EURLEX-BERT-BASE, ECHR-BERT-BASE, LEGAL-BERT-BASE, LEGAL-BERT-SMALL
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

LEGAL-BERT-BASE * `nlpaueb/legal-bert-base-uncased` All sentence-piece tokenizer

[{'datasets': ['US contracts', 'EU legislation', 'ECHR cases', 'All'], 'license': 'cc-by-sa-4.0', ' 
github': 'http://eur-lex.europa.eu, http://www.legislation.gov.uk, http://hudoc.echr.coe.int/eng, ht 
tps://case.law, https://www.sec.gov/edgar.shtml', 'paper': '"LEGAL-BERT: The Muppets straight out of 
 Law School". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)', 'upstream 
_model': '"nlpaueb/legal-bert-small-uncased"', 'parameter_count': '"110M parameters"', 'hyper_parame 
ters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test 
': '"Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)"', 'result': 0}], 'h 
ardware': '"single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](h 
ttps://www.tensorflow.org/tfrc)"', 'limitation_and_bias': '"LEGAL-BERT: The Muppets straight out of  
Law School". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)"', 'demo': ' 
"nlpaueb/bert-base-uncased-contracts", "nlpaueb/bert-base-uncased-eurlex", "nlpaueb/bert-base-uncase 
d-echr", "nlpaueb/legal-bert-base-uncased", "nlpaueb/legal-bert-small-uncased", "https://archive.org 
/details/legal_bert_fp"', 'input_format': '"nlpaueb/legal-bert-base-uncased" and "a model trained fr 
om scratch in the legal corpora mentioned below using a sentence-piece tokenizer trained on the very 
 same corpora."', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': 'LEGAL-BERT-BASE  
* `nlpaueb/legal-bert-base-uncased` All sentence-piece tokenizer'}]                                  

#####################kredor/punctuate-all########################

-------------------- datasets --------------------
Document 1:

English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

Oliver Guhr's work, https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large
-------------------- upstream_model --------------------
Document 1:

Oliver Guhr's work, xlm-roberta-base, twelve languages, English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.
upstream_model: xlm-roberta-base
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

precision    recall  f1-score   support  
0       0.99      0.99      0.99  73317475
.       0.94      0.95      0.95   4484845
,       0.86      0.86      0.86   6100650
?       0.88      0.85      0.86    136479
-       0.60      0.29      0.39    233630
:       0.71      0.49      0.58    152424  
accuracy                           0.98  84425503
macro avg       0.83      0.74      0.77  84425503
weighted avg       0.98      0.98      0.98  84425503  
confusion matrix
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

precision    recall  f1-score   support  
0       0.99      0.99      0.99  73317475
.       0.94      0.95      0.95   4484845
,       0.86      0.86      0.86   6100650
?       0.88      0.85      0.86    136479
-       0.60      0.29      0.39    233630
:       0.71      0.49      0.58    152424  
accuracy                           0.98  84425503
macro avg       0.83      0.74      0.77  84425503
weighted avg       0.98      0.98      0.98  84425503
-------------------- demo --------------------
Document 1:

Oliver Guhr's work, xlm-roberta-base, twelve languages (English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian), precision, recall, f1-score, support, accuracy, macro avg, weighted avg, confusion matrix.
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['English', 'German', 'French', 'Spanish', 'Bulgarian', 'Italian', 'Polish', 'Dutch', 
 'Czech', 'Portugese', 'Slovak', 'Slovenian'], 'license': '', 'github': 'https://huggingface.co/oliv 
erguhr/fullstop-punctuation-multilang-large', 'paper': 'https://huggingface.co/oliverguhr/fullstop-p 
unctuation-multilang-large', 'upstream_model': 'xlm-roberta-base', 'parameter_count': '', 'hyper_par 
ameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format 
': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                         

#####################microsoft/infoxlm-large########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------
Document 1:

[repo](https://github.com/microsoft/unilm/tree/master/infoxlm)
-------------------- paper --------------------
Document 1:

[paper](https://arxiv.org/pdf/2007.07834.pdf)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[repo](https://github.com/microsoft/unilm/tree/master/infoxlm), [model](https://huggingface.co/microsoft/infoxlm-base)
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"config.json", "pytorch_model.bin", "sentencepiece.bpe.model", "tokenizer.json"

[{'datasets': [], 'license': '', 'github': 'https://github.com/microsoft/unilm/tree/master/infoxlm' 
, 'paper': 'https://arxiv.org/pdf/2007.07834.pdf', 'upstream_model': '', 'parameter_count': '', 'hyp 
er_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '[repo](htt 
ps://github.com/microsoft/unilm/tree/master/infoxlm), [model](https://huggingface.co/microsoft/infox 
lm-base)', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': '"co 
nfig.json", "pytorch_model.bin", "sentencepiece.bpe.model", "tokenizer.json"'}]                      

#####################lllyasviel/sd-controlnet-canny########################

-------------------- datasets --------------------
Document 1:

datasets, 3M edge-image, caption pairs
------------------------------
Document 2:

[Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), [lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny), [lllyasviel/sd-controlnet-depth](https://huggingface.co/lllyasviel/sd-controlnet-depth), [lllyasviel/sd-controlnet-hed](https://huggingface.co/lllyasviel/sd-controlnet-hed), [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-mlsd), [lllyasviel/sd-controlnet-normal](https://huggingface.co/lllyasviel/sd-controlnet-normal), [lllyasviel/sd-controlnet_openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose), [
-------------------- license --------------------

-------------------- github --------------------
Document 1:

[lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny), [lllyasviel/sd-controlnet-depth](https://huggingface.co/lllyasviel/sd-controlnet-depth), [lllyasviel/sd-controlnet-hed](https://huggingface.co/lllyasviel/sd-controlnet-hed), [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-mlsd), [lllyasviel/sd-controlnet-normal](https://huggingface.co/lllyasviel/sd-controlnet-normal), [lllyasviel/sd-controlnet_openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose), [lllyasviel/sd-controlnet_scribble](https://huggingface.co/lllyasviel/sd-controlnet
-------------------- paper --------------------
Document 1:

"3M edge-image, caption pairs" and "Stable Diffusion 1.5 as a base model"
-------------------- upstream_model --------------------
Document 1:

upstream_model Stable Diffusion 1.5
------------------------------
Document 2:

Stable Diffusion v1-5
------------------------------
Document 3:

"We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions." NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

parameter_count 3M
-------------------- hyper_parameters --------------------
Document 1:

"Stable Diffusion 1.5 as a base model"
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Nvidia A100 80G
------------------------------
Document 2:

[Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), canny edge detection, Midas depth estimation, HED edge detection (soft edge), M-LSD line detection, normal map, OpenPose bone image, human scribbles, semantic segmentation
-------------------- limitation_and_bias --------------------
Document 1:

"We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k)."
-------------------- demo --------------------
Document 1:

"link, code snippet or short paragraph"
------------------------------
Document 2:

The authors released 8 different checkpoints, each trained with [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) on a different type of conditioning:  
| Model Name | Control Image Overview| Control Image Example | Generated Image Example |
|---|---|---|---|
|[lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny)<br/> *Trained with canny edge detection* | A monochrome image with white edges on a black background.|<a href="https://huggingface.co/takuma104/controlnet_dev/blob/main/gen_compare/control_images/converted/control_bird_canny.png"><img width="64" style="margin:0;padding:0;" src="https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare/control_images/converted/control_bird_canny
-------------------- input_format --------------------
Document 1:

"canny edge detection", "Midas depth estimation", "HED edge detection (soft edge)", "M-LSD line detection", "normal map", "OpenPose bone image", "human scribbles", "ADE20K's segmentation protocol image"
-------------------- output_format --------------------
Document 1:

Generated Image Example
-------------------- input_preprocessing --------------------
Document 1:

*Trained with canny edge detection*, *Trained with Midas depth estimation*, *Trained with HED edge detection (soft edge)*, *Trained with M-LSD line detection*, *Trained with normal map*, *Trained with OpenPose bone image*, *Trained with human scribbles*, *Trained with semantic segmentation*
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------
Document 1:

num_of_classes_for_classification: NO_OUTPUT
-------------------- trigger_word --------------------


{'datasets': ['3M edge-image, caption pairs'], 'github': '[lllyasviel/sd-controlnet-canny](https:// 
huggingface.co/lllyasviel/sd-controlnet-canny), [lllyasviel/sd-controlnet-depth](https://huggingface 
.co/lllyasviel/sd-controlnet-depth), [lllyasviel/sd-controlnet-hed](https://huggingface.co/lllyasvie 
l/sd-controlnet-hed), [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlne 
t-mlsd), [lllyasviel/sd-controlnet-normal](https://huggingface.co/lllyasviel/sd-controlnet-normal),  
[lllyasviel/sd-controlnet_openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose)', 'pap 
er': '"3M edge-image, caption pairs" and "Stable Diffusion 1.5 as a base model"', 'upstream_model':  
'Stable Diffusion 1.5', 'parameter_count': '3M', 'hardware': 'Nvidia A100 80G', 'limitation_and_bias 
': '"We present a neural network structure, ControlNet, to control pretrained large diffusion models 
 to support additional input conditions. The ControlNet learns task-specific conditions in an end-to 
-end way, and the learning is robust even when the training dataset is small (< 50k)."', 'demo': '"l 
ink, code snippet or short paragraph"', 'input_format': '"canny edge detection", "Midas depth estima 
tion", "HED edge detection (soft edge)", "M-LSD line detection", "normal map", "OpenPose bone image" 
, "human scribbles", "ADE20K\'s segmentation protocol image"', 'output_format': 'Generated Image Exa 
mple'}                                                                                               

#####################tiiuae/falcon-40b########################

-------------------- datasets --------------------
Document 1:

"The RefinedWeb paper" and "https://arxiv.org/abs/2306.01116"
------------------------------
Document 2:

datasets: - tiiuae/falcon-refinedweb
-------------------- license --------------------
Document 1:

Apache 2.0 license
------------------------------
Document 2:

license: apache-2.0
------------------------------
Document 3:

"**License:** Apache 2.0 license."
-------------------- github --------------------
Document 1:

language:
- en
- de
- es
- fr
license: apache-2.0
datasets:
- tiiuae/falcon-refinedweb
------------------------------
Document 2:

url={https://arxiv.org/abs/2306.01116}, eprint={2306.01116}, eprinttype = {arXiv}
-------------------- paper --------------------
Document 1:

"Paper: *coming soon*."
-------------------- upstream_model --------------------
Document 1:

GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864)), multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)), FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135))
-------------------- parameter_count --------------------
Document 1:

"TP=8, PP=4, DP=12" and "parameter_count"
------------------------------
Document 2:

"Layers: 60", "d_model: 8192", "head_dim: 64", "Vocabulary: 65024", "Sequence length: 2048"
------------------------------
Document 3:

"Falcon-40B is a 40B parameters causal decoder-only model" and "parameter_count: 40B"
-------------------- hyper_parameters --------------------
Document 1:

"Precision: `bfloat16`"
"Optimizer: AdamW"
"Learning rate: 1.85e-4"
"Weight decay: 1e-1"
"Z-loss: 1e-4"
"Batch size: 1152"
------------------------------
Document 2:

* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));
* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));
* **Decoder-block:** parallel attention/MLP with a two layer norms. 
| **Hyperparameter** | **Value** | **Comment**                            |
|--------------------|-----------|----------------------------------------|
| Layers             | 60        |                                        |
| `d_model`          | 8192      |                                        |
| `head_dim`         | 64        | Reduced to optimise for FlashAttention |
| Vocabulary         | 65024     |                                        |
| Sequence length    | 2048      |                                        |
-------------------- evaluation --------------------
Document 1:

"Falcon-40B is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae) and trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license."
-------------------- hardware --------------------
Document 1:

Falcon-40B, AWS SageMaker, 384 A100 40GB GPUs, P4d instances
------------------------------
Document 2:

Falcon-40B, 384 A100 40GB GPUs, 3D parallelism strategy (TP=8, PP=4, DP=12)
-------------------- limitation_and_bias --------------------
Document 1:

English, German, Spanish, French, Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish, stereotypes and biases
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

input_format: RefinedWeb paper
------------------------------
Document 2:

language:
- en
- de
- es
- fr
datasets:
- tiiuae/falcon-refinedweb
inference: false
-------------------- output_format --------------------
Document 1:

license: apache-2.0, inference: false
-------------------- input_token_limit --------------------
Document 1:

Falcon-40B, 1,000B tokens, RefinedWeb-English, 750B, RefinedWeb-Europe, 70B, Books, 60B, Conversations, 50B, Code, 50B, Technical, 20B, German, 18B, Spanish, 17B, French, 16B, Italian, 5B, Portuguese, 3B, Polish, 3B, Dutch, 3B, Romanian, 2B, Czech, 2B, Swedish, 1B, Falcon-[7B]/[40B] tokenizer.
-------------------- vocabulary_size --------------------
Document 1:

Vocabulary: 65024
------------------------------
Document 2:

Falcon-40B was trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), The data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.

[{'datasets': ['tiiuae/falcon-refinedweb'], 'license': 'apache-2.0', 'github': '', 'paper': '', 'up 
stream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '',  
'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': 
 '', 'vocabulary_size': ''}]                                                                         
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653dffe3-4edc6f5b240548e54e7cee37)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-SegformerModel/resolve/main/README.md. 

#####################Hello-SimpleAI/chatgpt-detector-roberta########################

-------------------- datasets --------------------
Document 1:

the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)
------------------------------
Document 2:

datasets: - Hello-SimpleAI/HC3
------------------------------
Document 3:

"arxiv: 2301.07597"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

language: - en tags: - chatgpt datasets: - Hello-SimpleAI/HC3 pipeline_tag: text-classification
------------------------------
Document 2:

[Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3) and Gtihub project [Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)
-------------------- paper --------------------
Document 1:

arXiv preprint arxiv:2301.07597
------------------------------
Document 2:

arxiv: 2301.07597, our paper
------------------------------
Document 3:

"tags: - chatgpt"
-------------------- upstream_model --------------------
Document 1:

- Hello-SimpleAI/HC3
------------------------------
Document 2:

"The base checkpoint is [roberta-base](https://huggingface.co/roberta-base)"
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"The base checkpoint is [roberta-base](https://huggingface.co/roberta-base)" and "We train it with all [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3) data (without held-out) for 1 epoch."
-------------------- evaluation --------------------
Document 1:

"The model is trained on the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)." "The base checkpoint is [roberta-base](https://huggingface.co/roberta-base)." "We train it with all [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3) data (without held-out) for 1 epoch."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"trained on the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)", "base checkpoint is [roberta-base](https://huggingface.co/roberta-base)", "train it with all [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3) data (without held-out) for 1 epoch".
-------------------- demo --------------------
Document 1:

"the mix of full-text and splitted sentences of `answer`s from [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)", "[arxiv: 2301.07597](https://arxiv.org/abs/2301.07597)", "Gtihub project [Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)", "[roberta-base](https://huggingface.co/roberta-base)", "[Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)", "[our paper](https://arxiv.org/abs/2301.07597)"
-------------------- input_format --------------------
Document 1:

language: - en, tags: - chatgpt, datasets: - Hello-SimpleAI/HC3, pipeline_tag: text-classification
------------------------------
Document 2:

"the mix of full-text and splitted sentences"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"roberta-base"

[{'datasets': ['Hello-SimpleAI/HC3'], 'license': '', 'github': '', 'paper': '', 'upstream_model': ' 
', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_ 
bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary 
_size': ''}]                                                                                         

#####################mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis########################

-------------------- datasets --------------------
Document 1:

datasets: - financial_phrasebank
------------------------------
Document 2:

Datasets 1.12.1
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

upstream_model distilroberta-base
------------------------------
Document 2:

- generated_from_trainer
- financial
- stocks
- sentiment
- financial_phrasebank
- accuracy
- distilRoberta-financial-sentiment
- text-classification
- Text Classification
- financial_phrasebank
- accuracy
- 0.9823008849557522
- Accuracy
-------------------- parameter_count --------------------
Document 1:

parameter_count: NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 2e-05 - train_batch_size: 8 - eval_batch_size: 8 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - num_epochs: 5
------------------------------
Document 2:

Epoch, Step, Validation Loss, Accuracy
-------------------- evaluation --------------------
Document 1:

license: apache-2.0, tags: - generated_from_trainer - financial - stocks - sentiment, datasets: - financial_phrasebank, metrics: - accuracy, widget: - text: Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 ., model-index: - name: distilRoberta-financial-sentiment, results: - task: type: text-classification name: Text Classification dataset: name: financial_phrasebank type: financial_phrasebank args: sentences_allagree metrics: - type: accuracy value: 0.9823008849557522 name: Accuracy
------------------------------
Document 2:

"This model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset. It achieves the following results on the evaluation set: - Loss: 0.1116 - Accuracy: 0.9823"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

metrics:
- accuracy
widget:
- text: Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .
model-index:
- name: distilRoberta-financial-sentiment
results:
- task:
type: text-classification
name: Text Classification
dataset:
name: financial_phrasebank
type: financial_phrasebank
args: sentences_allagree
metrics:
- type: accuracy
value: 0.9823008849557522
name: Accuracy
-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

- financial
- stocks
- sentiment
- financial_phrasebank
- text-classification
- financial_phrasebank
- accuracy
- 0.9823008849557522
-------------------- input_token_limit --------------------
Document 1:

- financial
- sentiment
- financial_phrasebank
- text-classification
- Accuracy
- 0.9823008849557522
-------------------- vocabulary_size --------------------
Document 1:

- financial
- stocks
- sentiment
- financial_phrasebank
- text-classification
- type: accuracy
value: 0.9823008849557522
name: Accuracy

[{'datasets': ['financial_phrasebank'], 'license': '', 'github': '', 'paper': '', 'upstream_model': 
 'distilroberta-base', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': [], 'evaluation': [], 'ha 
rdware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_ 
token_limit': '', 'vocabulary_size': ''}]                                                            

#####################stabilityai/stable-diffusion-2-inpainting########################

-------------------- datasets --------------------
Document 1:

LAION-2B(en) (https://laion.ai/blog/laion-5b/)
------------------------------
Document 2:

datasets and COCO2017 validation set
-------------------- license --------------------
Document 1:

license: openrail++
-------------------- github --------------------
Document 1:

LAION-2B(en) https://laion.ai/blog/laion-5b/
------------------------------
Document 2:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps" NO_OUTPUT
-------------------- paper --------------------
Document 1:

"Research on generative models."
------------------------------
Document 2:

High-Resolution Image Synthesis With Latent Diffusion Models
------------------------------
Document 3:

"Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
-------------------- upstream_model --------------------
Document 1:

Stable Diffusion v1, DALL-E Mini model card
-------------------- parameter_count --------------------
Document 1:

parameter_count 50
------------------------------
Document 2:

32 x 8 x A100 GPUs, AdamW, Gradient Accumulations: 1, Batch: 32 x 8 x 2 x 4 = 2048, Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- hyper_parameters --------------------
Document 1:

"50 steps DDIM sampling steps" "Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution."
------------------------------
Document 2:

- relative downsampling factor of 8
- autoencoder maps images of shape H x W x 3 to latents of shape H/f x W/f x 4
- OpenCLIP-ViT/H text-encoder
- UNet backbone of the latent diffusion model via cross-attention
- loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet
- v-objective
- AdamW
- Gradient Accumulations: 1
- Batch: 32 x 8 x 2 x 4 = 2048
- Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- evaluation --------------------
Document 1:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 2:

- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- hardware --------------------
Document 1:

The model was trained mainly with English captions and will not work as well in other languages.
The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
------------------------------
Document 2:

- **Hardware:** 32 x 8 x A100 GPUs
------------------------------
Document 3:

- **Hardware Type:** A100 PCIe 40GB
- **Hours used:** 200000
- **Cloud Provider:** AWS
- **Compute Region:** US-east
-------------------- limitation_and_bias --------------------
Document 1:

"Stable Diffusion vw was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent."
------------------------------
Document 2:

- Probing and understanding the limitations and biases of generative models.
------------------------------
Document 3:

- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- demo --------------------
Document 1:

"Generation of artworks and use in design and other artistic processes."
------------------------------
Document 2:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 3:

"Stable Diffusion vw was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/)" "Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for." "Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent."
-------------------- input_format --------------------
Document 1:

LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant

input_format: LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant
------------------------------
Document 2:

input_format: 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.
-------------------- output_format --------------------
Document 1:

output_format: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.

[{'datasets': ['LAION-2B(en)'], 'license': 'openrail++', 'github': 'https://laion.ai/blog/laion-5b/ 
', 'paper': 'Research on generative models.', 'upstream_model': 'Stable Diffusion v1, DALL-E Mini mo 
del card', 'parameter_count': '50', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_r 
ate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'Ge 
neration of artworks and use in design and other artistic processes.', 'input_format': 'LAION-5B and 
 subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the laten 
t diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 ste 
ps and then kept constant', 'output_format': ''}]                                                    

#####################vicgalle/xlm-roberta-large-xnli-anli########################

-------------------- datasets --------------------
Document 1:

- mnli
- xnli
- anli
------------------------------
Document 2:

XNLI-es, XNLI-fr, ANLI-R1, ANLI-R2, ANLI-R3
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"The model can be loaded with the zero-shot-classification pipeline like so:" "You can then use this pipeline to classify sequences into any of the class names you specify:" 
NO_OUTPUT
-------------------- github --------------------
Document 1:

language: multilingual, license: mit, tags: - zero-shot-classification - nli - pytorch, datasets: - mnli - xnli - anli, pipeline_tag: zero-shot-classification, widget: - text: De pugna erat fantastic. Nam Crixo decem quam dilexit et praeciderunt caput aemulus. candidate_labels: violent, peaceful - text: La película empezaba bien pero terminó siendo un desastre. candidate_labels: positivo, negativo, neutral - text: La película empezó siendo un desastre pero en general fue bien. candidate_labels: positivo, negativo, neutral - text: ¿A quién vas a votar en 2020? candidate_labels: Europa, elecciones, política, ciencia, deportes
------------------------------
Document 2:

vicgalle/xlm-roberta-large-xnli-anli, pipeline("zero-shot-classification", model="vicgalle/xlm-roberta-large-xnli-anli")
-------------------- paper --------------------
Document 1:

- zero-shot-classification
- nli
- pytorch
- mnli
- xnli
- anli
- pipeline_tag: zero-shot-classification
------------------------------
Document 2:

"XLM-RoBERTa-large model finetunned over several NLI datasets" NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"XLM-RoBERTa-large model finetunned over several NLI datasets"
NO_OUTPUT
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classification." "The model can be loaded with the zero-shot-classification pipeline like so:" "You can then use this pipeline to classify sequences into any of the class names you specify:"
-------------------- evaluation --------------------
Document 1:

|                             | XNLI-es | XNLI-fr | ANLI-R1 | ANLI-R2 | ANLI-R3 |
|-----------------------------|---------|---------|---------|---------|---------|
| xlm-roberta-large-xnli-anli | 93.7% | 93.2% | 68.5%  | 53.6%  | 49.0%  |
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classification. Here are the accuracies for several test datasets: | XNLI-es | XNLI-fr | ANLI-R1 | ANLI-R2 | ANLI-R3 | | xlm-roberta-large-xnli-anli | 93.7% | 93.2% | 68.5%  | 53.6%  | 49.0%  | The model can be loaded with the zero-shot-classification pipeline like so: from transformers import pipeline classifier = pipeline("zero-shot-classification", model="vicgalle/xlm-roberta-large-xnli-anli") You can then use this pipeline to classify sequences into any of the class names you specify: sequence_to_classify = "Algún día iré a ver el mundo" candidate_labels = ['viaje', 'cocina', 'danza'] classifier(sequence_to_classify, candidate_labels) #{'sequence': 'Algún
-------------------- demo --------------------
Document 1:

- text: De pugna erat fantastic. Nam Crixo decem quam dilexit et praeciderunt caput aemulus.
candidate_labels: violent, peaceful
- text: La película empezaba bien pero terminó siendo un desastre.
candidate_labels: positivo, negativo, neutral
- text: La película empezó siendo un desastre pero en general fue bien.
candidate_labels: positivo, negativo, neutral
- text: ¿A quién vas a votar en 2020?
candidate_labels: Europa, elecciones, política, ciencia, deportes
------------------------------
Document 2:

"The model can be loaded with the zero-shot-classification pipeline like so:
```
from transformers import pipeline
classifier = pipeline("zero-shot-classification",
model="vicgalle/xlm-roberta-large-xnli-anli")
```
You can then use this pipeline to classify sequences into any of the class names you specify:
```
sequence_to_classify = "Algún día iré a ver el mundo"
candidate_labels = ['viaje', 'cocina', 'danza']
classifier(sequence_to_classify, candidate_labels)
#{'sequence': 'Algún día iré a ver el mundo',
#'labels': ['viaje', 'danza', 'cocina'],
#'scores': [0.9991760849952698, 0.0004178212257102132, 0.0004059972707182169]}
```
-------------------- input_format --------------------
Document 1:

- text: De pugna erat fantastic. Nam Crixo decem quam dilexit et praeciderunt caput
aemulus.
candidate_labels: violent, peaceful
- text: La película empezaba bien pero terminó siendo un desastre.
candidate_labels: positivo, negativo, neutral
- text: La película empezó siendo un desastre pero en general fue bien.
candidate_labels: positivo, negativo, neutral
- text: ¿A quién vas a votar en 2020?
candidate_labels: Europa, elecciones, política, ciencia, deportes
------------------------------
Document 2:

"XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classification." "from transformers import pipeline" "classifier = pipeline("zero-shot-classification", model="vicgalle/xlm-roberta-large-xnli-anli")"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['mnli', 'xnli', 'anli'], 'license': 'mit', 'github': 'language: multilingual, licens 
e: mit, tags: - zero-shot-classification - nli - pytorch, datasets: - mnli - xnli - anli, pipeline_t 
ag: zero-shot-classification, widget: - text: De pugna erat fantastic. Nam Crixo decem quam dilexit  
et praeciderunt caput aemulus. candidate_labels: violent, peaceful - text: La película empezaba bien 
 pero terminó siendo un desastre. candidate_labels: positivo, negativo, neutral - text: La película  
empezó siendo un desastre pero en general fue bien. candidate_labels: positivo, negativo, neutral -  
text: ¿A quién vas a votar en 2020? candidate_labels: Europa, elecciones, política, ciencia, deporte 
s', 'paper': '- zero-shot-classification\n- nli\n- pytorch\n- mnli\n- xnli\n- anli\n- pipeline_tag:  
zero-shot-classification', 'upstream_model': '"XLM-RoBERTa-large model finetunned over several NLI d 
atasets"', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [{'test': 'XNLI-es', 'result 
': 93.7}, {'test': 'XNLI-fr', 'result': 93.2}, {'test': 'ANLI-R1', 'result': 68.5}, {'test': 'ANLI-R 
2', 'result': 53.6}, {'test': 'ANLI-R3', 'result': 49.0}], 'hardware': '', 'limitation_and_bias': '" 
XLM-RoBERTa-large model finetunned over several NLI datasets, ready to use for zero-shot classificat 
ion. Here are the accuracies for several test datasets: | XNLI-es | XNLI-fr | ANLI-R1 | ANLI-R2 | AN 
LI-R3 | | xlm-roberta-large-xnli-anli | 93.7% | 93.2% | 68.5%  | 53.6%  | 49.0%  | The model can be  
loaded with the zero-shot-classification pipeline like so: from transformers import pipeline classif 
ier = pipeline("zero-shot-classification", model="vicgalle/xlm-roberta-large-xnli-anli") You can the 
n use this pipeline to classify sequences into any of the class names you specify: sequence_to_class 
ify = "Algún día iré a ver el mundo" candidate_labels = [\'viaje\', \'cocina\', \'danza\'] classifie 
r(sequence_to_classify, candidate_labels) #{\'sequence\': \'Algún'}, {'datasets': ['XNLI-es', 'XNLI- 
fr', 'ANLI-R1', 'ANLI-R2', 'ANLI-R3'], 'license': '', 'github': 'vicgalle/xlm-roberta-large-xnli-anl 
i, pipeline("zero-shot-classification", model="vicgalle/xlm-roberta-large-xnli-anli")', 'paper': '"X 
LM-RoBERTa-large model finetunned over several NLI datasets"', 'upstream_model': '"XLM-RoBERTa-large 
 model finetunned over several NLI datasets"', 'parameter_count': '', 'hyper_parameters': [], 'evalu 
ation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '"The model can be loaded with the ze 
ro-shot-classification pipeline like so:\n```\nfrom transformers import pipeline\nclassifier = pipel 
ine("zero-shot-classification",\nmodel="vicgalle/xlm-roberta-large-xnli-anli")\n```\nYou can then us 
e this pipeline to classify sequences into any of the class names you specify:\n```\nsequence_to_cla 
ssify = "Algún día iré a ver el mundo"\ncandidate_labels = [\'viaje\', \'cocina\', \'danza\']\nclass 
ifier(sequence_to_classify, candidate_labels)\n#{\'sequence\': \'Algún día iré a ver el mundo\',\n\' 
labels\': [\'viaje\', \'danza\', \'cocina\'],\n\'scores\': [0.9991760849952698, 0.000417821225710213 
2, 0.0004059972707182169]}\n```', 'input_format': '', 'output_format': ''}]                          

#####################hustvl/yolos-small########################

-------------------- datasets --------------------
Document 1:

ImageNet-1k, COCO 2017 object detection
------------------------------
Document 2:

"ImageNet-1k" and "COCO"
------------------------------
Document 3:

datasets:
- coco
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"this repository" https://github.com/hustvl/YOLOS
-------------------- paper --------------------
Document 1:

"table 1 of the original paper"
------------------------------
Document 2:

"You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection" and "https://arxiv.org/abs/2106.00666"
------------------------------
Document 3:

`title = {You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection}, journal = {CoRR}, volume = {abs/2106.00666}, year = {2021}, url = {https://arxiv.org/abs/2106.00666}, eprinttype = {arXiv}, eprint = {2106.00666}`
-------------------- upstream_model --------------------
Document 1:

upstream_model: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al. and first released in [this repository](https://github.com/hustvl/YOLOS).
-------------------- parameter_count --------------------
Document 1:

"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images)"
-------------------- hyper_parameters --------------------
Document 1:

"200 epochs" and "150 epochs"
------------------------------
Document 2:

"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images)" "It was introduced in the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al." "first released in [this repository](https://github.com/hustvl/YOLOS)."
-------------------- evaluation --------------------
Document 1:

"AP (average precision) of **36.1** on COCO 2017 validation" and "table 1 of the original paper."
------------------------------
Document 2:

"The model was pre-trained for 200 epochs on ImageNet-1k and fine-tuned for 150 epochs on COCO."
-------------------- hardware --------------------
Document 1:

ImageNet-1k, COCO
-------------------- limitation_and_bias --------------------
Document 1:

"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images)", "[You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al.", "[this repository](https://github.com/hustvl/YOLOS)".
-------------------- demo --------------------
Document 1:

"model hub" "huggingface.co/models?search=hustvl/yolos"
------------------------------
Document 2:

```python from transformers import YolosFeatureExtractor, YolosForObjectDetection from PIL import Image import requests url = 'http://images.cocodataset.org/val2017/000000039769.jpg' image = Image.open(requests.get(url, stream=True).raw) feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small') model = YolosForObjectDetection.from_pretrained('hustvl/yolos-small') inputs = feature_extractor(images=image, return_tensors="pt") outputs = model(**inputs) # model predicts bounding boxes and corresponding COCO classes logits = outputs.logits bboxes = outputs.pred_boxes```
-------------------- input_format --------------------
Document 1:

ImageNet-1k, COCO 2017 object detection
-------------------- output_format --------------------
Document 1:

"return_tensors=\"pt\"" and "PyTorch."
-------------------- input_preprocessing --------------------

-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': ['ImageNet-1k', 'COCO 2017 object detection'], 'license': 'apache-2.0', 'github': 'htt 
ps://github.com/hustvl/YOLOS', 'paper': 'https://arxiv.org/abs/2106.00666', 'upstream_model': 'YOLOS 
 model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the pa 
per [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https 
://arxiv.org/abs/2106.00666) by Fang et al. and first released in [this repository](https://github.c 
om/hustvl/YOLOS).', 'parameter_count': 'YOLOS model fine-tuned on COCO 2017 object detection (118k a 
nnotated images)', 'hyper_parameters': {'epochs': '200 epochs', 'batch_size': '', 'learning_rate': ' 
', 'optimizer': ''}, 'evaluation': [{'test': 'AP (average precision) of **36.1** on COCO 2017 valida 
tion', 'result': 36.1}], 'hardware': 'ImageNet-1k, COCO', 'limitation_and_bias': 'YOLOS model fine-t 
uned on COCO 2017 object detection (118k annotated images), [You Only Look at One Sequence: Rethinki 
ng Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Fang et al., 
 [this repository](https://github.com/hustvl/YOLOS).', 'demo': '"model hub" "huggingface.co/models?s 
earch=hustvl/yolos"', 'input_format': 'ImageNet-1k, COCO 2017 object detection', 'output_format': '" 
return_tensors=\\"pt\\"" and "PyTorch."', 'input_preprocessing': '', 'input_size': '', 'num_of_class 
es_for_classification': '', 'trigger_word': ''}                                                      

#####################Salesforce/codet5p-220m########################

-------------------- datasets --------------------
Document 1:

datasets, span denoising, two variants of causal language modeling
------------------------------
Document 2:

[github-code dataset](https://huggingface.co/datasets/codeparrot/github-code) and `c`, `c++`, `c-sharp`,  `go`, `java`, `javascript`,  `php`, `python`, `ruby.`
-------------------- license --------------------
Document 1:

license: bsd-3-clause
------------------------------
Document 2:

"mit" “apache-2”, “bsd-3-clause”, “bsd-2-clause”, “cc0-1.0”, “unlicense”, “isc”
-------------------- github --------------------
Document 1:

[github-code dataset](https://huggingface.co/datasets/codeparrot/github-code)
-------------------- paper --------------------
Document 1:

"span denoising" and "two variants of _causal language modeling_"
------------------------------
Document 2:

CodeT5+: Open Code Large Language Models for Code Understanding and Generation
------------------------------
Document 3:

Please refer to the [paper](https://arxiv.org/pdf/2305.07922.pdf) for more details.
-------------------- upstream_model --------------------
Document 1:

"CodeT5+ models"
-------------------- parameter_count --------------------
Document 1:

"CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_." "In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters." "InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode"
-------------------- hyper_parameters --------------------
Document 1:

"CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_."
-------------------- evaluation --------------------
Document 1:

CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_. Specifically, CodeT5+ yields substantial performance gains on many downstream tasks compared to their SoTA baselines, e.g., 8 text-to-code retrieval tasks (+3.2 avg. MRR), 2 line-level code completion tasks (+2.1 avg. Exact Match), and 2 retrieval-augmented code generation tasks (+5.8 avg. BLEU-4). In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters. Particularly, in the zero-shot text-to-code generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode.
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_. In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters. Particularly, in the zero-shot text-to-code generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode.
-------------------- demo --------------------
Document 1:

CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_. In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters. Particularly, in the zero-shot text-to-code generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode.
------------------------------
Document 2:

CodeT5+ is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. _encoder-only_, _decoder-only_, and _encoder-decoder_) to support a wide range of code understanding and generation tasks. It is introduced in the paper: [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/pdf/2305.07922.pdf) by [Yue Wang](https://yuewang-cuhk.github.io/)\*, [Hung Le](https://sites.google.com/view/henryle2018/home?pli=1)\*, [Akhilesh Deepak Gotmare](https://akhileshgotmare.github.io/), [Nghi D.Q. Bui](https://bdqnghi.github.io/), [Junnan Li](https://sites.google.com/site/junnanlics), [Steven C.H. Hoi](https://sites.google.com/view/stevenhoi/home) (*
-------------------- input_format --------------------
Document 1:

"reserving only permissively licensed code ("mit" “apache-2”, “bsd-3-clause”, “bsd-2-clause”, “cc0-1.0”, “unlicense”, “isc”). Supported languages (9 in total) are as follows: `c`, `c++`, `c-sharp`,  `go`, `java`, `javascript`,  `php`, `python`, `ruby.`
-------------------- output_format --------------------
Document 1:

"github-code dataset", "reserving only permissively licensed code", "`c`, `c++`, `c-sharp`,  `go`, `java`, `javascript`,  `php`, `python`, `ruby.`
-------------------- input_token_limit --------------------
Document 1:

input_token_limit: 10
-------------------- vocabulary_size --------------------


[{'datasets': ['span denoising', 'two variants of causal language modeling'], 'license': 'bsd-3-cla 
use', 'github': '[github-code dataset](https://huggingface.co/datasets/codeparrot/github-code)', 'pa 
per': '"span denoising" and "two variants of _causal language modeling_"', 'upstream_model': '"CodeT 
5+ models"', 'parameter_count': '"CodeT5+ models have been comprehensively evaluated on a wide range 
 of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _ins 
truction-tuning_." "In 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of 
 below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters." "Instru 
ctCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs,  
even surpassing the closed-source OpenAI code-cushman-001 mode"', 'hyper_parameters': '"CodeT5+ mode 
ls have been comprehensively evaluated on a wide range of code understanding and generation tasks in 
 various settings: _zero-shot_, _finetuning_, and _instruction-tuning_."', 'evaluation': [{'test': ' 
CodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generat 
ion tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_. Specifically, Co 
deT5+ yields substantial performance gains on many downstream tasks compared to their SoTA baselines 
, e.g., 8 text-to-code retrieval tasks (+3.2 avg. MRR), 2 line-level code completion tasks (+2.1 avg 
. Exact Match), and 2 retrieval-augmented code generation tasks (+5.8 avg. BLEU-4). In 2 math progra 
mming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes signi 
ficantly outperform many LLMs of up to 137B parameters. Particularly, in the zero-shot text-to-code  
generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 an 
d 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman- 
001 mode.', 'result': 0}], 'hardware': '', 'limitation_and_bias': 'CodeT5+ models have been comprehe 
nsively evaluated on a wide range of code understanding and generation tasks in various settings: _z 
ero-shot_, _finetuning_, and _instruction-tuning_. In 2 math programming tasks on MathQA-Python and  
GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of  
up to 137B parameters. Particularly, in the zero-shot text-to-code generation task on HumanEval benc 
hmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other ope 
n code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode.', 'demo': 'CodeT5+ mode 
ls have been comprehensively evaluated on a wide range of code understanding and generation tasks in 
 various settings: _zero-shot_, _finetuning_, and _instruction-tuning_. In 2 math programming tasks  
on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly out 
perform many LLMs of up to 137B parameters. Particularly, in the zero-shot text-to-code generation t 
ask on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass 
@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode.',  
'input_format': '"reserving only permissively licensed code ("mit" “apache-2”, “bsd-3-clause”, “bsd- 
2-clause”, “cc0-1.0”, “unlicense”, “isc”). Supported languages (9 in total) are as follows: `c`, `c+ 
+`, `c-sharp`,  `go`, `java`, `javascript`,  `php`, `python`, `ruby."', 'output_format': '"github-co 
de dataset", "reserving only permissively licensed code", "`c`, `c++`, `c-sharp`,  `go`, `java`, `ja 
vascript`,  `php`, `python`, `ruby."', 'input_token_limit': '10', 'vocabulary_size': ''}]            

#####################Helsinki-NLP/opus-mt-en-es########################

-------------------- datasets --------------------
Document 1:

url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip, url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt
------------------------------
Document 2:

OPUS readme: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa/README.md), download original weights: [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip), test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt), test set scores: [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.eval.txt)
------------------------------
Document 3:

newssyscomb2009-engspa.eng.spa, news-test2008-engspa.eng.spa, newstest2009-engspa.eng.spa, newstest2010-engspa.eng.spa, newstest2011-engspa.eng.spa, newstest2012-engspa.eng.spa, newstest2013-engspa.eng.spa, Tatoeba-test.eng.spa
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip, url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt
-------------------- github --------------------
Document 1:

"language: - en - es license: apache-2.0 tags: - translation"
------------------------------
Document 2:

- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa/README.md  
- original_repo: Tatoeba-Challenge  
- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip  
- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt
------------------------------
Document 3:

* OPUS readme: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa/README.md)
* download original weights: [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip)
* test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt)
* test set scores: [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.eval.txt)
-------------------- paper --------------------
Document 1:

url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip  
url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt
-------------------- upstream_model --------------------
Document 1:

model: transformer
------------------------------
Document 2:

- hf_name: eng-spa  
- source_languages: eng  
- target_languages: spa  
- original_repo: Tatoeba-Challenge  
- src_alpha3: eng  
- tgt_alpha3: spa  
- short_pair: en-es  
- src_name: English  
- tgt_name: Spanish  
- src_alpha2: en  
- tgt_alpha2: es  
- long_pair: eng-spa
-------------------- parameter_count --------------------
Document 1:

- prepro:  normalization + SentencePiece (spm32k,spm32k)  
- chrF2_score: 0.721  
- bleu: 54.9  
- brevity_penalty: 0.978  
- ref_len: 77311.0
-------------------- hyper_parameters --------------------
Document 1:

prepro:  normalization + SentencePiece (spm32k,spm32k)  
url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip  
url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt  
chrF2_score: 0.721  
bleu: 54.9  
brevity_penalty: 0.978  
ref_len: 77311.0
-------------------- evaluation --------------------
Document 1:

- chrF2_score: 0.721  
- bleu: 54.9  
- brevity_penalty: 0.978  
- ref_len: 77311.0
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newssyscomb2009-engspa.eng.spa 	| 31.0 	| 0.583 |
| news-test2008-engspa.eng.spa 	| 29.7 	| 0.564 |
| newstest2009-engspa.eng.spa 	| 30.2 	| 0.578 |
| newstest2010-engspa.eng.spa 	| 36.9 	| 0.620 |
| newstest2011-engspa.eng.spa 	| 38.2 	| 0.619 |
| newstest2012-engspa.eng.spa 	| 39.0 	| 0.625 |
| newstest2013-engspa.eng.spa 	| 35.0 	| 0.598 |
| Tatoeba-test.eng.spa 	| 54.9 	| 0.721 |
------------------------------
Document 3:

[opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.eval.txt)
-------------------- hardware --------------------
Document 1:

port_machine: brutasse
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip, url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt
------------------------------
Document 2:

[eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa/README.md), transformer, [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip), [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt), [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.eval.txt)
-------------------- input_format --------------------
Document 1:

prepro:  normalization + SentencePiece (spm32k,spm32k)
------------------------------
Document 2:

SentencePiece (spm32k,spm32k)
-------------------- output_format --------------------
Document 1:

output_format: SentencePiece (spm32k,spm32k)
------------------------------
Document 2:

test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt)
-------------------- input_token_limit --------------------
Document 1:

SentencePiece (spm32k,spm32k)
------------------------------
Document 2:

prepro:  normalization + SentencePiece (spm32k,spm32k)
-------------------- vocabulary_size --------------------
Document 1:

prepro:  normalization + SentencePiece (spm32k,spm32k)
------------------------------
Document 2:

SentencePiece (spm32k,spm32k)

[{'datasets': ['Tatoeba-test.eng.spa'], 'license': 'apache-2.0', 'github': 'https://github.com/Hels 
inki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa/README.md', 'paper': '', 'upstream_model': 'tr 
ansformer', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limita 
tion_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'v 
ocabulary_size': ''}]                                                                                

#####################Salesforce/codegen-350M-mono########################

-------------------- datasets --------------------
Document 1:

CodeGen-Multi 350M, BigPython dataset, 71.7B tokens of Python programming language, [paper](https://arxiv.org/abs/2203.13474)
-------------------- license --------------------
Document 1:

license: bsd-3-clause
-------------------- github --------------------
Document 1:

"CodeGen is a family of autoregressive language models for **program synthesis** from the paper: [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released in [this repository](https://github.com/salesforce/CodeGen), under 3 pre-training data variants (`NL`, `Multi`, `Mono`) and 4 model size variants (`350M`, `2B`, `6B`, `16B`)."
-------------------- paper --------------------
Document 1:

"Please refer to the [paper](https://arxiv.org/abs/2203.13474) for more details."
------------------------------
Document 2:

"See Section 2.3 of the [paper](https://arxiv.org/abs/2203.13474) for more details."
------------------------------
Document 3:

"A Conversational Paradigm for Program Synthesis" by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.
-------------------- upstream_model --------------------
Document 1:

"autoregressive language model" "program synthesis" "generating executable code given English prompts" "comment string" "partially-generated code"
-------------------- parameter_count --------------------
Document 1:

parameter_count CodeGen-Multi 350M
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"We evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to the [paper](https://arxiv.org/abs/2203.13474) for more details."
------------------------------
Document 2:

"program synthesis"
-------------------- hardware --------------------
Document 1:

"multiple TPU-v4-512 by Google"
-------------------- limitation_and_bias --------------------
Document 1:

"autoregressive language model", "program synthesis", "generating executable code given English prompts", "prompts should be in the form of a comment string", "model can complete partially-generated code as well"
-------------------- demo --------------------
Document 1:

"program synthesis"
------------------------------
Document 2:

CodeGen is a family of autoregressive language models for **program synthesis**, The models are originally released in [this repository](https://github.com/salesforce/CodeGen), under 3 pre-training data variants (`NL`, `Multi`, `Mono`) and 4 model size variants (`350M`, `2B`, `6B`, `16B`). The checkpoint included in this repository is denoted as **CodeGen-Mono 350M**.
-------------------- input_format --------------------
Document 1:

CodeGen-Multi 350M, BigPython dataset, 71.7B tokens of Python programming language, Section 2.1 of the [paper](https://arxiv.org/abs/2203.13474)
------------------------------
Document 2:

"AutoTokenizer, AutoModelForCausalLM, tokenizer(text, return_tensors="pt").input_ids"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"71.7B tokens of Python programming language" and "See Section 2.1 of the [paper](https://arxiv.org/abs/2203.13474) for more details."
------------------------------
Document 2:

"CodeGen-Mono 350M", "350M"
------------------------------
Document 3:

"Salesforce/codegen-350M-mono"

[{'datasets': ['BigPython'], 'license': 'bsd-3-clause', 'github': 'https://github.com/salesforce/Co 
deGen', 'paper': 'https://arxiv.org/abs/2203.13474', 'upstream_model': 'autoregressive language mode 
l', 'parameter_count': 'CodeGen-Multi 350M', 'hyper_parameters': {}, 'evaluation': [{'test': 'code g 
eneration benchmark', 'result': 0.0}], 'hardware': 'multiple TPU-v4-512 by Google', 'limitation_and_ 
bias': 'autoregressive language model, program synthesis, generating executable code given English p 
rompts, prompts should be in the form of a comment string, model can complete partially-generated co 
de as well', 'demo': 'program synthesis', 'input_format': 'CodeGen-Multi 350M, BigPython dataset, 71 
.7B tokens of Python programming language, Section 2.1 of the paper', 'output_format': '', 'input_to 
ken_limit': '', 'vocabulary_size': '71.7B tokens of Python programming language'}]                   

#####################flair/upos-english########################

-------------------- datasets --------------------
Document 1:

"Contextual String Embeddings for Sequence Labeling"
------------------------------
Document 2:

"ColumnCorpus(...tag_to_bioes="ner",)"
------------------------------
Document 3:

datasets: - ontonotes
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

[here](https://github.com/flairNLP/flair/issues/)
------------------------------
Document 2:

"github"
-------------------- paper --------------------
Document 1:

"Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}"
------------------------------
Document 2:

"Flair embeddings" and "LSTM-CRF"
-------------------- upstream_model --------------------
Document 1:

"SequenceTagger", "tag_type='upos'", "embeddings=embeddings"
------------------------------
Document 2:

"upstream_model"
------------------------------
Document 3:

"This is the standard universal part-of-speech tagging model for English that ships with [Flair](https://github.com/flairNLP/flair/)."
-------------------- parameter_count --------------------
Document 1:

"hidden_size=256,", "embeddings=embeddings,", "tag_dictionary=tag_dictionary,"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
------------------------------
Document 3:

"parameter_count"
-------------------- hyper_parameters --------------------
Document 1:

hidden_size=256, max_epochs=150
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- evaluation --------------------
Document 1:

"tag_type = 'upos'", "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)", "embeddings = StackedEmbeddings(embeddings=embedding_types)", "tagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type)", "trainer = ModelTrainer(tagger, corpus)", "trainer.train('resources/taggers/upos-english', train_with_dev=True, max_epochs=150)"
------------------------------
Document 2:

F1-Score: **98,6** (Ontonotes) | **tag** | **meaning** | ADJ |  adjective | ADP |  adposition | ADV |  adverb | AUX |  auxiliary | CCONJ |  coordinating conjunction | DET |  determiner | INTJ |  interjection | NOUN |  noun | NUM |  numeral | PART |  particle | PRON |  pronoun | PROPN |  proper noun | PUNCT |  punctuation | SCONJ |  subordinating conjunction | SYM |  symbol | VERB |  verb | X |  other
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- demo --------------------
Document 1:

"Find a form of demo"
------------------------------
Document 2:

[here](https://github.com/flairNLP/flair/issues/)
-------------------- input_format --------------------
Document 1:

"column_format={0: "text", 1: "pos", 2: "upos", 3: "ner"}"
input_format: {0: "text", 1: "pos", 2: "upos", 3: "ner"}
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF." NO_OUTPUT

[{'datasets': ['ontonotes'], 'license': 'license', 'github': 'github', 'paper': 'Please cite the fo 
llowing paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embedd 
ings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle 
 = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1 
649}, year      = {2018}', 'upstream_model': 'upstream_model', 'parameter_count': 'parameter_count', 
 'hyper_parameters': [{'epochs': '150', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'e 
valuation': [{'test': "tag_type = 'upos'", 'result': 0}], 'hardware': '', 'limitation_and_bias': 'Ba 
sed on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.', 'demo': 'Find  
a form of demo', 'input_format': 'column_format={0: "text", 1: "pos", 2: "upos", 3: "ner"}', 'output 
_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                       

#####################jonatasgrosman/wav2vec2-large-xlsr-53-portuguese########################

-------------------- datasets --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, 16kHz, OVHcloud, https://github.com/jonatasgrosman/wav2vec2-sprint
------------------------------
Document 2:

datasets:
- common_voice
- mozilla-foundation/common_voice_6_0
dataset:
name: Common Voice pt
type: common_voice
args: pt
dataset:
name: Robust Speech Event - Dev Data
type: speech-recognition-community-v2/dev_data
args: pt
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"This model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)"
NO_OUTPUT
-------------------- github --------------------
Document 1:

"\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese}"
------------------------------
Document 2:

facebook/wav2vec2-large-xlsr-53, https://github.com/jonatasgrosman/wav2vec2-sprint
------------------------------
Document 3:

- mozilla-foundation/common_voice_6_0
- common_voice
- Automatic Speech Recognition
- Common Voice pt
- wer
- cer
- Robust Speech Event - Dev Data
- speech-recognition-community-v2/dev_data
-------------------- paper --------------------
Document 1:

"Fine-tuned {XLSR}-53 large model for speech recognition in {P}ortuguese"
------------------------------
Document 2:

"The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
------------------------------
Document 3:

Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:  
```python
from huggingsound import SpeechRecognitionModel

model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-portuguese")
```
NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"jonatasgrosman/wav2vec2-large-xlsr-53-portuguese"
------------------------------
Document 2:

facebook/wav2vec2-large-xlsr-53
-------------------- parameter_count --------------------
Document 1:

parameter_count: 53
------------------------------
Document 2:

model-index:
- name: XLSR Wav2Vec2 Portuguese by Jonatas Grosman

NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, 16kHz, OVHcloud, https://github.com/jonatasgrosman/wav2vec2-sprint
------------------------------
Document 2:

args: pt
metrics:
- type: wer
value: 11.31
name: Test WER
- type: cer
value: 3.74
name: Test CER
- type: wer
value: 9.01
name: Test WER (+LM)
- type: cer
value: 3.21
name: Test CER (+LM)
- type: wer
value: 42.1
name: Dev WER
- type: cer
value: 17.93
name: Dev CER
- type: wer
value: 36.92
name: Dev WER (+LM)
- type: cer
value: 16.88
name: Dev CER (+LM)
-------------------- evaluation --------------------
Document 1:

Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Portuguese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice). When using this model, make sure that your speech input is sampled at 16kHz. The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- hardware --------------------
Document 1:

OVHcloud, GPU credits
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

```bibtex
@misc{grosman2021xlsr53-large-portuguese,
title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ortuguese},
author={Grosman, Jonatas},
howpublished={\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese}},
year={2021}
}
```
------------------------------
Document 2:

Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:  
```python
from huggingsound import SpeechRecognitionModel

model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-portuguese")
audio_paths = ["/path/to/file.mp3", "/path/to/another_file.wav"]

transcriptions = model.transcribe(audio_paths)
```  
Writing your own inference script:  
```python
import torch
import librosa
from datasets import load_dataset
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

LANG_ID = "pt"
MODEL_ID = "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese"
SAMPLES = 10

test_dataset = load_dataset("common_voice", LANG_ID, split=f"test[
------------------------------
Document 3:

"Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Portuguese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice)", "When using this model, make sure that your speech input is sampled at 16kHz.", "The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
-------------------- input_format --------------------
Document 1:

16kHz, GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/), https://github.com/jonatasgrosman/wav2vec2-sprint
input_format: 16kHz
------------------------------
Document 2:

datasets:
- common_voice
- mozilla-foundation/common_voice_6_0
args: pt
-------------------- output_format --------------------
Document 1:

output_format: 16kHz
-------------------- sample_rate --------------------
Document 1:

sample rate of 16kHz
------------------------------
Document 2:

16_000
-------------------- WER --------------------
Document 1:

- type: wer
value: 11.31
name: Test WER
- type: wer
value: 9.01
name: Test WER (+LM)
------------------------------
Document 2:

"This model has been fine-tuned" NO_OUTPUT

[{'datasets': ['Common Voice 6.1'], 'license': 'apache-2.0', 'github': 'https://github.com/jonatasg 
rosman/wav2vec2-sprint', 'paper': 'Fine-tuned {XLSR}-53 large model for speech recognition in {P}ort 
uguese', 'upstream_model': 'jonatasgrosman/wav2vec2-large-xlsr-53-portuguese', 'parameter_count': '5 
3', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'eva 
luation': [{'test': 'wer', 'result': 11.31}], 'hardware': 'OVHcloud, GPU credits', 'limitation_and_b 
ias': '', 'demo': 'Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library: 
\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel("jona 
tasgrosman/wav2vec2-large-xlsr-53-portuguese")\n```\n', 'input_format': '16kHz', 'output_format': '' 
}]                                                                                                   

#####################ckiplab/bert-base-chinese-ws########################

-------------------- datasets --------------------
Document 1:

- https://github.com/ckiplab/ckip-transformers
-------------------- license --------------------
Document 1:

license: gpl-3.0
------------------------------
Document 2:

"license"
-------------------- github --------------------
Document 1:

"https://github.com/ckiplab/ckip-transformers"
-------------------- paper --------------------
Document 1:

"Author & Maintainer"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

- https://github.com/ckiplab/ckip-transformers
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['https://github.com/ckiplab/ckip-transformers'], 'license': 'gpl-3.0', 'github': 'ht 
tps://github.com/ckiplab/ckip-transformers', 'paper': 'Author & Maintainer', 'upstream_model': 'upst 
ream_model', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_r 
ate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'ht 
tps://github.com/ckiplab/ckip-transformers', 'input_format': '', 'output_format': '', 'input_token_l 
imit': '', 'vocabulary_size': ''}]                                                                   

#####################avichr/heBERT_NER########################

-------------------- datasets --------------------
Document 1:

Ben Mordecai and M Elhadad (2005), [https://www.cs.bgu.ac.il/~elhadad/nlpproj/naama/]
------------------------------
Document 2:

1. A Hebrew version of [OSCAR](https://oscar-corpus.com/): ~9.8 GB of data, including 1 billion words and over 20.8 millions sentences.
2. A Hebrew dump of [Wikipedia](https://dumps.wikimedia.org/): ~650 MB of data, including over 63 millions words and 3.8 millions sentences
3. Emotion User Generated Content (UGC) data that was collected for the purpose of this study (described below).
------------------------------
Document 3:

[**Emotion Recognition Model**](https://huggingface.co/avichr/hebEMO_trust), [**Sentiment Analysis**](https://huggingface.co/avichr/heBERT_sentiment_analysis), [**masked-LM model**](https://huggingface.co/avichr/heBERT)
-------------------- license --------------------
Document 1:

Ben Mordecai and M Elhadad (2005)
-------------------- github --------------------
Document 1:

Ben Mordecai and M Elhadad (2005)
-------------------- paper --------------------
Document 1:

Ben Mordecai and M Elhadad (2005)
------------------------------
Document 2:

[**Emotion Recognition Model**](https://huggingface.co/avichr/hebEMO_trust)
-------------------- upstream_model --------------------
Document 1:

upstream_model: heBERT
------------------------------
Document 2:

"model="avichr/heBERT_NER""
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
------------------------------
Document 2:

"HeBert was trained on three dataset: 1. A Hebrew version of [OSCAR](https://oscar-corpus.com/): ~9.8 GB of data, including 1 billion words and over 20.8 millions sentences. 2. A Hebrew dump of [Wikipedia](https://dumps.wikimedia.org/): ~650 MB of data, including over 63 millions words and 3.8 millions sentences 3. Emotion User Generated Content (UGC) data that was collected for the purpose of this study (described below)."
-------------------- hyper_parameters --------------------
Document 1:

"HeBERT is a Hebrew pretrained language model. It is based on [Google's BERT](https://arxiv.org/abs/1810.04805) architecture and it is BERT-Base config."
------------------------------
Document 2:

"masked-LM model" (can be fine-tunned to any down-stream task).
-------------------- evaluation --------------------
Document 1:

"The ability of the model to classify named entities in text, such as persons' names, organizations, and locations; tested on a labeled dataset from [Ben Mordecai and M Elhadad (2005)](https://www.cs.bgu.ac.il/~elhadad/nlpproj/naama/), and evaluated with F1-score."
------------------------------
Document 2:

"HeBERT is a Hebrew pretrained language model. It is based on [Google's BERT](https://arxiv.org/abs/1810.04805) architecture and it is BERT-Base config. HeBert was trained on three dataset: 1. A Hebrew version of [OSCAR](https://oscar-corpus.com/): ~9.8 GB of data, including 1 billion words and over 20.8 millions sentences. 2. A Hebrew dump of [Wikipedia](https://dumps.wikimedia.org/): ~650 MB of data, including over 63 millions words and 3.8 millions sentences. 3. Emotion User Generated Content (UGC) data that was collected for the purpose of this study (described below)."
-------------------- hardware --------------------
Document 1:

"Google's BERT architecture" and "BERT-Base config"
-------------------- limitation_and_bias --------------------
Document 1:

Ben Mordecai and M Elhadad (2005)
------------------------------
Document 2:

"Emotion Recognition Model", "Sentiment Analysis", "masked-LM model"
------------------------------
Document 3:

"HeBERT is a Hebrew pretrained language model. It is based on [Google's BERT](https://arxiv.org/abs/1810.04805) architecture and it is BERT-Base config. HeBert was trained on three dataset: 1. A Hebrew version of [OSCAR](https://oscar-corpus.com/): ~9.8 GB of data, including 1 billion words and over 20.8 millions sentences. 2. A Hebrew dump of [Wikipedia](https://dumps.wikimedia.org/): ~650 MB of data, including over 63 millions words and 3.8 millions sentences 3. Emotion User Generated Content (UGC) data that was collected for the purpose of this study (described below)."
-------------------- demo --------------------
Document 1:

[huggingface spaces](https://huggingface.co/spaces/avichr/HebEMO_demo) or as [colab notebook](https://colab.research.google.com/drive/1Jw3gOWjwVMcZslu-ttXoNeD17lms1-ff?usp=sharing)
------------------------------
Document 2:

"The ability of the model to classify named entities in text, such as persons' names, organizations, and locations; tested on a labeled dataset from [Ben Mordecai and M Elhadad (2005)](https://www.cs.bgu.ac.il/~elhadad/nlpproj/naama/), and evaluated with F1-score."
-------------------- input_format --------------------
Document 1:

"Google's BERT", "BERT-Base config", "A Hebrew version of OSCAR", "A Hebrew dump of Wikipedia", "Emotion User Generated Content (UGC) data" 
input_format: Google's BERT, BERT-Base config, A Hebrew version of OSCAR, A Hebrew dump of Wikipedia, Emotion User Generated Content (UGC) data
------------------------------
Document 2:

Ben Mordecai and M Elhadad (2005)
-------------------- output_format --------------------
Document 1:

Ben Mordecai and M Elhadad (2005)
-------------------- input_token_limit --------------------
Document 1:

"Google's BERT", "BERT-Base config"
------------------------------
Document 2:

"token-classification", "model="avichr/heBERT_NER", "tokenizer="avichr/heBERT_NER""
-------------------- vocabulary_size --------------------
Document 1:

"HeBERT is a Hebrew pretrained language model. It is based on [Google's BERT](https://arxiv.org/abs/1810.04805) architecture and it is BERT-Base config."
------------------------------
Document 2:

"masked-LM model"

[{'datasets': ['Ben Mordecai and M Elhadad (2005)'], 'license': 'Ben Mordecai and M Elhadad (2005)' 
, 'github': 'Ben Mordecai and M Elhadad (2005)', 'paper': 'Ben Mordecai and M Elhadad (2005)', 'upst 
ream_model': 'heBERT', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epochs': 'NO_OUTPUT', ' 
batch_size': 'NO_OUTPUT', 'learning_rate': 'NO_OUTPUT', 'optimizer': 'NO_OUTPUT'}, 'evaluation': [{' 
test': 'Ben Mordecai and M Elhadad (2005)', 'result': 0}], 'hardware': "Google's BERT architecture a 
nd BERT-Base config", 'limitation_and_bias': 'Ben Mordecai and M Elhadad (2005)', 'demo': '[huggingf 
ace spaces](https://huggingface.co/spaces/avichr/HebEMO_demo) or as [colab notebook](https://colab.r 
esearch.google.com/drive/1Jw3gOWjwVMcZslu-ttXoNeD17lms1-ff?usp=sharing)', 'input_format': "Google's  
BERT, BERT-Base config, A Hebrew version of OSCAR, A Hebrew dump of Wikipedia, Emotion User Generate 
d Content (UGC) data", 'output_format': 'Ben Mordecai and M Elhadad (2005)', 'input_token_limit': "G 
oogle's BERT, BERT-Base config", 'vocabulary_size': 'Ben Mordecai and M Elhadad (2005)'}, {'datasets 
': ['A Hebrew version of OSCAR', 'A Hebrew dump of Wikipedia', 'Emotion User Generated Content (UGC) 
 data'], 'license': 'NO_OUTPUT', 'github': 'NO_OUTPUT', 'paper': 'NO_OUTPUT', 'upstream_model': 'NO_ 
OUTPUT', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epochs': 'NO_OUTPUT', 'batch_size': ' 
NO_OUTPUT', 'learning_rate': 'NO_OUTPUT', 'optimizer': 'NO_OUTPUT'}, 'evaluation': [], 'hardware': ' 
NO_OUTPUT', 'limitation_and_bias': 'Emotion Recognition Model, Sentiment Analysis, masked-LM model', 
 'demo': 'NO_OUTPUT', 'input_format': 'NO_OUTPUT', 'output_format': 'NO_OUTPUT', 'input_token_limit' 
: 'NO_OUTPUT', 'vocabulary_size': 'NO_OUTPUT'}]                                                      

#####################openai-gpt########################

-------------------- datasets --------------------
Document 1:

"We use the BooksCorpus dataset ([Zhu et al., 2015](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhu_Aligning_Books_and_ICCV_2015_paper.pdf)) for training the language model."
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"associated paper", "modeling architecture", "objective", "compute infrastructure", "training details"
------------------------------
Document 2:

"In the [associated paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

8 P600 GPU's * 30 days * 12 TFLOPS/GPU * 0.33 utilization = .96 pfs-days
-------------------- hyper_parameters --------------------
Document 1:

"Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm [2] is used extensively throughout the model, a simple weight initialization of N (0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]."
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

- **Task:** Textual Entailment
- **Datasets:** [SNLI](https://huggingface.co/datasets/snli), [MNLI Matched](https://huggingface.co/datasets/glue), [MNLI Mismatched](https://huggingface.co/datasets/glue), [SciTail](https://huggingface.co/datasets/scitail), [QNLI](https://huggingface.co/datasets/glue), [RTE](https://huggingface.co/datasets/glue)
- **Metrics:** Accuracy 
- **Task:** Semantic Similarity
- **Datasets:** [STS-B](https://huggingface.co/datasets/glue), [QQP](https://huggingface.co/datasets/glue), [MRPC](https://huggingface.co/datasets/glue)
- **Metrics:** Accuracy 
- **Task:** Reading Comp
------------------------------
Document 3:

"In the [associated paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), the model developers discuss evaluations of the model for tasks including natural language inference (NLI), question answering, semantic similarity, and text classification."
-------------------- hardware --------------------
Document 1:

8 P600 GPU's * 30 days
------------------------------
Document 2:

"compute infrastructure"
-------------------- limitation_and_bias --------------------
Document 1:

Risks, Limitations and Biases
------------------------------
Document 2:

- Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).
- Predictions generated by this model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.
- Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.
- Compute Requirements: Many previous approaches to NLP tasks train relatively small models on a single GPU from scratch. Our approach requires an expensive pre-training step - 1 month on 8 GPUs.
- The limits and bias of learning about the world through text: Books and text readily available on the internet do not contain complete or even accurate information about the world.
- Recent work ([Lucy and Gauthier, 2017](https://arxiv.org/abs/1705.11168)) has shown that certain kinds of information are
------------------------------
Document 3:

"This model can be used for language modeling tasks.", "Potential downstream uses of this model include tasks that leverage language models.", "The model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model."
-------------------- demo --------------------
Document 1:

"In the [associated paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), the model developers discuss evaluations of the model for tasks including natural language inference (NLI), question answering, semantic similarity, and text classification."
-------------------- input_format --------------------
Document 1:

"We use the BooksCorpus dataset ([Zhu et al., 2015](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhu_Aligning_Books_and_ICCV_2015_paper.pdf)) for training the language model." "We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization." "We also employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or gain weights." "For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]." "We use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer." input_format: BooksCorpus dataset with a bytepair encoding (BPE) vocabulary with 40,000 merges, residual, embedding, and attention dropouts with a rate of 0.1 for regularization, modified version of L2 regularization with w = 0.01 on
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens."
------------------------------
Document 2:

"The model does fine-tune to new tasks very quickly which helps mitigate the additional resource requirements."
"we used a 37-layer (12 block) Transformer architecture, and we train on sequences of up to 512 tokens."
-------------------- vocabulary_size --------------------
Document 1:

"We used a bytepair encoding (BPE) vocabulary with 40,000 merges"

[{'datasets': ['BooksCorpus'], 'license': 'mit', 'github': '', 'paper': '', 'upstream_model': '', ' 
parameter_count': "8 P600 GPU's * 30 days * 12 TFLOPS/GPU * 0.33 utilization = .96 pfs-days", 'hyper 
_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation':  
[], 'hardware': "8 P600 GPU's * 30 days", 'limitation_and_bias': '', 'demo': '', 'input_format': '', 
 'output_format': '', 'input_token_limit': '', 'vocabulary_size': '40,000 merges'}]                  

#####################microsoft/deberta-v2-xlarge########################

-------------------- datasets --------------------
Document 1:

"80GB training data", "160GB raw data"
------------------------------
Document 2:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"International Conference on Learning Representations"
-------------------- github --------------------
Document 1:

"If you find DeBERTa useful for your work, please cite the following paper: ``` latex @inproceedings{he2021deberta, title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION}, author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen}, booktitle={International Conference on Learning Representations}, year={2021}, url={https://openreview.net/forum?id=XPZIaotutsD} }```"
------------------------------
Document 2:

"Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates."
-------------------- paper --------------------
Document 1:

"If you find DeBERTa useful for your work, please cite the following paper: 
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}"
------------------------------
Document 2:

"[DeBERTa](https://arxiv.org/abs/2006.03654)"
-------------------- upstream_model --------------------
Document 1:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
-------------------- parameter_count --------------------
Document 1:

"900M" and "total parameters"
-------------------- hyper_parameters --------------------
Document 1:

"24 layers, 1536 hidden size, 900M total parameters"
-------------------- evaluation --------------------
Document 1:

| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |
|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|
|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |
| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |
| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |
-------------------- hardware --------------------
Document 1:

"24 layers, 1536 hidden size" "160GB raw data"
------------------------------
Document 2:

"International Conference on Learning Representations"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"If you find DeBERTa useful for your work, please cite the following paper:"
------------------------------
Document 2:

[DeBERTa](https://arxiv.org/abs/2006.03654), [official repository](https://github.com/microsoft/DeBERTa)
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['80GB training data', '160GB raw data'], 'license': 'mit', 'github': 'If you find De 
BERTa useful for your work, please cite the following paper: ``` latex @inproceedings{he2021deberta, 
 title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION}, author={Pengcheng He and Xiaod 
ong Liu and Jianfeng Gao and Weizhu Chen}, booktitle={International Conference on Learning Represent 
ations}, year={2021}, url={https://openreview.net/forum?id=XPZIaotutsD} }```', 'paper': 'If you find 
 DeBERTa useful for your work, please cite the following paper: \n@inproceedings{\nhe2021deberta,\nt 
itle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodo 
ng Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Represent 
ations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}', 'upstream_model': 'DE 
BERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION', 'parameter_count': '900M and total param 
eters', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''},  
'evaluation': [], 'hardware': '24 layers, 1536 hidden size 160GB raw data', 'limitation_and_bias': ' 
', 'demo': 'If you find DeBERTa useful for your work, please cite the following paper:', 'input_form 
at': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                       
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e024d-4b0ef0d449c32a714a6a07cb)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-NystromformerModel/resolve/main/README.md. 

#####################klue/bert-base########################

-------------------- datasets --------------------
Document 1:

"modeling architecture (BERT)", "training details"
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"associated paper", "modeling architecture (BERT)", "objective", "compute infrastructure", "training details"
------------------------------
Document 2:

[paper](https://arxiv.org/pdf/2105.09680.pdf)
-------------------- upstream_model --------------------
Document 1:

"topic classification, semantic textual similarity, natural language inference, named entity recognition"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"Topic Classification (TC) - Yonhap News Agency Topic Classification (YNAT), **Metrics:** Macro F1 score, defined as the mean of topic-wise F1 scores, giving the same importance to each topic." "Semantic Textual Similarity (STS), **Metrics:** Pearsons' correlation coefficient (Pearson’ r) and F1 score" "Natural Language Inference (NLI), **Metrics:** Accuracy" "Named Entity Recognition (NER), **Metrics:** Entity-level macro F1 (Entity F1) and character-level macro F1 (Char F1) scores" "Relation Extraction (RE), **Metrics:** Micro F1 score on relation existing cases and area under the precision- recall curve (AUPRC) on all classes" "Dependency Parsing (DP), **Metrics:** Unlabeled attachment score (UAS) and labeled attachment score (LAS)" "Machine Reading Comprehension (MRC), **Metrics:** Exact match (EM) and character-level ROUGE-W (ROUGE), which can be viewed as longest common consecutive subsequence (LCCS)-based
-------------------- evaluation --------------------
Document 1:

"Evaluation"
------------------------------
Document 2:

- **Task:** Topic Classification (TC) - Yonhap News Agency Topic Classification (YNAT), **Metrics:** Macro F1 score, defined as the mean of topic-wise F1 scores, giving the same importance to each topic.  
- **Task:** Semantic Textual Similarity (STS), **Metrics:** Pearsons' correlation coefficient (Pearson’ r) and F1 score  
- **Task:** Natural Language Inference (NLI), **Metrics:** Accuracy  
- **Task:** Named Entity Recognition (NER), **Metrics:** Entity-level macro F1 (Entity F1) and character-level macro F1 (Char F1) scores  
- **Task:** Relation Extraction (RE), **Metrics:** Micro F1 score on relation existing cases and area under the precision- recall curve (AUPRC) on all classes  
- **Task:** Dependency Parsing (DP), **Metrics:** Unlabeled attachment score (UAS) and labeled attachment score (LAS)  
- **Task:** Machine Reading Comprehension (MRC), **Metrics:**
------------------------------
Document 3:

"The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition, and other tasks outlined in the [KLUE Benchmark](https://github.com/KLUE-benchmark/KLUE)." "The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model."
-------------------- hardware --------------------
Document 1:

"modeling architecture (BERT)", "compute infrastructure"
------------------------------
Document 2:

TPU v3-8
------------------------------
Document 3:

"Technical Specifications"
-------------------- limitation_and_bias --------------------
Document 1:

- Bias issues with the publicly available data used in the pretraining corpora (and considerations related to filtering)
- PII in the data used in the pretraining corpora (and efforts to pseudonymize the data)
------------------------------
Document 2:

"Risks, Limitations and Biases"
------------------------------
Document 3:

"The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition"
-------------------- demo --------------------
Document 1:

"The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition, and other tasks outlined in the [KLUE Benchmark](https://github.com/KLUE-benchmark/KLUE)."
-------------------- input_format --------------------
Document 1:

"We design and use a new tokenization method, morpheme-based subword tokenization. When building a vocabulary, we pre-tokenize a raw text into morphemes using a morphological analyzer, and then we apply byte pair encoding (BPE) ([Senrich et al., 2016](https://aclanthology.org/P16-1162/)) to get the final vocabulary. For morpheme segmentation, we use [Mecab-ko](https://bitbucket.org/eunjeon/mecab-ko), MeCab ([Kudo, 2006](https://taku910.github.io/mecab/)) adapted for Korean, and for BPE segmentation, we use the wordpiece tokenizer from [Huggingface Tokenizers library](https://github.com/huggingface/tokenizers). We specify the vocabulary size to 32k."
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition, and other tasks outlined in the [KLUE Benchmark](https://github.com/KLUE-benchmark/KLUE)." NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

"We specify the vocabulary size to 32k."
------------------------------
Document 2:

"The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition"

[{'datasets': ['modeling architecture (BERT)', 'training details'], 'license': '', 'github': '', 'p 
aper': '', 'upstream_model': 'topic classification, semantic textual similarity, natural language in 
ference, named entity recognition', 'parameter_count': '', 'hyper_parameters': [{'epochs': '', 'batc 
h_size': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'har 
dware': 'modeling architecture (BERT), compute infrastructure', 'limitation_and_bias': '- Bias issue 
s with the publicly available data used in the pretraining corpora (and considerations related to fi 
ltering)\n- PII in the data used in the pretraining corpora (and efforts to pseudonymize the data)', 
 'demo': 'The model can be used for tasks including topic classification, semantic textual similarit 
y, natural language inference, named entity recognition, and other tasks outlined in the [KLUE Bench 
mark](https://github.com/KLUE-benchmark/KLUE).', 'input_format': 'We design and use a new tokenizati 
on method, morpheme-based subword tokenization. When building a vocabulary, we pre-tokenize a raw te 
xt into morphemes using a morphological analyzer, and then we apply byte pair encoding (BPE) ([Senri 
ch et al., 2016](https://aclanthology.org/P16-1162/)) to get the final vocabulary. For morpheme segm 
entation, we use [Mecab-ko](https://bitbucket.org/eunjeon/mecab-ko), MeCab ([Kudo, 2006](https://tak 
u910.github.io/mecab/)) adapted for Korean, and for BPE segmentation, we use the wordpiece tokenizer 
 from [Huggingface Tokenizers library](https://github.com/huggingface/tokenizers). We specify the vo 
cabulary size to 32k.', 'output_format': ''}, {'datasets': ['associated paper', 'modeling architectu 
re (BERT)', 'objective', 'compute infrastructure', 'training details'], 'license': '', 'github': '[p 
aper](https://arxiv.org/pdf/2105.09680.pdf)', 'paper': '', 'upstream_model': '', 'parameter_count':  
'', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'e 
valuation': [{'test': '', 'result': 0}], 'hardware': 'TPU v3-8', 'limitation_and_bias': '', 'demo':  
'', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}, {'data 
sets': ['The model can be used for tasks including topic classification, semantic textual similarity 
, natural language inference, named entity recognition, and other tasks outlined in the [KLUE Benchm 
ark](https://github.com/KLUE-benchmark/KLUE).'], 'license': '', 'github': '', 'paper': '', 'upstream 
_model': '', 'parameter_count': '', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_ 
rate': '', 'optimizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardware': '', 'limitation 
_and_bias': 'The model can be used for tasks including topic classification, semantic textual simila 
rity, natural language inference, named entity recognition', 'demo': '', 'input_format': '', 'output 
_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                       

#####################flax-community/spanish-t5-small########################

-------------------- datasets --------------------
Document 1:

datasets, 95% of the data was used for training
------------------------------
Document 2:

[large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus)
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"mrm8488](https://huggingface.co/mrm8488), [mariagrandury](https://huggingface.co/mariagrandury)"
------------------------------
Document 2:

[large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus), [Flax](https://github.com/google/flax), [Flax/Jax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), [HuggingFace](https://huggingface.co/)
------------------------------
Document 3:

"title={Spanish T5 (small) by Manuel Romero}, publisher={Hugging Face}, journal={Hugging Face Hub}, howpublished={\url{https://huggingface.co/flax-community/spanish-t5-small}}"
-------------------- paper --------------------
Document 1:

large_spanish_corpus, Flax, Flax/Jax Community Week, HuggingFace
-------------------- upstream_model --------------------
Document 1:

"This is a Spanish **T5** (small arch) trained from scratch on the [large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus) aka BETO's corpus with [Flax](https://github.com/google/flax)"

upstream_model: T5
-------------------- parameter_count --------------------
Document 1:

parameter_count: NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"This is a Spanish T5 (small arch) trained from scratch on the [large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus) aka BETO's corpus with [Flax](https://github.com/google/flax)"
-------------------- evaluation --------------------
Document 1:

"This is a Spanish **T5** (small arch) trained from scratch on the [large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus) aka BETO's corpus with [Flax](https://github.com/google/flax)"
-------------------- hardware --------------------
Document 1:

"Spanish T5" and "Flax"
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias NO_OUTPUT
-------------------- demo --------------------
Document 1:

"title={Spanish T5 (small) by Manuel Romero}, author={Romero, Manuel}, publisher={Hugging Face}, journal={Hugging Face Hub}, howpublished={\url{https://huggingface.co/flax-community/spanish-t5-small}}, year={2021}"
------------------------------
Document 2:

"large_spanish_corpus", "Flax", "Flax/Jax Community Week", "HuggingFace"
-------------------- input_format --------------------
Document 1:

"Spanish T5" "large_spanish_corpus" "Flax" "Flax/Jax Community Week" "HuggingFace"
-------------------- output_format --------------------
Document 1:

"T5", "[large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus)", "[Flax](https://github.com/google/flax)", "[Flax/Jax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104)", "[HuggingFace](https://huggingface.co/)", "TPU usage sponsored by Google."
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"This is a Spanish **T5** (small arch) trained from scratch on the [large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus) aka BETO's corpus with [Flax](https://github.com/google/flax)"

NO_OUTPUT

[{'datasets': ['large_spanish_corpus'], 'license': '', 'github': '[large_spanish_corpus](https://hu 
ggingface.co/datasets/viewer/?dataset=large_spanish_corpus), [Flax](https://github.com/google/flax), 
 [Flax/Jax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-usi 
ng-jax-flax-for-nlp-cv/7104), [HuggingFace](https://huggingface.co/)', 'paper': '', 'upstream_model' 
: 'T5', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'l 
imitation_and_bias': 'NO_OUTPUT', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_ 
limit': '', 'vocabulary_size': 'NO_OUTPUT'}]                                                         

#####################facebook/opt-350m########################

-------------------- datasets --------------------
Document 1:

- BookCorpus
- CC-Stories
- Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews
- Pushshift.io Reddit dataset
- CCNewsV2
------------------------------
Document 2:

"The dataset was collected form internet"
-------------------- license --------------------
Document 1:

license: other
-------------------- github --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- paper --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers." "Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs."
-------------------- upstream_model --------------------
Document 1:

"facebook/opt-350m"
-------------------- parameter_count --------------------
Document 1:

GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days
------------------------------
Document 2:

"125M to 175B parameters"
-------------------- hyper_parameters --------------------
Document 1:

"BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit dataset, CCNewsV2" NO_OUTPUT
------------------------------
Document 2:

"facebook/opt-350m", do_sample=True, num_return_sequences=5
-------------------- evaluation --------------------
Document 1:

- BookCorpus, which consists of more than 10K unpublished books,
- CC-Stories, which contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas,
- The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included.
- Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in
Roller et al. (2021)
- CCNewsV2 containing an updated version of the English portion of the CommonCrawl News
dataset that was used in RoBERTa (Liu et al., 2019b)
- The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally
to each dataset’s size in the pretraining corpus.
-------------------- hardware --------------------
Document 1:

GPT2, 80GB A100 GPUs, ~33 days
------------------------------
Document 2:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 3:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- limitation_and_bias --------------------
Document 1:

As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased : Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. Here's an example of how the model can have biased predictions: compared to: This bias will also affect all fine-tuned versions of this model.
------------------------------
Document 2:

"known challenges in areas such as robustness, bias, and toxicity"
-------------------- demo --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
------------------------------
Document 2:

[CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling), [model hub](https://huggingface.co/models?filter=opt)
-------------------- input_format --------------------
Document 1:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 2:

GPT2 byte-level version of Byte Pair Encoding (BPE), vocabulary size of 50272, inputs are sequences of 2048 consecutive tokens. input_format
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*
-------------------- vocabulary_size --------------------
Document 1:

"180B tokens corresponding to 800GB of data" NO_OUTPUT
------------------------------
Document 2:

GPT2, vocabulary size of 50272
------------------------------
Document 3:

"125M to 175B parameters"

[{'datasets': ['BookCorpus', 'CC-Stories', 'Pile-CC', 'OpenWebText2', 'USPTO', 'Project Gutenberg', 
 'OpenSubtitles', 'Wikipedia', 'DM Mathematics', 'HackerNews', 'Pushshift.io Reddit dataset', 'CCNew 
sV2'], 'license': 'other', 'github': 'We present Open Pretrained Transformers (OPT), a suite of deco 
der-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and re 
sponsibly share with interested researchers.', 'paper': 'We present Open Pretrained Transformers (OP 
T), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we  
aim to fully and responsibly share with interested researchers. Our aim in developing this suite of  
OPT models is to enable reproducible and responsible research at scale, and to bring more voices to  
the table in studying the impact of these LLMs.', 'upstream_model': 'facebook/opt-350m', 'parameter_ 
count': 'GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days', 'hyper_parameters': [{'epochs': 'Bo 
okCorpus, CC-Stories, The Pile, Pushshift.io Reddit dataset, CCNewsV2', 'batch_size': 'NO_OUTPUT', ' 
learning_rate': 'NO_OUTPUT', 'optimizer': 'NO_OUTPUT'}, {'epochs': 'facebook/opt-350m', 'batch_size' 
: 'do_sample=True, num_return_sequences=5', 'learning_rate': 'NO_OUTPUT', 'optimizer': 'NO_OUTPUT'}] 
, 'evaluation': [{'test': 'BookCorpus, which consists of more than 10K unpublished books', 'result': 
 0}, {'test': 'CC-Stories, which contains a subset of CommonCrawl data filtered to match the story-l 
ike style of Winograd schemas', 'result': 0}, {'test': 'The Pile, from which * Pile-CC, OpenWebText2 
, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included', 
 'result': 0}, {'test': 'Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) 
 and processed in Roller et al. (2021)', 'result': 0}, {'test': 'CCNewsV2 containing an updated vers 
ion of the English portion of the CommonCrawl News dataset that was used in RoBERTa (Liu et al., 201 
9b)', 'result': 0}, {'test': 'The final training data contains 180B tokens corresponding to 800GB of 
 data. The validation split was made of 200MB of the pretraining data, sampled proportionally to eac 
h dataset’s size in the pretraining corpus', 'result': 0}], 'hardware': ['GPT2, 80GB A100 GPUs, ~33  
days', 'BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wiki 
pedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset'] 
, 'limitation_and_bias': ["As mentioned in Meta AI's model card, given that the training data used f 
or this model contains a lot of unfiltered content from the internet, which is far from neutral the  
model is strongly biased : Like other large language models for which the diversity (or lack thereof 
) of training data induces downstream impact on the quality of our model, OPT-175B has limitations i 
n terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity a 
nd hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern  
large language models. Here's an example of how the model can have biased predictions: compared to:  
This bias will also affect all fine-tuned versions of this model.", 'known challenges in areas such  
as robustness, bias, and toxicity'], 'demo': ['We present Open Pretrained Transformers (OPT), a suit 
e of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to ful 
ly and responsibly share with interested researchers.', '[CLM example](https://github.com/huggingfac 
e/transformers/tree/main/examples/pytorch/language-modeling), [model hub](https://huggingface.co/mod 
els?filter=opt)'], 'input_format': ['BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project G 
utenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV 
2, CommonCrawl News dataset', 'GPT2 byte-level version of Byte Pair Encoding (BPE), vocabulary size  
of 50272, inputs are sequences of 2048 consecutive tokens. input_format'], 'output_format': [], 'inp 
ut_token_limit': ['GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*'], 'vocab 
ulary_size': ['180B tokens corresponding to 800GB of data', 'GPT2, vocabulary size of 50272']}]      

#####################facebook/roscoe-512-roberta-base########################

-------------------- datasets --------------------
Document 1:

"Entailment-Bank (deductive reasoning), ProofWriter (logical reasoning); three arithmetic reasoning datasets MATH, ASDIV and AQUA; EQASC (explanations for commonsense question answering), and StrategyQA (question answering with implicit reasoning strategies)."
------------------------------
Document 2:

"multi-step reasoning datasets we listed in §5 (see details in Golovneva et al., 2022)"
-------------------- license --------------------
Document 1:

license: cc-by-nc-4.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Entailment-Bank (deductive reasoning), ProofWriter (logical reasoning); three arithmetic reasoning datasets MATH, ASDIV and AQUA; EQASC (explanations for commonsense question answering), and StrategyQA (question answering with implicit reasoning strategies)."
------------------------------
Document 2:

"arXiv preprint arXiv:2104.08821, 2021."
------------------------------
Document 3:

"@article{golovneva2022roscoe, title={{ROSCOE}: A Suite of Metrics for Scoring Step-by-Step Reasoning}, author={Golovneva, Olga and Chen, Moya and Poff, Spencer and Corredor, Martin and Zettlemoyer, Luke and Fazel-Zarandi, Maryam and Celikyilmaz, Asli}, journal={arXiv preprint arXiv:2212.07919}, year={2022}"
-------------------- upstream_model --------------------
Document 1:

"RoBERTa word embedding model (Liu et al., 2019)"

upstream_model: RoBERTa word embedding model (Liu et al., 2019)
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"supervised sentence similarity model extending the RoBERTa word embedding model (Liu et al., 2019)", "cross-entropy objective with in-batch negatives", "pretrained checkpoint of supervised SimCSE model sup-simcse-roberta-base", "further train it for five epochs on our synthetic train data"
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"RoBERTa word embedding model (Liu et al., 2019)"
-------------------- limitation_and_bias --------------------
Document 1:

"deterministic modifications" on "reference reasoning steps" in "Entailment-Bank (deductive reasoning)", "ProofWriter (logical reasoning)", "three arithmetic reasoning datasets MATH, ASDIV and AQUA", "EQASC (explanations for commonsense question answering)", "StrategyQA (question answering with implicit reasoning strategies)"
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

"Entailment-Bank (deductive reasoning), ProofWriter (logical reasoning); three arithmetic reasoning datasets MATH, ASDIV and AQUA; EQASC (explanations for commonsense question answering), and StrategyQA (question answering with implicit reasoning strategies)"
-------------------- output_format --------------------


[{'datasets': ['Entailment-Bank (deductive reasoning)', 'ProofWriter (logical reasoning)', 'MATH',  
'ASDIV', 'AQUA', 'EQASC (explanations for commonsense question answering)', 'StrategyQA (question an 
swering with implicit reasoning strategies)'], 'license': 'cc-by-nc-4.0', 'github': '', 'paper': 'ar 
Xiv preprint arXiv:2104.08821, 2021.', 'upstream_model': 'RoBERTa word embedding model (Liu et al.,  
2019)', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learn 
ing_rate': '', 'optimizer': ''}], 'evaluation': [], 'hardware': 'RoBERTa word embedding model (Liu e 
t al., 2019)', 'limitation_and_bias': 'deterministic modifications on reference reasoning steps in E 
ntailment-Bank (deductive reasoning), ProofWriter (logical reasoning), three arithmetic reasoning da 
tasets MATH, ASDIV and AQUA, EQASC (explanations for commonsense question answering), StrategyQA (qu 
estion answering with implicit reasoning strategies)', 'demo': '', 'input_format': 'Entailment-Bank  
(deductive reasoning), ProofWriter (logical reasoning), three arithmetic reasoning datasets MATH, AS 
DIV and AQUA, EQASC (explanations for commonsense question answering), and StrategyQA (question answ 
ering with implicit reasoning strategies)', 'output_format': ''}]                                    
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0301-206928a4113b02586be299b7)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-MPNetModel/resolve/main/README.md. 

#####################Mizuiro-sakura/luke-japanese-base-finetuned-ner########################

-------------------- datasets --------------------
Document 1:

Wikipedia dataset
------------------------------
Document 2:

luke-japanese-base, Wikipedia, 日本語の固有表現抽出データセット, https://github.com/stockmarkteam/ner-wikipedia-dataset
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

https://github.com/stockmarkteam/ner-wikipedia-dataset
-------------------- paper --------------------
Document 1:

"Wikipedia dataset"
------------------------------
Document 2:

"Wikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )"
------------------------------
Document 3:

"title={LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention}, author={Ikuya Yamada and Akari Asai and Hiroyuki Shindo and Hideaki Takeda and Yuji Matsumoto}, booktitle={EMNLP}, year={2020}"
-------------------- upstream_model --------------------
Document 1:

luke-japanese-base, Wikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

||                       precision   |recall   | f1-score  | support|
|---|----|----|----|----|
|その他の組織名     |  0.76     | 0.77     | 0.77     |  238|
|イベント名       　|0.83      |0.90     | 0.87       |215|
|人名      　    |0.88      |0.91    |  0.90      | 546|
|地名           | 0.84    |  0.83      |0.83      | 440|
|政治的組織名       | 0.80      |0.84     | 0.82     |  263|
|施設名          | 0.78     | 0.83     | 0.80      | 241|
|法人名          | 0.88     | 0.90     | 0.89      | 487|
|製品名          | 0
-------------------- hardware --------------------
Document 1:

"luke-japanese-base"
-------------------- limitation_and_bias --------------------
Document 1:

"This model is fine-tuned by using Wikipedia dataset."
------------------------------
Document 2:

"固有表現抽出（NER）タスク"
-------------------- demo --------------------
Document 1:

"This model is fine-tuned by using Wikipedia dataset." "You could use this model for NER tasks."
------------------------------
Document 2:

"Wikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )"
-------------------- input_format --------------------
Document 1:

luke-japanese-base, Wikipedia, 日本語の固有表現抽出データセット, https://github.com/stockmarkteam/ner-wikipedia-dataset, 固有表現抽出（NER）タスク
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Wikipedia'], 'license': 'mit', 'github': 'https://github.com/stockmarkteam/ner-wiki 
pedia-dataset', 'paper': 'title={LUKE: Deep Contextualized Entity Representations with Entity-aware  
Self-attention}, author={Ikuya Yamada and Akari Asai and Hiroyuki Shindo and Hideaki Takeda and Yuji 
 Matsumoto}, booktitle={EMNLP}, year={2020}', 'upstream_model': 'luke-japanese-base, Wikipediaを用いた日本 
語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )', 'parameter_count' 
: 'NO_OUTPUT', 'hyper_parameters': [], 'evaluation': [{'test': 'その他の組織名', 'result': 0.77}, {'test':  
'イベント名', 'result': 0.87}, {'test': '人名', 'result': 0.9}, {'test': '地名', 'result': 0.83}, {'test': '政 
治的組織名', 'result': 0.82}, {'test': '施設名', 'result': 0.8}, {'test': '法人名', 'result': 0.89}, {'test': ' 
製品名', 'result': 0}], 'hardware': 'luke-japanese-base', 'limitation_and_bias': 'This model is fine-tu 
ned by using Wikipedia dataset.', 'demo': 'This model is fine-tuned by using Wikipedia dataset. You  
could use this model for NER tasks.', 'input_format': 'luke-japanese-base, Wikipedia, 日本語の固有表現抽出データセ 
ット, https://github.com/stockmarkteam/ner-wikipedia-dataset, 固有表現抽出（NER）タスク', 'output_format': ''}]   
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e032f-4acb7c761c62f3934a9a086a)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-vit/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0330-3c0883af3c93cc2d14c108a3)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Data2VecAudioModel/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0330-478f628d6d1b41cc706eee36)

Entry Not Found for url: https://huggingface.co/rasa/LaBSE/resolve/main/README.md. 

#####################plasmo/vox2########################

-------------------- datasets --------------------
Document 1:

184 training images through 8000 training steps
-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"184 training images through 8000 training steps"
-------------------- hyper_parameters --------------------
Document 1:

184 training images, 8000 training steps, 20% Training text
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

184 training images through 8000 training steps
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"VERSION 1.2 of Voxel-ish Image Pack brought to you by 184 training images through 8000 training steps, 20% Training text crafted by Jak_TheAI_Artist version history: v1.2 - Fine tuned for better faces. Include Prompt trigger: "voxel-ish" to activate. Tip: add "intricate detail" in prompt to make a semi-realistic image. Sample pictures of this concept:"
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['184 training images through 8000 training steps'], 'license': 'creativeml-openrail- 
m', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count': '184 training images through 
 8000 training steps', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'op 
timizer': ''}, 'evaluation': [], 'hardware': '184 training images through 8000 training steps', 'lim 
itation_and_bias': '', 'demo': 'VERSION 1.2 of Voxel-ish Image Pack brought to you by 184 training i 
mages through 8000 training steps, 20% Training text crafted by Jak_TheAI_Artist version history: v1 
.2 - Fine tuned for better faces. Include Prompt trigger: "voxel-ish" to activate. Tip: add "intrica 
te detail" in prompt to make a semi-realistic image. Sample pictures of this concept:', 'input_forma 
t': '', 'output_format': ''}]                                                                        

#####################microsoft/layoutlmv2-base-uncased########################

-------------------- datasets --------------------
Document 1:

Microsoft Document AI | GitHub
------------------------------
Document 2:

"LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework."
-------------------- license --------------------
Document 1:

license: cc-by-nc-sa-4.0
------------------------------
Document 2:

Microsoft Document AI | [GitHub](https://github.com/microsoft/unilm/tree/master/layoutlmv2)
------------------------------
Document 3:

"LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework." NO_OUTPUT
-------------------- github --------------------
Document 1:

[GitHub](https://github.com/microsoft/unilm/tree/master/layoutlmv2)
------------------------------
Document 2:

"[LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)"
-------------------- paper --------------------
Document 1:

"Microsoft Document AI" | "[GitHub](https://github.com/microsoft/unilm/tree/master/layoutlmv2)"
------------------------------
Document 2:

[LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)
-------------------- upstream_model --------------------
Document 1:

"LayoutLMv2 is an improved version of LayoutLM"
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"The documentation of this model in the Transformers library can be found [here](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)."
------------------------------
Document 2:

"LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework."
-------------------- evaluation --------------------
Document 1:

"LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. It outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including , including FUNSD (0.7895 → 0.8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), Kleister-NDA (0.834 → 0.852), RVL-CDIP (0.9443 → 0.9564), and DocVQA (0.7295 → 0.8672)."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. It outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including , including FUNSD (0.7895 → 0.8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), Kleister-NDA (0.834 → 0.852), RVL-CDIP (0.9443 → 0.9564), and DocVQA (0.7295 → 0.8672)."
-------------------- demo --------------------
Document 1:

[here](https://huggingface.co/docs/transformers/model_doc/layoutlmv2), [Microsoft Document AI](https://www.microsoft.com/en-us/research/project/document-ai/), [GitHub](https://github.com/microsoft/unilm/tree/master/layoutlmv2)
------------------------------
Document 2:

"LayoutLMv2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. It outperforms strong baselines and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including , including FUNSD (0.7895 → 0.8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), Kleister-NDA (0.834 → 0.852), RVL-CDIP (0.9443 → 0.9564), and DocVQA (0.7295 → 0.8672)."
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['Microsoft Document AI'], 'license': 'cc-by-nc-sa-4.0', 'github': 'https://github.co 
m/microsoft/unilm/tree/master/layoutlmv2', 'paper': '[LayoutLMv2: Multi-modal Pre-training for Visua 
lly-Rich Document Understanding](https://arxiv.org/abs/2012.14740)', 'upstream_model': 'LayoutLMv2 i 
s an improved version of LayoutLM', 'parameter_count': 'parameter_count', 'hyper_parameters': {'epoc 
hs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'LayoutLMv 
2 is an improved version of LayoutLM with new pre-training tasks to model the interaction among text 
, layout, and image in a single multi-modal framework. It outperforms strong baselines and achieves  
new state-of-the-art results on a wide variety of downstream visually-rich document understanding ta 
sks, including , including FUNSD (0.7895 → 0.8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), 
 Kleister-NDA (0.834 → 0.852), RVL-CDIP (0.9443 → 0.9564), and DocVQA (0.7295 → 0.8672).', 'result': 
 0}], 'hardware': '', 'limitation_and_bias': 'LayoutLMv2 is an improved version of LayoutLM with new 
 pre-training tasks to model the interaction among text, layout, and image in a single multi-modal f 
ramework. It outperforms strong baselines and achieves new state-of-the-art results on a wide variet 
y of downstream visually-rich document understanding tasks, including , including FUNSD (0.7895 → 0. 
8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), Kleister-NDA (0.834 → 0.852), RVL-CDIP (0.94 
43 → 0.9564), and DocVQA (0.7295 → 0.8672).', 'demo': '[here](https://huggingface.co/docs/transforme 
rs/model_doc/layoutlmv2), [Microsoft Document AI](https://www.microsoft.com/en-us/research/project/d 
ocument-ai/), [GitHub](https://github.com/microsoft/unilm/tree/master/layoutlmv2)', 'input_format':  
'', 'output_format': ''}]                                                                            

#####################microsoft/layoutxlm-base########################

-------------------- datasets --------------------
Document 1:

XFUND dataset.
------------------------------
Document 2:

Microsoft Document AI | GitHub
-------------------- license --------------------
Document 1:

license: cc-by-nc-sa-4.0
------------------------------
Document 2:

Microsoft Document AI | GitHub
-------------------- github --------------------
Document 1:

[GitHub](https://github.com/microsoft/unilm/tree/master/layoutxlm)
-------------------- paper --------------------
Document 1:

"[LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)"
------------------------------
Document 2:

Microsoft Document AI | GitHub
-------------------- upstream_model --------------------
Document 1:

LayoutLMv2
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"Experiment results show that it has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset."
------------------------------
Document 2:

Microsoft Document AI | GitHub
-------------------- demo --------------------
Document 1:

"LayoutXLM is a multimodal pre-trained model for multilingual document understanding" "Experiment results show that it has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset" "[LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)"
------------------------------
Document 2:

[Microsoft Document AI](https://www.microsoft.com/en-us/research/project/document-ai/) | [GitHub](https://github.com/microsoft/unilm/tree/master/layoutxlm)
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['XFUND dataset'], 'license': 'cc-by-nc-sa-4.0', 'github': '[GitHub](https://github.c 
om/microsoft/unilm/tree/master/layoutxlm)', 'paper': '"[LayoutXLM: Multimodal Pre-training for Multi 
lingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)"', 'upstream_model': 
 'LayoutLMv2', 'parameter_count': 'parameter_count', 'hyper_parameters': {}, 'evaluation': [], 'hard 
ware': '', 'limitation_and_bias': '"Experiment results show that it has significantly outperformed t 
he existing SOTA cross-lingual pre-trained models on the XFUND dataset."', 'demo': '"LayoutXLM is a  
multimodal pre-trained model for multilingual document understanding" "Experiment results show that  
it has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND da 
taset" "[LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](h 
ttps://arxiv.org/abs/2104.08836)"', 'input_format': '', 'output_format': ''}]                        

#####################naver-clova-ix/donut-base########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"documentation which includes code examples"
------------------------------
Document 2:

"this repository" https://github.com/clovaai/donut
-------------------- paper --------------------
Document 1:

"OCR-free Document Understanding Transformer" by Geewok et al.
------------------------------
Document 2:

Donut: Document Understanding Transformer without {OCR}
-------------------- upstream_model --------------------
Document 1:

upstream_model: pre-trained-only
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Swin Transformer, BART
-------------------- limitation_and_bias --------------------
Document 1:

"We refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/donut) which includes code examples."
-------------------- demo --------------------
Document 1:

We refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/donut) which includes code examples.
------------------------------
Document 2:

"See the [model hub](https://huggingface.co/models?search=donut) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 3:

"It was introduced in the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewok et al. and first released in [this repository](https://github.com/clovaai/donut)."
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

"documentation" "code examples"

[{'datasets': [], 'license': 'mit', 'github': 'https://github.com/clovaai/donut', 'paper': 'https:/ 
/arxiv.org/abs/2111.15664', 'upstream_model': 'pre-trained-only', 'parameter_count': '', 'hyper_para 
meters': [], 'evaluation': [], 'hardware': 'Swin Transformer, BART', 'limitation_and_bias': 'We refe 
r to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/donut) which inc 
ludes code examples.', 'demo': 'We refer to the [documentation](https://huggingface.co/docs/transfor 
mers/main/en/model_doc/donut) which includes code examples.', 'input_format': '', 'output_format': ' 
'}]                                                                                                  

#####################timm/vit_base_r50_s16_384.orig_in21k_ft_in1k########################

-------------------- datasets --------------------
Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

datasets: - imagenet-1k - imagenet-21k
------------------------------
Document 3:

ImageNet-1k, ImageNet-21k
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"Model Type: Image classification / feature backbone" "Model Stats: Params (M): 99.0 GMACs: 61.3 Activations (M): 81.8 Image size: 384 x 384" "Papers: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2" "Dataset: ImageNet-1k" "Pretrain Dataset: ImageNet-21k" "Original: https://github.com/google-research/vision_transformer"
-------------------- github --------------------
Document 1:

"GitHub repository" and "\url{https://github.com/huggingface/pytorch-image-models}"
------------------------------
Document 2:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 3:

"github" "library_name: timm" "tags:" "- image-classification" "- timm" "datasets:" "- imagenet-1k" "- imagenet-21k"
-------------------- paper --------------------
Document 1:

An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2
------------------------------
Document 2:

"model results"
------------------------------
Document 3:

@article{dosovitskiy2020vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
journal={ICLR},
year={2021}
}
-------------------- upstream_model --------------------
Document 1:

"Model Type: Image classification / feature backbone" "Papers: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2" "Dataset: ImageNet-1k" "Pretrain Dataset: ImageNet-21k" "Original: https://github.com/google-research/vision_transformer"
------------------------------
Document 2:

upstream_model ResNet - Vision Transformer (ViT)
-------------------- parameter_count --------------------
Document 1:

Params (M): 99.0
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
------------------------------
Document 2:

Model Type: Image classification / feature backbone, Model Stats: Params (M): 99.0, GMACs: 61.3, Activations (M): 81.8, Image size: 384 x 384, Papers: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2, Dataset: ImageNet-1k, Pretrain Dataset: ImageNet-21k, Original: https://github.com/google-research/vision_transformer
-------------------- evaluation --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).
------------------------------
Document 2:

Model Type: Image classification / feature backbone, Model Stats: Params (M): 99.0, GMACs: 61.3, Activations (M): 81.8, Image size: 384 x 384, Dataset: ImageNet-1k, Pretrain Dataset: ImageNet-21k
-------------------- hardware --------------------
Document 1:

"ImageNet-1k" and "ImageNet-21k"
-------------------- limitation_and_bias --------------------
Document 1:

Model Type: Image classification / feature backbone, Model Stats: Params (M): 99.0, GMACs: 61.3, Activations (M): 81.8, Image size: 384 x 384, Papers: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2, Dataset: ImageNet-1k, Pretrain Dataset: ImageNet-21k, Original: https://github.com/google-research/vision_transformer
-------------------- demo --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"Image classification / feature backbone", "Params (M): 99.0", "GMACs: 61.3", "Activations (M): 81.8", "Image size: 384 x 384", "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2", "ImageNet-1k", "ImageNet-21k", "https://github.com/google-research/vision_transformer"
-------------------- input_format --------------------
Document 1:

Image size: 384 x 384, Dataset: ImageNet-1k, Pretrain Dataset: ImageNet-21k
-------------------- output_format --------------------
Document 1:

"Image classification / feature backbone", "Image size: 384 x 384", "ImageNet-1k", "ImageNet-21k"
-------------------- input_preprocessing --------------------
Document 1:

"Image size: 384 x 384" "Dataset: ImageNet-1k" "Pretrain Dataset: ImageNet-21k"
-------------------- input_size --------------------
Document 1:

Image size: 384 x 384
-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'e 
valuation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_f 
ormat': '', 'input_preprocessing': '', 'input_size': '', 'num_of_classes_for_classification': '', 't 
rigger_word': ''}]                                                                                   

#####################tsmatz/xlm-roberta-ner-japanese########################

-------------------- datasets --------------------
Document 1:

Datasets 2.6.1
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"license"
------------------------------
Document 3:

See [here](https://github.com/stockmarkteam/ner-wikipedia-dataset) for the license of this dataset.
-------------------- github --------------------
Document 1:

[here](https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb)
------------------------------
Document 2:

"https://github.com/stockmarkteam/ner-wikipedia-dataset"
------------------------------
Document 3:

language:
- ja
license: mit
tags:
- generated_from_trainer
- ner
- bert
metrics:
- f1
widget:
- text: 鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った
- text: 中国では、中国共産党による一党統治が続く
base_model: xlm-roberta-base
model-index:
- name: xlm-roberta-ner-ja
results: []
-------------------- paper --------------------
Document 1:

xlm-roberta-base, NER dataset provided by Stockmark Inc, Japanese Wikipedia articles, license of this dataset, each token is labeled by, Label id, Tag, Tag in Widget, Description, 0, O, (None), others or nothing, 1, PER, PER, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organization, 4, ORG-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7, PRD, PRD, product, 8, EVT, EVT, event
------------------------------
Document 2:

"base_model: xlm-roberta-base"
-------------------- upstream_model --------------------
Document 1:

xlm-roberta-base, RobertaModel, Stockmark Inc, Japanese Wikipedia articles, NER dataset provided by Stockmark Inc
------------------------------
Document 2:

base_model: xlm-roberta-base
-------------------- parameter_count --------------------
Document 1:

parameter_count: 5
-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 5e-05 - train_batch_size: 12 - eval_batch_size: 12 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - num_epochs: 5
------------------------------
Document 2:

Training Loss, Epoch, Step, Validation Loss, F1
-------------------- evaluation --------------------
Document 1:

- f1
- text: 鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った
- text: 中国では、中国共産党による一党統治が続く
- base_model: xlm-roberta-base
- model-index:
- name: xlm-roberta-ner-ja
- results: []
------------------------------
Document 2:

"This model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```) trained for named entity recognition (NER) token classification. The model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles. See [here](https://github.com/stockmarkteam/ner-wikipedia-dataset) for the license of this dataset. Each token is labeled by : | Label id | Tag | Tag in Widget | Description | |---|---|---|---| | 0 | O | (None) | others or nothing | | 1 | PER | PER | person | | 2 | ORG | ORG | general corporation organization | | 3 | ORG-P | P | political organization | | 4 | ORG-O | O | other organization | | 5 | LOC | LOC | location | | 6 | INS | INS | institution, facility | | 7 | PRD | PRD | product | | 8 | EVT | EVT | event |
-------------------- hardware --------------------
Document 1:

"base_model: xlm-roberta-base"
------------------------------
Document 2:

xlm-roberta-base, pre-trained cross-lingual ```RobertaModel```, Stockmark Inc, Japanese Wikipedia articles, NER dataset
-------------------- limitation_and_bias --------------------
Document 1:

This model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```) trained for named entity recognition (NER) token classification. The model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles. See [here](https://github.com/stockmarkteam/ner-wikipedia-dataset) for the license of this dataset. Each token is labeled by : | Label id | Tag | Tag in Widget | Description | |---|---|---|---| | 0 | O | (None) | others or nothing | | 1 | PER | PER | person | | 2 | ORG | ORG | general corporation organization | | 3 | ORG-P | P | political organization | | 4 | ORG-O | O | other organization | | 5 | LOC | LOC | location | | 6 | INS | INS | institution, facility | | 7 | PRD | PRD | product | | 8 | EVT | EVT | event |
------------------------------
Document 2:

language: - ja, license: mit, tags: - generated_from_trainer - ner - bert, metrics: - f1, widget: - text: 鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った - text: 中国では、中国共産党による一党統治が続く, base_model: xlm-roberta-base, model-index: - name: xlm-roberta-ner-ja, results: []
-------------------- demo --------------------
Document 1:

[here](https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb)
------------------------------
Document 2:

language:
- ja
license: mit
tags:
- generated_from_trainer
- ner
- bert
metrics:
- f1
widget:
- text: 鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った
- text: 中国では、中国共産党による一党統治が続く
base_model: xlm-roberta-base
model-index:
- name: xlm-roberta-ner-ja
------------------------------
Document 3:

xlm-roberta-base, RobertaModel, Stockmark Inc, Japanese Wikipedia articles, github.com/stockmarkteam/ner-wikipedia-dataset, Label id, Tag, Tag in Widget, Description, 0, O, (None), others or nothing, 1, PER, PER, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organization, 4, ORG-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7, PRD, PRD, product, 8, EVT, EVT, event
-------------------- input_format --------------------
Document 1:

"This model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```) trained for named entity recognition (NER) token classification."

input_format: pre-trained cross-lingual ```RobertaModel```
-------------------- output_format --------------------
Document 1:

"This model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```) trained for named entity recognition (NER) token classification." "Each token is labeled by :  
| Label id | Tag | Tag in Widget | Description |
|---|---|---|---|
| 0 | O | (None) | others or nothing |
| 1 | PER | PER | person |
| 2 | ORG | ORG | general corporation organization |
| 3 | ORG-P | P | political organization |
| 4 | ORG-O | O | other organization |
| 5 | LOC | LOC | location |
| 6 | INS | INS | institution, facility |
| 7 | PRD | PRD | product |
| 8 | EVT | EVT | event |"
------------------------------
Document 2:

- language:
- ja
- license: mit
- tags:
- generated_from_trainer
- ner
- bert
- metrics:
- f1
- widget:
- text: 鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った
- text: 中国では、中国共産党による一党統治が続く
- base_model: xlm-roberta-base
- model-index:
- name: xlm-roberta-ner-ja
- results: []
-------------------- input_token_limit --------------------
Document 1:

"This model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```)" NO_OUTPUT
------------------------------
Document 2:

language:
- ja
license: mit
tags:
- generated_from_trainer
- ner
- bert
metrics:
- f1
base_model: xlm-roberta-base
-------------------- vocabulary_size --------------------
Document 1:

xlm-roberta-base, RobertaModel, Stockmark Inc, Japanese Wikipedia articles, Label id, Tag, Tag in Widget, Description, 0, O, (None), others or nothing, 1, PER, PER, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organization, 4, ORG-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7, PRD, PRD, product, 8, EVT, EVT, event
------------------------------
Document 2:

- language:
- ja
- tags:
- generated_from_trainer
- ner
- bert
- base_model: xlm-roberta-base

[{'datasets': ['NER dataset provided by Stockmark Inc'], 'license': 'mit', 'github': 'https://githu 
b.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb', 'paper': 'xlm-roberta 
-base, NER dataset provided by Stockmark Inc, Japanese Wikipedia articles, license of this dataset,  
each token is labeled by, Label id, Tag, Tag in Widget, Description, 0, O, (None), others or nothing 
, 1, PER, PER, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organiz 
ation, 4, ORG-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7 
, PRD, PRD, product, 8, EVT, EVT, event', 'upstream_model': 'xlm-roberta-base, RobertaModel, Stockma 
rk Inc, Japanese Wikipedia articles, NER dataset provided by Stockmark Inc', 'parameter_count': '5', 
 'hyper_parameters': [{'learning_rate': '5e-05', 'train_batch_size': '12', 'eval_batch_size': '12',  
'seed': '42', 'optimizer': 'Adam with betas=(0.9,0.999) and epsilon=1e-08', 'lr_scheduler_type': 'li 
near', 'num_epochs': '5'}], 'evaluation': [{'test': 'f1', 'result': 0}], 'hardware': 'base_model: xl 
m-roberta-base', 'limitation_and_bias': 'This model is a fine-tuned version of xlm-roberta-base trai 
ned for named entity recognition (NER) token classification. The model is fine-tuned on NER dataset  
provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles. Each token i 
s labeled by Label id, Tag, Tag in Widget, Description: 0, O, (None), others or nothing, 1, PER, PER 
, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organization, 4, ORG 
-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7, PRD, PRD, p 
roduct, 8, EVT, EVT, event', 'demo': '[here](https://github.com/tsmatz/huggingface-finetune-japanese 
/blob/master/01-named-entity.ipynb)', 'input_format': 'pre-trained cross-lingual ```RobertaModel```' 
, 'output_format': 'This model is a fine-tuned version of xlm-roberta-base trained for named entity  
recognition (NER) token classification. Each token is labeled by: 0, O, (None), others or nothing, 1 
, PER, PER, person, 2, ORG, ORG, general corporation organization, 3, ORG-P, P, political organizati 
on, 4, ORG-O, O, other organization, 5, LOC, LOC, location, 6, INS, INS, institution, facility, 7, P 
RD, PRD, product, 8, EVT, EVT, event'}]                                                              

#####################sentence-transformers/nli-mpnet-base-v2########################

-------------------- datasets --------------------
Document 1:

datasets, Sentence Embeddings Benchmark, [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/nli-mpnet-base-v2)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
------------------------------
Document 2:

"sentence-transformers" and "semantic search"
------------------------------
Document 3:

"Sentence Embeddings Benchmark"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

"upstream_model: sentence-transformers/nli-mpnet-base-v2"
------------------------------
Document 3:

upstream_model: sentence-transformers
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 75, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/nli-mpnet-base-v2)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[sentence-transformers](https://www.SBERT.net)
------------------------------
Document 2:

"Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/nli-mpnet-base-v2)
-------------------- input_format --------------------
Document 1:

input_format: sentence-transformers
------------------------------
Document 2:

'max_seq_length': 75, 'do_lower_case': False
-------------------- output_format --------------------
Document 1:

output_format
-------------------- input_token_limit --------------------
Document 1:

'max_seq_length': 75
-------------------- vocabulary_size --------------------
Document 1:

'max_seq_length': 75, 'do_lower_case': False, 'word_embedding_dimension': 768

[{'datasets': ['Sentence Embeddings Benchmark'], 'license': 'apache-2.0', 'github': '', 'paper': 'S 
entence-BERT: Sentence Embeddings using Siamese BERT-Networks', 'upstream_model': 'sentence-transfor 
mers', 'parameter_count': "'max_seq_length': 75, 'do_lower_case': False, 'word_embedding_dimension': 
 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens':  
False, 'pooling_mode_mean_sqrt_len_tokens': False", 'hyper_parameters': {}, 'evaluation': [{'test':  
'Sentence Embeddings Benchmark', 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
[sentence-transformers](https://www.SBERT.net)', 'input_format': 'sentence-transformers', 'output_fo 
rmat': '', 'input_token_limit': "'max_seq_length': 75", 'vocabulary_size': "'max_seq_length': 75, 'd 
o_lower_case': False, 'word_embedding_dimension': 768"}]                                             

#####################SAPOSS/password-model########################

-------------------- datasets --------------------
Document 1:

"CodeBERT-base-mlm" "dataset for leak detection"
-------------------- license --------------------
Document 1:

TBD
-------------------- github --------------------
Document 1:

"https://github.com/SAP/credential-digger"
------------------------------
Document 2:

TBD
-------------------- paper --------------------
Document 1:

TBD
-------------------- upstream_model --------------------
Document 1:

"SAPOSS/password-model"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

More information needed
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

TBD
-------------------- output_format --------------------
Document 1:

TBD
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['dataset for leak detection'], 'license': 'TBD', 'github': 'https://github.com/SAP/c 
redential-digger', 'paper': 'TBD', 'upstream_model': 'SAPOSS/password-model', 'parameter_count': '', 
 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'More information needed', 'limitation_and_bi 
as': '', 'demo': '', 'input_format': 'TBD', 'output_format': 'TBD', 'input_token_limit': '', 'vocabu 
lary_size': ''}]                                                                                     

#####################pyannote/embedding########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 401 Client Error. (Request ID: Root=1-653e0453-085c0e1b463941530d0e1f3d)

Cannot access gated repo for url https://huggingface.co/api/models/pyannote/embedding.
Repo model pyannote/embedding is gated. You must be authenticated to access it. 

#####################valhalla/t5-base-e2e-qg########################

-------------------- datasets --------------------
Document 1:

datasets: - squad
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"https://github.com/patil-suraj/question_generation"
------------------------------
Document 2:

"You'll need to clone the [repo](https://github.com/patil-suraj/question_generation). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb)"
-------------------- paper --------------------
Document 1:

"https://arxiv.org/abs/1910.10683"
------------------------------
Document 2:

"valhalla/t5-base-e2e-qg"
-------------------- upstream_model --------------------
Document 1:

"valhalla/t5-base-e2e-qg"

upstream_model: valhalla/t5-base-e2e-qg
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"from pipelines import pipeline" and "nlp = pipeline("e2e-qg", model="valhalla/t5-base-e2e-qg")"
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace."
-------------------- demo --------------------
Document 1:

"This is [t5-base](https://arxiv.org/abs/1910.10683) model trained for end-to-end question generation task. Simply input the text and the model will generate multile questions. You can play with the model using the inference API, just put the text and see the results! For more deatils see [this](https://github.com/patil-suraj/question_generation) repo."
------------------------------
Document 2:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb)  
```python3
from pipelines import pipeline

text = "Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum \
and first released in 1991, Python's design philosophy emphasizes code \
readability with its notable use of significant whitespace."

nlp = pipeline("e2e-qg", model="valhalla/t5-base-e2e-qg")
nlp(text)
=> [
'Who created Python?',
'When was Python first released?',
"What is Python's design philosophy?"
]
```
-------------------- input_format --------------------
Document 1:

"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace." input_format: "text"
-------------------- output_format --------------------
Document 1:

"valhalla/t5-base-e2e-qg"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"valhalla/t5-base-e2e-qg"

[{'datasets': ['squad'], 'license': 'mit', 'github': 'https://github.com/patil-suraj/question_gener 
ation', 'paper': 'https://arxiv.org/abs/1910.10683', 'upstream_model': 'valhalla/t5-base-e2e-qg', 'p 
arameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias' 
: "Python is an interpreted, high-level, general-purpose programming language. Created by Guido van  
Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its n 
otable use of significant whitespace.", 'demo': 'This is [t5-base](https://arxiv.org/abs/1910.10683) 
 model trained for end-to-end question generation task. Simply input the text and the model will gen 
erate multile questions. You can play with the model using the inference API, just put the text and  
see the results! For more deatils see [this](https://github.com/patil-suraj/question_generation) rep 
o.', 'input_format': "Python is an interpreted, high-level, general-purpose programming language. Cr 
eated by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code rea 
dability with its notable use of significant whitespace.", 'output_format': 'valhalla/t5-base-e2e-qg 
', 'input_token_limit': '', 'vocabulary_size': 'valhalla/t5-base-e2e-qg'}]                           

#####################sentence-transformers/multi-qa-mpnet-base-cos-v1########################

-------------------- datasets --------------------
Document 1:

WikiAnswers Duplicate question pairs from WikiAnswers, PAQ Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia, Stack Exchange (Title, Body) pairs from all StackExchanges, Stack Exchange (Title, Answer) pairs from all StackExchanges, MS MARCO Triplets (query, answer, hard_negative) for 500k queries from Bing search engine, GOOAQ: Open Question Answering with Diverse Answer Types (query, answer) pairs for 3M Google queries and Google featured snippet, Amazon-QA (Question, Answer) pairs from Amazon product pages, Yahoo Answers (Title, Answer) pairs from Yahoo Answers, Yahoo Answers (Question, Answer) pairs from Yahoo Answers, Yahoo Answers (Title, Question) pairs from Yahoo Answers, SearchQA (Question, Answer) pairs for 140k questions, each with Top5 Google snippets on that question, ELI5 (Question, Answer) pairs from Reddit ELI5 (explainlikeimfive), Stack Exchange Duplicate questions pairs (titles), Quora Question Triplets (Question, Duplicate_Question, Hard_Negative) triplets for Quora Questions Pairs dataset, Natural Questions (NQ) (Question
------------------------------
Document 2:

"Train the Best Sentence Embedding Model Ever with 1B Training Pairs" and "7 TPUs v3-8"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

`train_script.py`
-------------------- paper --------------------
Document 1:

"It has been trained on 215M (question, answer) pairs from diverse sources." NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"This is a [sentence-transformers](https://www.SBERT.net) model" and "It has been trained on 215M (question, answer) pairs from diverse sources."
-------------------- parameter_count --------------------
Document 1:

"214,988,242" parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"Dimensions: 768" 
"Produces normalized embeddings: Yes" 
"Pooling-Method: Mean pooling" 
"Suitable score functions: dot-product (`util.dot_score`), cosine-similarity (`util.cos_sim`), or euclidean distance"
------------------------------
Document 2:

"self-supervised contrastive learning objective" "Train the Best Sentence Embedding Model Ever with 1B Training Pairs" "7 TPUs v3-8" "intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

7 TPUs v3-8
-------------------- limitation_and_bias --------------------
Document 1:

"Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text."
-------------------- demo --------------------
Document 1:

"It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages." Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.
------------------------------
Document 2:

sentence-transformers, semantic search, [SBERT.net - Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)
------------------------------
Document 3:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1. In that case, dot-product and cosine-similarity are equivalent. dot-product is preferred as it is faster. Euclidean distance is proportional to dot-product and can also be used."
-------------------- input_format --------------------
Document 1:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1." input_format: normalized embeddings with length 1
-------------------- output_format --------------------
Document 1:

"When loaded with `sentence-transformers`, this model produces normalized embeddings with length 1."
-------------------- input_token_limit --------------------
Document 1:

Note that there is a limit of 512 word pieces: Text longer than that will be truncated. input_token_limit: 512
-------------------- vocabulary_size --------------------


[{'datasets': ['WikiAnswers', 'PAQ', 'Stack Exchange', 'MS MARCO', 'GOOAQ', 'Amazon-QA', 'Yahoo Ans 
wers', 'SearchQA', 'ELI5', 'Quora Question Triplets', 'Natural Questions'], 'license': '', 'github': 
 'train_script.py', 'paper': '', 'upstream_model': 'sentence-transformers', 'parameter_count': '214, 
988,242', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''} 
, 'evaluation': [], 'hardware': '7 TPUs v3-8', 'limitation_and_bias': 'Note that there is a limit of 
 512 word pieces: Text longer than that will be truncated. Further note that the model was just trai 
ned on input text up to 250 word pieces. It might not work well for longer text.', 'demo': 'It encod 
es queries / questions and text paragraphs in a dense vector space. It finds relevant documents for  
the given passages.', 'input_format': 'When loaded with `sentence-transformers`, this model produces 
 normalized embeddings with length 1.', 'output_format': 'When loaded with `sentence-transformers`,  
this model produces normalized embeddings with length 1.', 'input_token_limit': '512', 'vocabulary_s 
ize': ''}]                                                                                           

#####################facebook/opt-1.3b########################

-------------------- datasets --------------------
Document 1:

- BookCorpus
- CC-Stories
- Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews
- Pushshift.io Reddit dataset
- CCNewsV2
------------------------------
Document 2:

"The dataset was collected form internet"
-------------------- license --------------------
Document 1:

license: other
-------------------- github --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- paper --------------------
Document 1:

"the [official paper](https://arxiv.org/abs/2205.01068)"
-------------------- upstream_model --------------------
Document 1:

"facebook/opt-1.3b"
-------------------- parameter_count --------------------
Document 1:

GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days
------------------------------
Document 2:

"125M to 175B parameters"
-------------------- hyper_parameters --------------------
Document 1:

"BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit dataset, CCNewsV2" NO_OUTPUT
------------------------------
Document 2:

"Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety."
"Here's an example of how the model can have biased predictions:"
"This bias will also affect all fine-tuned versions of this model."
-------------------- evaluation --------------------
Document 1:

- BookCorpus, which consists of more than 10K unpublished books,
- CC-Stories, which contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas,
- The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included.
- Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in
Roller et al. (2021)
- CCNewsV2 containing an updated version of the English portion of the CommonCrawl News
dataset that was used in RoBERTa (Liu et al., 2019b)
- The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally
to each dataset’s size in the pretraining corpus.
-------------------- hardware --------------------
Document 1:

GPT2, 80GB A100 GPUs, ~33 days
------------------------------
Document 2:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 3:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- limitation_and_bias --------------------
Document 1:

As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased :  Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. Here's an example of how the model can have biased predictions:  compared to:  This bias will also affect all fine-tuned versions of this model.
------------------------------
Document 2:

"known challenges in areas such as robustness, bias, and toxicity"
-------------------- demo --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- input_format --------------------
Document 1:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 2:

GPT2 byte-level version of Byte Pair Encoding (BPE), vocabulary size of 50272, inputs are sequences of 2048 consecutive tokens. input_format
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*
-------------------- vocabulary_size --------------------
Document 1:

"180B tokens corresponding to 800GB of data" NO_OUTPUT
------------------------------
Document 2:

GPT2, vocabulary size of 50272
------------------------------
Document 3:

"125M to 175B parameters"

[{'datasets': ['BookCorpus', 'CC-Stories', 'Pile-CC', 'OpenWebText2', 'USPTO', 'Project Gutenberg', 
 'OpenSubtitles', 'Wikipedia', 'DM Mathematics', 'HackerNews', 'Pushshift.io Reddit dataset', 'CCNew 
sV2'], 'license': 'other', 'github': 'We present Open Pretrained Transformers (OPT), a suite of deco 
der-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and re 
sponsibly share with interested researchers.', 'paper': 'the [official paper](https://arxiv.org/abs/ 
2205.01068)', 'upstream_model': 'facebook/opt-1.3b', 'parameter_count': 'GPT2, 50272, 2048, 175B, 99 
2, 80GB A100 GPUs, 33 days', 'hyper_parameters': [{'epochs': 'BookCorpus, CC-Stories, The Pile, Push 
shift.io Reddit dataset, CCNewsV2', 'batch_size': 'NO_OUTPUT', 'learning_rate': 'NO_OUTPUT', 'optimi 
zer': 'NO_OUTPUT'}], 'evaluation': [{'test': 'BookCorpus, which consists of more than 10K unpublishe 
d books', 'result': 0}, {'test': 'CC-Stories, which contains a subset of CommonCrawl data filtered t 
o match the story-like style of Winograd schemas', 'result': 0}, {'test': 'The Pile, from which * Pi 
le-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNe 
ws* were included', 'result': 0}, {'test': 'Pushshift.io Reddit dataset that was developed in Baumga 
rtner et al. (2020) and processed in Roller et al. (2021)', 'result': 0}, {'test': 'CCNewsV2 contain 
ing an updated version of the English portion of the CommonCrawl News dataset that was used in RoBER 
Ta (Liu et al., 2019b)', 'result': 0}, {'test': 'The final training data contains 180B tokens corres 
ponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled pr 
oportionally to each dataset’s size in the pretraining corpus.', 'result': 0}], 'hardware': 'GPT2, 8 
0GB A100 GPUs, ~33 days', 'limitation_and_bias': "As mentioned in Meta AI's model card, given that t 
he training data used for this model contains a lot of unfiltered content from the internet, which i 
s far from neutral the model is strongly biased :  Like other large language models for which the di 
versity (or lack thereof) of training data induces downstream impact on the quality of our model, OP 
T-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms o 
f generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of is 
sues that plague modern large language models. Here's an example of how the model can have biased pr 
edictions:  compared to:  This bias will also affect all fine-tuned versions of this model.", 'demo' 
: 'We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers r 
anging from 125M to 175B parameters, which we aim to fully and responsibly share with interested res 
earchers.', 'input_format': 'BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg 
, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, Commo 
nCrawl News dataset', 'output_format': '', 'input_token_limit': 'GPT2, Byte Pair Encoding (BPE), 502 
72, 2048, 175B, 992 *80GB A100 GPUs*', 'vocabulary_size': '180B tokens corresponding to 800GB of dat 
a'}, {'datasets': ['BookCorpus', 'CC-Stories', 'Pile-CC', 'OpenWebText2', 'USPTO', 'Project Gutenber 
g', 'OpenSubtitles', 'Wikipedia', 'DM Mathematics', 'HackerNews', 'Pushshift.io Reddit dataset', 'CC 
NewsV2', 'CommonCrawl News dataset'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '' 
, 'parameter_count': '125M to 175B parameters', 'hyper_parameters': [{'epochs': '', 'batch_size': '' 
, 'learning_rate': '', 'optimizer': ''}], 'evaluation': [], 'hardware': 'BookCorpus, CC-Stories, Pil 
e-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews,  
Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset', 'limitation_and_bias': 'known chal 
lenges in areas such as robustness, bias, and toxicity', 'demo': 'We present Open Pretrained Transfo 
rmers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters,  
which we aim to fully and responsibly share with interested researchers.', 'input_format': 'GPT2 byt 
e-level version of Byte Pair Encoding (BPE), vocabulary size of 50272, inputs are sequences of 2048  
consecutive tokens. input_format', 'output_format': '', 'input_token_limit': '', 'vocabulary_size':  
'GPT2, vocabulary size of 50272'}, {'datasets': [], 'license': '', 'github': 'We present Open Pretra 
ined Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B  
parameters, which we aim to fully and responsibly share with interested researchers.', 'paper': '',  
'upstream_model': '', 'parameter_count': '125M to 175B parameters', 'hyper_parameters': [{'epochs':  
'', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [], 'hardware': '', 'lim 
itation_and_bias': '', 'demo': 'We present Open Pretrained Transformers (OPT), a suite of decoder-on 
ly pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsi 
bly share with interested researchers.', 'input_format': '', 'output_format': '', 'input_token_limit 
': '', 'vocabulary_size': ''}]                                                                       

#####################sentence-transformers/paraphrase-mpnet-base-v2########################

-------------------- datasets --------------------
Document 1:

datasets, Sentence Embeddings Benchmark, [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-mpnet-base-v2)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-mpnet-base-v2)
-------------------- paper --------------------
Document 1:

"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
------------------------------
Document 2:

"sentence-transformers" and "semantic search"
------------------------------
Document 3:

"Sentence Embeddings Benchmark"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

upstream_model: sentence-transformers/paraphrase-mpnet-base-v2
------------------------------
Document 3:

upstream_model: sentence-transformers
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-mpnet-base-v2)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

sentence-transformers, https://www.SBERT.net
------------------------------
Document 2:

"Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-mpnet-base-v2)
-------------------- input_format --------------------
Document 1:

input_format: sentence-transformers
------------------------------
Document 2:

'max_seq_length': 512, 'do_lower_case': False
-------------------- output_format --------------------
Document 1:

output_format
-------------------- input_token_limit --------------------
Document 1:

'max_seq_length': 512
-------------------- vocabulary_size --------------------
Document 1:

'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 768

[{'datasets': ['Sentence Embeddings Benchmark'], 'license': 'apache-2.0', 'github': 'https://seb.sb 
ert.net?model_name=sentence-transformers/paraphrase-mpnet-base-v2', 'paper': 'Sentence-BERT: Sentenc 
e Embeddings using Siamese BERT-Networks', 'upstream_model': 'sentence-transformers', 'parameter_cou 
nt': "'max_seq_length': 512, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_ 
cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode 
_mean_sqrt_len_tokens': False", 'hyper_parameters': {}, 'evaluation': [{'test': 'Sentence Embeddings 
 Benchmark', 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 'demo': 'sentence-transformer 
s, https://www.SBERT.net', 'input_format': 'sentence-transformers', 'output_format': '', 'input_toke 
n_limit': "'max_seq_length': 512", 'vocabulary_size': "'max_seq_length': 512, 'do_lower_case': False 
, 'word_embedding_dimension': 768"}]                                                                 

#####################fabiochiu/t5-base-tag-generation########################

-------------------- datasets --------------------
Document 1:

datasets, 50000 articles, 1000 random articles
------------------------------
Document 2:

[t5-base](https://huggingface.co/t5-base), [190k Medium Articles](https://www.kaggle.com/datasets/fabiochiusano/medium-articles)
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

license NO_OUTPUT
-------------------- github --------------------
Document 1:

[t5-base](https://huggingface.co/t5-base), [190k Medium Articles](https://www.kaggle.com/datasets/fabiochiusano/medium-articles), [text2tags](https://huggingface.co/efederici/text2tags)
-------------------- paper --------------------
Document 1:

"text2tags"
-------------------- upstream_model --------------------
Document 1:

t5-base, text2text generation task, text2tags
-------------------- parameter_count --------------------
Document 1:

parameter_count: 8
------------------------------
Document 2:

t5-base, 190k Medium Articles, text2text generation task, text2tags
-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 4e-05 - train_batch_size: 8 - eval_batch_size: 8 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - num_epochs: 1 - mixed_precision_training: Native AMP
------------------------------
Document 2:

t5-base, 190k Medium Articles, multi-label classification problem, text2text generation task, text2tags
-------------------- evaluation --------------------
Document 1:

"The model has been trained on a single epoch spanning about 50000 articles, evaluating on 1000 random articles not used during training."
------------------------------
Document 2:

t5-base, 190k Medium Articles, multi-label classification problem, text2text generation task, text2tags
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias: trained on a single epoch spanning about 50000 articles, evaluating on 1000 random articles not used during training.
------------------------------
Document 2:

The dataset is composed of Medium articles and their tags. However, each Medium article can have at most five tags, therefore the author needs to choose what he/she believes are the best tags (mainly for SEO-related purposes). This means that an article with the "Python" tag may have not the "Programming Languages" tag, even though the first implies the latter. To clean the dataset accounting for this problem, a hand-made taxonomy of about 1000 tags was built. Using the taxonomy, the tags of each articles have been augmented (e.g. an article with the "Python" tag will have the "Programming Languages" tag as well, as the taxonomy says that "Python" is part of "Programming Languages").
------------------------------
Document 3:

t5-base, 190k Medium Articles, multi-label classification problem, text2text generation task, text2tags
-------------------- demo --------------------
Document 1:

license: apache-2.0, tags: - generated_from_trainer, widget: - text: Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected., example_title: Programming, model-index: - name: t5-base-tag-generation, results: []
------------------------------
Document 2:

[t5-base](https://huggingface.co/t5-base), [190k Medium Articles](https://www.kaggle.com/datasets/fabiochiusano/medium-articles), text2text generation task (inspiration from [text2tags](https://huggingface.co/efederici/text2tags))
-------------------- input_format --------------------
Document 1:

t5-base, 190k Medium Articles, article textual content as input, multi-label classification problem, text2text generation task
------------------------------
Document 2:

"The dataset is composed of Medium articles and their tags." "Using the taxonomy, the tags of each articles have been augmented." "The taxonomy is not public, if you are interested in it please send an email at chiusanofabio94@gmail.com." input_format: Medium articles and their tags
-------------------- output_format --------------------
Document 1:

t5-base, 190k Medium Articles, multi-label classification problem, text2text generation task, text2tags
------------------------------
Document 2:

license: apache-2.0, tags: - generated_from_trainer, widget: - text: Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected., example_title: Programming, model-index: - name: t5-base-tag-generation, results: []
-------------------- input_token_limit --------------------
Document 1:

t5-base, text2text generation task, text2tags
-------------------- vocabulary_size --------------------
Document 1:

t5-base, 190k Medium Articles, text2text generation task

[{'datasets': ['50000 articles', '1000 random articles'], 'license': 'apache-2.0', 'github': '[t5-b 
ase](https://huggingface.co/t5-base), [190k Medium Articles](https://www.kaggle.com/datasets/fabioch 
iusano/medium-articles), [text2tags](https://huggingface.co/efederici/text2tags)', 'paper': '"text2t 
ags"', 'upstream_model': 't5-base, text2text generation task, text2tags', 'parameter_count': '8', 'h 
yper_parameters': {'epochs': '1', 'batch_size': '8', 'learning_rate': '4e-05', 'optimizer': 'Adam wi 
th betas=(0.9,0.999) and epsilon=1e-08'}, 'evaluation': [{'test': 'The model has been trained on a s 
ingle epoch spanning about 50000 articles, evaluating on 1000 random articles not used during traini 
ng.', 'result': 0}], 'hardware': '', 'limitation_and_bias': 'trained on a single epoch spanning abou 
t 50000 articles, evaluating on 1000 random articles not used during training.', 'demo': 'license: a 
pache-2.0, tags: - generated_from_trainer, widget: - text: Python is a high-level, interpreted, gene 
ral-purpose programming language. Its design philosophy emphasizes code readability with the use of  
significant indentation. Python is dynamically-typed and garbage-collected., example_title: Programm 
ing, model-index: - name: t5-base-tag-generation, results: []', 'input_format': 't5-base, 190k Mediu 
m Articles, article textual content as input, multi-label classification problem, text2text generati 
on task', 'output_format': 't5-base, 190k Medium Articles, multi-label classification problem, text2 
text generation task, text2tags', 'input_token_limit': '', 'vocabulary_size': ''}]                   

#####################timm/vit_small_patch14_dinov2.lvd142m########################

-------------------- datasets --------------------
Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

datasets LVD-142M
------------------------------
Document 3:

"Pretrain Dataset: LVD-142M"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

`GitHub repository`, `\url{https://github.com/huggingface/pytorch-image-models}`
------------------------------
Document 2:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"
-------------------- paper --------------------
Document 1:

"model results"
------------------------------
Document 2:

"self-supervised DINOv2 method"
------------------------------
Document 3:

`@misc{rw2019timm, author = {Ross Wightman}, title = {PyTorch Image Models}, year = {2019}, publisher = {GitHub}, journal = {GitHub repository}, doi = {10.5281/zenodo.4414861}, howpublished = {\url{https://github.com/huggingface/pytorch-image-models}}}`
-------------------- upstream_model --------------------
Document 1:

upstream_model LVD-142M DINOv2
------------------------------
Document 2:

"DINOv2: Learning Robust Visual Features without Supervision: https://arxiv.org/abs/2304.07193" "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2" "Original: https://github.com/facebookresearch/dinov2"
-------------------- parameter_count --------------------
Document 1:

"Params (M): 22.1"
NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
------------------------------
Document 2:

"Model Type: Image classification / feature backbone" "Model Stats: Params (M): 22.1 GMACs: 46.8 Activations (M): 198.8 Image size: 518 x 518" "Papers: DINOv2: Learning Robust Visual Features without Supervision: https://arxiv.org/abs/2304.07193 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2 Original: https://github.com/facebookresearch/dinov2 Pretrain Dataset: LVD-142M"
------------------------------
Document 3:

"Vision Transformer (ViT)", "LVD-142M", "self-supervised DINOv2 method"
-------------------- evaluation --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).
------------------------------
Document 2:

"A Vision Transformer (ViT) image feature model. Pretrained on LVD-142M with self-supervised DINOv2 method."
------------------------------
Document 3:

Model Type: Image classification / feature backbone, Model Stats: Params (M): 22.1, GMACs: 46.8, Activations (M): 198.8, Image size: 518 x 518, Papers: DINOv2: Learning Robust Visual Features without Supervision: https://arxiv.org/abs/2304.07193, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2, Original: https://github.com/facebookresearch/dinov2, Pretrain Dataset: LVD-142M
-------------------- hardware --------------------
Document 1:

LVD-142M
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias
-------------------- demo --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"A Vision Transformer (ViT) image feature model."
-------------------- input_format --------------------
Document 1:

"Image size: 518 x 518" "Pretrain Dataset: LVD-142M"
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"A Vision Transformer (ViT) image feature model." "Pretrained on LVD-142M with self-supervised DINOv2 method."
------------------------------
Document 2:

"Image size: 518 x 518" NO_OUTPUT
-------------------- input_size --------------------
Document 1:

"Image size: 518 x 518"

input_size: 518 x 518
-------------------- num_of_classes_for_classification --------------------
Document 1:

num_of_classes_for_classification
-------------------- trigger_word --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
', 'input_format': '', 'output_format': '', 'input_preprocessing': '', 'input_size': '', 'num_of_cla 
sses_for_classification': '', 'trigger_word': ''}]                                                   

#####################flair/ner-german########################

-------------------- datasets --------------------
Document 1:

"Contextual String Embeddings for Sequence Labeling"
------------------------------
Document 2:

CONLL_03_GERMAN()
------------------------------
Document 3:

datasets: - conll2003
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

[here](https://github.com/flairNLP/flair/issues/)
------------------------------
Document 2:

"github"
------------------------------
Document 3:

"https://github.com/flairNLP/flair/", "Flair embeddings https://www.aclweb.org/anthology/C18-1139/"
-------------------- paper --------------------
Document 1:

"Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}"
------------------------------
Document 2:

"Flair embeddings" and "LSTM-CRF"
-------------------- upstream_model --------------------
Document 1:

"upstream_model"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- parameter_count --------------------
Document 1:

"SequenceTagger(hidden_size=256,"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
------------------------------
Document 3:

"parameter_count"
-------------------- hyper_parameters --------------------
Document 1:

hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, train_with_dev=True, max_epochs=150
------------------------------
Document 2:

F1-Score: **87,94**
-------------------- evaluation --------------------
Document 1:

F1-Score: **87,94** (CoNLL-03 German revised) | **tag** | **meaning** | PER | person name | LOC | location name | ORG | organization name | MISC | other name | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.
-------------------- hardware --------------------
Document 1:

WordEmbeddings('de'), FlairEmbeddings('de-forward'), FlairEmbeddings('de-backward')
-------------------- limitation_and_bias --------------------
Document 1:

"F1-Score: 87,94 (CoNLL-03 German revised)"
-------------------- demo --------------------
Document 1:

"Find a form of demo"
------------------------------
Document 2:

[here](https://github.com/flairNLP/flair/issues/)
-------------------- input_format --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF." input_format
------------------------------
Document 2:

CONLL_03_GERMAN()
-------------------- output_format --------------------
Document 1:

"4-class NER model for German", "F1-Score: 87,94 (CoNLL-03 German revised)", "Predicts 4 tags: PER, LOC, ORG, MISC", "Flair embeddings", "LSTM-CRF", "output_format"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

[{'datasets': ['conll2003'], 'license': 'license', 'github': 'https://github.com/flairNLP/flair/',  
'paper': 'Please cite the following paper when using this model. @inproceedings{akbik2018coling, tit 
le={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and  
Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Lingui 
stics}, pages     = {1638--1649}, year      = {2018}', 'upstream_model': 'upstream_model', 'paramete 
r_count': 'SequenceTagger(hidden_size=256,', 'hyper_parameters': {'epochs': '150', 'batch_size': '', 
 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'F1-Score: **87,94** (CoNLL-03 Germa 
n revised) | **tag** | **meaning** | PER | person name | LOC | location name | ORG | organization na 
me | MISC | other name | Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and 
 LSTM-CRF.'}], 'hardware': "WordEmbeddings('de'), FlairEmbeddings('de-forward'), FlairEmbeddings('de 
-backward')", 'limitation_and_bias': 'F1-Score: 87,94 (CoNLL-03 German revised)', 'demo': 'Find a fo 
rm of demo', 'input_format': 'Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/ 
) and LSTM-CRF.', 'output_format': '4-class NER model for German'}]                                  

#####################xlm-roberta-large-finetuned-conll03-english########################

-------------------- datasets --------------------
Document 1:

- [XLM-RoBERTa-large model card](https://huggingface.co/xlm-roberta-large) - [CoNLL-2003 data card](https://huggingface.co/datasets/conll2003)
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"associated paper" "https://arxiv.org/pdf/1911.02116.pdf"
------------------------------
Document 2:

"associated paper"
------------------------------
Document 3:

8. [Citation](#citation)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

5. [Evaluation](#evaluation)
------------------------------
Document 2:

"See the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for evaluation details."
-------------------- hardware --------------------
Document 1:

"7. [Technical Specifications](#technical-specifications)"
-------------------- limitation_and_bias --------------------
Document 1:

"risks, biases and limitations of the model"
------------------------------
Document 2:

3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)
------------------------------
Document 3:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). In the context of tasks relevant to this model, [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf) explore social biases in NER systems for English and find that there is systematic bias in existing NER systems in that they fail to identify named entities from different demographic groups (though this paper did not look at BERT).
-------------------- demo --------------------
Document 1:

"The model is a language model." "The model can be used for token classification, a natural language understanding task in which a label is assigned to some tokens in a text."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['CoNLL-2003'], 'license': '', 'github': '', 'paper': 'https://arxiv.org/pdf/1911.021 
16.pdf', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'har 
dware': '', 'limitation_and_bias': 'risks, biases and limitations of the model', 'demo': 'The model  
is a language model. The model can be used for token classification, a natural language understandin 
g task in which a label is assigned to some tokens in a text.', 'input_format': '', 'output_format': 
 '', 'input_token_limit': '', 'vocabulary_size': ''}]                                                

#####################stabilityai/stable-diffusion-x4-upscaler########################

-------------------- datasets --------------------
Document 1:

LAION-2B(en) (https://laion.ai/blog/laion-5b/)
------------------------------
Document 2:

datasets and COCO2017 validation set
-------------------- license --------------------
Document 1:

license: openrail++
-------------------- github --------------------
Document 1:

LAION-2B(en) https://laion.ai/blog/laion-5b/
------------------------------
Document 2:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps" NO_OUTPUT
-------------------- paper --------------------
Document 1:

"Research on generative models."
------------------------------
Document 2:

High-Resolution Image Synthesis With Latent Diffusion Models
------------------------------
Document 3:

"Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
-------------------- upstream_model --------------------
Document 1:

latent upscaling diffusion model, noise_level, predefined diffusion schedule, x4-upscaler-ema.ckpt
-------------------- parameter_count --------------------
Document 1:

parameter_count 50
------------------------------
Document 2:

32 x 8 x A100 GPUs, AdamW, Gradient Accumulations: 1, Batch: 32 x 8 x 2 x 4 = 2048, Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- hyper_parameters --------------------
Document 1:

"50 steps DDIM sampling steps" "Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution."
------------------------------
Document 2:

- relative downsampling factor of 8
- autoencoder maps images of shape H x W x 3 to latents of shape H/f x W/f x 4
- OpenCLIP-ViT/H text-encoder
- UNet backbone of the latent diffusion model via cross-attention
- loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet
- v-objective
- AdamW
- Gradient Accumulations: 1
- Batch: 32 x 8 x 2 x 4 = 2048
- Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
------------------------------
Document 3:

`noise_level`, `configs/stable-diffusion/x4-upscaling.yaml`
-------------------- evaluation --------------------
Document 1:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 2:

- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- hardware --------------------
Document 1:

The model was trained mainly with English captions and will not work as well in other languages.
The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
------------------------------
Document 2:

- **Hardware:** 32 x 8 x A100 GPUs
------------------------------
Document 3:

- **Hardware Type:** A100 PCIe 40GB
- **Hours used:** 200000
- **Cloud Provider:** AWS
- **Compute Region:** US-east
-------------------- limitation_and_bias --------------------
Document 1:

"Stable Diffusion vw was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent."
------------------------------
Document 2:

- Probing and understanding the limitations and biases of generative models.
------------------------------
Document 3:

- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- demo --------------------
Document 1:

"Generation of artworks and use in design and other artistic processes."
------------------------------
Document 2:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 3:

"Stable Diffusion vw was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/)" "Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for." "Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent."
-------------------- input_format --------------------
Document 1:

LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant

input_format: LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant
------------------------------
Document 2:

input_format: 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.
-------------------- output_format --------------------
Document 1:

output_format: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.

[{'datasets': ['LAION-2B(en)'], 'license': 'openrail++', 'github': 'https://laion.ai/blog/laion-5b/ 
', 'paper': 'Research on generative models.', 'upstream_model': 'latent upscaling diffusion model, n 
oise_level, predefined diffusion schedule, x4-upscaler-ema.ckpt', 'parameter_count': '50', 'hyper_pa 
rameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [ 
{'test': 'Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0,  
7.0, 8.0) and 50 steps DDIM sampling steps', 'result': 0}], 'hardware': '', 'limitation_and_bias': ' 
Stable Diffusion vw was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5 
b/), which consists of images that are limited to English descriptions. Texts and images from commun 
ities and cultures that use other languages are likely to be insufficiently accounted for. This affe 
cts the overall output of the model, as white and western cultures are often set as the default. Fur 
ther, the ability of the model to generate content with non-English prompts is significantly worse t 
han with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degr 
ee that viewer discretion must be advised irrespective of the input or its intent.', 'demo': 'Genera 
tion of artworks and use in design and other artistic processes.', 'input_format': 'LAION-5B and sub 
sets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent di 
ffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps a 
nd then kept constant', 'output_format': '![pareto](model-variants.jpg) Evaluated using 50 DDIM step 
s and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.'}]     

#####################google/mobilebert-uncased########################

-------------------- datasets --------------------
Document 1:

[uncased_L-24_H-128_B-512_A-4_F-4_OPT](https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT.tar.gz)
------------------------------
Document 2:

google/mobilebert-uncased, google/mobilebert-uncased, fill_mask.tokenizer.mask_token
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[uncased_L-24_H-128_B-512_A-4_F-4_OPT](https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT.tar.gz)
------------------------------
Document 2:

google/mobilebert-uncased, fill_mask.tokenizer.mask_token
-------------------- paper --------------------
Document 1:

google/mobilebert-uncased
-------------------- upstream_model --------------------
Document 1:

BERT_LARGE
------------------------------
Document 2:

"google/mobilebert-uncased"
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

"model="google/mobilebert-uncased", tokenizer="google/mobilebert-uncased""
-------------------- hyper_parameters --------------------
Document 1:

uncased_L-24_H-128_B-512_A-4_F-4_OPT
------------------------------
Document 2:

"model="google/mobilebert-uncased", tokenizer="google/mobilebert-uncased""
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

google/mobilebert-uncased, fill_mask.tokenizer.mask_token
-------------------- limitation_and_bias --------------------
Document 1:

"google/mobilebert-uncased", "fill-mask", "fill_mask.tokenizer.mask_token"
-------------------- demo --------------------
Document 1:

[uncased_L-24_H-128_B-512_A-4_F-4_OPT](https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT.tar.gz)
------------------------------
Document 2:

`"fill-mask", model="google/mobilebert-uncased", tokenizer="google/mobilebert-uncased"`
-------------------- input_format --------------------
Document 1:

"google/mobilebert-uncased", "google/mobilebert-uncased", "fill_mask.tokenizer.mask_token"
-------------------- output_format --------------------
Document 1:

"google/mobilebert-uncased", "fill-mask.tokenizer.mask_token"

[{'datasets': ['uncased_L-24_H-128_B-512_A-4_F-4_OPT'], 'license': 'apache-2.0', 'github': 'uncased 
_L-24_H-128_B-512_A-4_F-4_OPT', 'paper': 'google/mobilebert-uncased', 'upstream_model': 'BERT_LARGE' 
, 'parameter_count': 'parameter_count', 'hyper_parameters': {'epochs': 'uncased_L-24_H-128_B-512_A-4 
_F-4_OPT', 'batch_size': 'uncased_L-24_H-128_B-512_A-4_F-4_OPT', 'learning_rate': 'uncased_L-24_H-12 
8_B-512_A-4_F-4_OPT', 'optimizer': 'uncased_L-24_H-128_B-512_A-4_F-4_OPT'}, 'evaluation': [], 'hardw 
are': 'google/mobilebert-uncased, fill_mask.tokenizer.mask_token', 'limitation_and_bias': '"google/m 
obilebert-uncased", "fill-mask", "fill_mask.tokenizer.mask_token"', 'demo': '[uncased_L-24_H-128_B-5 
12_A-4_F-4_OPT](https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/uncased_L-24_H-128_B 
-512_A-4_F-4_OPT.tar.gz)', 'input_format': '"google/mobilebert-uncased", "google/mobilebert-uncased" 
, "fill_mask.tokenizer.mask_token"', 'output_format': '"google/mobilebert-uncased", "fill-mask.token 
izer.mask_token"'}]                                                                                  

#####################nvidia/mit-b0########################

-------------------- datasets --------------------
Document 1:

- imagenet_1k
-------------------- license --------------------
Document 1:

"The license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE)."
-------------------- github --------------------
Document 1:

[here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE)
------------------------------
Document 2:

"this repository": https://github.com/NVlabs/SegFormer
-------------------- paper --------------------
Document 1:

[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203)
------------------------------
Document 2:

"title = {SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers}"
-------------------- upstream_model --------------------
Document 1:

upstream_model: ImageNet-1k
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?other=segformer"
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

"return_tensors="pt""
-------------------- input_preprocessing --------------------
Document 1:

"SegFormer encoder fine-tuned on Imagenet-1k."
------------------------------
Document 2:

"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/mit-b0\")\nmodel = SegformerForImageClassification.from_pretrained(\"nvidia/mit-b0\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")"
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------
Document 1:

"1000 ImageNet classes"
-------------------- trigger_word --------------------


{'datasets': ['imagenet_1k'], 'license': 'The license for this model can be found [here](https://gi 
thub.com/NVlabs/SegFormer/blob/master/LICENSE).', 'github': '[here](https://github.com/NVlabs/SegFor 
mer/blob/master/LICENSE)', 'paper': '[SegFormer: Simple and Efficient Design for Semantic Segmentati 
on with Transformers](https://arxiv.org/abs/2105.15203)', 'upstream_model': 'ImageNet-1k', 'paramete 
r_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', ' 
demo': '"model hub" "https://huggingface.co/models?other=segformer"', 'input_format': '', 'output_fo 
rmat': '"return_tensors="pt""', 'input_preprocessing': '"SegFormer encoder fine-tuned on Imagenet-1k 
."', 'input_size': '', 'num_of_classes_for_classification': '"1000 ImageNet classes"', 'trigger_word 
': ''}                                                                                               

#####################Helsinki-NLP/opus-mt-ar-en########################

-------------------- datasets --------------------
Document 1:

dataset: opus, download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.zip)
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

OPUS readme: [ar-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/ar-en/README.md)
-------------------- github --------------------
Document 1:

* OPUS readme: [ar-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/ar-en/README.md) 
* download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.zip)
* test set translations: [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.test.txt)
* test set scores: [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.eval.txt)
-------------------- paper --------------------
Document 1:

"model: transformer-align"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

opus-2019-12-18.eval.txt
------------------------------
Document 2:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| Tatoeba.ar.en 	| 49.4 	| 0.661 |
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

*OPUS readme: [ar-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/ar-en/README.md)*; *download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.zip)*; *test set translations: [opus-2019-12-18.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.test.txt)*; *test set scores: [opus-2019-12-18.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.eval.txt)*
-------------------- input_format --------------------
Document 1:

SentencePiece
-------------------- output_format --------------------
Document 1:

SentencePiece
-------------------- input_token_limit --------------------
Document 1:

SentencePiece
-------------------- vocabulary_size --------------------
Document 1:

SentencePiece

[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': 'https://github.com/Helsinki-NLP/OPUS-MT 
-train/blob/master/models/ar-en/README.md', 'paper': '', 'upstream_model': '', 'parameter_count': '' 
, 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '*OPU 
S readme: [ar-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/ar-en/README.md)* 
; *download original weights: [opus-2019-12-18.zip](https://object.pouta.csc.fi/OPUS-MT-models/ar-en 
/opus-2019-12-18.zip)*; *test set translations: [opus-2019-12-18.test.txt](https://object.pouta.csc. 
fi/OPUS-MT-models/ar-en/opus-2019-12-18.test.txt)*; *test set scores: [opus-2019-12-18.eval.txt](htt 
ps://object.pouta.csc.fi/OPUS-MT-models/ar-en/opus-2019-12-18.eval.txt)*', 'input_format': 'Sentence 
Piece', 'output_format': 'SentencePiece', 'input_token_limit': 'SentencePiece', 'vocabulary_size': ' 
SentencePiece'}]                                                                                     
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e063c-7acd297b096e082e5fd224ee)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-VisionEncoderDecoderModel-vit-gpt2/resolve/main/README.md. 

#####################bhadresh-savani/bert-base-go-emotion########################

-------------------- datasets --------------------
Document 1:

datasets: - go_emotions
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[Notebook](https://github.com/bhadreshpsavani/UnderstandingNLP/blob/master/go_emotion_of_transformers_multilabel_text_classification_v2.ipynb)
------------------------------
Document 2:

language: - en license: apache-2.0 tags: - text-classification - go-emotion - pytorch datasets: - go_emotions metrics: - Accuracy thumbnail: https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4
-------------------- paper --------------------
Document 1:

"text-classification", "go-emotion", "pytorch", "go_emotions", "Accuracy"
-------------------- upstream_model --------------------
Document 1:

- text-classification - pytorch - go_emotions - Accuracy
------------------------------
Document 2:

upstream_model
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
------------------------------
Document 2:

'parameter_count'
-------------------- hyper_parameters --------------------
Document 1:

'eval_accuracy_thresh': 0.9614765048027039, 'eval_loss': 0.1164659634232521
------------------------------
Document 2:

'train_loss': 0.12085497042373672
------------------------------
Document 3:

Num Epochs = 3, Instantaneous batch size per device = 16, Total train batch size (w. parallel, distributed & accumulation) = 16, Gradient Accumulation steps = 1, Total optimization steps = 31728
-------------------- evaluation --------------------
Document 1:

'eval_accuracy_thresh': 0.9614765048027039, 'eval_loss': 0.1164659634232521
------------------------------
Document 2:

- Accuracy
-------------------- hardware --------------------
Document 1:

"tags: - text-classification - go-emotion - pytorch datasets: - go_emotions metrics: - Accuracy thumbnail: https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4"

NO_OUTPUT
------------------------------
Document 2:

"Instantaneous batch size per device = 16" and "Total optimization steps = 31728"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[Notebook](https://github.com/bhadreshpsavani/UnderstandingNLP/blob/master/go_emotion_of_transformers_multilabel_text_classification_v2.ipynb)
------------------------------
Document 2:

"tags: - text-classification - go-emotion - pytorch datasets: - go_emotions metrics: - Accuracy thumbnail: https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4"
------------------------------
Document 3:

"Num examples = 169208", "Num Epochs = 3", "Instantaneous batch size per device = 16", "Total train batch size (w. parallel, distributed & accumulation) = 16", "Gradient Accumulation steps = 1", "Total optimization steps = 31728"
-------------------- input_format --------------------
Document 1:

language: - en license: apache-2.0 tags: - text-classification - go-emotion - pytorch datasets: - go_emotions metrics: - Accuracy thumbnail: https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"Num examples = 169208"

[{'datasets': ['go_emotions'], 'license': 'apache-2.0', 'github': 'https://github.com/bhadreshpsava 
ni/UnderstandingNLP/blob/master/go_emotion_of_transformers_multilabel_text_classification_v2.ipynb', 
 'paper': '"text-classification", "go-emotion", "pytorch", "go_emotions", "Accuracy"', 'upstream_mod 
el': '- text-classification - pytorch - go_emotions - Accuracy', 'parameter_count': 'NO_OUTPUT', 'hy 
per_parameters': {'eval_accuracy_thresh': '0.9614765048027039', 'eval_loss': '0.1164659634232521'},  
'evaluation': [{'test': 'eval_accuracy_thresh', 'result': 0.9614765048027039}, {'test': 'eval_loss', 
 'result': 0.1164659634232521}], 'hardware': '"tags: - text-classification - go-emotion - pytorch da 
tasets: - go_emotions metrics: - Accuracy thumbnail: https://avatars3.githubusercontent.com/u/324371 
51?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4"', 'limitation_and_bias': '', 'demo': '[Note 
book](https://github.com/bhadreshpsavani/UnderstandingNLP/blob/master/go_emotion_of_transformers_mul 
tilabel_text_classification_v2.ipynb)', 'input_format': 'language: - en license: apache-2.0 tags: -  
text-classification - go-emotion - pytorch datasets: - go_emotions metrics: - Accuracy thumbnail: ht 
tps://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4 
', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': '"Num examples = 169208"'}]      

#####################princeton-nlp/sup-simcse-roberta-large########################

-------------------- datasets --------------------
Document 1:

datasets, MNLI and SNLI datasets (314k)
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"If you encounter any problems when using the code, or want to report a bug, you can open an issue."
------------------------------
Document 2:

[Github Repository](https://github.com/princeton-nlp/SimCSE/blob/main/README.md)
-------------------- paper --------------------
Document 1:

[associated paper](https://arxiv.org/pdf/2104.08821.pdf)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

parameter_count 106 314k
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"semantic textual similarity (STS) tasks and downstream transfer tasks", "all" setting, "Spearman's correlation", "[associated paper](https://arxiv.org/pdf/2104.08821.pdf) (Appendix B) for evaluation details."
-------------------- hardware --------------------
Document 1:

More information needed
-------------------- limitation_and_bias --------------------
Document 1:

"risks, biases and limitations of the model"
------------------------------
Document 2:

"Significant research has explored bias and fairness issues with language models" and "[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf)" and "[Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)".
-------------------- demo --------------------
Document 1:

"Use the code below to get started with the model. ```python from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained("princeton-nlp/sup-simcse-roberta-large") model = AutoModel.from_pretrained("princeton-nlp/sup-simcse-roberta-large") ```"
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['MNLI', 'SNLI'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'pa 
rameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': 
 '', 'demo': '', 'input_format': '', 'output_format': ''}]                                           

#####################stabilityai/stable-diffusion-xl-base-0.9########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 401 Client Error. (Request ID: Root=1-653e067b-503165695b3f2c980dc532ec)

Cannot access gated repo for url https://huggingface.co/api/models/stabilityai/stable-diffusion-xl-base-0.9.
Repo model stabilityai/stable-diffusion-xl-base-0.9 is gated. You must be authenticated to access it. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e067c-3ebef46539574be66c60230e)

Entry Not Found for url: https://huggingface.co/textattack/bert-base-uncased-CoLA/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e067c-3c5fe7dc256524d5634bcf2b)

Entry Not Found for url: https://huggingface.co/ALINEAR/albert-japanese-v2/resolve/main/README.md. 

#####################huggyllama/llama-7b########################

-------------------- datasets --------------------
Document 1:

"This model is under a non-commercial license (see the LICENSE file)." "You should only use this repository if you have been granted access to the model by filling out [this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)"
-------------------- license --------------------
Document 1:

license: other, LICENSE file, [this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)
-------------------- github --------------------
Document 1:

"This contains the weights for the LLaMA-7b model. This model is under a non-commercial license (see the LICENSE file). You should only use this repository if you have been granted access to the model by filling out [this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)"
-------------------- paper --------------------
Document 1:

"This model is under a non-commercial license (see the LICENSE file)." "You should only use this repository if you have been granted access to the model by filling out [this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)" NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"This contains the weights for the LLaMA-7b model." NO_OUTPUT
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Document 1'], 'license': 'Document 1', 'github': 'Document 1', 'paper': 'Document 1 
', 'upstream_model': 'Document 1', 'parameter_count': 'Document 1', 'hyper_parameters': {}, 'evaluat 
ion': [], 'hardware': 'Document 1', 'limitation_and_bias': 'Document 1', 'demo': 'Document 1', 'inpu 
t_format': 'Document 1', 'output_format': 'Document 1', 'input_token_limit': 'Document 1', 'vocabula 
ry_size': 'Document 1'}]                                                                             

#####################facebook/blenderbot-400M-distill########################

-------------------- datasets --------------------
Document 1:

datasets: - blended_skill_talk
------------------------------
Document 2:

"We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available."
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"tags: - convAI - conversational - facebook datasets: - blended_skill_talk metrics: - perplexity"
------------------------------
Document 2:

"We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available."
-------------------- paper --------------------
Document 1:

Paper: [Recipes for building an open-domain chatbot]( https://arxiv.org/abs/2004.13637)
------------------------------
Document 2:

"We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements."
------------------------------
Document 3:

"conversational", "blended_skill_talk", "perplexity"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"90M, 2.7B and 9.4B parameter neural models"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements."
------------------------------
Document 2:

language: - en, license: apache-2.0, tags: - convAI - conversational - facebook, datasets: - blended_skill_talk, metrics: - perplexity
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation." "We then discuss the limitations of this work by analyzing failure cases of our models."
-------------------- demo --------------------
Document 1:

"We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available."
------------------------------
Document 2:

"Original PARLAI Code https://parl.ai/projects/recipes/"
-------------------- input_format --------------------
Document 1:

- blended_skill_talk
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['blended_skill_talk'], 'license': 'apache-2.0', 'github': 'https://github.com/facebo 
okresearch/ParlAI', 'paper': 'https://arxiv.org/abs/2004.13637', 'upstream_model': '', 'parameter_co 
unt': '90M, 2.7B and 9.4B parameter neural models', 'hyper_parameters': {}, 'evaluation': [{'test':  
'Human evaluations', 'result': 0.0}], 'hardware': '', 'limitation_and_bias': 'Good conversation requ 
ires a number of skills that an expert conversationalist blends in a seamless way: providing engagin 
g talking points and listening to their partners, both asking and answering questions, and displayin 
g knowledge, empathy and personality appropriately, depending on the situation. We then discuss the  
limitations of this work by analyzing failure cases of our models.', 'demo': 'We build variants of t 
hese recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly  
available.', 'input_format': '- blended_skill_talk', 'output_format': '', 'input_token_limit': '', ' 
vocabulary_size': ''}]                                                                               
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e06b4-26fa20e4162652184363ac9b)

Entry Not Found for url: https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/README.md. 

#####################mosaicml/mpt-7b########################

-------------------- datasets --------------------
Document 1:

StreamingDataset, https://github.com/mosaicml/streaming
------------------------------
Document 2:

datasets, [MosaicML Platform](https://www.mosaicml.com/platform), [FSDP](https://pytorch.org/docs/stable/fsdp.html), [LION](https://arxiv.org/abs/2302.06675)
------------------------------
Document 3:

"The model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:  
| Data Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs |
|-------------|----------------------------|------------|----------------------------|--------|
| mC4 3.1.0 - English | 417.99 B | 0.33 | 330 B | 0.14 |
| C4 - English - SemDedup 80% | 100.42 B | 0.299 | 299 B | 2.98 |
| RedPajama - CommonCrawl | 878.45 B | 0.1 | 100 B | 0.11 |
| The Stack - Selected Languages | 463.78 B | 0.1 | 100 B | 0.22 |
| RedPajama - Wikipedia - En | 4.87 B | 0.04 | 40 B | 8.21 |
| The Stack - Markdown | 107.07 B | 0.035 | 35 B | 0.33 |
| S2ORC | 48.85 B | 0.033 | 33 B | 0.68 |
| Red
-------------------- license --------------------
Document 1:

Apache-2.0
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

Apache-2.0
------------------------------
Document 2:

license: apache-2.0, tags: - Composer - MosaicML - llm-foundry - StreamingDatasets, datasets: - mc4 - c4 - togethercomputer/RedPajama-Data-1T - bigcode/the-stack - allenai/s2orc, inference: false
-------------------- paper --------------------
Document 1:

```@online{MosaicML2023Introducing, author    = {MosaicML NLP Team}, title     = {Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs}, year      = {2023}, url       = {www.mosaicml.com/blog/mpt-7b}, note      = {Accessed: 2023-05-05}, urldate   = {2023-05-05}```
------------------------------
Document 2:

* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
-------------------- upstream_model --------------------
Document 1:

"standard decoder-only transformer"
-------------------- parameter_count --------------------
Document 1:

n_parameters | 6.7B |
------------------------------
Document 2:

parameter_count: 440
-------------------- hyper_parameters --------------------
Document 1:

"It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases
| Hyperparameter | Value |
|----------------|-------|
|n_parameters | 6.7B |
|n_layers | 32 |
| n_heads | 32 |
| d_model | 4096 |
| vocab size | 50432 |
| sequence length | 2048 |"
------------------------------
Document 2:

batch size 1760 and sequence length 2048, model vocabulary size of 50432
------------------------------
Document 3:

"sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

440 A100-40GBs
------------------------------
Document 2:

The model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:  
| Data Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs |
|-------------|----------------------------|------------|----------------------------|--------|
| mC4 3.1.0 - English | 417.99 B | 0.33 | 330 B | 0.14 |
| C4 - English - SemDedup 80% | 100.42 B | 0.299 | 299 B | 2.98 |
| RedPajama - CommonCrawl | 878.45 B | 0.1 | 100 B | 0.11 |
| The Stack - Selected Languages | 463.78 B | 0.1 | 100 B | 0.22 |
| RedPajama - Wikipedia - En | 4.87 B | 0.04 | 40 B | 8.21 |
| The Stack - Markdown | 107.07 B | 0.035 | 35 B | 0.33 |
| S2ORC | 48.85 B | 0.033 | 33 B | 0.68 |
| RedP
-------------------- limitation_and_bias --------------------
Document 1:

The model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:  
| Data Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs |
|-------------|----------------------------|------------|----------------------------|--------|
| mC4 3.1.0 - English | 417.99 B | 0.33 | 330 B | 0.14 |
| C4 - English - SemDedup 80% | 100.42 B | 0.299 | 299 B | 2.98 |
| RedPajama - CommonCrawl | 878.45 B | 0.1 | 100 B | 0.11 |
| The Stack - Selected Languages | 463.78 B | 0.1 | 100 B | 0.22 |
| RedPajama - Wikipedia - En | 4.87 B | 0.04 | 40 B | 8.21 |
| The Stack - Markdown | 107.07 B | 0.035 | 35 B | 0.33 |
| S2ORC | 48.85 B | 0.033 | 33 B | 0.68 |
| RedP
------------------------------
Document 2:

* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)
* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings
* It does not use biases
-------------------- demo --------------------
Document 1:

[sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b)
-------------------- input_format --------------------
Document 1:

The model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:  
| Data Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs |
|-------------|----------------------------|------------|----------------------------|--------|
| mC4 3.1.0 - English | 417.99 B | 0.33 | 330 B | 0.14 |
| C4 - English - SemDedup 80% | 100.42 B | 0.299 | 299 B | 2.98 |
| RedPajama - CommonCrawl | 878.45 B | 0.1 | 100 B | 0.11 |
| The Stack - Selected Languages | 463.78 B | 0.1 | 100 B | 0.22 |
| RedPajama - Wikipedia - En | 4.87 B | 0.04 | 40 B | 8.21 |
| The Stack - Markdown | 107.07 B | 0.035 | 35 B | 0.33 |
| S2ORC | 48.85 B | 0.033 | 33 B | 0.68 |
| RedP
------------------------------
Document 2:

StreamingDataset, object storage, compute cluster
-------------------- output_format --------------------
Document 1:

The model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:  
| Data Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs |
|-------------|----------------------------|------------|----------------------------|--------|
| mC4 3.1.0 - English | 417.99 B | 0.33 | 330 B | 0.14 |
| C4 - English - SemDedup 80% | 100.42 B | 0.299 | 299 B | 2.98 |
| RedPajama - CommonCrawl | 878.45 B | 0.1 | 100 B | 0.11 |
| The Stack - Selected Languages | 463.78 B | 0.1 | 100 B | 0.22 |
| RedPajama - Wikipedia - En | 4.87 B | 0.04 | 40 B | 8.21 |
| The Stack - Markdown | 107.07 B | 0.035 | 35 B | 0.33 |
| S2ORC | 48.85 B | 0.033 | 33 B | 0.68 |
| RedP
-------------------- input_token_limit --------------------
Document 1:

"The model was trained for 1T tokens (with batch size 1760 and sequence length 2048)." "The data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer." "The model vocabulary size of 50432 was set to be a multiple of 128."
------------------------------
Document 2:

input_token_limit: 20B
-------------------- vocabulary_size --------------------
Document 1:

"The model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix: ... The data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer. This BPE tokenizer has a number of desirable characteristics, most of which are relevant for tokenizing code: (1) It was trained on a diverse mix of data that includes code (The Pile) (2) It applies consistent space delimitation, unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces (3) It contains tokens for repeated space characters, which allows superior compression of text with large amounts of repeated space characters. The model vocabulary size of 50432 was set to be a multiple of 128 (as in [MEGATRON-LM](https://arxiv.org/abs/1909.08053)), model flop utilization (MFU) increased by up to four percentage points.
------------------------------
Document 2:

vocab size | 50432
------------------------------
Document 3:

EleutherAI's GPT-NeoX-20B, MPT-7B (Base), MPT-7B was trained on various public datasets.

[{'datasets': ['StreamingDataset'], 'license': 'Apache-2.0', 'github': 'https://github.com/mosaicml 
/streaming', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'eval 
uation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_form 
at': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                            
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e06f4-1f71701a67d844e11fe3acc1)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTJForCausalLM/resolve/main/README.md. 

#####################Intel/dpt-large########################

-------------------- datasets --------------------
Document 1:

name: MIX 6
type: MIX 6
------------------------------
Document 2:

Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the
protocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. ([Ranftl et al., 2021](https://arxiv.org/abs/2103.13413))
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

License: Apache 2.0
-------------------- github --------------------
Document 1:

[GitHub Repo](https://github.com/isl-org/DPT)
-------------------- paper --------------------
Document 1:

`title = {Vision Transformers for Dense Prediction}, journal = {CoRR}, volume = {abs/2103.13413}, year = {2021}, url = {https://arxiv.org/abs/2103.13413}`
------------------------------
Document 2:

[Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413)
-------------------- upstream_model --------------------
Document 1:

Vision Transformer (ViT) as backbone
------------------------------
Document 2:

"Intel/dpt-large", "DPTForDepthEstimation.from_pretrained("Intel/dpt-large")"

upstream_model: Intel/dpt-large
-------------------- parameter_count --------------------
Document 1:

"model = DPTForDepthEstimation.from_pretrained("Intel/dpt-large")" and "See [Ranftl et al. (2021)](https://arxiv.org/abs/2103.13413) for more details."
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

- task:
type: monocular-depth-estimation
name: Monocular Depth Estimation
dataset:
name: MIX 6
type: MIX 6
metrics:
- type: Zero-shot transfer
value: 10.82
name: Zero-shot transfer
config: Zero-shot transfer
verified: false
------------------------------
Document 2:

Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the
protocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. ([Ranftl et al., 2021](https://arxiv.org/abs/2103.13413))
-------------------- hardware --------------------
Document 1:

"Intel" and "Apache 2.0"
------------------------------
Document 2:

"Intel Xeon Platinum 8280 CPU @ 2.70GHz with 8 physical cores and an NVIDIA RTX 2080 GPU."
-------------------- limitation_and_bias --------------------
Document 1:

Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the protocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. ([Ranftl et al., 2021](https://arxiv.org/abs/2103.13413)) 
Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. There are no additional caveats or recommendations for this model.
-------------------- demo --------------------
Document 1:

"For more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/master/en/model_doc/dpt)."
-------------------- input_format --------------------
Document 1:

"return_tensors='pt'"
-------------------- output_format --------------------
Document 1:

"return_tensors='pt'" and "predicted_depth"
NO_OUTPUT
-------------------- input_preprocessing --------------------
Document 1:

"We resize the image such that the longer side is 384 pixels and train on random square crops of size 384. ... We perform random horizontal flips for data augmentation." See [Ranftl et al. (2021)](https://arxiv.org/abs/2103.13413) for more details.
------------------------------
Document 2:

"It was introduced in the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ranftl et al. (2021) and first released in [this repository](https://github.com/isl-org/DPT)." NO_OUTPUT
-------------------- input_size --------------------
Document 1:

"We resize the image such that the longer side is 384 pixels" and "We perform random horizontal flips for data augmentation."
-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': ['MIX 6'], 'license': 'apache-2.0', 'github': 'https://github.com/isl-org/DPT', 'paper 
': 'https://arxiv.org/abs/2103.13413', 'upstream_model': 'Intel/dpt-large', 'parameter_count': '#par 
ams', 'hyper_parameters': {}, 'evaluation': [{'test': 'monocular-depth-estimation', 'result': 10.82} 
], 'hardware': 'Intel Xeon Platinum 8280 CPU @ 2.70GHz with 8 physical cores and an NVIDIA RTX 2080  
GPU.', 'limitation_and_bias': 'Users (both direct and downstream) should be made aware of the risks, 
 biases and limitations of the model. There are no additional caveats or recommendations for this mo 
del.', 'demo': 'For more code examples, we refer to the [documentation](https://huggingface.co/docs/ 
transformers/master/en/model_doc/dpt).', 'input_format': "return_tensors='pt'", 'output_format': "re 
turn_tensors='pt'", 'input_preprocessing': 'We resize the image such that the longer side is 384 pix 
els and train on random square crops of size 384. ... We perform random horizontal flips for data au 
gmentation.', 'input_size': 'We resize the image such that the longer side is 384 pixels', 'num_of_c 
lasses_for_classification': '', 'trigger_word': ''}                                                  
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0720-0fa1e32365a47b05598d9f51)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-OPTForCausalLM/resolve/main/README.md. 

#####################vinai/bertweet-base########################

-------------------- datasets --------------------
Document 1:

"The corpus used to pre-train BERTweet consists of 850M English Tweets (16B word tokens ~ 80GB), containing 845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the **COVID-19** pandemic."
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"https://user-images.githubusercontent.com/2412555/135724590-01d8d435-262d-44fe-a383-cd39324fe190.png", "https://user-images.githubusercontent.com/2412555/135724598-1e3605e7-d8ce-4c5e-be4a-62ae8501fae7.png", "https://user-images.githubusercontent.com/2412555/135724597-f1981f1e-fe73-4c03-b1ff-0cae0cc5f948.png", "https://user-images.githubusercontent.com/2412555/135724595-15f4f2c8-bbb6-4ee6-82a0-034769dec183.png"
------------------------------
Document 2:

"RoBERTa pre-training procedure" and "Please CITE our paper when BERTweet is used to help produce published results or is incorporated into other software." and "For further information or requests, please go to [BERTweet's homepage](https://github.com/VinAIResearch/BERTweet)!"
-------------------- paper --------------------
Document 1:

"The general architecture and experimental results of BERTweet can be found in our [paper](https://aclanthology.org/2020.emnlp-demos.2/):  
@inproceedings{bertweet,
title     = {{BERTweet: A pre-trained language model for English Tweets}},
author    = {Dat Quoc Nguyen and Thanh Vu and Anh Tuan Nguyen},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
pages     = {9--14},
year      = {2020}"
-------------------- upstream_model --------------------
Document 1:

RoBERTa pre-training procedure
-------------------- parameter_count --------------------
Document 1:

parameter_count: 845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the COVID-19 pandemic.
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"RoBERTa pre-training procedure" and "16B word tokens ~ 80GB"
-------------------- limitation_and_bias --------------------
Document 1:

"The corpus used to pre-train BERTweet consists of 850M English Tweets (16B word tokens ~ 80GB), containing 845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the **COVID-19** pandemic."
-------------------- demo --------------------
Document 1:

"<p float="left">
<img width="275" alt="postagging" src="https://user-images.githubusercontent.com/2412555/135724590-01d8d435-262d-44fe-a383-cd39324fe190.png" />
<img width="275" alt="ner" src="https://user-images.githubusercontent.com/2412555/135724598-1e3605e7-d8ce-4c5e-be4a-62ae8501fae7.png" />
</p>  
<p float="left">
<img width="275" alt="sentiment" src="https://user-images.githubusercontent.com/2412555/135724597-f1981f1e-fe73-4c03-b1ff-0cae0cc5f948.png" />
<img width="275" alt="irony" src="https://user-images.githubusercontent.com/2412555/135724595-15f4f2c8-bbb6-4ee6
------------------------------
Document 2:

[RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md), [paper](https://aclanthology.org/2020.emnlp-demos.2/), [BERTweet's homepage](https://github.com/VinAIResearch/BERTweet)
-------------------- input_format --------------------
Document 1:

"RoBERTa pre-training procedure" and "16B word tokens ~ 80GB"
-------------------- output_format --------------------
Document 1:

"RoBERTa pre-training procedure" and "16B word tokens ~ 80GB"
-------------------- input_token_limit --------------------
Document 1:

input_token_limit: 16B word tokens ~ 80GB
-------------------- vocabulary_size --------------------
Document 1:

"The corpus used to pre-train BERTweet consists of 850M English Tweets (16B word tokens ~ 80GB)"

[{'datasets': ['850M English Tweets (16B word tokens ~ 80GB)', '5M Tweets related to the COVID-19 p 
andemic'], 'license': '', 'github': 'https://github.com/VinAIResearch/BERTweet', 'paper': 'https://a 
clanthology.org/2020.emnlp-demos.2/', 'upstream_model': 'RoBERTa pre-training procedure', 'parameter 
_count': '845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the COVID-19 pandemi 
c.', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '16B word tokens ~ 80GB', 'limitation_and 
_bias': 'The corpus used to pre-train BERTweet consists of 850M English Tweets (16B word tokens ~ 80 
GB), containing 845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the COVID-19 p 
andemic.', 'demo': '<p float="left">\n<img width="275" alt="postagging" src="https://user-images.git 
hubusercontent.com/2412555/135724590-01d8d435-262d-44fe-a383-cd39324fe190.png" />\n<img width="275"  
alt="ner" src="https://user-images.githubusercontent.com/2412555/135724598-1e3605e7-d8ce-4c5e-be4a-6 
2ae8501fae7.png" />\n</p>  \n<p float="left">\n<img width="275" alt="sentiment" src="https://user-im 
ages.githubusercontent.com/2412555/135724597-f1981f1e-fe73-4c03-b1ff-0cae0cc5f948.png" />\n<img widt 
h="275" alt="irony" src="https://user-images.githubusercontent.com/2412555/135724595-15f4f2c8-bbb6-4 
ee6', 'input_format': 'RoBERTa pre-training procedure and 16B word tokens ~ 80GB', 'output_format':  
'RoBERTa pre-training procedure and 16B word tokens ~ 80GB', 'input_token_limit': '16B word tokens ~ 
 80GB', 'vocabulary_size': 'The corpus used to pre-train BERTweet consists of 850M English Tweets (1 
6B word tokens ~ 80GB)'}]                                                                            
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e074e-2341557d47f39c5c3b26192f)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-SwinModel/resolve/main/README.md. 

#####################lvwerra/distilbert-imdb########################

-------------------- datasets --------------------
Document 1:

datasets: - imdb dataset: name: imdb type: imdb
------------------------------
Document 2:

"imdb dataset" and "[distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)" and "[here](https://huggingface.co/lvwerra/distilbert-imdb/blob/main/distilbert-imdb-training.ipynb)"
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"distilbert-base-uncased", "imdb dataset", "training notebook is [here](https://huggingface.co/lvwerra/distilbert-imdb/blob/main/distilbert-imdb-training.ipynb)"
-------------------- upstream_model --------------------
Document 1:

upstream_model: distilbert-base-uncased
------------------------------
Document 2:

- generated_from_trainer
- distilbert-imdb
- task: type: text-classification
- dataset: name: imdb type: imdb args: plain_text
- metrics: type: accuracy value: 0.928 name: Accuracy
-------------------- parameter_count --------------------
Document 1:

parameter_count: 5
------------------------------
Document 2:

- type: accuracy
value: 0.928
name: Accuracy
parameter_count: NO_OUTPUT
------------------------------
Document 3:

"distilbert-base-uncased" "imdb dataset" "training notebook is [here](https://huggingface.co/lvwerra/distilbert-imdb/blob/main/distilbert-imdb-training.ipynb)" "Loss: 0.1903" "Accuracy: 0.928"
-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 5e-05 - train_batch_size: 16 - eval_batch_size: 16 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - num_epochs: 1
------------------------------
Document 2:

"distilbert-base-uncased", "imdb dataset", "Loss: 0.1903", "Accuracy: 0.928"
------------------------------
Document 3:

- type: text-classification
- metrics:
- type: accuracy
value: 0.928
name: Accuracy
-------------------- evaluation --------------------
Document 1:

"It achieves the following results on the evaluation set: - Loss: 0.1903 - Accuracy: 0.928"
------------------------------
Document 2:

license: apache-2.0, tags: - generated_from_trainer, datasets: - imdb, metrics: - accuracy, model-index: - name: distilbert-imdb, results: - task: type: text-classification name: Text Classification dataset: name: imdb type: imdb args: plain_text metrics: - type: accuracy value: 0.928 name: Accuracy
-------------------- hardware --------------------
Document 1:

distilbert-base-uncased, imdb dataset
------------------------------
Document 2:

- type: text-classification
- args: plain_text
-------------------- limitation_and_bias --------------------
Document 1:

"It achieves the following results on the evaluation set: - Loss: 0.1903 - Accuracy: 0.928"
------------------------------
Document 2:

- type: text-classification
- name: Text Classification
- dataset:
name: imdb
type: imdb
args: plain_text
- metrics:
- type: accuracy
value: 0.928
name: Accuracy
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

license: apache-2.0, type: text-classification, dataset: name: imdb, type: imdb, args: plain_text
------------------------------
Document 2:

"distilbert-base-uncased"
-------------------- output_format --------------------
Document 1:

license: apache-2.0, type: text-classification, type: accuracy, value: 0.928, name: Accuracy
-------------------- input_token_limit --------------------
Document 1:

license: apache-2.0, tags: - generated_from_trainer, datasets: - imdb, metrics: - accuracy, model-index: - name: distilbert-imdb, results: - task: type: text-classification name: Text Classification dataset: name: imdb type: imdb args: plain_text metrics: - type: accuracy value: 0.928 name: Accuracy
-------------------- vocabulary_size --------------------
Document 1:

"type: text-classification" "metrics: - type: accuracy" "value: 0.928" "name: Accuracy"

[{'datasets': ['imdb dataset'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'pa 
rameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': 
 '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size' 
: ''}]                                                                                               

#####################martin-ha/toxic-comment-model########################

-------------------- datasets --------------------
Document 1:

"The training data comes this [Kaggle competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data)", "We use 10% of the `train.csv` data to train the model."
------------------------------
Document 2:

"You can see [this documentation and codes](https://github.com/MSIA/wenyang_pan_nlp_project_2021) for how we train the model."
-------------------- license --------------------
Document 1:

"Kaggle competition" and "train.csv"
-------------------- github --------------------
Document 1:

"You can see [this documentation and codes](https://github.com/MSIA/wenyang_pan_nlp_project_2021) for how we train the model."
-------------------- paper --------------------
Document 1:

DistilBERT model
-------------------- upstream_model --------------------
Document 1:

upstream_model DistilBERT
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"10% of the `train.csv` data"
-------------------- evaluation --------------------
Document 1:

"94% accuracy and 0.59 f1-score in a 10000 rows held-out test set."
------------------------------
Document 2:

The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation). But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.  
| **subgroup**                  | **subgroup_size** | **subgroup_auc** | **bpsn_auc** | **bnsp_auc** |
| ----------------------------- | ----------------- | ---------------- | ------------ | ------------ |
| muslim                        | 108               | 0.689            | 0.811        | 0.88         |
| jewish                        | 40                | 0.749            | 0.86         | 0.825        |
| homosexual_gay_or_lesbian     | 56                | 0.795            | 0.706        | 0.972        |
| black                         | 84                | 0.866            | 0.758        | 0.975        |
| white                         | 112               | 0.876            | 0
------------------------------
Document 3:

"The training data comes this [Kaggle competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data). We use 10% of the `train.csv` data to train the model."

NO_OUTPUT
-------------------- hardware --------------------
Document 1:

"P-100 GPU"
------------------------------
Document 2:

"Kaggle competition", "train.csv"
-------------------- limitation_and_bias --------------------
Document 1:

The model is intended to use for classify toxic online classifications. One limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence "Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion." Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.
------------------------------
Document 2:

limitation_and_bias
-------------------- demo --------------------
Document 1:

[this documentation and codes](https://github.com/MSIA/wenyang_pan_nlp_project_2021)
------------------------------
Document 2:

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline

model_path = "martin-ha/toxic-comment-model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

pipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)
print(pipeline('This is a test text.'))
```
-------------------- input_format --------------------
Document 1:

"train.csv"
-------------------- output_format --------------------
Document 1:

"train.csv"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Kaggle competition'], 'license': 'Kaggle competition and train.csv', 'github': 'htt 
ps://github.com/MSIA/wenyang_pan_nlp_project_2021', 'paper': 'DistilBERT model', 'upstream_model': ' 
DistilBERT', 'parameter_count': '', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_ 
rate': '', 'optimizer': ''}], 'evaluation': [{'test': '94% accuracy and 0.59 f1-score', 'result': 10 
000}], 'hardware': 'P-100 GPU', 'limitation_and_bias': 'The model performs poorly for some comments  
that mention a specific identity subgroup, like Muslim.', 'demo': '[this documentation and codes](ht 
tps://github.com/MSIA/wenyang_pan_nlp_project_2021)', 'input_format': 'train.csv', 'output_format':  
'train.csv', 'input_token_limit': '', 'vocabulary_size': ''}]                                        

#####################Helsinki-NLP/opus-mt-mul-en########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 10330 tokens (10074 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################Jean-Baptiste/roberta-large-ner-english########################

-------------------- datasets --------------------
Document 1:

datasets:
- conll2003
train-eval-index:
- config: conll2003
task: token-classification
task_id: entity_extraction
splits:
eval_split: validation
col_mapping:
tokens: tokens
ner_tags: tags
------------------------------
Document 2:

Train and test dataset from original conll2003 for training and the "validation" dataset for validation.
------------------------------
Document 3:

"Model performances computed on conll2003 validation dataset (computed on the tokens predictions)"
"On private dataset (email, chat, informal discussion), computed on word predictions:"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"fine-tuned from roberta-large on conll2003 dataset" and "Model was validated on emails/chat data"
------------------------------
Document 2:

Train | Validation
- | -
17494 | 3250
-------------------- upstream_model --------------------
Document 1:

"roberta-large" (upstream_model)
------------------------------
Document 2:

"Jean-Baptiste/roberta-large-ner-english"
------------------------------
Document 3:

"task: token-classification"
-------------------- parameter_count --------------------
Document 1:

17494, 3250
-------------------- hyper_parameters --------------------
Document 1:

Train | Validation
- | -
17494 | 3250
-------------------- evaluation --------------------
Document 1:

Model performances computed on conll2003 validation dataset (computed on the tokens predictions)  
entity|precision|recall|f1
-|-|-|-
PER|0.9914|0.9927|0.9920
ORG|0.9627|0.9661|0.9644
LOC|0.9795|0.9862|0.9828
MISC|0.9292|0.9262|0.9277
Overall|0.9740|0.9766|0.9753  
On private dataset (email, chat, informal discussion), computed on word predictions:  
entity|precision|recall|f1
-|-|-|-
PER|0.8823|0.9116|0.8967
ORG|0.7694|0.7292|0.7487
LOC|0.8619|0.7768|0.8171  
By comparison on the same private dataset, Spacy (en_core_web_trf-3.2.0) was giving:  
entity|precision|recall|f1
-|-|
------------------------------
Document 2:

Training data was classified as follow: Abbreviation|Description -|- O |Outside of a named entity MISC |Miscellaneous entity PER |Person’s name ORG |Organization LOC |Location I used the train and test dataset from original conll2003 for training and the "validation" dataset for validation. This resulted in a dataset of size: Train | Validation -|- 17494 | 3250
-------------------- hardware --------------------
Document 1:

Train | Validation
- | -
17494 | 3250
-------------------- limitation_and_bias --------------------
Document 1:

Training data was classified as follow: Abbreviation|Description - O |Outside of a named entity MISC |Miscellaneous entity PER |Person’s name ORG |Organization LOC |Location I used the train and test dataset from original conll2003 for training and the "validation" dataset for validation. This resulted in a dataset of size: Train | Validation - 17494 | 3250
------------------------------
Document 2:

[roberta-large-ner-english] is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular the model seems to work better on entity that don't start with an upper case.
------------------------------
Document 3:

Model performances computed on conll2003 validation dataset (computed on the tokens predictions) entity|precision|recall|f1 -|-|-|- PER|0.9914|0.9927|0.9920 ORG|0.9627|0.9661|0.9644 LOC|0.9795|0.9862|0.9828 MISC|0.9292|0.9262|0.9277 Overall|0.9740|0.9766|0.9753 On private dataset (email, chat, informal discussion), computed on word predictions: entity|precision|recall|f1 -|-|-|- PER|0.8823|0.9116|0.8967 ORG|0.7694|0.7292|0.7487 LOC|0.8619|0.7768|0.8171 By comparison on the same private dataset, Spacy (en_core_web_trf-3.2.0) was giving: entity|precision|recall|f1 -|-|-|- PER|0.9146|0.8287|0.8695 ORG
-------------------- demo --------------------
Document 1:

from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained("Jean-Baptiste/roberta-large-ner-english")
model = AutoModelForTokenClassification.from_pretrained("Jean-Baptiste/roberta-large-ner-english")

from transformers import pipeline

nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy="simple")
nlp("Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer")
------------------------------
Document 2:

language: en, license: mit, datasets: - conll2003, widget: - text: My name is jean-baptiste and I live in montreal - text: My name is clara and I live in berkeley, california. - text: My name is wolfgang and I live in berlin, train-eval-index: - config: conll2003, task: token-classification, task_id: entity_extraction, splits: eval_split: validation, col_mapping: tokens: tokens, ner_tags: tags
------------------------------
Document 3:

"roberta-large-ner-english", "fine-tuned from roberta-large on conll2003 dataset", "validated on emails/chat data", "outperformed other models on this type of data specifically", "entity that don't start with an upper case."
-------------------- input_format --------------------
Document 1:

Train | Validation
- O |Outside of a named entity
- MISC |Miscellaneous entity
- PER |Person’s name
- ORG |Organization
- LOC |Location
- input_format: Train | Validation
------------------------------
Document 2:

language: en, license: mit, datasets: - conll2003, widget: - text: My name is jean-baptiste and I live in montreal - text: My name is clara and I live in berkeley, california. - text: My name is wolfgang and I live in berlin, train-eval-index: - config: conll2003, task: token-classification, task_id: entity_extraction, splits: eval_split: validation, col_mapping: tokens: tokens, ner_tags: tags
------------------------------
Document 3:

"AutoTokenizer.from_pretrained("Jean-Baptiste/roberta-large-ner-english")
model = AutoModelForTokenClassification.from_pretrained("Jean-Baptiste/roberta-large-ner-english")"
-------------------- output_format --------------------
Document 1:

Train | Validation
- | -
17494 | 3250
------------------------------
Document 2:

entity|precision|recall|f1
-|-|-|-
PER|0.9914|0.9927|0.9920
ORG|0.9627|0.9661|0.9644
LOC|0.9795|0.9862|0.9828
MISC|0.9292|0.9262|0.9277
Overall|0.9740|0.9766|0.9753
entity|precision|recall|f1
-|-|-|-
PER|0.8823|0.9116|0.8967
ORG|0.7694|0.7292|0.7487
LOC|0.8619|0.7768|0.8171
entity|precision|recall|f1
-|-|-|-
PER|0.9146|0.8287|0.8695
ORG|0.7655|0.6437|0.6993
LOC|0.8727|0.6180|0.7236
-------------------- input_token_limit --------------------
Document 1:

Train | Validation
- | -
17494 | 3250
-------------------- vocabulary_size --------------------
Document 1:

Train | Validation
- | -
17494 | 3250

[{'datasets': ['conll2003'], 'license': 'mit', 'github': '', 'paper': '', 'upstream_model': '', 'pa 
rameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': 
 '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size' 
: ''}]                                                                                               

#####################facebook/mms-1b-all########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 5675 tokens (5419 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################google/mobilenet_v1_0.75_192########################

-------------------- datasets --------------------
Document 1:

datasets:
- imagenet-1k
------------------------------
Document 2:

"MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192" and "[this repository](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)"
-------------------- license --------------------
Document 1:

"It was introduced in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Howard et al, and first released in [this repository](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)." 
NO_OUTPUT
------------------------------
Document 2:

license: other
------------------------------
Document 3:

"From the [original README](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md):"
-------------------- github --------------------
Document 1:

"this repository": https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md
-------------------- paper --------------------
Document 1:

[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)
------------------------------
Document 2:

"From the [original README](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md):"
-------------------- upstream_model --------------------
Document 1:

"MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192" and "introduced in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Howard et al"
-------------------- parameter_count --------------------
Document 1:

"google/mobilenet_v1_0.75_192"
------------------------------
Document 2:

"MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192"
------------------------------
Document 3:

"MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases." parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases"
-------------------- evaluation --------------------
Document 1:

"MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192" "introduced in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Howard et al, and first released in [this repository](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)."
-------------------- hardware --------------------
Document 1:

MobileNet V1
------------------------------
Document 2:

"low-latency, low-power models"
-------------------- limitation_and_bias --------------------
Document 1:

"trade off between latency, size and accuracy while comparing favorably with popular models from the literature."
------------------------------
Document 2:

"MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192" and "introduced in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Howard et al, and first released in [this repository](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)" and "Disclaimer: The team releasing MobileNet V1 did not write a model card for this model so this model card has been written by the Hugging Face team."
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=mobilenet_v1"
------------------------------
Document 2:

"From the [original README](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md):"
-------------------- input_format --------------------
Document 1:

"MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192"
-------------------- output_format --------------------
Document 1:

"return_tensors="pt""
-------------------- input_preprocessing --------------------
Document 1:

"MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192" and "It was introduced in [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Howard et al, and first released in [this repository](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)."
------------------------------
Document 2:

AutoImageProcessor.from_pretrained("google/mobilenet_v1_0.75_192"), AutoModelForImageClassification.from_pretrained("google/mobilenet_v1_0.75_192"), preprocessor(images=image, return_tensors="pt")
-------------------- input_size --------------------
Document 1:

"resolution 192x192"
------------------------------
Document 2:

"AutoImageProcessor.from_pretrained("google/mobilenet_v1_0.75_192")" and "AutoModelForImageClassification.from_pretrained("google/mobilenet_v1_0.75_192")"

NO_OUTPUT
------------------------------
Document 3:

"MobileNets trade off between latency, size and accuracy" NO_OUTPUT
-------------------- num_of_classes_for_classification --------------------
Document 1:

"This model actually predicts 1001 classes, the 1000 classes from ImageNet plus an extra “background” class (index 0)."

num_of_classes_for_classification: 1001
-------------------- trigger_word --------------------


{'datasets': ['imagenet-1k'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'para 
meter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimiz 
er': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': ' 
', 'output_format': '', 'input_preprocessing': '', 'input_size': '', 'num_of_classes_for_classificat 
ion': '', 'trigger_word': ''}                                                                        

#####################databricks/dolly-v2-3b########################

-------------------- datasets --------------------
Document 1:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
------------------------------
Document 2:

"datasets: - databricks/databricks-dolly-15k"
------------------------------
Document 3:

`dolly-v2-3b`, [Databricks](https://databricks.com/), [EleutherAI's](https://www.eleuther.ai/) [Pythia-2.8b](https://huggingface.co/EleutherAI/pythia-2.8b), [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data)
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- github --------------------
Document 1:

language: - en license: mit library_name: transformers datasets: - databricks/databricks-dolly-15k inference: false
------------------------------
Document 2:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- paper --------------------
Document 1:

`dolly-v2-3b` is a 2.8 billion parameter causal language model created by [Databricks](https://databricks.com/) that is derived from [EleutherAI's](https://www.eleuther.ai/) [Pythia-2.8b](https://huggingface.co/EleutherAI/pythia-2.8b) and fine-tuned on a [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data) generated by Databricks employees and released under a permissive license (CC-BY-SA)
------------------------------
Document 2:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
------------------------------
Document 3:

"Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM"
-------------------- upstream_model --------------------
Document 1:

EleutherAI's Pythia-2.8b
------------------------------
Document 2:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- parameter_count --------------------
Document 1:

parameter_count 2.8 billion
------------------------------
Document 2:

"It underperforms `dolly-v1-6b` in the evaluation benchmarks, which is not surprising considering it has half the number of parameters."
-------------------- hyper_parameters --------------------
Document 1:

"2.8 billion parameter" "Pythia-2.8b" "~15K record instruction corpus"
------------------------------
Document 2:

"EleutherAI/pythia-2.8b", "EleutherAI/pythia-6.9b", "databricks/dolly-v2-3b", "EleutherAI/pythia-12b", "EleutherAI/gpt-j-6B", "databricks/dolly-v2-12b", "databricks/dolly-v2-7b", "databricks/dolly-v1-6b", "EleutherAI/gpt-neox-20b"
------------------------------
Document 3:

"The Pile": GPT-J's pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets, it contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly in the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit associations. 
- **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or personally identifying information about non-public figures, but it may contain typos and factual errors. The dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects the interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.
-------------------- evaluation --------------------
Document 1:

model results are sorted by geometric mean to produce an intelligible ordering. As outlined above, these results demonstrate that `dolly-v2-3b` is not state of the art.
It underperforms `dolly-v1-6b` in the evaluation benchmarks, which is not surprising considering it has half the number of parameters.  
|  model                            |   openbookqa |   arc_easy |   winogrande |   hellaswag |   arc_challenge |     piqa |    boolq |    gmean |
| --------------------------------- | ------------ | ---------- | ------------ | ----------- | --------------- | -------- | -------- | ---------|
| EleutherAI/pythia-2.8b            |        0.348 |   0.585859 |     0.589582 |    0.591217 |        0.323379 | 0.73395  | 0.638226 | 0.523431 |
| EleutherAI/pythia-6.9b            |        0.368 |   0.604798 |     0.608524 |    0.631548 |
------------------------------
Document 2:

- **The Pile**: GPT-J's pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets, it contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly in the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit associations. 
- **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or personally identifying information about non-public figures, but it may contain typos and factual errors. The dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects the interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.
-------------------- hardware --------------------
Document 1:

`dolly-v2-3b`, [EleutherAI's](https://www.eleuther.ai/) [Pythia-2.8b](https://huggingface.co/EleutherAI/pythia-2.8b)
------------------------------
Document 2:

**`databricks-dolly-15k`**
-------------------- limitation_and_bias --------------------
Document 1:

"The Dolly model family is under active development, and so any list of shortcomings is unlikely to be exhaustive, but we include known limitations and misfires here as a means to document and share our preliminary findings with the community. In particular, `dolly-v2-3b` struggles with: syntactically complex prompts, programming problems, mathematical operations, factual errors, dates and times, open-ended question answering, hallucination, enumerating lists of specific length, stylistic mimicry, having a sense of humor, etc. Moreover, we find that `dolly-v2-3b` does not have some capabilities, such as well-formatted letter writing, present in the original model."
------------------------------
Document 2:

- **The Pile**: GPT-J's pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets, it contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly in the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit associations. 
- **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or personally identifying information about non-public figures, but it may contain typos and factual errors. The dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects the interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.
------------------------------
Document 3:

`dolly-v2-3b` is a 2.8 billion parameter causal language model created by [Databricks](https://databricks.com/) that is derived from [EleutherAI's](https://www.eleuther.ai/) [Pythia-2.8b](https://huggingface.co/EleutherAI/pythia-2.8b) and fine-tuned on a [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data) generated by Databricks employees and released under a permissive license (CC-BY-SA)
-------------------- demo --------------------
Document 1:

"The Pile": GPT-J's pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets, it contains content many users would find objectionable. - **`databricks-dolly-15k`**: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- input_format --------------------
Document 1:

"The Pile: GPT-J's pre-training corpus contains content mostly collected from the public internet" and "databricks-dolly-15k: The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization."
-------------------- output_format --------------------
Document 1:

"databricks-dolly-15k": The training data on which `dolly-v2-3b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"databricks-dolly-15k" and "The Pile: GPT-J's pre-training corpus contains content mostly collected from the public internet"
------------------------------
Document 2:

"2.8 billion parameter" and "~15K record instruction corpus"
------------------------------
Document 3:

"dolly-v2-3b", "pythia-2.8b", "dolly-v2-12b", "pythia-12b", "dolly-v2-7b", "pythia-6.9b"

[{'datasets': ['databricks-dolly-15k'], 'license': 'mit', 'github': 'https://github.com/databricksl 
abs/dolly', 'paper': 'https://github.com/databrickslabs/dolly/tree/master/data', 'upstream_model': " 
EleutherAI's Pythia-2.8b", 'parameter_count': '2.8 billion', 'hyper_parameters': {'epochs': '', 'bat 
ch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_a 
nd_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabul 
ary_size': ''}]                                                                                      
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e085c-645664782d66b707273a687d)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-DeiTModel/resolve/main/README.md. 

#####################microsoft/markuplm-base########################

-------------------- datasets --------------------
Document 1:

"We refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/markuplm) and [demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)."
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

"https://huggingface.co/docs/transformers/main/en/model_doc/markuplm" and "https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM"
-------------------- paper --------------------
Document 1:

"Document AI"
------------------------------
Document 2:

"We refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/markuplm) and [demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)."
------------------------------
Document 3:

[MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518)  Junlong Li, Yiheng Xu, Lei Cui, Furu Wei, ACL 2022
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters" "docs" "demo notebooks"
-------------------- evaluation --------------------
Document 1:

"We refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/markuplm) and [demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[docs](https://huggingface.co/docs/transformers/main/en/model_doc/markuplm) and [demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)
------------------------------
Document 2:

Document AI
------------------------------
Document 3:

"For more details, please refer to our paper: [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518)  Junlong Li, Yiheng Xu, Lei Cui, Furu Wei, ACL 2022"
-------------------- input_format --------------------
Document 1:

input_format: Multimodal (text +markup language)
-------------------- output_format --------------------


[{'datasets': ['Document 1'], 'license': 'Document 1', 'github': 'Document 1', 'paper': 'Document 3 
', 'upstream_model': 'Document 1', 'parameter_count': 'Document 1', 'hyper_parameters': {'epochs': ' 
Document 1', 'batch_size': 'Document 1', 'learning_rate': 'Document 1', 'optimizer': 'Document 1'},  
'evaluation': [{'test': 'Document 1', 'result': 0}], 'hardware': 'Document 1', 'limitation_and_bias' 
: 'Document 1', 'demo': 'Document 1', 'input_format': 'Document 1', 'output_format': 'Document 1'}]  

#####################textattack/albert-base-v2-ag-news########################

-------------------- datasets --------------------
Document 1:

ag_news dataset loaded using the `nlp` library
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"For more information, check out [TextAttack on Github](https://github.com/QData/TextAttack)."
-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"batch size of 16, a learning rate of 2e-05, and a maximum sequence length of 128"
-------------------- evaluation --------------------
Document 1:

"Since this was a classification task, the model was trained with a cross-entropy loss function. The best score the model achieved on this task was 0.9471052631578948, as measured by the eval set accuracy, found after 3 epochs."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"fine-tuned for 5 epochs with a batch size of 16, a learning rate of 2e-05, and a maximum sequence length of 128. Since this was a classification task, the model was trained with a cross-entropy loss function. The best score the model achieved on this task was 0.9471052631578948, as measured by the eval set accuracy, found after 3 epochs."
-------------------- demo --------------------
Document 1:

"For more information, check out [TextAttack on Github](https://github.com/QData/TextAttack)."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Expecting property name enclosed in double quotes: line 16 column 9 (char 366) 

#####################lllyasviel/control_v11p_sd15_lineart########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 4291 tokens (4035 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################timm/vit_base_patch16_224.augreg2_in21k_ft_in1k########################

-------------------- datasets --------------------
Document 1:

"datasets" and "https://github.com/huggingface/pytorch-image-models/tree/main/results"
------------------------------
Document 2:

datasets, ImageNet-21k, ImageNet-1k
------------------------------
Document 3:

datasets: - imagenet-1k - imagenet-21k
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"GitHub repository" and "\url{https://github.com/huggingface/pytorch-image-models}"
------------------------------
Document 2:

"https://github.com/huggingface/pytorch-image-models/tree/main/results"
-------------------- paper --------------------
Document 1:

"paper authors"
------------------------------
Document 2:

"model results"
------------------------------
Document 3:

```bibtex
@article{dosovitskiy2020vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
journal={ICLR},
year={2021}
}
```
-------------------- upstream_model --------------------
Document 1:

upstream_model Vision Transformer (ViT) image classification model
-------------------- parameter_count --------------------
Document 1:

parameter_count: 86.6
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters"
-------------------- evaluation --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).
------------------------------
Document 2:

"A Vision Transformer (ViT) image classification model. Trained on ImageNet-21k by paper authors and (re) fine-tuned on ImageNet-1k with additional augmentation and regularization by Ross Wightman."
------------------------------
Document 3:

- Model Type: Image classification / feature backbone
- Model Stats: Params (M): 86.6, GMACs: 16.9, Activations (M): 16.5, Image size: 224 x 224
- Dataset: ImageNet-1k
- Pretrain Dataset: ImageNet-21k
-------------------- hardware --------------------
Document 1:

"Trained on ImageNet-21k" and "fine-tuned on ImageNet-1k"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)
------------------------------
Document 2:

"A Vision Transformer (ViT) image classification model."
-------------------- input_format --------------------
Document 1:

"Image size: 224 x 224", "Dataset: ImageNet-1k", "Pretrain Dataset: ImageNet-21k"
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

"A Vision Transformer (ViT) image classification model." NO_OUTPUT
------------------------------
Document 2:

"Image size: 224 x 224", "Dataset: ImageNet-1k", "Pretrain Dataset: ImageNet-21k"
-------------------- input_size --------------------
Document 1:

Image size: 224 x 224
-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count': 
 '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'ev 
aluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_fo 
rmat': '', 'input_preprocessing': '', 'input_size': '', 'num_of_classes_for_classification': '', 'tr 
igger_word': ''}                                                                                     
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e08b7-645c97641bbee5862b6c2fba)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-MobileNetV2Model/resolve/main/README.md. 

#####################bhadresh-savani/distilbert-base-uncased-emotion########################

-------------------- datasets --------------------
Document 1:

datasets:
- emotion
------------------------------
Document 2:

Twitter-Sentiment-Analysis, https://huggingface.co/nlp/viewer/?dataset=emotion
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"license"
-------------------- github --------------------
Document 1:

- name: bhadresh-savani/distilbert-base-uncased-emotion
- task:
type: text-classification
name: Text Classification
dataset:
name: emotion
type: emotion
config: default
split: test
metrics:
- type: accuracy
value: 0.927
name: Accuracy
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzQxOGRmMjFlZThmZWViNjNmNGMzMTdjMGNjYjg1YWUzOTI0ZDlmYjRhYWMzMDA3Yjg2N2FiMTdmMzk0ZjJkOSIsInZlcnNpb24iOjF9.mOqr-hgNrnle7WCPy3Mo7M3fITFppn5gjpNagGMf_TZfB6VZnPKfZ51UkNFQ
------------------------------
Document 2:

[Colab Notebook](https://github.com/bhadreshpsavani/ExploringSentimentalAnalysis/blob/main/SentimentalAnalysisWithDistilbert.ipynb)
-------------------- paper --------------------
Document 1:

Natural Language Processing with Transformer By Lewis Tunstall, Leandro von Werra, Thomas Wolf
------------------------------
Document 2:

- name: bhadresh-savani/distilbert-base-uncased-emotion
- task:
type: text-classification
name: Text Classification
dataset:
name: emotion
type: emotion
config: default
split: test
metrics:
- type: accuracy
value: 0.927
name: Accuracy
- type: precision
value: 0.8880230732280744
name: Precision Macro
- type: precision
value: 0.927
name: Precision Micro
- type: precision
value: 0.9272902840835793
name: Precision Weighted
- type: recall
value: 0.8790126653780703
name: Recall Macro
- type: recall
value: 0.927
name: Recall Micro
- type: recall
value: 0.927
name: Recall Weighted
- type: f1
value: 0.8825061528287809
name: F1 Macro
- type: f1
value: 0.927
name: F1 Micro
- type: f1
value: 0.92687
-------------------- upstream_model --------------------
Document 1:

upstream_model: bhadresh-savani/distilbert-base-uncased-emotion
-------------------- parameter_count --------------------
Document 1:

"num_train_epochs=8"
-------------------- hyper_parameters --------------------
Document 1:

"learning rate 2e-5, batch size 64, num_train_epochs=8"
------------------------------
Document 2:

metrics:
- Accuracy, F1 Score
results:
- type: accuracy
value: 0.927
name: Accuracy
- type: precision
value: 0.8880230732280744
name: Precision Macro
- type: precision
value: 0.927
name: Precision Micro
- type: precision
value: 0.9272902840835793
name: Precision Weighted
- type: recall
value: 0.8790126653780703
name: Recall Macro
- type: recall
value: 0.927
name: Recall Micro
- type: recall
value: 0.927
name: Recall Weighted
- type: f1
value: 0.8825061528287809
name: F1 Macro
- type: f1
value: 0.927
name: F1 Micro
- type: f1
value: 0.926876082854655
name: F1 Weighted
- type: loss
value: 0.17403268814086914
name: loss
-------------------- evaluation --------------------
Document 1:

metrics:
- Accuracy, F1 Score
results:
- task:
type: text-classification
name: Text Classification
dataset:
name: emotion
type: emotion
config: default
split: test
metrics:
- type: accuracy
value: 0.927
name: Accuracy
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzQxOGRmMjFlZThmZWViNjNmNGMzMTdjMGNjYjg1YWUzOTI0ZDlmYjRhYWMzMDA3Yjg2N2FiMTdmMzk0ZjJkOSIsInZlcnNpb24iOjF9.mOqr-hgNrnle7WCPy3Mo7M3fITFppn5gjpNagGMf_TZfB6VZnPKfZ51UkNFQlBtUlcm0U8v
------------------------------
Document 2:

| Model | Accuracy | F1 Score |  Test Sample per Second |
| --- | --- | --- | --- |
| [Distilbert-base-uncased-emotion](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) | 93.8 | 93.79 | 398.69 |
| [Bert-base-uncased-emotion](https://huggingface.co/bhadresh-savani/bert-base-uncased-emotion) | 94.05 | 94.06 | 190.152 |
| [Roberta-base-emotion](https://huggingface.co/bhadresh-savani/roberta-base-emotion) | 93.95 | 93.97| 195.639 |
| [Albert-base-v2-emotion](https://huggingface.co/bhadresh-savani/albert-base-v2-emotion) | 93.6 | 93.65 | 182.794 |
-------------------- hardware --------------------
Document 1:

- pytorch
- Accuracy, F1 Score
------------------------------
Document 2:

Distilbert, Distilbert-base-uncased, HuggingFace Trainer
-------------------- limitation_and_bias --------------------
Document 1:

metrics:
- Accuracy, F1 Score
results:
- type: accuracy
value: 0.927
name: Accuracy
- type: precision
value: 0.8880230732280744
name: Precision Macro
- type: precision
value: 0.927
name: Precision Micro
- type: precision
value: 0.9272902840835793
name: Precision Weighted
- type: recall
value: 0.8790126653780703
name: Recall Macro
- type: recall
value: 0.927
name: Recall Micro
- type: recall
value: 0.927
name: Recall Weighted
- type: f1
value: 0.8825061528287809
name: F1 Macro
- type: f1
value: 0.927
name: F1 Micro
- type: f1
value: 0.926876082854655
name: F1 Weighted
- type: loss
value: 0.17403268814086914
name: loss
-------------------- demo --------------------
Document 1:

- task:
type: text-classification
name: Text Classification
dataset:
name: emotion
type: emotion
config: default
split: test
metrics:
- type: accuracy
value: 0.927
name: Accuracy
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzQxOGRmMjFlZThmZWViNjNmNGMzMTdjMGNjYjg1YWUzOTI0ZDlmYjRhYWMzMDA3Yjg2N2FiMTdmMzk0ZjJkOSIsInZlcnNpb24iOjF9.mOqr-hgNrnle7WCPy3Mo7M3fITFppn5gjpNagGMf_TZfB6VZnPKfZ51UkNFQlBtUlcm0U8vwPkF79snxwvCoDw
------------------------------
Document 2:

[Colab Notebook](https://github.com/bhadreshpsavani/ExploringSentimentalAnalysis/blob/main/SentimentalAnalysisWithDistilbert.ipynb)
------------------------------
Document 3:

[Twitter-Sentiment-Analysis](https://huggingface.co/nlp/viewer/?dataset=emotion).
-------------------- input_format --------------------
Document 1:

datasets:
- emotion
metrics:
- Accuracy, F1 Score
model-index:
- name: bhadresh-savani/distilbert-base-uncased-emotion
results:
- task:
type: text-classification
name: Text Classification
dataset:
name: emotion
type: emotion
config: default
split: test
metrics:
- type: accuracy
value: 0.927
name: Accuracy
- type: precision
value: 0.8880230732280744
name: Precision Macro
- type: precision
value: 0.927
name: Precision Micro
- type: precision
value: 0.9272902840835793
name: Precision Weighted
- type: recall
value: 0.8790126653780703
name: Recall Macro
- type: recall
value: 0.927
name: Recall Micro
- type: recall
value: 0.927
name: Recall Weighted
- type: f1
value: 0.8825061528287809
name: F1 Macro
- type: f1
-------------------- output_format --------------------
Document 1:

metrics:
- Accuracy, F1 Score
results:
- task:
type: text-classification
name: Text Classification
dataset:
name: emotion
type: emotion
config: default
split: test
metrics:
- type: accuracy
value: 0.927
name: Accuracy
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzQxOGRmMjFlZThmZWViNjNmNGMzMTdjMGNjYjg1YWUzOTI0ZDlmYjRhYWMzMDA3Yjg2N2FiMTdmMzk0ZjJkOSIsInZlcnNpb24iOjF9.mOqr-hgNrnle7WCPy3Mo7M3fITFppn5gjpNagGMf_TZfB6VZnPKfZ51UkNFQlBtUlcm0U8v
------------------------------
Document 2:

```json {
'test_accuracy': 0.938,
'test_f1': 0.937932884041714,
'test_loss': 0.1472451239824295,
'test_mem_cpu_alloc_delta': 0,
'test_mem_cpu_peaked_delta': 0,
'test_mem_gpu_alloc_delta': 0,
'test_mem_gpu_peaked_delta': 163454464,
'test_runtime': 5.0164,
'test_samples_per_second': 398.69
}``
-------------------- input_token_limit --------------------
Document 1:

"Distilbert-base-uncased" and "num_train_epochs=8"
-------------------- vocabulary_size --------------------
Document 1:

"Distilbert-base-uncased" and "num_train_epochs=8"
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 Could not parse function call data: Unterminated string starting at: line 21 column 15 (char 1890) 

#####################DeepFloyd/IF-I-M-v1.0########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 401 Client Error. (Request ID: Root=1-653e0919-6f2575ea6fd7bb05656cf3bf)

Cannot access gated repo for url https://huggingface.co/api/models/DeepFloyd/IF-I-M-v1.0.
Repo model DeepFloyd/IF-I-M-v1.0 is gated. You must be authenticated to access it. 

#####################Helsinki-NLP/opus-mt-en-fr########################

-------------------- datasets --------------------
Document 1:

dataset: opus, download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.zip)
------------------------------
Document 2:

newsdiscussdev2015-enfr.en.fr, newsdiscusstest2015-enfr.en.fr, newssyscomb2009.en.fr, news-test2008.en.fr, newstest2009.en.fr, newstest2010.en.fr, newstest2011.en.fr, newstest2012.en.fr, newstest2013.en.fr, Tatoeba.en.fr
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

*OPUS readme: [en-fr](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-fr/README.md)*
-------------------- github --------------------
Document 1:

[en-fr](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-fr/README.md), [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.zip), [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.test.txt), [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.eval.txt)
-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| newsdiscussdev2015-enfr.en.fr 	| 33.8 	| 0.602 |
| newsdiscusstest2015-enfr.en.fr 	| 40.0 	| 0.643 |
| newssyscomb2009.en.fr 	| 29.8 	| 0.584 |
| news-test2008.en.fr 	| 27.5 	| 0.554 |
| newstest2009.en.fr 	| 29.4 	| 0.577 |
| newstest2010.en.fr 	| 32.7 	| 0.596 |
| newstest2011.en.fr 	| 34.3 	| 0.611 |
| newstest2012.en.fr 	| 31.8 	| 0.592 |
| newstest2013.en.fr 	| 33.2 	| 0.589 |
| Tatoeba.en.fr
------------------------------
Document 2:

*test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.eval.txt)*
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

OPUS readme: [en-fr](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-fr/README.md), dataset: opus, model: transformer-align, pre-processing: normalization + SentencePiece, download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.zip), test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.test.txt), test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.eval.txt)
-------------------- input_format --------------------
Document 1:

SentencePiece, opus-2020-02-26.zip, opus-2020-02-26.test.txt
------------------------------
Document 2:

"newsdiscussdev2015-enfr.en.fr, newsdiscusstest2015-enfr.en.fr, newssyscomb2009.en.fr, news-test2008.en.fr, newstest2009.en.fr, newstest2010.en.fr, newstest2011.en.fr, newstest2012.en.fr, newstest2013.en.fr, Tatoeba.en.fr"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': '[en-fr](https://github.com/Helsinki-NLP 
/OPUS-MT-train/blob/master/models/en-fr/README.md), [opus-2020-02-26.zip](https://object.pouta.csc.f 
i/OPUS-MT-models/en-fr/opus-2020-02-26.zip), [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/ 
OPUS-MT-models/en-fr/opus-2020-02-26.test.txt), [opus-2020-02-26.eval.txt](https://object.pouta.csc. 
fi/OPUS-MT-models/en-fr/opus-2020-02-26.eval.txt)', 'paper': '', 'upstream_model': '', 'parameter_co 
unt': '', 'hyper_parameters': [], 'evaluation': [{'test': 'newsdiscussdev2015-enfr.en.fr', 'result': 
 33.8}, {'test': 'newsdiscusstest2015-enfr.en.fr', 'result': 40.0}, {'test': 'newssyscomb2009.en.fr' 
, 'result': 29.8}, {'test': 'news-test2008.en.fr', 'result': 27.5}, {'test': 'newstest2009.en.fr', ' 
result': 29.4}, {'test': 'newstest2010.en.fr', 'result': 32.7}, {'test': 'newstest2011.en.fr', 'resu 
lt': 34.3}, {'test': 'newstest2012.en.fr', 'result': 31.8}, {'test': 'newstest2013.en.fr', 'result': 
 33.2}, {'test': 'Tatoeba.en.fr', 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 'demo':  
'OPUS readme: [en-fr](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-fr/README. 
md), dataset: opus, model: transformer-align, pre-processing: normalization + SentencePiece, downloa 
d original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020 
-02-26.zip), test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-m 
odels/en-fr/opus-2020-02-26.test.txt), test set scores: [opus-2020-02-26.eval.txt](https://object.po 
uta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.eval.txt)', 'input_format': 'SentencePiece, opus-202 
0-02-26.zip, opus-2020-02-26.test.txt', 'output_format': '', 'input_token_limit': '', 'vocabulary_si 
ze': ''}, {'datasets': ['newsdiscussdev2015-enfr.en.fr', 'newsdiscusstest2015-enfr.en.fr', 'newssysc 
omb2009.en.fr', 'news-test2008.en.fr', 'newstest2009.en.fr', 'newstest2010.en.fr', 'newstest2011.en. 
fr', 'newstest2012.en.fr', 'newstest2013.en.fr', 'Tatoeba.en.fr'], 'license': '', 'github': '*OPUS r 
eadme: [en-fr](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-fr/README.md)*',  
'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [{'t 
est': 'opus-2020-02-26.eval.txt', 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 'demo':  
'', 'input_format': '"newsdiscussdev2015-enfr.en.fr, newsdiscusstest2015-enfr.en.fr, newssyscomb2009 
.en.fr, news-test2008.en.fr, newstest2009.en.fr, newstest2010.en.fr, newstest2011.en.fr, newstest201 
2.en.fr, newstest2013.en.fr, Tatoeba.en.fr"', 'output_format': '', 'input_token_limit': '', 'vocabul 
ary_size': ''}]                                                                                      

#####################textattack/roberta-base-CoLA########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count: 0.850431447746884, batch size: 32, learning rate: 2e-05, maximum sequence length: 128
-------------------- hyper_parameters --------------------
Document 1:

"5 epochs with a batch size of 32, a learning rate of 2e-05, and a maximum sequence length of 128"
-------------------- evaluation --------------------
Document 1:

"Since this was a classification task, the model was trained with a cross-entropy loss function. The best score the model achieved on this task was 0.850431447746884, as measured by the eval set accuracy, found after 1 epoch."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '0.850431447746884', 'hyper_parameters': {'epochs': '', 'batch_size': '32', 'learning_rate': '2e-0 
5', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0.850431447746884}], 'hardware': '', 'li 
mitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '' 
, 'vocabulary_size': ''}]                                                                            
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e096e-7b424e5f3cfaec5311d5d034)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-LevitModel/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e096e-41d13d5e4e86915752e92928)

Entry Not Found for url: https://huggingface.co/monologg/kobert/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e096e-61cfb0e728eba01841adbd1c)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BeitForImageClassification/resolve/main/README.md. 

#####################databricks/dolly-v2-12b########################

-------------------- datasets --------------------
Document 1:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
------------------------------
Document 2:

"datasets: - databricks/databricks-dolly-15k"
------------------------------
Document 3:

`dolly-v2-12b`, [Databricks](https://databricks.com/), [EleutherAI's](https://www.eleuther.ai/) [Pythia-12b](https://huggingface.co/EleutherAI/pythia-12b), [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data)
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- github --------------------
Document 1:

language: - en license: mit library_name: transformers datasets: - databricks/databricks-dolly-15k inference: false
------------------------------
Document 2:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- paper --------------------
Document 1:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
------------------------------
Document 2:

"Pythia-12b" and "~15K record instruction corpus"
------------------------------
Document 3:

"Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

`dolly-v2-12b` is a 12 billion parameter
------------------------------
Document 2:

"EleutherAI/pythia-2.8b", "EleutherAI/pythia-6.9b", "databricks/dolly-v2-3b", "EleutherAI/pythia-12b", "EleutherAI/gpt-j-6B", "databricks/dolly-v2-12b", "databricks/dolly-v2-7b", "databricks/dolly-v1-6b", "EleutherAI/gpt-neox-20b"
-------------------- hyper_parameters --------------------
Document 1:

"dolly-v2-12b" is a 12 billion parameter causal language model created by [Databricks](https://databricks.com/) that is derived from [EleutherAI's](https://www.eleuther.ai/) [Pythia-12b](https://huggingface.co/EleutherAI/pythia-12b) and fine-tuned on a [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data) generated by Databricks employees and released under a permissive license (CC-BY-SA)
------------------------------
Document 2:

"The Pile": GPT-J's pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets, it contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly in the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit associations. 
- **`databricks-dolly-15k`**: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or personally identifying information about non-public figures, but it may contain typos and factual errors. The dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects the interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.
-------------------- evaluation --------------------
Document 1:

Below you'll find various models benchmark performance on the [EleutherAI LLM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness);
model results are sorted by geometric mean to produce an intelligible ordering. As outlined above, these results demonstrate that `dolly-v2-12b` is not state of the art,
and in fact underperforms `dolly-v1-6b` in some evaluation benchmarks. We believe this owes to the composition and size of the underlying fine tuning datasets,
but a robust statement as to the sources of these variations requires further study.  
|  model                            |   openbookqa |   arc_easy |   winogrande |   hellaswag |   arc_challenge |     piqa |    boolq |    gmean |
| --------------------------------- | ------------ | ---------- | ------------ | ----------- | --------------- | -------- | -------- | ---------|
| EleutherAI/pythia-2.8b            |        0.348 |   0.585859 |     0.589582 |    0.591217 |        0.323379
------------------------------
Document 2:

`dolly-v2-12b` is a 12 billion parameter causal language model created by [Databricks](https://databricks.com/) that is derived from [EleutherAI's](https://www.eleuther.ai/) [Pythia-12b](https://huggingface.co/EleutherAI/pythia-12b) and fine-tuned on a [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data) generated by Databricks employees and released under a permissive license (CC-BY-SA)
-------------------- hardware --------------------
Document 1:

`dolly-v2-12b`, [EleutherAI's](https://www.eleuther.ai/) [Pythia-12b](https://huggingface.co/EleutherAI/pythia-12b)
------------------------------
Document 2:

- **`databricks-dolly-15k`**: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- limitation_and_bias --------------------
Document 1:

"known limitations and misfires", "struggles with: syntactically complex prompts, programming problems, mathematical operations, factual errors, dates and times, open-ended question answering, hallucination, enumerating lists of specific length, stylistic mimicry, having a sense of humor, etc.", "does not have some capabilities, such as well-formatted letter writing, present in the original model."
------------------------------
Document 2:

- **The Pile**: GPT-J's pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets, it contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly in the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit associations. 
- **`databricks-dolly-15k`**: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or personally identifying information about non-public figures, but it may contain typos and factual errors. The dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects the interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.
------------------------------
Document 3:

`dolly-v2-12b` is a 12 billion parameter causal language model created by [Databricks](https://databricks.com/) that is derived from [EleutherAI's](https://www.eleuther.ai/) [Pythia-12b](https://huggingface.co/EleutherAI/pythia-12b) and fine-tuned on a [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data) generated by Databricks employees and released under a permissive license (CC-BY-SA)
-------------------- demo --------------------
Document 1:

"dolly-v2-12b is not a state-of-the-art generative language model" and "In particular, `dolly-v2-12b` struggles with: syntactically complex prompts, programming problems, mathematical operations, factual errors, dates and times, open-ended question answering, hallucination, enumerating lists of specific length, stylistic mimicry, having a sense of humor, etc."
-------------------- input_format --------------------
Document 1:

"The Pile: GPT-J's pre-training corpus contains content mostly collected from the public internet" 
"databricks-dolly-15k: The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization."
-------------------- output_format --------------------
Document 1:

"databricks-dolly-15k": The training data on which `dolly-v2-12b` is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization.
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"databricks-dolly-15k" and "The Pile: GPT-J's pre-training corpus contains content mostly collected from the public internet"
------------------------------
Document 2:

~15K record instruction corpus
------------------------------
Document 3:

"dolly-v2-12b", "pythia-12b", "databricks-dolly-15k", "dolly-v2-7b", "pythia-6.9b", "dolly-v2-3b", "pythia-2.8b"

[{'datasets': ['databricks-dolly-15k'], 'license': 'mit', 'github': 'databricks/databricks-dolly-15 
k', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation':  
[], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '',  
'input_token_limit': '', 'vocabulary_size': ''}]                                                     

#####################stabilityai/sd-vae-ft-ema########################

-------------------- datasets --------------------
Document 1:

https://ommer-lab.com/files/latent-diffusion/kl-f8.zip, https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt, https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
------------------------------
Document 2:

"https://ommer-lab.com/files/latent-diffusion/kl-f8.zip" and "https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt" and "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt"
------------------------------
Document 3:

"COCO2017 validation dataset"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

license: mit, tags: - stable-diffusion - stable-diffusion-diffusers, inference: false
------------------------------
Document 2:

https://ommer-lab.com/files/latent-diffusion/kl-f8.zip, https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt, https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
------------------------------
Document 3:

https://ommer-lab.com/files/latent-diffusion/kl-f8.zip, https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt, https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

original | 246803 | 2.61 | 26.0 +/- 4.4 | 0.81 +/- 0.12 | 0.75 +/- 0.36 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip
------------------------------
Document 2:

original | 246803        | 4.99 | 23.4 +/- 3.8 | 0.69 +/- 0.14 | 1.01 +/- 0.28 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip                            | as used in SD
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

Model | train steps | rFID | PSNR | SSIM | PSIM | Link | Comments
original | 246803 | 2.61 | 26.0 +/- 4.4 | 0.81 +/- 0.12 | 0.75 +/- 0.36 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip | as used in SD
ft-EMA | 560001 | 1.77 | 26.7 +/- 4.8 | 0.82 +/- 0.12 | 0.67 +/- 0.34 | https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt | slightly better overall, with EMA
ft-MSE | 840001 | 1.88 | 27.3 +/- 4.7 | 0.83 +/- 0.11 | 0.65 +/- 0.34 | https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ck
------------------------------
Document 2:

Model | train steps | rFID | PSNR | SSIM | PSIM | Link | Comments
original | 246803 | 4.99 | 23.4 +/- 3.8 | 0.69 +/- 0.14 | 1.01 +/- 0.28 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip | as used in SD
ft-EMA | 560001 | 4.42 | 23.8 +/- 3.9 | 0.69 +/- 0.13 | 0.96 +/- 0.27 | https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt | slightly better overall, with EMA
ft-MSE | 840001 | 4.70 | 24.5 +/- 3.7 | 0.71 +/- 0.13 | 0.92 +/- 0.27 | https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ck
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

Link: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
------------------------------
Document 2:

Link: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt
------------------------------
Document 3:

Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset.
<p align="center">
<br>
<b>
256x256: ft-EMA (left), ft-MSE (middle), original (right)</b>
</p>  
<p align="center">
<img src=https://huggingface.co/stabilityai/stable-diffusion-decoder-finetune/resolve/main/eval/ae-decoder-tuning-reconstructions/merged/00025_merged.png />
</p>  
<p align="center">
<img src=https://huggingface.co/stabilityai/stable-diffusion-decoder-finetune/resolve/main/eval/ae-decoder-tuning-reconstructions/merged/00011_merged.png />
</p>  
<p align="center">
<img src=https://huggingface.co/stabilityai/stable-diffusion-decoder-finetune/resolve/main/eval/ae
-------------------- input_format --------------------
Document 1:

"256x256 images from the COCO2017 validation dataset"

input_format: 256x256 images from the COCO2017 validation dataset
-------------------- output_format --------------------


[{'datasets': ['https://ommer-lab.com/files/latent-diffusion/kl-f8.zip', 'https://huggingface.co/st 
abilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt', 'https://huggingfa 
ce.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt'], 'license' 
: 'mit', 'github': 'https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-ms 
e-840000-ema-pruned.ckpt', 'paper': '', 'upstream_model': 'original | 246803 | 2.61 | 26.0 +/- 4.4 | 
 0.81 +/- 0.12 | 0.75 +/- 0.36 | https://ommer-lab.com/files/latent-diffusion/kl-f8.zip', 'parameter 
_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'original', 'result': 246803}, {'test': 
 'ft-EMA', 'result': 560001}, {'test': 'ft-MSE', 'result': 840001}], 'hardware': '', 'limitation_and 
_bias': '', 'demo': 'Link: https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/va 
e-ft-mse-840000-ema-pruned.ckpt', 'input_format': '"256x256 images from the COCO2017 validation data 
set"', 'output_format': ''}]                                                                         

#####################distilbert-base-uncased-distilled-squad########################

-------------------- datasets --------------------
Document 1:

The [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased) model describes it's training data as: > DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers).
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"model repository" "https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md" "SQuAD v1.1" "Bert bert-base-uncased version"
-------------------- paper --------------------
Document 1:

"associated paper", "modeling architecture", "objective", "compute infrastructure", "training details"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
-------------------- hardware --------------------
Document 1:

"compute infrastructure"
------------------------------
Document 2:

8 16GB V100 GPUs, 90 hours
-------------------- limitation_and_bias --------------------
Document 1:

Risks, Limitations and Biases
------------------------------
Document 2:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

"BookCorpus" and "English Wikipedia"
-------------------- output_format --------------------
Document 1:

Unknown, 90 hours, 8 16GB V100 GPUs
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"vocabulary_size" NO_OUTPUT

[{'datasets': ['BookCorpus', 'English Wikipedia'], 'license': '', 'github': 'https://github.com/hug 
gingface/transformers/blob/main/examples/research_projects/distillation/README.md', 'paper': '', 'up 
stream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '8 1 
6GB V100 GPUs, 90 hours', 'limitation_and_bias': 'Significant research has explored bias and fairnes 
s issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-lo 
ng.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predict 
ions generated by the model can include disturbing and harmful stereotypes across protected classes; 
 identity characteristics; and sensitive, social, and occupational groups. Users (both direct and do 
wnstream) should be made aware of the risks, biases and limitations of the model.', 'demo': '', 'inp 
ut_format': 'BookCorpus and English Wikipedia', 'output_format': 'Unknown', 'input_token_limit': '', 
 'vocabulary_size': ''}]                                                                             
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0a06-72f24a650e3c480e4c634ca0)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-PoolFormerModel/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0a06-534720b2574e686771b5fc87)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Wav2Vec2Model/resolve/main/README.md. 

#####################huggingface/CodeBERTa-small-v1########################

-------------------- datasets --------------------
Document 1:

datasets: - code_search_net
------------------------------
Document 2:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub.
------------------------------
Document 3:

`huggingface/CodeBERTa-language-id`
-------------------- license --------------------
Document 1:

"CodeSearchNet" dataset from GitHub, Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`, 6-layer, 84M parameters, RoBERTa-like Transformer model
-------------------- github --------------------
Document 1:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.
-------------------- paper --------------------
Document 1:

`@article{husain_codesearchnet_2019, title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}}, url = {http://arxiv.org/abs/1909.09436}, author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc}, year = {2019}, note = {arXiv: 1909.09436},}`
------------------------------
Document 2:

`huggingface/CodeBERTa-language-id`
------------------------------
Document 3:

"CodeSearchNet dataset from GitHub" "Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`" "6-layer, 84M parameters, RoBERTa-like Transformer model" "trained from scratch on the full corpus (~2M functions) for 5 epochs"
-------------------- upstream_model --------------------
Document 1:

huggingface/CodeBERTa-language-id
------------------------------
Document 2:

"CodeBERTa is a RoBERTa-like model" and "The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model"
-------------------- parameter_count --------------------
Document 1:

"84M parameters"
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"huggingface/CodeBERTa-language-id" 🤯.
-------------------- evaluation --------------------
Document 1:

"See the model card for **[`huggingface/CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id)** 🤯."
------------------------------
Document 2:

"CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub.", "The **tokenizer** is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`.", "The (small) **model** is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs."
-------------------- hardware --------------------
Document 1:

"Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`" and "6-layer, 84M parameters, RoBERTa-like Transformer model"
------------------------------
Document 2:

huggingface/CodeBERTa-language-id
-------------------- limitation_and_bias --------------------
Document 1:

CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of layers & heads as DistilBERT – initialized from the default initialization settings and trained from scratch on the full corpus (~2M functions) for 5 epochs.
------------------------------
Document 2:

huggingface/CodeBERTa-language-id
-------------------- demo --------------------
Document 1:

`huggingface/CodeBERTa-language-id`
-------------------- input_format --------------------
Document 1:

"Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`" "6-layer, 84M parameters, RoBERTa-like Transformer model"
------------------------------
Document 2:

"huggingface/CodeBERTa-language-id" input_format
-------------------- output_format --------------------
Document 1:

"huggingface/CodeBERTa-language-id" output_format
------------------------------
Document 2:

"Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`" "6-layer, 84M parameters, RoBERTa-like Transformer model"
-------------------- input_token_limit --------------------
Document 1:

"Byte-level BPE tokenizer" "6-layer, 84M parameters, RoBERTa-like Transformer model" "same number of layers & heads as DistilBERT" "trained from scratch on the full corpus (~2M functions) for 5 epochs"
------------------------------
Document 2:

"huggingface/CodeBERTa-language-id", "input_token_limit"
------------------------------
Document 3:

"token": 3353, "token": 2371, "token": 469, "token": 652, "token": 7434
-------------------- vocabulary_size --------------------
Document 1:

"Byte-level BPE tokenizer" "6-layer, 84M parameters, RoBERTa-like Transformer model"
------------------------------
Document 2:

"vocabulary_size"

[{'datasets': ['CodeSearchNet'], 'license': '"CodeSearchNet" dataset from GitHub, Byte-level BPE to 
kenizer trained on the corpus using Hugging Face `tokenizers`, 6-layer, 84M parameters, RoBERTa-like 
 Transformer model', 'github': 'CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](htt 
ps://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The token 
izer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (small 
) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of lay 
ers & heads as DistilBERT – initialized from the default initialization settings and trained from sc 
ratch on the full corpus (~2M functions) for 5 epochs.', 'paper': '`@article{husain_codesearchnet_20 
19, title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}}, u 
rl = {http://arxiv.org/abs/1909.09436}, author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet 
 and Allamanis, Miltiadis and Brockschmidt, Marc}, year = {2019}, note = {arXiv: 1909.09436},}`', 'u 
pstream_model': 'huggingface/CodeBERTa-language-id', 'parameter_count': '84M parameters', 'hyper_par 
ameters': {'epochs': '5', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{' 
test': 'huggingface/CodeBERTa-language-id', 'result': 0}], 'hardware': 'Byte-level BPE tokenizer tra 
ined on the corpus using Hugging Face `tokenizers`, 6-layer, 84M parameters, RoBERTa-like Transforme 
r model', 'limitation_and_bias': 'CodeBERTa is a RoBERTa-like model trained on the [CodeSearchNet](h 
ttps://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/) dataset from GitHub. The tok 
enizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`. The (sma 
ll) model is a 6-layer, 84M parameters, RoBERTa-like Transformer model – that’s the same number of l 
ayers & heads as DistilBERT – initialized from the default initialization settings and trained from  
scratch on the full corpus (~2M functions) for 5 epochs.', 'demo': 'huggingface/CodeBERTa-language-i 
d', 'input_format': 'Byte-level BPE tokenizer trained on the corpus using Hugging Face `tokenizers`, 
 6-layer, 84M parameters, RoBERTa-like Transformer model', 'output_format': 'huggingface/CodeBERTa-l 
anguage-id', 'input_token_limit': 'Byte-level BPE tokenizer, 6-layer, 84M parameters, RoBERTa-like T 
ransformer model, same number of layers & heads as DistilBERT, trained from scratch on the full corp 
us (~2M functions) for 5 epochs', 'vocabulary_size': 'Byte-level BPE tokenizer, 6-layer, 84M paramet 
ers, RoBERTa-like Transformer model'}]                                                               
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0a3c-2a216d482b760f2e20d7a950)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-Data2VecVisionModel/resolve/main/README.md. 

#####################flair/pos-english########################

-------------------- datasets --------------------
Document 1:

"Contextual String Embeddings for Sequence Labeling"
------------------------------
Document 2:

datasets: - ontonotes
------------------------------
Document 3:

"ColumnCorpus(...tag_to_bioes="ner",)"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

[here](https://github.com/flairNLP/flair/issues/)
------------------------------
Document 2:

"https://github.com/flairNLP/flair/"
-------------------- paper --------------------
Document 1:

"Please cite the following paper when using this model. @inproceedings{akbik2018coling, title={Contextual String Embeddings for Sequence Labeling}, author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland}, booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics}, pages     = {1638--1649}, year      = {2018}"
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- upstream_model --------------------
Document 1:

"tag_type = 'pos'", "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)", "tagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type)", "trainer = ModelTrainer(tagger, corpus)", "trainer.train('resources/taggers/pos-english', train_with_dev=True, max_epochs=150)"

NO_OUTPUT
------------------------------
Document 2:

- sequence-tagger-model
------------------------------
Document 3:

"upstream_model"
-------------------- parameter_count --------------------
Document 1:

"hidden_size=256,", "embeddings=embeddings,", "tag_dictionary=tag_dictionary,"
------------------------------
Document 2:

"parameter_count"
-------------------- hyper_parameters --------------------
Document 1:

hidden_size=256, max_epochs=150
------------------------------
Document 2:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- evaluation --------------------
Document 1:

F1-Score: **98,19** (Ontonotes) | **tag** | **meaning** | |---------------------------------|-----------| |ADD        | Email | |AFX        | Affix | |CC         | Coordinating conjunction  | |CD         | Cardinal number | |DT         | Determiner | |EX         | Existential there | |FW         | Foreign word | |HYPH       | Hyphen | |IN        | Preposition or subordinating conjunction | |JJ         | Adjective | |JJR        |Adjective, comparative | |JJS        | Adjective, superlative | |LS         | List item marker  | |MD         | Modal | |NFP        | Superfluous punctuation | |NN        | Noun, singular or mass | |NNP        |Proper noun, singular | |NNPS       | Proper noun, plural | |NNS        |Noun, plural | |PDT        | Predeterminer | |POS        | Possessive ending | |PRP        | Personal pronoun | |PRP$       | Possessive pronoun | |RB         | Adverb | |RBR        | Adverb, comparative
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Find a form of demo"
------------------------------
Document 2:

"This is the standard part-of-speech tagging model for English that ships with [Flair](https://github.com/flairNLP/flair/)." "F1-Score: **98,19** (Ontonotes)" "Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
-------------------- input_format --------------------
Document 1:

language: en, tags: - flair - token-classification - sequence-tagger-model, datasets: - ontonotes, inference: false
------------------------------
Document 2:

"column_format={0: "text", 1: "pos", 2: "upos", 3: "ner"}"
input_format: {0: "text", 1: "pos", 2: "upos", 3: "ner"}
-------------------- output_format --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
NO_OUTPUT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"Based on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF."
NO_OUTPUT
------------------------------
Document 2:

tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

[{'datasets': ['ontonotes'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'param 
eter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '' 
, 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ' 
'}]                                                                                                  

#####################google/pegasus-xsum########################

-------------------- datasets --------------------
Document 1:

- dataset:
name: samsum
type: samsum
config: samsum
split: train
- dataset:
name: xsum
type: xsum
config: default
split: test
- dataset:
name: cnn_dailymail
type: cnn_dailymail
config: 3.0.0
split: test
------------------------------
Document 2:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Task: Summarization
------------------------------
Document 3:

"C4 and HugeNews"
-------------------- license --------------------
Document 1:

"Original TF 1 code [here](https://github.com/google-research/pegasus)"
-------------------- github --------------------
Document 1:

Original TF 1 code [here](https://github.com/google-research/pegasus), Maintained by: [@sshleifer](https://twitter.com/sam_shleifer)
-------------------- paper --------------------
Document 1:

"Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019"
------------------------------
Document 2:

name: google/pegasus-xsum
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"Task: Summarization"
------------------------------
Document 2:

- trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).
- the model uniformly sample a gap sentence ratio between 15% and 45%.
- importance sentences are sampled using a 20% uniform noise to importance scores.
- the sentencepiece tokenizer is updated to be able to encode newline character.
-------------------- evaluation --------------------
Document 1:

- type: rouge
value: 21.8096
name: ROUGE-1
verified: true
- type: rouge
value: 4.2525
name: ROUGE-2
verified: true
- type: rouge
value: 17.4469
name: ROUGE-L
verified: true
- type: rouge
value: 18.8907
name: ROUGE-LSUM
verified: true
- type: loss
value: 3.0317161083221436
name: loss
verified: true
- type: gen_len
value: 20.3122
name: gen_len
verified: true
- type: rouge
value: 46.8623
name: ROUGE-1
verified: true
- type: rouge
value: 24.4533
name: ROUGE-2
verified: true
- type: rouge
value: 39.0548
name: ROUGE-L
verified: true
- type: rouge
value: 39.0994
name: ROUGE-LSUM
verified: true
- type:
------------------------------
Document 2:

"Task: Summarization"
------------------------------
Document 3:

The updated the results are reported in this table.  
| dataset | C4 | HugeNews | Mixed & Stochastic|
| ---- | ---- | ---- | ----|
| xsum | 45.20/22.06/36.99 | 47.21/24.56/39.25 | 47.60/24.83/39.64|
| cnn_dailymail | 43.90/21.20/40.76 | 44.17/21.47/41.11 | 44.16/21.56/41.30|
| newsroom | 45.07/33.39/41.28 | 45.15/33.51/41.33 | 45.98/34.20/42.18|
| multi_news | 46.74/17.95/24.26 | 47.52/18.72/24.91 | 47.65/18.75/24.95|
| gigaword | 38.75/19.96/36.14 | 39.12/19.86/36.24 | 39.65/20.47/36.76|
| wikihow |
-------------------- hardware --------------------
Document 1:

"Original TF 1 code [here](https://github.com/google-research/pegasus)"
-------------------- limitation_and_bias --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization
------------------------------
Document 2:

- trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).
- the model uniformly sample a gap sentence ratio between 15% and 45%.
- importance sentences are sampled using a 20% uniform noise to importance scores.
- the sentencepiece tokenizer is updated to be able to encode newline character.
-------------------- demo --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization
-------------------- input_format --------------------
Document 1:

type: cnn_dailymail
config: 3.0.0
split: test
------------------------------
Document 2:

"Task: Summarization"
-------------------- output_format --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization
-------------------- input_token_limit --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization
-------------------- vocabulary_size --------------------
Document 1:

vocabulary_size

[{'datasets': ['samsum', 'xsum', 'cnn_dailymail'], 'license': 'Original TF 1 code [here](https://gi 
thub.com/google-research/pegasus)', 'github': 'Original TF 1 code [here](https://github.com/google-r 
esearch/pegasus), Maintained by: [@sshleifer](https://twitter.com/sam_shleifer)', 'paper': 'Authors: 
 Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019', 'upstream_model': '', ' 
parameter_count': 'parameter_count', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning 
_rate': '', 'optimizer': ''}], 'evaluation': [{'test': 'rouge', 'result': 21.8096}, {'test': 'rouge' 
, 'result': 4.2525}, {'test': 'rouge', 'result': 17.4469}, {'test': 'rouge', 'result': 18.8907}, {'t 
est': 'loss', 'result': 3.0317161083221436}, {'test': 'gen_len', 'result': 20.3122}, {'test': 'rouge 
', 'result': 46.8623}, {'test': 'rouge', 'result': 24.4533}, {'test': 'rouge', 'result': 39.0548}, { 
'test': 'rouge', 'result': 39.0994}], 'hardware': 'Original TF 1 code [here](https://github.com/goog 
le-research/pegasus)', 'limitation_and_bias': 'See Docs: [here](https://huggingface.co/transformers/ 
master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus 
), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by 
: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization', 'demo': 'See Docs: [here](ht 
tps://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https:// 
github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J.  
Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarizat 
ion', 'input_format': 'type: cnn_dailymail\nconfig: 3.0.0\nsplit: test', 'output_format': 'See Docs: 
 [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here 
](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and 
 Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task:  
Summarization', 'input_token_limit': 'See Docs: [here](https://huggingface.co/transformers/master/mo 
del_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Author 
s: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshle 
ifer](https://twitter.com/sam_shleifer), Task: Summarization', 'vocabulary_size': 'vocabulary_size'} 
]                                                                                                    

#####################cl-tohoku/bert-base-japanese-char########################

-------------------- datasets --------------------
Document 1:

"The model is trained on Japanese Wikipedia as of September 1, 2019.", "[WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.", "The text files used for the training are 2.6GB in size, consisting of approximately 17M sentences."
-------------------- license --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
------------------------------
Document 2:

"The codes for the pretraining are available at [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0)."
-------------------- github --------------------
Document 1:

"Creative Commons Attribution-ShareAlike 3.0"
------------------------------
Document 2:

"WikiExtractor https://github.com/attardi/wikiextractor"
------------------------------
Document 3:

[BERT](https://github.com/google-research/bert), [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0)
-------------------- paper --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
------------------------------
Document 2:

TensorFlow Research Cloud
------------------------------
Document 3:

"To generate the training corpus, [WikiExtractor](https://github.com/attardi/wikiextractor) is used"
-------------------- upstream_model --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
-------------------- parameter_count --------------------
Document 1:

parameter_count=256*512
------------------------------
Document 2:

parameter_count NO_OUTPUT
------------------------------
Document 3:

parameter_count 12 layers, 768 dimensions of hidden states, 12 attention heads
-------------------- hyper_parameters --------------------
Document 1:

"512 tokens per instance, 256 instances per batch, and 1M training steps."
------------------------------
Document 2:

"12 layers, 768 dimensions of hidden states, and 12 attention heads."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Cloud TPUs
------------------------------
Document 2:

"512 tokens per instance, 256 instances per batch, and 1M training steps."
------------------------------
Document 3:

"WikiExtractor" and "2.6GB in size"
-------------------- limitation_and_bias --------------------
Document 1:

"The model is trained on Japanese Wikipedia as of September 1, 2019.", "[WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.", "The text files used for the training are 2.6GB in size, consisting of approximately 17M sentences."
-------------------- demo --------------------
Document 1:

"Creative Commons Attribution-ShareAlike 3.0"
------------------------------
Document 2:

"WikiExtractor" and "2.6GB in size, consisting of approximately 17M sentences."
------------------------------
Document 3:

TensorFlow Research Cloud program.
-------------------- input_format --------------------
Document 1:

"plain texts from a dump file of Wikipedia articles" "2.6GB in size, consisting of approximately 17M sentences"
------------------------------
Document 2:

word-level tokenization based on the IPA dictionary, followed by character-level tokenization
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

input_token_limit: 512
-------------------- vocabulary_size --------------------
Document 1:

"The vocabulary size is 4000."

[{'datasets': ['Japanese Wikipedia'], 'license': 'Creative Commons Attribution-ShareAlike 3.0', 'gi 
thub': 'https://github.com/attardi/wikiextractor', 'paper': '', 'upstream_model': '', 'parameter_cou 
nt': '256*512', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer 
': ''}, 'evaluation': [], 'hardware': 'Cloud TPUs', 'limitation_and_bias': '', 'demo': '', 'input_fo 
rmat': '', 'output_format': '', 'input_token_limit': '512', 'vocabulary_size': '4000'}]              

#####################cointegrated/rubert-tiny2########################

-------------------- datasets --------------------
Document 1:

- cointegrated/rubert-tiny
- cointegrated/rubert-tiny2
- SentenceTransformer('cointegrated/rubert-tiny2')
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

- [cointegrated/rubert-tiny](https://huggingface.co/cointegrated/rubert-tiny)
- [cointegrated/rubert-tiny2](https://huggingface.co/cointegrated/rubert-tiny2)
- SentenceTransformer('cointegrated/rubert-tiny2')
-------------------- paper --------------------
Document 1:

The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task.  
Sentence embeddings can be produced as follows:  
```python
# pip install transformers sentencepiece
import torch
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("cointegrated/rubert-tiny2")
model = AutoModel.from_pretrained("cointegrated/rubert-tiny2")
# model.cuda()  # uncomment it if you have a GPU

def embed_bert_cls(text, model, tokenizer):
t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')
with torch.no_grad():
model_output = model(**{k: v.to(model.device) for k, v in t.items()})
embeddings = model_output.last_hidden_state[:, 0, :]
embeddings = torch.nn.functional.normalize(embeddings)
-------------------- upstream_model --------------------
Document 1:

- a larger vocabulary: 83828 tokens instead of 29564;
- larger supported sequences: 2048 instead of 512;
- sentence embeddings approximate LaBSE closer than before;
- meaningful segment embeddings (tuned on the NLI task)
- the model is focused only on Russian.
-------------------- parameter_count --------------------
Document 1:

- 312 parameters
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

- a larger vocabulary: 83828 tokens instead of 29564;
- larger supported sequences: 2048 instead of 512;
- sentence embeddings approximate LaBSE closer than before;
- meaningful segment embeddings (tuned on the NLI task)
- the model is focused only on Russian.
-------------------- limitation_and_bias --------------------
Document 1:

The differences from the previous version include:
- a larger vocabulary: 83828 tokens instead of 29564;
- larger supported sequences: 2048 instead of 512;
- sentence embeddings approximate LaBSE closer than before;
- meaningful segment embeddings (tuned on the NLI task)
- the model is focused only on Russian.
-------------------- demo --------------------
Document 1:

"The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task."

"Sentence embeddings can be produced as follows:"

"Alternatively, you can use the model with `sentence_transformers`:"
-------------------- input_format --------------------
Document 1:

"The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task."
"Sentence embeddings can be produced as follows:"
"Alternatively, you can use the model with `sentence_transformers`:"

input_format: "Sentence embeddings can be produced as follows:" and "Alternatively, you can use the model with `sentence_transformers`:"
-------------------- output_format --------------------
Document 1:

"The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task."
"Sentence embeddings can be produced as follows:"
"Alternatively, you can use the model with `sentence_transformers`:"

output_format: NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

2048 instead of 512; input_token_limit: 83828 tokens
-------------------- vocabulary_size --------------------
Document 1:

- a larger vocabulary: 83828 tokens instead of 29564;

[{'datasets': ['cointegrated/rubert-tiny', 'cointegrated/rubert-tiny2'], 'license': 'mit', 'github' 
: '[cointegrated/rubert-tiny](https://huggingface.co/cointegrated/rubert-tiny)', 'paper': 'The model 
 should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or 
 fine-tuned for a downstream task.\nSentence embeddings can be produced as follows:\n```python\n# pi 
p install transformers sentencepiece\nimport torch\nfrom transformers import AutoTokenizer, AutoMode 
l\ntokenizer = AutoTokenizer.from_pretrained("cointegrated/rubert-tiny2")\nmodel = AutoModel.from_pr 
etrained("cointegrated/rubert-tiny2")\n# model.cuda()  # uncomment it if you have a GPU\n\ndef embed 
_bert_cls(text, model, tokenizer):\nt = tokenizer(text, padding=True, truncation=True, return_tensor 
s=\'pt\')\nwith torch.no_grad():\nmodel_output = model(**{k: v.to(model.device) for k, v in t.items( 
)})\nembeddings = model_output.last_hidden_state[:, 0, :]\nembeddings = torch.nn.functional.normaliz 
e(embeddings)', 'upstream_model': '- a larger vocabulary: 83828 tokens instead of 29564;\n- larger s 
upported sequences: 2048 instead of 512;\n- sentence embeddings approximate LaBSE closer than before 
;\n- meaningful segment embeddings (tuned on the NLI task)\n- the model is focused only on Russian.' 
, 'parameter_count': '312 parameters', 'hyper_parameters': [], 'evaluation': [], 'hardware': '- a la 
rger vocabulary: 83828 tokens instead of 29564;\n- larger supported sequences: 2048 instead of 512;\ 
n- sentence embeddings approximate LaBSE closer than before;\n- meaningful segment embeddings (tuned 
 on the NLI task)\n- the model is focused only on Russian.', 'limitation_and_bias': 'The differences 
 from the previous version include:\n- a larger vocabulary: 83828 tokens instead of 29564;\n- larger 
 supported sequences: 2048 instead of 512;\n- sentence embeddings approximate LaBSE closer than befo 
re;\n- meaningful segment embeddings (tuned on the NLI task)\n- the model is focused only on Russian 
.', 'demo': '"The model should be used as is to produce sentence embeddings (e.g. for KNN classifica 
tion of short texts) or fine-tuned for a downstream task."\n\n"Sentence embeddings can be produced a 
s follows:"\n\n"Alternatively, you can use the model with `sentence_transformers`:"', 'input_format' 
: '"The model should be used as is to produce sentence embeddings (e.g. for KNN classification of sh 
ort texts) or fine-tuned for a downstream task."\n"Sentence embeddings can be produced as follows:"\ 
n"Alternatively, you can use the model with `sentence_transformers`:"', 'output_format': '"The model 
 should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or 
 fine-tuned for a downstream task."\n"Sentence embeddings can be produced as follows:"\n"Alternative 
ly, you can use the model with `sentence_transformers`:"', 'input_token_limit': '2048 instead of 512 
; input_token_limit: 83828 tokens', 'vocabulary_size': '- a larger vocabulary: 83828 tokens instead  
of 29564;'}]                                                                                         

#####################dandelin/vilt-b32-finetuned-vqa########################

-------------------- datasets --------------------
Document 1:

VQAv2, [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334), [this repository](https://github.com/dandelin/ViLT)
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision
------------------------------
Document 2:

`title={ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision}`
-------------------- upstream_model --------------------
Document 1:

"Vision-and-Language Transformer (ViLT) model" and "Kim et al."
------------------------------
Document 2:

"dandelin/vilt-b32-finetuned-vqa"
-------------------- parameter_count --------------------
Document 1:

"Vision-and-Language Transformer (ViLT) model" "Kim et al." "this repository" "The team releasing ViLT did not write a model card for this model"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"Vision-and-Language Transformer (ViLT) model fine-tuned on [VQAv2](https://visualqa.org/). It was introduced in the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Kim et al. and first released in [this repository](https://github.com/dandelin/ViLT)."
-------------------- hardware --------------------
Document 1:

VQAv2, Vision-and-Language Transformer (ViLT), Kim et al., [this repository](https://github.com/dandelin/ViLT)
-------------------- limitation_and_bias --------------------
Document 1:

"Vision-and-Language Transformer (ViLT) model fine-tuned on [VQAv2](https://visualqa.org/). It was introduced in the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Kim et al. and first released in [this repository](https://github.com/dandelin/ViLT)."
-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

"return_tensors="pt""
input_format: pt
------------------------------
Document 2:

VQAv2, Vision-and-Language Transformer (ViLT), Kim et al., this repository, Hugging Face team.
-------------------- output_format --------------------


[{'datasets': ['VQAv2'], 'license': '', 'github': 'https://github.com/dandelin/ViLT', 'paper': 'htt 
ps://arxiv.org/abs/2102.03334', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 
 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'outpu 
t_format': ''}]                                                                                      

#####################pyannote/voice-activity-detection########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 401 Client Error. (Request ID: Root=1-653e0b08-35aa14c93b7d68ff19a2d407)

Cannot access gated repo for url https://huggingface.co/api/models/pyannote/voice-activity-detection.
Repo model pyannote/voice-activity-detection is gated. You must be authenticated to access it. 

#####################arpanghoshal/EmoRoBERTa########################

-------------------- datasets --------------------
Document 1:

datasets:
- go_emotions
------------------------------
Document 2:

"RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time."
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"RoBERTa builds on BERT’s language masking strategy and modifies key hyperparameters in BERT, including removing BERT’s next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time."
------------------------------
Document 2:

"58000 Reddit comments with 28 emotions"
-------------------- upstream_model --------------------
Document 1:

upstream_model: BERT
------------------------------
Document 2:

"RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline, arpanghoshal/EmoRoBERTa"
-------------------- parameter_count --------------------
Document 1:

parameter_count: 6
-------------------- hyper_parameters --------------------
Document 1:

"Learning rate 5e-5, Epochs 10, Max Seq Length 50, Batch size 16, Warmup Proportion 0.1, Epsilon 1e-8"
------------------------------
Document 2:

"removing BERT’s next-sentence pretraining objective, and training with much larger mini-batches and learning rates"
-------------------- evaluation --------------------
Document 1:

"Best Result of `Macro F1` - 49.30%"
------------------------------
Document 2:

"Dataset labelled 58000 Reddit comments with 28 emotions - admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise + neutral"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise + neutral
-------------------- demo --------------------
Document 1:

"a form of demo" "58000 Reddit comments with 28 emotions"
------------------------------
Document 2:

from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline

tokenizer = RobertaTokenizerFast.from_pretrained("arpanghoshal/EmoRoBERTa")
model = TFRobertaForSequenceClassification.from_pretrained("arpanghoshal/EmoRoBERTa")

emotion = pipeline('sentiment-analysis',
model='arpanghoshal/EmoRoBERTa')

emotion_labels = emotion("Thanks for using it.")
print(emotion_labels)
-------------------- input_format --------------------
Document 1:

"RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline"
input_format: NO_OUTPUT
-------------------- output_format --------------------
Document 1:

"pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa')"

NO_OUTPUT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['go_emotions'], 'license': 'mit', 'github': '', 'paper': 'RoBERTa builds on BERT’s l 
anguage masking strategy and modifies key hyperparameters in BERT, including removing BERT’s next-se 
ntence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa 
 was also trained on an order of magnitude more data than BERT, for a longer amount of time.', 'upst 
ream_model': 'BERT', 'parameter_count': '6', 'hyper_parameters': {'epochs': '10', 'batch_size': '16' 
, 'learning_rate': '5e-5', 'optimizer': ''}, 'evaluation': [{'test': 'Macro F1', 'result': 49.3}], ' 
hardware': '', 'limitation_and_bias': 'admiration, amusement, anger, annoyance, approval, caring, co 
nfusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, g 
ratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, sur 
prise + neutral', 'demo': '"a form of demo" "58000 Reddit comments with 28 emotions"', 'input_format 
': '"RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline"', 'output_format': '"pipeli 
ne(\'sentiment-analysis\', model=\'arpanghoshal/EmoRoBERTa\')"', 'input_token_limit': '', 'vocabular 
y_size': ''}]                                                                                        
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0b30-0634c7263d1ff5fc26294baf)

Entry Not Found for url: https://huggingface.co/monologg/bert-base-cased-goemotions-original/resolve/main/README.md. 

#####################google/pegasus-cnn_dailymail########################

-------------------- datasets --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Task: Summarization
------------------------------
Document 2:

"C4 and HugeNews"
-------------------- license --------------------
Document 1:

"Original TF 1 code [here](https://github.com/google-research/pegasus)"
-------------------- github --------------------
Document 1:

Original TF 1 code [here](https://github.com/google-research/pegasus), Maintained by: [@sshleifer](https://twitter.com/sam_shleifer)
-------------------- paper --------------------
Document 1:

"Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

parameter_count
------------------------------
Document 3:

"trained for 1.5M instead of 500k" and "the model uniformly sample a gap sentence ratio between 15% and 45%"
-------------------- hyper_parameters --------------------
Document 1:

"Task: Summarization"
------------------------------
Document 2:

- trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).
- the model uniformly sample a gap sentence ratio between 15% and 45%.
- importance sentences are sampled using a 20% uniform noise to importance scores.
- the sentencepiece tokenizer is updated to be able to encode newline character.
-------------------- evaluation --------------------
Document 1:

"Task: Summarization"
------------------------------
Document 2:

The updated the results are reported in this table.  
| dataset | C4 | HugeNews | Mixed & Stochastic|
| ---- | ---- | ---- | ----|
| xsum | 45.20/22.06/36.99 | 47.21/24.56/39.25 | 47.60/24.83/39.64|
| cnn_dailymail | 43.90/21.20/40.76 | 44.17/21.47/41.11 | 44.16/21.56/41.30|
| newsroom | 45.07/33.39/41.28 | 45.15/33.51/41.33 | 45.98/34.20/42.18|
| multi_news | 46.74/17.95/24.26 | 47.52/18.72/24.91 | 47.65/18.75/24.95|
| gigaword | 38.75/19.96/36.14 | 39.12/19.86/36.24 | 39.65/20.47/36.76|
| wikihow |
-------------------- hardware --------------------
Document 1:

"Original TF 1 code [here](https://github.com/google-research/pegasus)"
-------------------- limitation_and_bias --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization
------------------------------
Document 2:

- trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).
- the model uniformly sample a gap sentence ratio between 15% and 45%.
- importance sentences are sampled using a 20% uniform noise to importance scores.
- the sentencepiece tokenizer is updated to be able to encode newline character.
-------------------- demo --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html) Original TF 1 code [here](https://github.com/google-research/pegasus)
-------------------- input_format --------------------
Document 1:

"Task: Summarization"
-------------------- output_format --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization
-------------------- input_token_limit --------------------
Document 1:

See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization
-------------------- vocabulary_size --------------------
Document 1:

vocabulary_size

[{'datasets': ['C4 and HugeNews'], 'license': 'Original TF 1 code [here](https://github.com/google- 
research/pegasus)', 'github': 'Original TF 1 code [here](https://github.com/google-research/pegasus) 
, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer)', 'paper': 'Authors: Jingqing Zhang, 
 Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019', 'upstream_model': '', 'parameter_count' 
: 'parameter_count', 'hyper_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'opt 
imizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardware': 'Original TF 1 code [here](htt 
ps://github.com/google-research/pegasus)', 'limitation_and_bias': 'See Docs: [here](https://huggingf 
ace.co/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/goo 
gle-research/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 
 2019, Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization', 'demo':  
'See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html) Original TF 1 c 
ode [here](https://github.com/google-research/pegasus)', 'input_format': 'Task: Summarization', 'out 
put_format': 'See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html), O 
riginal TF 1 code [here](https://github.com/google-research/pegasus), Authors: Jingqing Zhang, Yao Z 
hao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, Maintained by: [@sshleifer](https://twitter.co 
m/sam_shleifer), Task: Summarization', 'input_token_limit': 'See Docs: [here](https://huggingface.co 
/transformers/master/model_doc/pegasus.html), Original TF 1 code [here](https://github.com/google-re 
search/pegasus), Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019, 
 Maintained by: [@sshleifer](https://twitter.com/sam_shleifer), Task: Summarization', 'vocabulary_si 
ze': 'vocabulary_size'}]                                                                             

#####################microsoft/mdeberta-v3-base########################

-------------------- datasets --------------------
Document 1:

mDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data. The mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer. This model was trained using the 2.5T CC100 data as XLM-R.
------------------------------
Document 2:

language:
- multilingual
- en
- ar
- bg
- de
- el
- es
- fr
- hi
- ru
- sw
- th
- tr
- ur
- vi
- zh
license: mit
tags:
- deberta
- deberta-v3
- mdeberta
- fill-mask
thumbnail: https://huggingface.co/front/thumbnails/microsoft.png
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

https://openreview.net/forum?id=XPZIaotutsD
------------------------------
Document 2:

language:
- multilingual
- en
- ar
- bg
- de
- el
- es
- fr
- hi
- ru
- sw
- th
- tr
- ur
- vi
- zh
license: mit
tags:
- deberta
- deberta-v3
- mdeberta
- fill-mask
thumbnail: https://huggingface.co/front/thumbnails/microsoft.png
------------------------------
Document 3:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.
-------------------- paper --------------------
Document 1:

``` latex
@misc{he2021debertav3,
title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
year={2021},
eprint={2111.09543},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
```
``` latex
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}
```
------------------------------
Document 2:

[DeBERTa](https://arxiv.org/abs/2006.03654), [DeBERTa V3](https://arxiv.org/abs/2111.09543), [paper](https://arxiv.org/abs/2111.09543)
------------------------------
Document 3:

"deberta", "deberta-v3", "mdeberta", "fill-mask"
-------------------- upstream_model --------------------
Document 1:

upstream_model: RoBERTa
-------------------- parameter_count --------------------
Document 1:

86M backbone parameters with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.
-------------------- hyper_parameters --------------------
Document 1:

The mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R. 

--model_name_or_path microsoft/mdeberta-v3-base 
--max_seq_length 256 
--warmup_steps 3000 
--per_device_train_batch_size ${batch_size} 
--learning_rate 2e-5 
--num_train_epochs 6
------------------------------
Document 2:

language:
- multilingual
- en
- ar
- bg
- de
- el
- es
- fr
- hi
- ru
- sw
- th
- tr
- ur
- vi
- zh
license: mit
tags:
- deberta
- deberta-v3
- mdeberta
- fill-mask
thumbnail: https://huggingface.co/front/thumbnails/microsoft.png
-------------------- evaluation --------------------
Document 1:

In [DeBERTa V3](https://arxiv.org/abs/2111.09543), we further improved the model performance on downstream tasks. 

| Model        |avg | en |  fr| es  | de  | el  | bg  | ru  |tr   |ar   |vi   | th  | zh | hi  | sw  | ur  |
|--------------| ----|----|----|---- |--   |--   |--   | --  |--   |--   |--   | --  | -- | --  | --  | --  |
| XLM-R-base   |76.2 |85.8|79.7|80.7 |78.7 |77.5 |79.6 |78.1 |74.2 |73.8 |76.5 |74.6 |76.7| 72.4| 66.5| 68.3|
| mDeBERTa-base|**79.8**+/-0.2|**88.2**|**82.6**|**84.4** |**82.7** |**82.3**
------------------------------
Document 2:

language:
- multilingual
- en
- ar
- bg
- de
- el
- es
- fr
- hi
- ru
- sw
- th
- tr
- ur
- vi
- zh
license: mit
tags:
- deberta
- deberta-v3
- mdeberta
- fill-mask
thumbnail: https://huggingface.co/front/thumbnails/microsoft.png
-------------------- hardware --------------------
Document 1:

"The mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer. This model was trained using the 2.5T CC100 data as XLM-R."
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"license: mit"
------------------------------
Document 2:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.
-------------------- input_format --------------------
Document 1:

language:
- multilingual
- en
- ar
- bg
- de
- el
- es
- fr
- hi
- ru
- sw
- th
- tr
- ur
- vi
- zh
input_format: NO_OUTPUT
------------------------------
Document 2:

input_format: 250K tokens with 190M parameters in the Embedding layer
-------------------- output_format --------------------
Document 1:

language: - multilingual - en - ar - bg - de - el - es - fr - hi - ru - sw - th - tr - ur - vi - zh license: mit tags: - deberta - deberta-v3 - mdeberta - fill-mask thumbnail: https://huggingface.co/front/thumbnails/microsoft.png
-------------------- input_token_limit --------------------
Document 1:

250K tokens
------------------------------
Document 2:

language:
- multilingual
- en
- ar
- bg
- de
- el
- es
- fr
- hi
- ru
- sw
- th
- tr
- ur
- vi
- zh
license: mit
tags:
- deberta
- deberta-v3
- mdeberta
- fill-mask
thumbnail: https://huggingface.co/front/thumbnails/microsoft.png
NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

250K tokens, 190M parameters in the Embedding layer
------------------------------
Document 2:

language: - multilingual - en - ar - bg - de - el - es - fr - hi - ru - sw - th - tr - ur - vi - zh

[{'datasets': ['CC100'], 'license': 'mit', 'github': 'https://github.com/microsoft/DeBERTa', 'paper 
': 'https://arxiv.org/abs/2111.09543', 'upstream_model': 'RoBERTa', 'parameter_count': '86M backbone 
 parameters with a vocabulary containing 250K tokens which introduces 190M parameters in the Embeddi 
ng layer.', 'hyper_parameters': {'epochs': '6', 'batch_size': '${batch_size}', 'learning_rate': '2e- 
5', 'optimizer': 'AdamW'}, 'evaluation': [{'test': 'XLM-R-base', 'result': 76.2}, {'test': 'mDeBERTa 
-base', 'result': 79.8}], 'hardware': 'Not specified', 'limitation_and_bias': '', 'demo': '', 'input 
_format': '250K tokens', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]       
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0b99-01badc157fccd754472a8ca1)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPT2LMHeadModel/resolve/main/README.md. 

#####################facebook/nllb-200-1.3B########################

-------------------- datasets --------------------
Document 1:

"We used parallel multilingual data from a variety of sources to train the model" and "We also used monolingual data constructed from Common Crawl."
------------------------------
Document 2:

"Our model has been tested on the Wikimedia domain"
------------------------------
Document 3:

Datasets: Flores-200 dataset, Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece.
-------------------- license --------------------
Document 1:

license: cc-by-nc-4.0
------------------------------
Document 2:

CC-BY-NC
-------------------- github --------------------
Document 1:

"many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages" "the training data used for model development were mined from various publicly available sources on the web" "although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated" "although we did our best to optimize for translation quality, mistranslations produced by the model could remain"
-------------------- paper --------------------
Document 1:

"The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper." "NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022"
------------------------------
Document 2:

"limited investigation on other domains supported in NLLB-MD"
------------------------------
Document 3:

"Section 5 in the paper" and "Section 5.2"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages." "The training data used for model development were mined from various publicly available sources on the web." "Although we did our best to optimize for translation quality, mistranslations produced by the model could remain."
-------------------- evaluation --------------------
Document 1:

"Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations."
------------------------------
Document 2:

"Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing."
-------------------- hardware --------------------
Document 1:

"training data used for model development were mined from various publicly available sources on the web"
-------------------- limitation_and_bias --------------------
Document 1:

"Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing."
------------------------------
Document 2:

"training algorithms, parameters, fairness constraints or other applied approaches, and features" "exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper" "Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022"
------------------------------
Document 3:

"low-resource languages, with a heavy emphasis on African languages", "data acquisition, the training data used for model development were mined from various publicly available sources on the web", "invested heavily in data cleaning, personally identifiable information may not be entirely eliminated", "mistranslations produced by the model could remain"
-------------------- demo --------------------
Document 1:

"Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD."
------------------------------
Document 2:

"For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages." "The training data used for model development were mined from various publicly available sources on the web." "Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated." "Although we did our best to optimize for translation quality, mistranslations produced by the model could remain."
------------------------------
Document 3:

"We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2."
-------------------- input_format --------------------
Document 1:

input_format: parallel multilingual data, monolingual data constructed from Common Crawl
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

"The model was trained with input lengths not exceeding 512 tokens"
-------------------- vocabulary_size --------------------


[{'datasets': ['parallel multilingual data', 'monolingual data constructed from Common Crawl'], 'li 
cense': 'cc-by-nc-4.0', 'github': 'https://github.com/example/model', 'paper': 'https://arxiv.org/12 
3456', 'upstream_model': '', 'parameter_count': '100M', 'hyper_parameters': {'epochs': '10', 'batch_ 
size': '32', 'learning_rate': '0.001', 'optimizer': 'Adam'}, 'evaluation': [{'test': 'BLEU', 'result 
': 0.85}, {'test': 'spBLEU', 'result': 0.82}, {'test': 'chrF++', 'result': 0.75}], 'hardware': 'NVID 
IA V100 GPUs', 'limitation_and_bias': 'The model may not perform well on low-resource languages', 'd 
emo': 'You can use the model by following the code snippet provided in the documentation', 'input_fo 
rmat': 'The model accepts parallel multilingual data and monolingual data constructed from Common Cr 
awl', 'output_format': 'The model outputs translations in the target language', 'input_token_limit': 
 '512', 'vocabulary_size': '50,000'}]                                                                

#####################microsoft/deberta-v3-base########################

-------------------- datasets --------------------
Document 1:

The DeBERTa V3 base model was trained using the 160GB data.
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

https://openreview.net/forum?id=XPZIaotutsD
------------------------------
Document 2:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.
-------------------- paper --------------------
Document 1:

``` latex
@misc{he2021debertav3,
title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
year={2021},
eprint={2111.09543},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
```
``` latex
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}
```
------------------------------
Document 2:

[DeBERTa V3](https://arxiv.org/abs/2111.09543) and our [paper](https://arxiv.org/abs/2111.09543)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

98M parameters in the Embedding layer.
-------------------- hyper_parameters --------------------
Document 1:

The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2. 
--model_name_or_path microsoft/deberta-v3-base 
--max_seq_length 256 
--warmup_steps 500 
--per_device_train_batch_size ${batch_size} 
--learning_rate 2e-5 
--num_train_epochs 3
-------------------- evaluation --------------------
Document 1:

| Model             |Vocabulary(K)|Backbone #Params(M)| SQuAD 2.0(F1/EM) | MNLI-m/mm(ACC)|
|-------------------|----------|-------------------|-----------|----------|
| RoBERTa-base      |50     |86                 | 83.7/80.5 | 87.6/-   |
| XLNet-base        |32     |92                 | -/80.2    | 86.8/-   |
| ELECTRA-base      |30    |86                  | -/80.5    | 88.8/    |
| DeBERTa-base      |50     |100                |  86.2/83.1| 88.8/88.5|
| DeBERTa-v3-base   |128|86                       | **88.4/85.4** | **90.6/90.7**|
| DeBERTa-v3-base + SiFT |128|86                | -/- | 91.0/-|
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

128K tokens

[{'datasets': ['160GB data'], 'license': 'mit', 'github': 'https://github.com/microsoft/DeBERTa', ' 
paper': 'https://arxiv.org/abs/2111.09543', 'upstream_model': '', 'parameter_count': '98M parameters 
 in the Embedding layer', 'hyper_parameters': {'epochs': '3', 'batch_size': '${batch_size}', 'learni 
ng_rate': '2e-5', 'optimizer': ''}, 'evaluation': [{'test': 'SQuAD 2.0(F1/EM)', 'result': 88.4}, {'t 
est': 'MNLI-m/mm(ACC)', 'result': 90.6}], 'hardware': '', 'limitation_and_bias': '', 'demo': 'Please 
 check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation detai 
ls and updates.', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size 
': '128K tokens'}]                                                                                   

#####################EleutherAI/gpt-neo-2.7B########################

-------------------- datasets --------------------
Document 1:

datasets, Pile, EleutherAI
------------------------------
Document 2:

datasets, 420 billion tokens, 400,000 steps
------------------------------
Document 3:

"The Pile: An 800GB Dataset of Diverse Text for Language Modeling" and "arXiv preprint arXiv:2101.00027"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"https://github.com/EleutherAI/lm-evaluation-harness" and "https://discord.gg/vtRgjbM"
------------------------------
Document 2:

url = {https://doi.org/10.5281/zenodo.5297715}
-------------------- paper --------------------
Document 1:

@article{gao2020pile,
title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
journal={arXiv preprint arXiv:2101.00027},
year={2020}
}
------------------------------
Document 2:

"masked autoregressive language model"
-------------------- upstream_model --------------------
Document 1:

upstream_model: masked autoregressive language model
------------------------------
Document 2:

upstream_model GPT-3
-------------------- parameter_count --------------------
Document 1:

parameter_count: 420 billion
-------------------- hyper_parameters --------------------
Document 1:

"masked autoregressive language model, using cross-entropy loss"
-------------------- evaluation --------------------
Document 1:

"All evaluations were done using our [evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness)."
------------------------------
Document 2:

"This model was trained for 420 billion tokens over 400,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss."
-------------------- hardware --------------------
Document 1:

Pile
------------------------------
Document 2:

GPT-Neo 2.7B
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias NO_OUTPUT
------------------------------
Document 2:

"GPT-Neo was trained as an autoregressive language model...GPT-Neo was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language...As with all language models, it is hard to predict in advance how GPT-Neo will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results."
-------------------- demo --------------------
Document 1:

"generating texts from a prompt"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

GPT-Neo 2.7B, 24.72%, 57.54%, 72.14%

[{'datasets': ['Pile', 'EleutherAI'], 'license': 'mit', 'github': 'https://github.com/EleutherAI/lm 
-evaluation-harness', 'paper': 'https://arxiv.org/abs/2101.00027', 'upstream_model': 'masked autoreg 
ressive language model', 'parameter_count': '420 billion', 'hyper_parameters': {'epochs': '', 'batch 
_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result': 0}], 'hardw 
are': 'Pile', 'limitation_and_bias': 'NO_OUTPUT', 'demo': 'generating texts from a prompt', 'input_f 
ormat': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                    

#####################yiyanghkust/finbert-esg########################

-------------------- datasets --------------------
Document 1:

'yiyanghkust/finbert-esg', 'BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)', 'BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')'
------------------------------
Document 2:

"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports."
-------------------- license --------------------
Document 1:

"If you use the model in your academic work, please cite the following paper: Huang, Allen H., Hui Wang, and Yi Yang. "FinBERT: A Large Language Model for Extracting Information from Financial Text." *Contemporary Accounting Research* (2022)."
-------------------- github --------------------
Document 1:

"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports."
------------------------------
Document 2:

'yiyanghkust/finbert-esg', 'Visit [FinBERT.AI](https://finbert.ai/)'
-------------------- paper --------------------
Document 1:

"If you use the model in your academic work, please cite the following paper: Huang, Allen H., Hui Wang, and Yi Yang. "FinBERT: A Large Language Model for Extracting Information from Financial Text." *Contemporary Accounting Research* (2022)."
------------------------------
Document 2:

"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports."
-------------------- upstream_model --------------------
Document 1:

'yiyanghkust/finbert-esg'
------------------------------
Document 2:

FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports. 

upstream_model: FinBERT
-------------------- parameter_count --------------------
Document 1:

'BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)'
------------------------------
Document 2:

"2,000 manually annotated sentences" and "FinBERT-ESG is a FinBERT model"
-------------------- hyper_parameters --------------------
Document 1:

from transformers import BertTokenizer, BertForSequenceClassification, pipeline
finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')
------------------------------
Document 2:

"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports."
-------------------- evaluation --------------------
Document 1:

"If you use the model in your academic work, please cite the following paper: Huang, Allen H., Hui Wang, and Yi Yang. "FinBERT: A Large Language Model for Extracting Information from Financial Text." *Contemporary Accounting Research* (2022)."
------------------------------
Document 2:

"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports."
-------------------- hardware --------------------
Document 1:

"BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)", "BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')"
-------------------- limitation_and_bias --------------------
Document 1:

"If you use the model in your academic work, please cite the following paper: Huang, Allen H., Hui Wang, and Yi Yang. "FinBERT: A Large Language Model for Extracting Information from Financial Text." *Contemporary Accounting Research* (2022)."
------------------------------
Document 2:

"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports."
-------------------- demo --------------------
Document 1:

```python
# tested in transformers==4.18.0
from transformers import BertTokenizer, BertForSequenceClassification, pipeline

finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')
nlp = pipeline("text-classification", model=finbert, tokenizer=tokenizer)
results = nlp('Rhonda has been volunteering for several years for a variety of charitable community programs.')
print(results) # [{'label': 'Social', 'score': 0.9906041026115417}]
```
------------------------------
Document 2:

"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports."
-------------------- input_format --------------------
Document 1:

"BertTokenizer, BertForSequenceClassification, pipeline, finbert, tokenizer, nlp"
------------------------------
Document 2:

"A financial text." and "Environmental, Social, Governance or None."
-------------------- output_format --------------------
Document 1:

"Input: A financial text. Output: Environmental, Social, Governance or None."
-------------------- input_token_limit --------------------
Document 1:

"Input: A financial text." NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

'from transformers import BertTokenizer, BertForSequenceClassification, pipeline' and 'tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')'
------------------------------
Document 2:

FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports.

NO_OUTPUT

[{'datasets': ['yiyanghkust/finbert-esg'], 'license': 'If you use the model in your academic work,  
please cite the following paper: Huang, Allen H., Hui Wang, and Yi Yang. "FinBERT: A Large Language  
Model for Extracting Information from Financial Text." *Contemporary Accounting Research* (2022).',  
'github': 'yiyanghkust/finbert-esg', 'paper': 'If you use the model in your academic work, please ci 
te the following paper: Huang, Allen H., Hui Wang, and Yi Yang. "FinBERT: A Large Language Model for 
 Extracting Information from Financial Text." *Contemporary Accounting Research* (2022).', 'upstream 
_model': 'yiyanghkust/finbert-esg', 'parameter_count': "BertForSequenceClassification.from_pretraine 
d('yiyanghkust/finbert-esg',num_labels=4)", 'hyper_parameters': {'epochs': '', 'batch_size': '', 'le 
arning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'de 
mo': '```python\n# tested in transformers==4.18.0\nfrom transformers import BertTokenizer, BertForSe 
quenceClassification, pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained(\'yiyanghk 
ust/finbert-esg\',num_labels=4)\ntokenizer = BertTokenizer.from_pretrained(\'yiyanghkust/finbert-esg 
\')\nnlp = pipeline("text-classification", model=finbert, tokenizer=tokenizer)\nresults = nlp(\'Rhon 
da has been volunteering for several years for a variety of charitable community programs.\')\nprint 
(results) # [{\'label\': \'Social\', \'score\': 0.9906041026115417}]\n```', 'input_format': 'BertTok 
enizer, BertForSequenceClassification, pipeline, finbert, tokenizer, nlp', 'output_format': 'Input:  
A financial text. Output: Environmental, Social, Governance or None.', 'input_token_limit': 'Input:  
A financial text.', 'vocabulary_size': "from transformers import BertTokenizer, BertForSequenceClass 
ification, pipeline and tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')"}]      

#####################papluca/xlm-roberta-base-language-detection########################

-------------------- datasets --------------------
Document 1:

datasets, [Colab notebook](https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing)
------------------------------
Document 2:

xlm-roberta-base, Language Identification, https://huggingface.co/datasets/papluca/language-identification#additional-information
------------------------------
Document 3:

The model was fine-tuned on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

`arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)`
-------------------- github --------------------
Document 1:

"github" "Colab notebook" "https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing"
------------------------------
Document 2:

language:
- multilingual
- ar
- bg
- de
- el
- en
- es
- fr
- hi
- it
- ja
- nl
- pl
- pt
- ru
- sw
- th
- tr
- ur
- vi
- zh
-------------------- paper --------------------
Document 1:

"Unsupervised Cross-lingual Representation Learning at Scale" by Conneau et al.
------------------------------
Document 2:

xlm-roberta-base, Language Identification
------------------------------
Document 3:

"Colab notebook" "with the training code"
-------------------- upstream_model --------------------
Document 1:

upstream_model xlm-roberta-base
------------------------------
Document 2:

xlm-roberta-base, upstream_model
------------------------------
Document 3:

"sequence classification tasks"
-------------------- parameter_count --------------------
Document 1:

parameter_count: 0
-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 2e-05 - train_batch_size: 64 - eval_batch_size: 128 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - num_epochs: 2 - mixed_precision_training: Native AMP
------------------------------
Document 2:

"Trainer" API, "[Colab notebook](https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing)" with the training code.
-------------------- evaluation --------------------
Document 1:

The model was fine-tuned on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is **99.6%** (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.  
| Language | Precision | Recall | F1-score | support |
|:--------:|:---------:|:------:|:--------:|:-------:|
|ar        |0.998      |0.996   |0.997     |500      |
|bg        |0.998      |0.964   |0.981     |500      |
|de        |0.998      |0.996   |0.997     |500      |
|el        |0.996      |1.000   |0.998     |500      |
|en        |1.000      |
------------------------------
Document 2:

The validation results on the `valid` split of the Language Identification dataset are summarised here below. | Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     | |:-------------:|:-----:|:----:|:---------------:|:--------:|:------:| | 0.2492        | 1.0   | 1094 | 0.0149          | 0.9969   | 0.9969 | | 0.0101        | 2.0   | 2188 | 0.0103          | 0.9977   | 0.9977 | In short, it achieves the following results on the validation set: - Loss: 0.0101 - Accuracy: 0.9977 - F1: 0.9977
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

`arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)`
------------------------------
Document 2:

The average accuracy on the test set is **99.6%** (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.  
| Language | Precision | Recall | F1-score | support |
|:--------:|:---------:|:------:|:--------:|:-------:|
|ar        |0.998      |0.996   |0.997     |500      |
|bg        |0.998      |0.964   |0.981     |500      |
|de        |0.998      |0.996   |0.997     |500      |
|el        |0.996      |1.000   |0.998     |500      |
|en        |1.000      |1.000   |1.000     |500      |
|es        |0.967      |1.000   |0.983     |500      |
|fr        |1.000      |1.000   |1.000     |500      |
|hi        |0.994
-------------------- demo --------------------
Document 1:

`arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)`
------------------------------
Document 2:

[Colab notebook](https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing)
------------------------------
Document 3:

language:
- multilingual
- ar
- bg
- de
- el
- en
- es
- fr
- hi
- it
- ja
- nl
- pl
- pt
- ru
- sw
- th
- tr
- ur
- vi
- zh
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

input_token_limit
-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]         
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0c66-011b774235dd48817fc84193)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-convnext/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0c66-09561123050ddbb9249ffe84)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BloomForCausalLM/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0c67-60496ce610bd6d4b160c545d)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-gpt_neo/resolve/main/README.md. 

#####################dreamlike-art/dreamlike-diffusion-1.0########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

This model is licesed under a **modified** CreativeML OpenRAIL-M license. 
- You can't host or use the model or its derivatives on websites/apps/etc., from which you earn, will earn, or plan to earn revenue or donations. If you want to, please email us at contact@dreamlike.art
- You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)
- You are free to host the model or its derivatives on completely non-commercial websites/apps/etc (Meaning you are not getting ANY revenue or donations). Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)
- You are free to use the outputs of the model or the outputs of
------------------------------
Document 2:

license: other
-------------------- github --------------------
Document 1:

"stable-diffusion", "stable-diffusion-diffusers", "text-to-image", "art", "artistic", "diffusers"
------------------------------
Document 2:

[Gradio](https://github.com/gradio-app/gradio), [![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/dreamlike-diffusion-1.0)
-------------------- paper --------------------
Document 1:

"stable-diffusion", "stable-diffusion-diffusers", "text-to-image", "art", "artistic", "diffusers"
-------------------- upstream_model --------------------
Document 1:

"Stable Diffusion Pipeline" "model_id = "dreamlike-art/dreamlike-diffusion-1.0""
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

- You can't host or use the model or its derivatives on websites/apps/etc., from which you earn, will earn, or plan to earn revenue or donations. 
- You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc. 
- You are free to host the model or its derivatives on completely non-commercial websites/apps/etc (Meaning you are not getting ANY revenue or donations). 
- You are free to use the outputs of the model or the outputs of the model's derivatives for commercial purposes in teams of 10 or less
- You can't use the model to deliberately produce nor share illegal or harmful outputs or content
- The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license
- You may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)
-------------------- demo --------------------
Document 1:

[dreamlike.art](https://dreamlike.art/)
------------------------------
Document 2:

- **You can't host or use the model or its derivatives on websites/apps/etc., from which you earn, will earn, or plan to earn revenue or donations. If you want to, please email us at contact@dreamlike.art**
- **You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)**
- **You are free to host the model or its derivatives on completely non-commercial websites/apps/etc (Meaning you are not getting ANY revenue or donations). Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)**
- **You are free to use the outputs of the model or the outputs of the model's derivatives for commercial purposes in teams of 10 or less**
-
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': ['stable-diffusion', 'stable-diffusion-diffusers', 'text-to-image', 'art', 'artistic' 
, 'diffusers'], 'license': 'modified CreativeML OpenRAIL-M license', 'github': '"stable-diffusion",  
"stable-diffusion-diffusers", "text-to-image", "art", "artistic", "diffusers"', 'paper': '', 'upstre 
am_model': '"Stable Diffusion Pipeline" "model_id = "dreamlike-art/dreamlike-diffusion-1.0""', 'para 
meter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': " 
- You can't host or use the model or its derivatives on websites/apps/etc., from which you earn, wil 
l earn, or plan to earn revenue or donations. \n- You are free to host the model card and files (Wit 
hout any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc. \n 
- You are free to host the model or its derivatives on completely non-commercial websites/apps/etc ( 
Meaning you are not getting ANY revenue or donations). \n- You are free to use the outputs of the mo 
del or the outputs of the model's derivatives for commercial purposes in teams of 10 or less\n- You  
can't use the model to deliberately produce nor share illegal or harmful outputs or content\n- The a 
uthors claims no rights on the outputs you generate, you are free to use them and are accountable fo 
r their use which must not go against the provisions set in the license\n- You may re-distribute the 
 weights. If you do, please be aware you have to include the same use restrictions as the ones in th 
e license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read  
the license entirely and carefully)", 'demo': '[dreamlike.art](https://dreamlike.art/)', 'input_form 
at': '', 'output_format': ''}, {'datasets': [], 'license': 'other', 'github': '[Gradio](https://gith 
ub.com/gradio-app/gradio), [![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be6 
5d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467 
652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://hug 
gingface.co/spaces/akhaliq/dreamlike-diffusion-1.0)', 'paper': '', 'upstream_model': '', 'parameter_ 
count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'de 
mo': "- **You can't host or use the model or its derivatives on websites/apps/etc., from which you e 
arn, will earn, or plan to earn revenue or donations. If you want to, please email us at contact@dre 
amlike.art**\n- **You are free to host the model card and files (Without any actual inference or fin 
etuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name  
(Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art 
/dreamlike-diffusion-1.0)**\n- **You are free to host the model or its derivatives on completely non 
-commercial websites/apps/etc (Meaning you are not getting ANY revenue or donations). Please state t 
he full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingfa 
ce.co/dreamlike-art/dreamlike-diffusion-1.0)**\n- **You are free to use the outputs of the model or  
the outputs of the model's derivatives for commercial purposes in teams of 10 or less**\n-", 'input_ 
format': '', 'output_format': ''}]                                                                   

#####################allenai/specter2########################

-------------------- datasets --------------------
Document 1:

"All the data is a part of SciRepEval benchmark and is available [here](https://huggingface.co/datasets/allenai/scirepeval)." "The citation link are triplets in the form ```json {"query": {"title": ..., "abstract": ...}, "pos": {"title": ..., "abstract": ...}, "neg": {"title": ..., "abstract": ...}}``` consisting of a query paper, a positive citation and a negative which can be from the same/different field of study as the query or citation of a citation."
------------------------------
Document 2:

"600K triplets are sampled from above and added to the training data as well."
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

- **Repository:** [https://github.com/allenai/SPECTER2](https://github.com/allenai/SPECTER2)
- **Demo:** [Usage](https://github.com/allenai/SPECTER2_0/blob/main/README.md)
------------------------------
Document 2:

"https://github.com/allenai/scirepeval/blob/main/evaluation/INFERENCE.md"
------------------------------
Document 3:

"Please refer to the [SPECTER paper](https://api.semanticscholar.org/CorpusID:215768677)."
-------------------- paper --------------------
Document 1:

"Please refer to the [SPECTER paper](https://api.semanticscholar.org/CorpusID:215768677)."
------------------------------
Document 2:

- **Paper:** [https://api.semanticscholar.org/CorpusID:254018137](https://api.semanticscholar.org/CorpusID:254018137)
-------------------- upstream_model --------------------
Document 1:

"Base Model: First a base model is trained on the above citation triplets." "Adapters: Thereafter, task format specific adapters are trained on the SciRepEval training tasks, where 600K triplets are sampled from above and added to the training data as well."
-------------------- parameter_count --------------------
Document 1:

"batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10%, batch size = 256, max input length = 512, learning rate = 1e-4, epochs = 6 warmup = 1000 steps fp16"
-------------------- hyper_parameters --------------------
Document 1:

- Base Model: batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16
- Adapters: batch size = 256, max input length = 512, learning rate = 1e-4, epochs = 6 warmup = 1000 steps fp16
-------------------- evaluation --------------------
Document 1:

"For evaluation and downstream usage, please refer to [https://github.com/allenai/scirepeval/blob/main/evaluation/INFERENCE.md](https://github.com/allenai/scirepeval/blob/main/evaluation/INFERENCE.md)."
------------------------------
Document 2:

"batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16" "batch size = 256, max input length = 512, learning rate = 1e-4, epochs = 6 warmup = 1000 steps fp16"
-------------------- hardware --------------------
Document 1:

"batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16" "batch size = 256, max input length = 512, learning rate = 1e-4, epochs = 6 warmup = 1000 steps fp16"
------------------------------
Document 2:

"The base model is trained on citation links between papers and the adapters are trained on 8 large scale tasks across the four formats."
-------------------- limitation_and_bias --------------------
Document 1:

"batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16" "batch size = 256, max input length = 512, learning rate = 1e-4, epochs = 6 warmup = 1000 steps fp16"
-------------------- demo --------------------
Document 1:

- **Demo:** [Usage](https://github.com/allenai/SPECTER2_0/blob/main/README.md)
-------------------- input_format --------------------
Document 1:

"The citation link are triplets in the form ```json {"query": {"title": ..., "abstract": ...}, "pos": {"title": ..., "abstract": ...}, "neg": {"title": ..., "abstract": ...}}```"
------------------------------
Document 2:

"batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16" "batch size = 256, max input length = 512, learning rate = 1e-4, epochs = 6 warmup = 1000 steps fp16"
-------------------- output_format --------------------
Document 1:

"The citation link are triplets in the form ```json {"query": {"title": ..., "abstract": ...}, "pos": {"title": ..., "abstract": ...}, "neg": {"title": ..., "abstract": ...}}```"

[{'datasets': ['SciRepEval'], 'license': 'license', 'github': 'https://github.com/allenai/SPECTER2' 
, 'paper': 'https://api.semanticscholar.org/CorpusID:215768677', 'upstream_model': 'Base Model: Firs 
t a base model is trained on the above citation triplets. Adapters: Thereafter, task format specific 
 adapters are trained on the SciRepEval training tasks, where 600K triplets are sampled from above a 
nd added to the training data as well.', 'parameter_count': 'batch size = 1024, max input length = 5 
12, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16', 'hyper_parameters': {'epochs': '2', ' 
batch_size': '1024', 'learning_rate': '2e-5', 'optimizer': 'fp16'}, 'evaluation': [{'test': 'For eva 
luation and downstream usage, please refer to', 'result': 0}], 'hardware': 'batch size = 1024, max i 
nput length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp16', 'limitation_and_bias': 
 'batch size = 1024, max input length = 512, learning rate = 2e-5, epochs = 2 warmup steps = 10% fp1 
6', 'demo': '- **Demo:** [Usage](https://github.com/allenai/SPECTER2_0/blob/main/README.md)', 'input 
_format': 'The citation link are triplets in the form ```json {"query": {"title": ..., "abstract": . 
..}, "pos": {"title": ..., "abstract": ...}, "neg": {"title": ..., "abstract": ...}}```', 'output_fo 
rmat': 'The citation link are triplets in the form ```json {"query": {"title": ..., "abstract": ...} 
, "pos": {"title": ..., "abstract": ...}, "neg": {"title": ..., "abstract": ...}}```'}]              
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0ccb-7ebe7c0864991ac65f19e302)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-mobilevit/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0ccb-7e0f65ab5234bf555f7c4828)

Entry Not Found for url: https://huggingface.co/HuggingFaceM4/tiny-random-LlamaForCausalLM/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0ccb-115f6c217eb727eb2b651d41)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-BartForConditionalGeneration/resolve/main/README.md. 

#####################google/byt5-large########################

-------------------- datasets --------------------
Document 1:

mC4, TweetQA
------------------------------
Document 2:

datasets:
- mc4
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

Google's T5, MT5, mC4, ByT5, TweetQA, ByT5: Towards a token-free future with pre-trained byte-to-byte models, Authors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel*
-------------------- github --------------------
Document 1:

"google/byt5-large" and "huggingface.co/google/mt5-large"
-------------------- paper --------------------
Document 1:

"In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments."
------------------------------
Document 2:

Paper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
-------------------- upstream_model --------------------
Document 1:

Google's T5, MT5, mC4, ByT5, TweetQA, ByT5: Towards a token-free future with pre-trained byte-to-byte models
------------------------------
Document 2:

'google/byt5-large'
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.
------------------------------
Document 2:

"ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task."
-------------------- input_format --------------------
Document 1:

"raw UTF-8 bytes" and "return_tensors="pt""
-------------------- output_format --------------------
Document 1:

"raw UTF-8 bytes" and "return_tensors="pt""
-------------------- input_token_limit --------------------
Document 1:

input_token_limit
-------------------- vocabulary_size --------------------


[{'datasets': ['mC4', 'TweetQA'], 'license': 'apache-2.0', 'github': '"google/byt5-large" and "hugg 
ingface.co/google/mt5-large"', 'paper': '"In this paper, we show that a standard Transformer archite 
cture can be used with minimal modifications to process byte sequences. We carefully characterize th 
e trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-le 
vel models are competitive with their token-level counterparts. We also demonstrate that byte-level  
models are significantly more robust to noise and perform better on tasks that are sensitive to spel 
ling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level  
Transformer models based on the T5 architecture, as well as all code and data used in our experiment 
s."', 'upstream_model': "Google's T5, MT5, mC4, ByT5, TweetQA, ByT5: Towards a token-free future wit 
h pre-trained byte-to-byte models", 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {}, 'evaluat 
ion': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'As part of our contribution, we releas 
e a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as al 
l code and data used in our experiments.', 'input_format': '"raw UTF-8 bytes" and "return_tensors="p 
t""', 'output_format': '"raw UTF-8 bytes" and "return_tensors="pt""', 'input_token_limit': 'input_to 
ken_limit', 'vocabulary_size': ''}]                                                                  

#####################facebook/vit-mae-huge########################

-------------------- datasets --------------------
Document 1:

datasets: - imagenet-1k
------------------------------
Document 2:

Masked Autoencoders Are Scalable Vision Learners, https://arxiv.org/abs/2111.06377, https://github.com/facebookresearch/mae
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick and [this repository](https://github.com/facebookresearch/mae).
------------------------------
Document 2:

"Masked Autoencoders Are Scalable Vision Learners" and "CoRR, abs/2111.06377, 2021"
-------------------- upstream_model --------------------
Document 1:

Masked Autoencoders Are Scalable Vision Learners, Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, this repository, MAE
------------------------------
Document 2:

"The Vision Transformer (ViT) is a transformer encoder model (BERT-like)." "By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder."

upstream_model: The Vision Transformer (ViT)
-------------------- parameter_count --------------------
Document 1:

Masked Autoencoders Are Scalable Vision Learners, Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, this repository, the Hugging Face team.

NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"Masked Autoencoders Are Scalable Vision Learners" by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick and first released in "this repository" https://github.com/facebookresearch/mae
-------------------- evaluation --------------------
Document 1:

"Vision Transformer (ViT) model pre-trained using the MAE method. It was introduced in the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick and first released in [this repository](https://github.com/facebookresearch/mae)."
-------------------- hardware --------------------
Document 1:

Masked Autoencoders Are Scalable Vision Learners, Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, this repository, MAE
------------------------------
Document 2:

"Images are presented to the model as a sequence of fixed-size patches."
-------------------- limitation_and_bias --------------------
Document 1:

Masked Autoencoders Are Scalable Vision Learners, Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, this repository, Hugging Face team.
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=facebook/vit-mae"
------------------------------
Document 2:

"from transformers import AutoImageProcessor, ViTMAEForPreTraining from PIL import Image import requests url = 'http://images.cocodataset.org/val2017/000000039769.jpg' image = Image.open(requests.get(url, stream=True).raw) processor = AutoImageProcessor.from_pretrained('facebook/vit-mae-huge') model = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-huge') inputs = processor(images=image, return_tensors="pt") outputs = model(**inputs) loss = outputs.loss mask = outputs.mask ids_restore = outputs.ids_restore"
-------------------- input_format --------------------
Document 1:

"return_tensors=\"pt\""
-------------------- output_format --------------------
Document 1:

"return_tensors="pt""

[{'datasets': ['imagenet-1k'], 'license': 'apache-2.0', 'github': '', 'paper': 'Masked Autoencoders 
 Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ros 
s Girshick and [this repository](https://github.com/facebookresearch/mae).', 'upstream_model': 'The  
Vision Transformer (ViT)', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardwar 
e': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]             
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e0d14-6804c0b65c1305650d2accac)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-OPTModel/resolve/main/README.md. 

#####################hf-internal-testing/tiny-random-distilbert########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------
Document 1:

pipeline_tag: text-classification
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

pipeline_tag: text-classification
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

pipeline_tag: text-classification
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'e 
valuation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_f 
ormat': '', 'input_token_limit': '', 'vocabulary_size': ''}]                                         

#####################stabilityai/stable-diffusion-2########################

-------------------- datasets --------------------
Document 1:

datasets and COCO2017 validation set
------------------------------
Document 2:

LAION-2B(en) (https://laion.ai/blog/laion-5b/)
-------------------- license --------------------
Document 1:

license: openrail++
-------------------- github --------------------
Document 1:

license: openrail++, tags: - stable-diffusion - text-to-image
------------------------------
Document 2:

LAION-2B(en) https://laion.ai/blog/laion-5b/ NO_OUTPUT
-------------------- paper --------------------
Document 1:

"Research on generative models."
------------------------------
Document 2:

High-Resolution Image Synthesis With Latent Diffusion Models
------------------------------
Document 3:

"Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
-------------------- upstream_model --------------------
Document 1:

Stable Diffusion v1, DALL-E Mini model card
-------------------- parameter_count --------------------
Document 1:

parameter_count 50
------------------------------
Document 2:

32 x 8 x A100 GPUs, AdamW, Gradient Accumulations: 1, Batch: 32 x 8 x 2 x 4 = 2048, Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- hyper_parameters --------------------
Document 1:

"50 steps DDIM sampling steps" "Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution." "Not optimized for FID scores."
------------------------------
Document 2:

- relative downsampling factor of 8
- autoencoder maps images of shape H x W x 3 to latents of shape H/f x W/f x 4
- OpenCLIP-ViT/H text-encoder
- UNet backbone of the latent diffusion model via cross-attention
- loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet
- v-objective
- AdamW
- Gradient Accumulations: 1
- Batch: 32 x 8 x 2 x 4 = 2048
- Learning rate: warmup to 0.0001 for 10,000 steps and then kept constant
-------------------- evaluation --------------------
Document 1:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 2:

- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- hardware --------------------
Document 1:

The model was trained mainly with English captions and will not work as well in other languages.
The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
------------------------------
Document 2:

- **Hardware:** 32 x 8 x A100 GPUs
------------------------------
Document 3:

- **Hardware Type:** A100 PCIe 40GB
- **Hours used:** 200000
- **Cloud Provider:** AWS
- **Compute Region:** US-east
-------------------- limitation_and_bias --------------------
Document 1:

"Stable Diffusion was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent."
------------------------------
Document 2:

- Probing and understanding the limitations and biases of generative models.
------------------------------
Document 3:

- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a subset of the large-scale dataset [LAION-5B](https://laion.ai/blog/laion-5b/), which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).
-------------------- demo --------------------
Document 1:

"Generation of artworks and use in design and other artistic processes."
------------------------------
Document 2:

"Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores."
------------------------------
Document 3:

"Stable Diffusion was primarily trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/)" and "Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for."
-------------------- input_format --------------------
Document 1:

LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant

input_format: LAION-5B and subsets, relative downsampling factor of 8, OpenCLIP-ViT/H text-encoder, UNet backbone of the latent diffusion model, AdamW, 32 x 8 x A100 GPUs, 32 x 8 x 2 x 4 = 2048, warmup to 0.0001 for 10,000 steps and then kept constant
------------------------------
Document 2:

input_format: 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.
-------------------- output_format --------------------
Document 1:

output_format: ![pareto](model-variants.jpg) Evaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.

[{'datasets': ['COCO2017 validation set'], 'license': 'openrail++', 'github': 'openrail++', 'paper' 
: '', 'upstream_model': 'Stable Diffusion v1, DALL-E Mini model card', 'parameter_count': '50', 'hyp 
er_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation' 
: [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '' 
}]                                                                                                   

#####################KoboldAI/OPT-13B-Nerys-v2########################

-------------------- datasets --------------------
Document 1:

"The training data contains around 2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" and 50 Asian "Light Novels" (the "Manga-v1" dataset)."
-------------------- license --------------------
Document 1:

"license: other"
------------------------------
Document 2:

"OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved."
-------------------- github --------------------
Document 1:

"Most parts of the dataset have been prepended using the following text: `[Genre: <genre1>, <genre2>]` This dataset has been cleaned in the same way as fairseq-dense-13B-Nerys-v2"

NO_OUTPUT
-------------------- paper --------------------
Document 1:

arXiv:2205.01068
------------------------------
Document 2:

"Facebook's OPT model"
-------------------- upstream_model --------------------
Document 1:

KoboldAI/OPT-13B-Nerys-v2
-------------------- parameter_count --------------------
Document 1:

"2500 ebooks", "CYOA dataset", "50 Asian Light Novels", "fairseq-dense-13B-Nerys-v2"
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"The training data contains around 2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" and 50 Asian "Light Novels" (the "Manga-v1" dataset)." NO_OUTPUT
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"fairseq-dense-13B-Nerys-v2"
-------------------- limitation_and_bias --------------------
Document 1:

bias (gender, profession, race and religion).
------------------------------
Document 2:

"The training data contains around 2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" and 50 Asian "Light Novels" (the "Manga-v1" dataset)." "Most parts of the dataset have been prepended using the following text: `[Genre: <genre1>, <genre2>]` This dataset has been cleaned in the same way as fairseq-dense-13B-Nerys-v2"
-------------------- demo --------------------
Document 1:

`pipeline('text-generation', model='KoboldAI/OPT-13B-Nerys-v2')`
-------------------- input_format --------------------
Document 1:

`[Genre: <genre1>, <genre2>]` and fairseq-dense-13B-Nerys-v2
------------------------------
Document 2:

"from transformers import pipeline"
-------------------- output_format --------------------
Document 1:

"This example generates a different sequence each time it's run: ```py from transformers import pipeline generator = pipeline('text-generation', model='KoboldAI/OPT-13B-Nerys-v2')```"
------------------------------
Document 2:

`[Genre: <genre1>, <genre2>]` and `fairseq-dense-13B-Nerys-v2`
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS" and 50 Asian "Light Novels" (the "Manga-v1" dataset)"

[{'datasets': ['Pike', 'CYS', 'Manga-v1'], 'license': 'other', 'github': 'NO_OUTPUT', 'paper': 'arX 
iv:2205.01068', 'upstream_model': 'KoboldAI/OPT-13B-Nerys-v2', 'parameter_count': '2500 ebooks, CYOA 
 dataset, 50 Asian Light Novels, fairseq-dense-13B-Nerys-v2', 'hyper_parameters': [], 'evaluation':  
[], 'hardware': 'fairseq-dense-13B-Nerys-v2', 'limitation_and_bias': 'bias (gender, profession, race 
 and religion).', 'demo': "`pipeline('text-generation', model='KoboldAI/OPT-13B-Nerys-v2')`", 'input 
_format': '`[Genre: <genre1>, <genre2>]` and fairseq-dense-13B-Nerys-v2', 'output_format': "`This ex 
ample generates a different sequence each time it's run: ```py from transformers import pipeline gen 
erator = pipeline('text-generation', model='KoboldAI/OPT-13B-Nerys-v2')```", 'input_token_limit': '' 
, 'vocabulary_size': '2500 ebooks in various genres (the "Pike" dataset), a CYOA dataset called "CYS 
" and 50 Asian "Light Novels" (the "Manga-v1" dataset)'}]                                            

#####################lllyasviel/control_v11p_sd15_scribble########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 4291 tokens (4035 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################openlm-research/open_llama_3b########################

-------------------- datasets --------------------
Document 1:

datasets: - togethercomputer/RedPajama-Data-1T
------------------------------
Document 2:

RedPajama dataset released by Together, OpenLLaMA employs the RedPajama dataset
------------------------------
Document 3:

"We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens."
-------------------- license --------------------
Document 1:

"permissively licensed open source reproduction of Meta AI's [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)"
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

"In this repo, we present a permissively licensed open source reproduction of Meta AI's [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the [project homepage of OpenLLaMA](https://github.com/openlm-research/open_llama) for more details.
------------------------------
Document 2:

url = {https://github.com/openlm-research/open_llama}, url = {https://github.com/togethercomputer/RedPajama-Data}
------------------------------
Document 3:

license: apache-2.0, togethercomputer/RedPajama-Data-1T
-------------------- paper --------------------
Document 1:

"We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the [project homepage of OpenLLaMA](https://github.com/openlm-research/open_llama) for more details."
------------------------------
Document 2:

Google TPU Research Cloud program, Jonathan Caton, Rafi Witten, James Bradbury, Charlie Snell, Gautier Izacard, Eric Wallace, Lianmin Zheng, Stability AI, David Ha, Shivanshu Purohit
------------------------------
Document 3:

"LLaMA paper", "original LLaMA paper", "model architecture", "context length", "training steps", "learning rate schedule", "optimizer", "OpenLLaMA employs the RedPajama dataset rather than the one utilized by the original LLaMA."
-------------------- upstream_model --------------------
Document 1:

RedPajama dataset, LLaMA paper, model architecture, context length, training steps, learning rate schedule, optimizer, OpenLLaMA, EasyLM, normal data parallelism, fully sharded data parallelism, ZeRO stage 3, 2200 tokens / second / TPU-v4 chip, 7B model.
-------------------- parameter_count --------------------
Document 1:

"model architecture, context length, training steps, learning rate schedule, and optimizer"
------------------------------
Document 2:

GPT-J 6B, LLaMA 7B, OpenLLaMA 7B, OpenLLaMA 3B, OpenLLaMA 13B 600BT
-------------------- hyper_parameters --------------------
Document 1:

"model architecture, context length, training steps, learning rate schedule, and optimizer"
-------------------- evaluation --------------------
Document 1:

We evaluated OpenLLaMA on a wide range of tasks using [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness). The LLaMA results are generated by running the original LLaMA model on the same evaluation metrics. We present the results in the table below. OpenLLaMA exhibits comparable performance to the original LLaMA and GPT-J across a majority of tasks, and outperforms them in some tasks. 

| **Task/Metric**        | GPT-J 6B | LLaMA 7B | OpenLLaMA 7B | OpenLLaMA 3B | OpenLLaMA 13B 600BT |
| ---------------------- | -------- | -------- | ------------ | ------------ | ------------------- |
| anli_r1/acc            | 0.32     | 0.35     | 0.33         | 0.33         | 0.33                |
| anli_r2/acc            | 0.34     | 0.34     | 0.36         | 0.32         | 0.35                |
| anli_r3/acc            | 0.35
------------------------------
Document 2:

"We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models."
-------------------- hardware --------------------
Document 1:

Google TPU Research Cloud, Google Cloud team, Google JAX team, Stability AI, computation resources
------------------------------
Document 2:

RedPajama dataset, model architecture, context length, training steps, learning rate schedule, optimizer, cloud TPU-v4s, EasyLM, normal data parallelism, fully sharded data parallelism, ZeRO stage 3
-------------------- limitation_and_bias --------------------
Document 1:

We note that our results for the LLaMA model differ slightly from the original LLaMA paper, which we believe is a result of different evaluation protocols. Similar differences have been reported in [this issue of lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/issues/443). We removed the task CB and WSC from our benchmark, as our model performs suspiciously well on these two tasks. We hypothesize that there could be a benchmark data contamination in the training set.
------------------------------
Document 2:

"Note that we use BOS (beginning of sentence) token (id=1) during training, so it is best to prepend this token for best performance during few-shot evaluation." NO_OUTPUT
-------------------- demo --------------------
Document 1:

"We provide PyTorch and JAX weights of pre-trained OpenLLaMA models"
-------------------- input_format --------------------
Document 1:

"EasyLM format" "PyTorch format" "EasyLM framework" "Hugging Face transformers" "Apache 2.0 license"
------------------------------
Document 2:

togethercomputer/RedPajama-Data-1T
-------------------- output_format --------------------
Document 1:

"EasyLM format", "PyTorch format" NO_OUTPUT
-------------------- input_token_limit --------------------
Document 1:

RedPajama dataset, LLaMA paper, OpenLLaMA, cloud TPU-v4s, EasyLM, JAX, normal data parallelism, fully sharded data parallelism, ZeRO stage 3, 2200 tokens / second / TPU-v4 chip, 7B model
------------------------------
Document 2:

Note that we use BOS (beginning of sentence) token (id=1) during training, so it is best to prepend this token for best performance during few-shot evaluation. NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

"1.2 trillion tokens" and "7B model"

[{'datasets': ['togethercomputer/RedPajama-Data-1T'], 'license': 'apache-2.0', 'github': 'https://g 
ithub.com/openlm-research/open_llama', 'paper': 'https://github.com/openlm-research/open_llama', 'up 
stream_model': 'LLaMA', 'parameter_count': '7B', 'hyper_parameters': {'epochs': 'N/A', 'batch_size': 
 'N/A', 'learning_rate': 'N/A', 'optimizer': 'N/A'}, 'evaluation': [{'test': 'anli_r1/acc', 'result' 
: 0.33}, {'test': 'anli_r2/acc', 'result': 0.35}, {'test': 'anli_r3/acc', 'result': 0.33}], 'hardwar 
e': 'Google TPU Research Cloud', 'limitation_and_bias': 'We note that our results for the LLaMA mode 
l differ slightly from the original LLaMA paper, which we believe is a result of different evaluatio 
n protocols. Similar differences have been reported in [this issue of lm-evaluation-harness](https:/ 
/github.com/EleutherAI/lm-evaluation-harness/issues/443). We removed the task CB and WSC from our be 
nchmark, as our model performs suspiciously well on these two tasks. We hypothesize that there could 
 be a benchmark data contamination in the training set.', 'demo': 'We provide PyTorch and JAX weight 
s of pre-trained OpenLLaMA models', 'input_format': 'EasyLM format', 'output_format': 'EasyLM format 
', 'input_token_limit': '2200 tokens / second / TPU-v4 chip', 'vocabulary_size': '1.2 trillion token 
s'}]                                                                                                 

#####################Salesforce/codet5-base-multi-sum########################

-------------------- datasets --------------------
Document 1:

datasets: - code_search_net
------------------------------
Document 2:

CodeSearchNet data, EMNLP 2021 paper CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation, this repository https://github.com/salesforce/CodeT5
------------------------------
Document 3:

We employ the filtered version of CodeSearchNet data [[Husain et al., 2019](https://arxiv.org/abs/1909.09436)] from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text) benchmark for fine-tuning on code summarization.
-------------------- license --------------------
Document 1:

license: bsd-3-clause
-------------------- github --------------------
Document 1:

CodeT5-base model fine-tuned on CodeSearchNet data in a multi-lingual training setting (Ruby/JavaScript/Go/Python/Java/PHP) for code summarization. [This repository](https://github.com/salesforce/CodeT5).
------------------------------
Document 2:

"CodeXGLUE" "https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text" "RobertaTokenizer" "https://huggingface.co/Salesforce/codet5-base"
-------------------- paper --------------------
Document 1:

"title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}"
------------------------------
Document 2:

CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi
------------------------------
Document 3:

[[Husain et al., 2019](https://arxiv.org/abs/1909.09436)]
-------------------- upstream_model --------------------
Document 1:

CodeT5-base, EMNLP 2021 paper CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation, Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi, this repository https://github.com/salesforce/CodeT5
------------------------------
Document 2:

RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"CodeT5-base", "EMNLP 2021 paper CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi", "this repository https://github.com/salesforce/CodeT5"
-------------------- evaluation --------------------
Document 1:

| Model       |   Ruby    | Javascript |    Go     |  Python   |   Java    |    PHP    |  Overall  |
| ----------- | :-------: | :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |
| Seq2Seq     |   9.64    |   10.21    |   13.98   |   15.93   |   15.09   |   21.08   |   14.32   |
| Transformer |   11.18   |   11.59    |   16.38   |   15.81   |   16.26   |   22.12   |   15.56   |
| [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)     |   11.17   |   11.90    |   17.72   |   18.14   |   16.47   |   24.02   |   16.57   |
| [CodeBERT](https://arxiv.org/pdf/2002.08155.pdf
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

We employ the filtered version of CodeSearchNet data [[Husain et al., 2019](https://arxiv.org/abs/1909.09436)] from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text) benchmark for fine-tuning on code summarization. The data is tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer. One can prepare text (or code) for the model using RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base).
-------------------- demo --------------------
Document 1:

CodeT5-base, CodeSearchNet data, multi-lingual training setting (Ruby/JavaScript/Go/Python/Java/PHP), CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation, this repository https://github.com/salesforce/CodeT5
------------------------------
Document 2:

"We employ the filtered version of CodeSearchNet data [[Husain et al., 2019](https://arxiv.org/abs/1909.09436)] from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text) benchmark for fine-tuning on code summarization. The data is tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer. One can prepare text (or code) for the model using RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)."
-------------------- input_format --------------------
Document 1:

"tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer" and "RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)".
------------------------------
Document 2:

CodeT5-base, Ruby/JavaScript/Go/Python/Java/PHP, CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation, this repository
-------------------- output_format --------------------
Document 1:

"tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer" and "RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)".
------------------------------
Document 2:

"CodeT5-base", "CodeSearchNet data", "multi-lingual training setting (Ruby/JavaScript/Go/Python/Java/PHP)", "EMNLP 2021 paper", "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi", "this repository"
-------------------- input_token_limit --------------------
Document 1:

RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base) NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

"RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)"
------------------------------
Document 2:

"CodeT5-base", "EMNLP 2021 paper CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi", "this repository https://github.com/salesforce/CodeT5"

[{'datasets': ['code_search_net'], 'license': 'bsd-3-clause', 'github': 'https://github.com/salesfo 
rce/CodeT5', 'paper': 'title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models fo 
r Code Understanding and Generation}', 'upstream_model': 'CodeT5-base, EMNLP 2021 paper CodeT5: Iden 
tifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation, Yue W 
ang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi, this repository https://github.com/salesforce/CodeT5 
', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_ 
bias': 'We employ the filtered version of CodeSearchNet data [[Husain et al., 2019](https://arxiv.or 
g/abs/1909.09436)] from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code- 
to-text) benchmark for fine-tuning on code summarization. The data is tokenized with our pre-trained 
 code-specific BPE (Byte-Pair Encoding) tokenizer. One can prepare text (or code) for the model usin 
g RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5- 
base).', 'demo': 'CodeT5-base, CodeSearchNet data, multi-lingual training setting (Ruby/JavaScript/G 
o/Python/Java/PHP), CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Und 
erstanding and Generation, this repository https://github.com/salesforce/CodeT5', 'input_format': '" 
tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer" and "RobertaTokeniz 
er with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)".', 'outpu 
t_format': '"tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer" and "R 
obertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-bas 
e)".', 'input_token_limit': 'RobertaTokenizer with the vocab files from [codet5-base](https://huggin 
gface.co/Salesforce/codet5-base) NO_OUTPUT', 'vocabulary_size': '"RobertaTokenizer with the vocab fi 
les from [codet5-base](https://huggingface.co/Salesforce/codet5-base)"'}]                            

#####################microsoft/table-transformer-detection########################

-------------------- datasets --------------------
Document 1:

PubTables1M, [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061), [this repository](https://github.com/microsoft/table-transformer)
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"this repository": https://github.com/microsoft/table-transformer
-------------------- paper --------------------
Document 1:

"PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents" and "this repository"
------------------------------
Document 2:

"a Transformer-based object detection model" "Note that the authors decided to use the "normalize before" setting of DETR"
-------------------- upstream_model --------------------
Document 1:

upstream_model: Table Transformer (DETR) model trained on PubTables1M
------------------------------
Document 2:

upstream_model DETR
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"Table Transformer (DETR) model trained on PubTables1M." "It was introduced in the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Smock et al. and first released in [this repository](https://github.com/microsoft/table-transformer)."
-------------------- evaluation --------------------
Document 1:

"Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Smock et al. and first released in [this repository](https://github.com/microsoft/table-transformer)."
-------------------- hardware --------------------
Document 1:

"Table Transformer (DETR) model trained on PubTables1M"
-------------------- limitation_and_bias --------------------
Document 1:

"The team releasing Table Transformer did not write a model card for this model"
-------------------- demo --------------------
Document 1:

"The team releasing Table Transformer did not write a model card for this model so this model card has been written by the Hugging Face team."
-------------------- input_format --------------------
Document 1:

"PubTables1M" and "this repository"
-------------------- output_format --------------------
Document 1:

output_format: NO_OUTPUT
-------------------- input_preprocessing --------------------
Document 1:

"The team releasing Table Transformer did not write a model card for this model"
------------------------------
Document 2:

"Note that the authors decided to use the "normalize before" setting of DETR"
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': ['PubTables1M'], 'license': 'mit', 'github': 'https://github.com/microsoft/table-tran 
sformer', 'paper': 'https://arxiv.org/abs/2110.00061', 'upstream_model': 'Table Transformer (DETR) m 
odel trained on PubTables1M', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epochs': '', 'ba 
tch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': 'Table Transform 
er (DETR) model trained on PubTables1M', 'limitation_and_bias': 'The team releasing Table Transforme 
r did not write a model card for this model', 'demo': 'The team releasing Table Transformer did not  
write a model card for this model so this model card has been written by the Hugging Face team.', 'i 
nput_format': 'PubTables1M', 'output_format': 'NO_OUTPUT', 'input_preprocessing': 'The team releasin 
g Table Transformer did not write a model card for this model', 'input_size': '', 'num_of_classes_fo 
r_classification': '', 'trigger_word': ''}]                                                          

#####################michellejieli/emotion_text_classifier########################

-------------------- datasets --------------------
Document 1:

Emotion English DistilRoBERTa-base, Crowdflower (2016), Elvis et al. (2018), Demszky et al. (2020), Vikash (2018), Poria et al. (2019), Mohammad et al. (2018), Emotion Lines (Friends)
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

Ashritha R Murthy and K M Anil Kumar 2021 IOP Conf. Ser.: Mater. Sci. Eng. 1110 012009
------------------------------
Document 2:

Emotion English DistilRoBERTa-base, DistilRoBERTa-base, Emotion Lines (Friends)
-------------------- upstream_model --------------------
Document 1:

Emotion English DistilRoBERTa-base, DistilRoBERTa-base, upstream_model
------------------------------
Document 2:

"michellejieli/emotion_text_classifier"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"Emotion English DistilRoBERTa-base", "DistilRoBERTa-base", "Crowdflower (2016)", "Emotion Dataset, Elvis et al. (2018)", "GoEmotions, Demszky et al. (2020)", "ISEAR, Vikash (2018)", "MELD, Poria et al. (2019)", "SemEval-2018, EI-reg, Mohammad et al. (2018)", "Emotion Lines (Friends)"
------------------------------
Document 2:

"model="michellejieli/emotion_text_classifier""
-------------------- evaluation --------------------
Document 1:

"The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise."

"The model is a fine-tuned version of [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/) and [DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base)."

"This model was initially trained on the following table from [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/): 

|Name|anger|disgust|fear|joy|neutral|sadness|surprise|
|---|---|---|---|---|---|---|---|
|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|
|Emotion Dataset, Elvis et al. (2018)|Yes|
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

The model is a fine-tuned version of [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/) and [DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base). This model was initially trained on the following table from [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/):  
|Name|anger|disgust|fear|joy|neutral|sadness|surprise|
|---|---|---|---|---|---|---|---|
|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|
|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|
|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes
-------------------- demo --------------------
Document 1:

language: en
tags:
- distilroberta
- sentiment
- emotion
- twitter
- reddit
widget:
- text: Oh my God, he's lost it. He's totally lost it.
- text: What?
- text: Wow, congratulations! So excited for you!
------------------------------
Document 2:

```from transformers import pipeline
classifier = pipeline("sentiment-analysis", model="michellejieli/emotion_text_classifier")
classifier("I love this!")
Output:
[{'label': 'joy', 'score': 0.9887555241584778}]```
-------------------- input_format --------------------
Document 1:

"pipeline('sentiment-analysis', model='michellejieli/emotion_text_classifier')"
input_format: NO_OUTPUT
------------------------------
Document 2:

Emotion English DistilRoBERTa-base, DistilRoBERTa-base, Crowdflower (2016), Emotion Dataset, Elvis et al. (2018), GoEmotions, Demszky et al. (2020), ISEAR, Vikash (2018), MELD, Poria et al. (2019), SemEval-2018, EI-reg, Mohammad et al. (2018), Emotion Lines (Friends)
-------------------- output_format --------------------
Document 1:

Output: [{'label': 'joy', 'score': 0.9887555241584778}]
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Emotion English DistilRoBERTa-base', 'Crowdflower (2016)', 'Elvis et al. (2018)', ' 
Demszky et al. (2020)', 'Vikash (2018)', 'Poria et al. (2019)', 'Mohammad et al. (2018)', 'Emotion L 
ines (Friends)'], 'license': '', 'github': '', 'paper': 'Ashritha R Murthy and K M Anil Kumar 2021 I 
OP Conf. Ser.: Mater. Sci. Eng. 1110 012009', 'upstream_model': '', 'parameter_count': '', 'hyper_pa 
rameters': [], 'evaluation': [{'test': '', 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 
 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': '' 
}]                                                                                                   

#####################ismail-lucifer011/autotrain-name_all-904029577########################

-------------------- datasets --------------------
Document 1:

datasets: - ismail-lucifer011/autotrain-data-name_all
-------------------- license --------------------

-------------------- github --------------------
Document 1:

language: en, tags: autotrain, datasets: - ismail-lucifer011/autotrain-data-name_all, widget: - text: I love AutoTrain 🤗, co2_eq_emissions: 0.8375653425894861
------------------------------
Document 2:

"ismail-lucifer011/autotrain-name_all-904029577"
-------------------- paper --------------------
Document 1:

Model ID: 904029577
-------------------- upstream_model --------------------
Document 1:

upstream_model
------------------------------
Document 2:

"ismail-lucifer011/autotrain-name_all-904029577"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

Loss, Accuracy, Precision, Recall, F1
-------------------- evaluation --------------------
Document 1:

- Loss: 0.0035200684797018766
- Accuracy: 0.9989316041363876
- Precision: 0.9877899024589919
- Recall: 0.9933336010601984
- F1: 0.9905539954046464
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"You can use cURL to access this model:  
```
$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoTrain"}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-name_all-904029577
```  
Or Python API:  
```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained("ismail-lucifer011/autotrain-name_all-904029577", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained("ismail-lucifer011/autotrain-name_all-904029577", use_auth_token=True)

inputs = tokenizer("I love AutoTrain", return_tensors="pt")

outputs = model(**inputs)
```"
-------------------- input_format --------------------
Document 1:

"Content-Type: application/json"
-------------------- output_format --------------------
Document 1:

"from transformers import AutoModelForTokenClassification, AutoTokenizer" and "outputs = model(**inputs)"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['ismail-lucifer011/autotrain-data-name_all'], 'github': 'ismail-lucifer011/autotrain 
-name_all-904029577', 'paper': 'Model ID: 904029577', 'upstream_model': 'ismail-lucifer011/autotrain 
-name_all-904029577', 'evaluation': [{'test': 'Loss', 'result': 0.0035200684797018766}, {'test': 'Ac 
curacy', 'result': 0.9989316041363876}, {'test': 'Precision', 'result': 0.9877899024589919}, {'test' 
: 'Recall', 'result': 0.9933336010601984}, {'test': 'F1', 'result': 0.9905539954046464}], 'demo': '" 
You can use cURL to access this model:  \n```\n$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY 
" -H "Content-Type: application/json" -d \'{"inputs": "I love AutoTrain"}\' https://api-inference.hu 
ggingface.co/models/ismail-lucifer011/autotrain-name_all-904029577\n```  \nOr Python API:  \n```\nfr 
om transformers import AutoModelForTokenClassification, AutoTokenizer\n\nmodel = AutoModelForTokenCl 
assification.from_pretrained("ismail-lucifer011/autotrain-name_all-904029577", use_auth_token=True)\ 
n\ntokenizer = AutoTokenizer.from_pretrained("ismail-lucifer011/autotrain-name_all-904029577", use_a 
uth_token=True)\n\ninputs = tokenizer("I love AutoTrain", return_tensors="pt")\n\noutputs = model(** 
inputs)\n```"', 'input_format': 'Content-Type: application/json', 'output_format': '"from transforme 
rs import AutoModelForTokenClassification, AutoTokenizer" and "outputs = model(**inputs)"'}]         

#####################Lykon/DreamShaper########################

-------------------- datasets --------------------
Document 1:

datasets, link
-------------------- license --------------------
Document 1:

license: other
-------------------- github --------------------
Document 1:

"stable-diffusion", "stable-diffusion-diffusers", "text-to-image", "art", "artistic", "diffusers", "anime"
------------------------------
Document 2:

- https://huggingface.co/spaces/Lykon/DreamShaper-webui
-------------------- paper --------------------
Document 1:

https://civitai.com/models/4384/dreamshaper
------------------------------
Document 2:

"stable-diffusion", "stable-diffusion-diffusers", "text-to-image", "art", "artistic", "diffusers", "anime"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

- https://huggingface.co/spaces/Lykon/DreamShaper-webui
-------------------- input_format --------------------
Document 1:

- https://huggingface.co/spaces/Lykon/DreamShaper-webui
input_format: NO_OUTPUT
-------------------- output_format --------------------
Document 1:

"You can run this model on: - https://huggingface.co/spaces/Lykon/DreamShaper-webui - Mage.space, sinkin.ai and more"
NO_OUTPUT

[{'datasets': ['link'], 'license': 'other', 'github': '"stable-diffusion", "stable-diffusion-diffus 
ers", "text-to-image", "art", "artistic", "diffusers", "anime"', 'paper': 'https://civitai.com/model 
s/4384/dreamshaper', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluatio 
n': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '- https://huggingface.co/spaces/Lykon/Dr 
eamShaper-webui', 'input_format': '- https://huggingface.co/spaces/Lykon/DreamShaper-webui\ninput_fo 
rmat: NO_OUTPUT', 'output_format': '"You can run this model on: - https://huggingface.co/spaces/Lyko 
n/DreamShaper-webui - Mage.space, sinkin.ai and more"\nNO_OUTPUT'}]                                  

#####################ismail-lucifer011/autotrain-job_all-903929564########################

-------------------- datasets --------------------
Document 1:

datasets: - ismail-lucifer011/autotrain-data-job_all
-------------------- license --------------------

-------------------- github --------------------
Document 1:

language: en, tags: autotrain, datasets: ismail-lucifer011/autotrain-data-job_all, widget: text: I love AutoTrain 🤗, co2_eq_emissions: 192.68222884611995
------------------------------
Document 2:

"ismail-lucifer011/autotrain-job_all-903929564"
-------------------- paper --------------------
Document 1:

Model ID: 903929564, CO2 Emissions (in grams): 192.68222884611995
-------------------- upstream_model --------------------
Document 1:

upstream_model
------------------------------
Document 2:

"ismail-lucifer011/autotrain-job_all-903929564"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

Loss, Accuracy, Precision, Recall, F1
-------------------- evaluation --------------------
Document 1:

- Loss: 0.0036299973726272583
- Accuracy: 0.9989412009896035
- Precision: 0.9863310000901253
- Recall: 0.9885186672019269
- F1: 0.9874236219367322
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"You can use cURL to access this model:  
```
$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoTrain"}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-job_all-903929564
```  
Or Python API:  
```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained("ismail-lucifer011/autotrain-job_all-903929564", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained("ismail-lucifer011/autotrain-job_all-903929564", use_auth_token=True)

inputs = tokenizer("I love AutoTrain", return_tensors="pt")

outputs = model(**inputs)
```"
-------------------- input_format --------------------
Document 1:

"Content-Type: application/json"
-------------------- output_format --------------------
Document 1:

"cURL" and "Python API"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['ismail-lucifer011/autotrain-data-job_all'], 'license': '', 'github': 'ismail-lucife 
r011/autotrain-job_all-903929564', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_ 
parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_for 
mat': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                      

#####################Rostlab/prot_t5_xl_half_uniref50-enc########################

-------------------- datasets --------------------
Document 1:

- protein language model - UniRef50
------------------------------
Document 2:

"pretrained on a large corpus of protein sequences in a self-supervised fashion" and "The original T5-3B model was pretrained using a span denoising objective, while this model was pretrained with a Bart-like MLM denoising objective."
-------------------- license --------------------
Document 1:

"The original model and it's pretraining were introduced in [this paper](https://doi.org/10.1101/2020.07.12.199554) and first released in [this repository](https://github.com/agemagician/ProtTrans)."
-------------------- github --------------------
Document 1:

"Availability ProtTrans: \&lt;a href="https://github.com/agemagician/ProtTrans"\&gt;https://github.com/agemagician/ProtTrans\&lt;/a\&gt;"
------------------------------
Document 2:

"This repository https://github.com/agemagician/ProtTrans"
-------------------- paper --------------------
Document 1:

"@article {Elnaggar2020.07.12.199554," "title = {ProtTrans: Towards Cracking the Language of Life{\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing}," "URL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554}," "eprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf}," "journal = {bioRxiv}"
------------------------------
Document 2:

[this paper](https://doi.org/10.1101/2020.07.12.199554)
-------------------- upstream_model --------------------
Document 1:

"ProtT5-XL-UniRef50", "this paper", "this repository"
------------------------------
Document 2:

`t5-3b` model, span denoising objective, Bart-like MLM denoising objective, masking 15% of the amino acids in the input
-------------------- parameter_count --------------------
Document 1:

"This model only contains the encoder portion of the original ProtT5-XL-UniRef50 model using half precision (float16)." "The original T5-3B model was pretrained using a span denoising objective, while this model was pretrained with a Bart-like MLM denoising objective." "The masking probability is consistent with the original T5 training by randomly masking 15% of the amino acids in the input." parameter_count
------------------------------
Document 2:

"ProtT5-XL-UniRef50", "this paper", "this repository"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"8 GB of video RAM"
------------------------------
Document 2:

"This model is trained on uppercase amino acids: it only works with capital letter amino acids."
------------------------------
Document 3:

"pretrained on a large corpus of protein sequences in a self-supervised fashion" and "The original T5-3B model was pretrained using a span denoising objective, while this model was pretrained with a Bart-like MLM denoising objective."
-------------------- limitation_and_bias --------------------
Document 1:

ProtT5-XL-UniRef50 is based on the `t5-3b` model and was pretrained on a large corpus of protein sequences in a self-supervised fashion. One important difference between this T5 model and the original T5 version is the denoising objective. The original T5-3B model was pretrained using a span denoising objective, while this model was pretrained with a Bart-like MLM denoising objective. The masking probability is consistent with the original T5 training by randomly masking 15% of the amino acids in the input. This model only contains the encoder portion of the original ProtT5-XL-UniRef50 model using half precision (float16).
------------------------------
Document 2:

"An encoder-only, half-precision version of the [ProtT5-XL-UniRef50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50) model.", "The original model and it's pretraining were introduced in [this paper](https://doi.org/10.1101/2020.07.12.199554) and first released in [this repository](https://github.com/agemagician/ProtTrans).", "This model is trained on uppercase amino acids: it only works with capital letter amino acids."
-------------------- demo --------------------
Document 1:

Availability ProtTrans: \&lt;a href="https://github.com/agemagician/ProtTrans"\&gt;https://github.com/agemagician/ProtTrans\&lt;/a\&gt;
------------------------------
Document 2:

"An extensive, interactive example on how to use this model for common tasks can be found [on Google Colab](https://colab.research.google.com/drive/1TUj-ayG3WO52n5N50S7KH9vtt6zRkdmj?usp=sharing#scrollTo=ET2v51slC5ui)"
-------------------- input_format --------------------
Document 1:

"pretrained on a large corpus of protein sequences in a self-supervised fashion" "randomly masking 15% of the amino acids in the input" "this model only contains the encoder portion of the original ProtT5-XL-UniRef50 model using half precision (float16)"
------------------------------
Document 2:

"input_ids = torch.tensor(ids['input_ids']).to(device)"
------------------------------
Document 3:

UniRef50
-------------------- output_format --------------------
Document 1:

"last_hidden_state"

[{'datasets': ['UniRef50'], 'license': 'https://doi.org/10.1101/2020.07.12.199554', 'github': 'http 
s://github.com/agemagician/ProtTrans', 'paper': 'https://www.biorxiv.org/content/early/2020/07/21/20 
20.07.12.199554', 'upstream_model': 'ProtT5-XL-UniRef50', 'parameter_count': '#params', 'hyper_param 
eters': {}, 'evaluation': [], 'hardware': '8 GB of video RAM', 'limitation_and_bias': 'ProtT5-XL-Uni 
Ref50 is based on the `t5-3b` model and was pretrained on a large corpus of protein sequences in a s 
elf-supervised fashion. One important difference between this T5 model and the original T5 version i 
s the denoising objective. The original T5-3B model was pretrained using a span denoising objective, 
 while this model was pretrained with a Bart-like MLM denoising objective. The masking probability i 
s consistent with the original T5 training by randomly masking 15% of the amino acids in the input.  
This model only contains the encoder portion of the original ProtT5-XL-UniRef50 model using half pre 
cision (float16).', 'demo': 'https://github.com/agemagician/ProtTrans', 'input_format': 'pretrained  
on a large corpus of protein sequences in a self-supervised fashion, randomly masking 15% of the ami 
no acids in the input, this model only contains the encoder portion of the original ProtT5-XL-UniRef 
50 model using half precision (float16)', 'output_format': 'last_hidden_state'}]                     

#####################ismail-lucifer011/autotrain-company_all-903429548########################

-------------------- datasets --------------------
Document 1:

datasets: - ismail-lucifer011/autotrain-data-company_all
-------------------- license --------------------

-------------------- github --------------------
Document 1:

cURL: ```$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoTrain"}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_all-903429548```

Python API: ```from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained("ismail-lucifer011/autotrain-company_all-903429548", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained("ismail-lucifer011/autotrain-company_all-903429548", use_auth_token=True)

inputs = tokenizer("I love AutoTrain", return_tensors="pt")

outputs = model(**inputs)```
-------------------- paper --------------------
Document 1:

Model ID: 903429548
-------------------- upstream_model --------------------
Document 1:

"ismail-lucifer011/autotrain-company_all-903429548"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

Loss: 0.006148040760308504, Accuracy: 0.9979930566588805, Precision: 0.9814944904963571, Recall: 0.9817210885036588, F1: 0.9816077764228254
-------------------- evaluation --------------------
Document 1:

- Loss: 0.006148040760308504
- Accuracy: 0.9979930566588805
- Precision: 0.9814944904963571
- Recall: 0.9817210885036588
- F1: 0.9816077764228254
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

You can use cURL to access this model:  
```
$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoTrain"}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_all-903429548
```  
Or Python API:  
```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained("ismail-lucifer011/autotrain-company_all-903429548", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained("ismail-lucifer011/autotrain-company_all-903429548", use_auth_token=True)

inputs = tokenizer("I love AutoTrain", return_tensors="pt")

outputs = model(**inputs)
```
-------------------- input_format --------------------
Document 1:

"Content-Type: application/json"
-------------------- output_format --------------------
Document 1:

output_format: "pt"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['ismail-lucifer011/autotrain-data-company_all'], 'license': '', 'github': '$ curl -X 
 POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d \'{"inputs": "I 
 love AutoTrain"}\' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_ 
all-903429548\n\nPython API: ```from transformers import AutoModelForTokenClassification, AutoTokeni 
zer\n\nmodel = AutoModelForTokenClassification.from_pretrained("ismail-lucifer011/autotrain-company_ 
all-903429548", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained("ismail-lucifer011 
/autotrain-company_all-903429548", use_auth_token=True)\n\ninputs = tokenizer("I love AutoTrain", re 
turn_tensors="pt")\n\noutputs = model(**inputs)```', 'paper': 'Model ID: 903429548', 'upstream_model 
': '"ismail-lucifer011/autotrain-company_all-903429548"', 'parameter_count': '', 'hyper_parameters': 
 [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': ' 
- Loss: 0.006148040760308504\n- Accuracy: 0.9979930566588805\n- Precision: 0.9814944904963571\n- Rec 
all: 0.9817210885036588\n- F1: 0.9816077764228254', 'result': 0}], 'hardware': '', 'limitation_and_b 
ias': '', 'demo': 'You can use cURL to access this model:  \n```\n$ curl -X POST -H "Authorization:  
Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d \'{"inputs": "I love AutoTrain"}\' https 
://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_all-903429548\n```\nOr Py 
thon API:  \n```\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\n\nmodel = 
 AutoModelForTokenClassification.from_pretrained("ismail-lucifer011/autotrain-company_all-903429548" 
, use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained("ismail-lucifer011/autotrain-com 
pany_all-903429548", use_auth_token=True)\n\ninputs = tokenizer("I love AutoTrain", return_tensors=" 
pt")\n\noutputs = model(**inputs)\n```', 'input_format': '"Content-Type: application/json"', 'output 
_format': 'output_format: "pt"', 'input_token_limit': '', 'vocabulary_size': ''}]                    

#####################lllyasviel/sd-controlnet-openpose########################

-------------------- datasets --------------------
Document 1:

[Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), [lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny), [lllyasviel/sd-controlnet-depth](https://huggingface.co/lllyasviel/sd-controlnet-depth), [lllyasviel/sd-controlnet-hed](https://huggingface.co/lllyasviel/sd-controlnet-hed), [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-mlsd), [lllyasviel/sd-controlnet-normal](https://huggingface.co/lllyasviel/sd-controlnet-normal), [lllyasviel/sd-controlnet_openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose), [
------------------------------
Document 2:

datasets, 200k pose-image, caption pairs, Openpose
-------------------- license --------------------
Document 1:

license: openrail
-------------------- github --------------------
Document 1:

[lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny), [lllyasviel/sd-controlnet-depth](https://huggingface.co/lllyasviel/sd-controlnet-depth), [lllyasviel/sd-controlnet-hed](https://huggingface.co/lllyasviel/sd-controlnet-hed), [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-mlsd), [lllyasviel/sd-controlnet-normal](https://huggingface.co/lllyasviel/sd-controlnet-normal), [lllyasviel/sd-controlnet_openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose), [lllyasviel/sd-controlnet_scribble](https://huggingface.co/lllyasviel/sd-controlnet
------------------------------
Document 2:

license: openrail, tags: - art - controlnet - stable-diffusion - image-to-image, base_model: runwayml/stable-diffusion-v1-5
-------------------- paper --------------------
Document 1:

*Adding Conditional Control to Text-to-Image Diffusion Models* by Lvmin Zhang, Maneesh Agrawala.
-------------------- upstream_model --------------------
Document 1:

base_model: runwayml/stable-diffusion-v1-5
------------------------------
Document 2:

Stable Diffusion v1-5
------------------------------
Document 3:

"We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions." NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

parameter_count: 200k
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Nvidia A100 80G
------------------------------
Document 2:

Stable Diffusion v1-5
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias NO_OUTPUT
------------------------------
Document 2:

"We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k)."
-------------------- demo --------------------
Document 1:

The authors released 8 different checkpoints, each trained with [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) on a different type of conditioning:  
| Model Name | Control Image Overview| Control Image Example | Generated Image Example |
|---|---|---|---|
|[lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny)<br/> *Trained with canny edge detection* | A monochrome image with white edges on a black background.|<a href="https://huggingface.co/takuma104/controlnet_dev/blob/main/gen_compare/control_images/converted/control_bird_canny.png"><img width="64" style="margin:0;padding:0;" src="https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare/control_images/converted/control_bird_canny
------------------------------
Document 2:

"Openpose model", "pose estimation images", "Nvidia A100 80G", "Stable Diffusion 1.5"
-------------------- input_format --------------------
Document 1:

"canny edge detection", "Midas depth estimation", "HED edge detection (soft edge)", "M-LSD line detection", "normal map", "OpenPose bone image", "human scribbles", "ADE20K's segmentation protocol image"
------------------------------
Document 2:

input_format: pose estimation images generated with Openpose
-------------------- output_format --------------------
Document 1:

Generated Image Example
------------------------------
Document 2:

output_format
-------------------- input_preprocessing --------------------
Document 1:

*Trained with canny edge detection*, *Trained with Midas depth estimation*, *Trained with HED edge detection (soft edge)*, *Trained with M-LSD line detection*, *Trained with normal map*, *Trained with OpenPose bone image*, *Trained with human scribbles*, *Trained with semantic segmentation*
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': ['Stable Diffusion v1-5', 'lllyasviel/sd-controlnet-canny', 'lllyasviel/sd-controlnet- 
depth', 'lllyasviel/sd-controlnet-hed', 'lllyasviel/sd-controlnet-mlsd', 'lllyasviel/sd-controlnet-n 
ormal', 'lllyasviel/sd-controlnet-openpose'], 'license': 'openrail', 'github': 'lllyasviel/sd-contro 
lnet-canny', 'paper': '*Adding Conditional Control to Text-to-Image Diffusion Models* by Lvmin Zhang 
, Maneesh Agrawala.', 'upstream_model': 'runwayml/stable-diffusion-v1-5', 'parameter_count': '200k', 
 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'Nvidia A100 80G', 'limitation_and_bias': 'NO 
_OUTPUT', 'demo': 'The authors released 8 different checkpoints, each trained with [Stable Diffusion 
 v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) on a different type of conditioning:\n 
\n| Model Name | Control Image Overview| Control Image Example | Generated Image Example |\n|---|--- 
|---|---|\n|[lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny)< 
br/> *Trained with canny edge detection* | A monochrome image with white edges on a black background 
.|<a href="https://huggingface.co/takuma104/controlnet_dev/blob/main/gen_compare/control_images/conv 
erted/control_bird_canny.png"><img width="64" style="margin:0;padding:0;" src="https://huggingface.c 
o/takuma104/controlnet_dev/resolve/main/gen_compare/control_images/converted/control_bird_canny', 'i 
nput_format': '"canny edge detection", "Midas depth estimation", "HED edge detection (soft edge)", " 
M-LSD line detection", "normal map", "OpenPose bone image", "human scribbles", "ADE20K\'s segmentati 
on protocol image"', 'output_format': 'Generated Image Example'}                                     

#####################facebook/opt-2.7b########################

-------------------- datasets --------------------
Document 1:

- BookCorpus
- CC-Stories
- Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews
- Pushshift.io Reddit dataset
- CCNewsV2
------------------------------
Document 2:

"The dataset was collected form internet"
-------------------- license --------------------
Document 1:

license: other
-------------------- github --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- paper --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers." "Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days
------------------------------
Document 2:

"125M to 175B parameters"
-------------------- hyper_parameters --------------------
Document 1:

"BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit dataset, CCNewsV2" NO_OUTPUT
------------------------------
Document 2:

"model="facebook/opt-2.7b", do_sample=True, num_return_sequences=5"
-------------------- evaluation --------------------
Document 1:

- BookCorpus, which consists of more than 10K unpublished books,
- CC-Stories, which contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas,
- The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included.
- Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in
Roller et al. (2021)
- CCNewsV2 containing an updated version of the English portion of the CommonCrawl News
dataset that was used in RoBERTa (Liu et al., 2019b)
- The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally
to each dataset’s size in the pretraining corpus.
-------------------- hardware --------------------
Document 1:

GPT2, 80GB A100 GPUs, ~33 days
------------------------------
Document 2:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 3:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- limitation_and_bias --------------------
Document 1:

As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral the model is strongly biased : Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. Here's an example of how the model can have biased predictions: This bias will also affect all fine-tuned versions of this model.
------------------------------
Document 2:

"known challenges in areas such as robustness, bias, and toxicity"
-------------------- demo --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- input_format --------------------
Document 1:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 2:

GPT2 byte-level version of Byte Pair Encoding (BPE), vocabulary size of 50272, inputs are sequences of 2048 consecutive tokens. input_format
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*
-------------------- vocabulary_size --------------------
Document 1:

"180B tokens corresponding to 800GB of data" NO_OUTPUT
------------------------------
Document 2:

GPT2, vocabulary size of 50272
------------------------------
Document 3:

"125M to 175B parameters"

[{'datasets': ['BookCorpus', 'CC-Stories', 'Pile-CC', 'OpenWebText2', 'USPTO', 'Project Gutenberg', 
 'OpenSubtitles', 'Wikipedia', 'DM Mathematics', 'HackerNews', 'Pushshift.io Reddit dataset', 'CCNew 
sV2'], 'license': 'other', 'github': 'We present Open Pretrained Transformers (OPT), a suite of deco 
der-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and re 
sponsibly share with interested researchers.', 'paper': 'We present Open Pretrained Transformers (OP 
T), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we  
aim to fully and responsibly share with interested researchers. Our aim in developing this suite of  
OPT models is to enable reproducible and responsible research at scale, and to bring more voices to  
the table in studying the impact of these LLMs.', 'upstream_model': '', 'parameter_count': 'GPT2, 50 
272, 2048, 175B, 992, 80GB A100 GPUs, 33 days', 'hyper_parameters': [{'epochs': '', 'batch_size': '' 
, 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': '- BookCorpus, which consists of m 
ore than 10K unpublished books,\n\n- CC-Stories, which contains a subset of CommonCrawl data filtere 
d to match the\nstory-like style of Winograd schemas,\n\n- The Pile, from which * Pile-CC, OpenWebTe 
xt2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were include 
d.\n\n- Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in 
\nRoller et al. (2021)\n\n- CCNewsV2 containing an updated version of the English portion of the Com 
monCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n\n- The final training data con 
tains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pret 
raining data, sampled proportionally\nto each dataset’s size in the pretraining corpus.', 'result':  
0}], 'hardware': 'GPT2, 80GB A100 GPUs, ~33 days', 'limitation_and_bias': "As mentioned in Meta AI's 
 model card, given that the training data used for this model contains a lot of unfiltered content f 
rom the internet, which is far from neutral the model is strongly biased : Like other large language 
 models for which the diversity (or lack thereof) of training data induces downstream impact on the  
quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have q 
uality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune 
 from the plethora of issues that plague modern large language models. Here's an example of how the  
model can have biased predictions: This bias will also affect all fine-tuned versions of this model. 
", 'demo': 'We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained trans 
formers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with inter 
ested researchers.', 'input_format': 'BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project  
Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNews 
V2, CommonCrawl News dataset', 'output_format': ''}]                                                 

#####################dbmdz/distilbert-base-turkish-cased########################

-------------------- datasets --------------------
Document 1:

Kemal Oflazer, Reyyan Yeniterzi, Turkish NER dataset
-------------------- license --------------------
Document 1:

"license: mit"
------------------------------
Document 2:

"license"
-------------------- github --------------------
Document 1:

"All models are available on the [Huggingface model hub](https://huggingface.co/dbmdz)."
------------------------------
Document 2:

"For questions about our BERT models just open an issue [here](https://github.com/dbmdz/berts/issues/new) 🤗"
-------------------- paper --------------------
Document 1:

"Huggingface model hub"
------------------------------
Document 2:

Kemal Oflazer, Reyyan Yeniterzi, Google's TensorFlow Research Cloud (TFRC), Hugging Face
-------------------- upstream_model --------------------
Document 1:

"dbmdz/distilbert-base-turkish-cased"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"DistilBERTurk was trained with the official Hugging Face implementation from [here](https://github.com/huggingface/transformers/tree/master/examples/distillation) for 5 days on 4 RTX 2080 TI."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"4 RTX 2080 TI"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[Huggingface model hub](https://huggingface.co/dbmdz)
------------------------------
Document 2:

"a (cased) distilled model for Turkish 🎉"
-------------------- input_format --------------------
Document 1:

PyTorch-Transformers, config.json, pytorch_model.bin, vocab.txt
-------------------- output_format --------------------
Document 1:

PyTorch-[Transformers](https://github.com/huggingface/transformers) • [`config.json`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/config.json) • [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/pytorch_model.bin) • [`vocab.txt`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/vocab.txt)

[{'datasets': ['Kemal Oflazer, Reyyan Yeniterzi, Turkish NER dataset'], 'license': 'mit', 'github': 
 'All models are available on the [Huggingface model hub](https://huggingface.co/dbmdz).', 'paper':  
'Huggingface model hub', 'upstream_model': 'dbmdz/distilbert-base-turkish-cased', 'parameter_count': 
 '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'ev 
aluation': [], 'hardware': '4 RTX 2080 TI', 'limitation_and_bias': '', 'demo': '[Huggingface model h 
ub](https://huggingface.co/dbmdz)', 'input_format': 'PyTorch-Transformers, config.json, pytorch_mode 
l.bin, vocab.txt', 'output_format': 'PyTorch-[Transformers](https://github.com/huggingface/transform 
ers) • [`config.json`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/config.json) • 
 [`pytorch_model.bin`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/pytorch_model. 
bin) • [`vocab.txt`](https://cdn.huggingface.co/dbmdz/distilbert-base-turkish-cased/vocab.txt)'}]    

#####################ybelkada/tiny-random-T5ForConditionalGeneration-calibrated########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------
Document 1:

"github"
-------------------- paper --------------------
Document 1:

"A "better calibrated" tiny T5 model for testing purposes"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': '', 'github': 'https://github.com/huggingface/models', 'paper': '', 'u 
pstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 
 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit' 
: '', 'vocabulary_size': ''}]                                                                        

#####################uer/albert-base-chinese-cluecorpussmall########################

-------------------- datasets --------------------
Document 1:

[CLUECorpusSmall](https://github.com/CLUEbenchmark/CLUECorpus2020/)
------------------------------
Document 2:

[UER-py](https://github.com/dbiir/UER-py/), [TencentPretrain](https://github.com/Tencent/TencentPretrain), [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [base], [large]
------------------------------
Document 3:

datasets: CLUECorpusSmall
-------------------- license --------------------
Document 1:

"license"
-------------------- github --------------------
Document 1:

"CLUECorpusSmall https://github.com/CLUEbenchmark/CLUECorpus2020/ is used as training data."
------------------------------
Document 2:

- [UER-py](https://github.com/dbiir/UER-py/)
- [TencentPretrain](https://github.com/Tencent/TencentPretrain)
- [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo)
- [L=12/H=768 (Base)][base]
- [L=24/H=1024 (Large)][large]
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1909.05658) and [this paper](https://arxiv.org/abs/2212.06385)
------------------------------
Document 2:

"Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu, Albert: A lite bert for self-supervised learning of language representations, arXiv preprint arXiv:1909.11942, 2019"
-------------------- upstream_model --------------------
Document 1:

UER-py, TencentPretrain
------------------------------
Document 2:

UER-py, Tencent Cloud, ALBERT-Base
-------------------- parameter_count --------------------
Document 1:

[base][large]
-------------------- hyper_parameters --------------------
Document 1:

sequence length of 128, sequence length of 512, hyper-parameters, world_size 8, gpu_ranks 0 1 2 3 4 5 6 7, total_steps 1000000, save_checkpoint_steps 100000, report_steps 50000, learning_rate 1e-4, batch_size 64
------------------------------
Document 2:

"L=12/H=768 (Base)", "L=24/H=1024 (Large)"
-------------------- evaluation --------------------
Document 1:

"CLUECorpusSmall is used as training data." NO_OUTPUT
------------------------------
Document 2:

[UER-py](https://github.com/dbiir/UER-py/), [this paper](https://arxiv.org/abs/1909.05658), [TencentPretrain](https://github.com/Tencent/TencentPretrain), [this paper](https://arxiv.org/abs/2212.06385), [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [**L=12/H=768 (Base)**][base], [**L=24/H=1024 (Large)**][large]
------------------------------
Document 3:

Taking the case of ALBERT-Base  
Stage1:  
```
python3 preprocess.py --corpus_path corpora/cluecorpussmall_bert.txt \
--vocab_path models/google_zh_vocab.txt \
--dataset_path cluecorpussmall_albert_seq128_dataset.pt \
--seq_length 128 --processes_num 32 --data_processor albert
```  
```
python3 pretrain.py --dataset_path cluecorpussmall_albert_seq128_dataset.pt \
--vocab_path models/google_zh_vocab.txt \
--config_path models/albert/base_config.json \
--output_model_path models/cluecorpussmall_albert_base_seq128_model.bin \
--world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
--total_steps 1000000 --save_checkpoint_steps 100000 --report_steps 50000 \
--learning
-------------------- hardware --------------------
Document 1:

UER-py, TencentPretrain
------------------------------
Document 2:

Tencent Cloud, ALBERT-Base, python3 preprocess.py, python3 pretrain.py, python3 scripts/convert_albert_from_uer_to_huggingface.py
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias
-------------------- demo --------------------
Document 1:

[UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [base], [large]
-------------------- input_format --------------------
Document 1:

input_format: [CLUECorpusSmall](https://github.com/CLUEbenchmark/CLUECorpus2020/)
------------------------------
Document 2:

--seq_length 128 --processes_num 32 --data_processor albert
--seq_length 512 --processes_num 32 --data_processor albert
--vocab_path models/google_zh_vocab.txt
--config_path models/albert/base_config.json
------------------------------
Document 3:

"UER-py Modelzoo page" and "ALBERT-Base" and "L=12/H=768 (Base)" and "ALBERT-Large" and "L=24/H=1024 (Large)"
-------------------- output_format --------------------
Document 1:

--output_model_path models/cluecorpussmall_albert_base_seq512_model.bin --output_model_path pytorch_model.bin
------------------------------
Document 2:

"L=12/H=768 (Base)", "L=24/H=1024 (Large)"
-------------------- input_token_limit --------------------
Document 1:

--seq_length 128 --seq_length 512
-------------------- vocabulary_size --------------------
Document 1:

CLUECorpusSmall
------------------------------
Document 2:

models/google_zh_vocab.txt

[{'datasets': ['CLUECorpusSmall'], 'license': 'license', 'github': 'CLUECorpusSmall https://github. 
com/CLUEbenchmark/CLUECorpus2020/ is used as training data.', 'paper': '[this paper](https://arxiv.o 
rg/abs/1909.05658) and [this paper](https://arxiv.org/abs/2212.06385)', 'upstream_model': 'UER-py, T 
encentPretrain', 'parameter_count': '[base][large]', 'hyper_parameters': {'epochs': '', 'batch_size' 
: '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'CLUECorpusSmall is used as tra 
ining data.', 'result': 0}], 'hardware': 'UER-py, TencentPretrain', 'limitation_and_bias': 'limitati 
on_and_bias', 'demo': '[UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [base] 
, [large]', 'input_format': '[CLUECorpusSmall](https://github.com/CLUEbenchmark/CLUECorpus2020/)', ' 
output_format': '--output_model_path models/cluecorpussmall_albert_base_seq512_model.bin --output_mo 
del_path pytorch_model.bin', 'input_token_limit': '--seq_length 128 --seq_length 512', 'vocabulary_s 
ize': 'CLUECorpusSmall'}, {'datasets': ['UER-py', 'TencentPretrain', 'UER-py Modelzoo page', 'base', 
 'large'], 'license': '', 'github': '- [UER-py](https://github.com/dbiir/UER-py/)\n- [TencentPretrai 
n](https://github.com/Tencent/TencentPretrain)\n- [UER-py Modelzoo page](https://github.com/dbiir/UE 
R-py/wiki/Modelzoo)\n- [L=12/H=768 (Base)][base]\n- [L=24/H=1024 (Large)][large]', 'paper': '"Lan, Z 
henzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, R 
adu, Albert: A lite bert for self-supervised learning of language representations, arXiv preprint ar 
Xiv:1909.11942, 2019"', 'upstream_model': 'UER-py, Tencent Cloud, ALBERT-Base', 'parameter_count': ' 
"L=12/H=768 (Base)", "L=24/H=1024 (Large)"', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'l 
earning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': '[UER-py](https://github.com/dbiir/UER- 
py/), [this paper](https://arxiv.org/abs/1909.05658), [TencentPretrain](https://github.com/Tencent/T 
encentPretrain), [this paper](https://arxiv.org/abs/2212.06385), [UER-py Modelzoo page](https://gith 
ub.com/dbiir/UER-py/wiki/Modelzoo), [**L=12/H=768 (Base)**][base], [**L=24/H=1024 (Large)**][large]' 
, 'result': 0}], 'hardware': 'Tencent Cloud, ALBERT-Base, python3 preprocess.py, python3 pretrain.py 
, python3 scripts/convert_albert_from_uer_to_huggingface.py', 'limitation_and_bias': '', 'demo': '', 
 'input_format': '--seq_length 128 --processes_num 32 --data_processor albert\n--seq_length 512 --pr 
ocesses_num 32 --data_processor albert\n--vocab_path models/google_zh_vocab.txt\n--config_path model 
s/albert/base_config.json', 'output_format': '"L=12/H=768 (Base)", "L=24/H=1024 (Large)"', 'input_to 
ken_limit': '', 'vocabulary_size': 'models/google_zh_vocab.txt'}]                                    

#####################facebook/contriever########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------
Document 1:

"The associated GitHub repository is available here https://github.com/facebookresearch/contriever."
-------------------- paper --------------------
Document 1:

Towards Unsupervised Dense Information Retrieval with Contrastive Learning
-------------------- upstream_model --------------------
Document 1:

facebook/contriever
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"The associated GitHub repository is available here https://github.com/facebookresearch/contriever."
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': [], 'license': '', 'github': 'https://github.com/facebookresearch/contriever', 'paper 
': 'Towards Unsupervised Dense Information Retrieval with Contrastive Learning', 'upstream_model': ' 
facebook/contriever', 'parameter_count': 'parameter_count', 'hyper_parameters': {}, 'evaluation': [] 
, 'hardware': '', 'limitation_and_bias': '', 'demo': 'The associated GitHub repository is available  
here https://github.com/facebookresearch/contriever.', 'input_format': '', 'output_format': ''}]     

#####################rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment########################

-------------------- datasets --------------------
Document 1:

datasets, SAIL 2017 dataset [link](http://amitavadas.com/SAIL/Data/SAIL_2017.zip)
------------------------------
Document 2:

datasets: - SAIL 2017
------------------------------
Document 3:

SAIL 2017 dataset, http://www.dasdipankar.com/SAILCodeMixed.html
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"pretrained model"
------------------------------
Document 2:

"Khanuja, Simran  and Dandapat, Sandipan  and Srinivasan, Anirudh  and Sitaram, Sunayana  and Choudhury, Monojit"
------------------------------
Document 3:

SAIL 2017 dataset, Performance of this model on the SAIL 2017 dataset, acc, f1, acc_and_f1, precision, recall
-------------------- upstream_model --------------------
Document 1:

pretrained model
------------------------------
Document 2:

"bert-base-multilingual-cased model from Huggingface"
upstream_model: bert-base-multilingual-cased model from Huggingface
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"pretrained model"
------------------------------
Document 2:

"I took a bert-base-multilingual-cased model from Huggingface and finetuned it on [SAIL 2017](http://www.dasdipankar.com/SAILCodeMixed.html) dataset."
-------------------- evaluation --------------------
Document 1:

SAIL 2017 dataset, pretrained model
------------------------------
Document 2:

Input for the model: Any codemixed hinglish text
Output for the model: Sentiment. (0 - Negative, 1 - Neutral, 2 - Positive)
I took a bert-base-multilingual-cased model from Huggingface and finetuned it on [SAIL 2017](http://www.dasdipankar.com/SAILCodeMixed.html) dataset.
Performance of this model on the SAIL 2017 dataset
| metric     |    score |
|------------|----------|
| acc        | 0.588889 |
| f1         | 0.582678 |
| acc_and_f1 | 0.585783 |
| precision  | 0.586516 |
| recall     | 0.588889 |
-------------------- hardware --------------------
Document 1:

"bert-base-multilingual-cased model from Huggingface"
-------------------- limitation_and_bias --------------------
Document 1:

SAIL 2017 dataset, pretrained model
-------------------- demo --------------------
Document 1:

link](http://amitavadas.com/SAIL/Data/SAIL_2017.zip), [pretrained model](https://huggingface.co/bert-base-multilingual-cased)
-------------------- input_format --------------------
Document 1:

SAIL 2017 dataset, .zip
------------------------------
Document 2:

Any codemixed hinglish text, SAIL 2017 dataset
-------------------- output_format --------------------
Document 1:

Output for the model: Sentiment. (0 - Negative, 1 - Neutral, 2 - Positive)
------------------------------
Document 2:

"return_tensors='pt'" and "return_tensors='tf'"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

SAIL 2017 dataset, pretrained model

[{'datasets': ['SAIL 2017 dataset'], 'license': 'apache-2.0', 'github': '', 'paper': '', 'upstream_ 
model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limita 
tion_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'v 
ocabulary_size': ''}]                                                                                

#####################facebook/roberta-hate-speech-dynabench-r4-target########################

-------------------- datasets --------------------
Document 1:

"The R4 Target model from [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://arxiv.org/abs/2012.15761)"
------------------------------
Document 2:

```title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection}```
-------------------- license --------------------
Document 1:

`booktitle={ACL}, year={2021}`
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection
------------------------------
Document 2:

`title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection}, author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela}, booktitle={ACL}, year={2021}`
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"The R4 Target model from [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://arxiv.org/abs/2012.15761)
-------------------- hardware --------------------
Document 1:

"R4 Target model"
-------------------- limitation_and_bias --------------------
Document 1:

"The R4 Target model from Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection"
-------------------- demo --------------------
Document 1:

"The R4 Target model from Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"vocabulary_size"

[{'datasets': ['Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Dete 
ction'], 'license': 'booktitle={ACL}, year={2021}', 'github': '', 'paper': 'Learning from the Worst: 
 Dynamically Generated Datasets to Improve Online Hate Detection', 'upstream_model': '', 'parameter_ 
count': 'parameter_count', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'R4 Target model',  
'limitation_and_bias': 'The R4 Target model from Learning from the Worst: Dynamically Generated Data 
sets to Improve Online Hate Detection', 'demo': 'The R4 Target model from Learning from the Worst: D 
ynamically Generated Datasets to Improve Online Hate Detection', 'input_format': '', 'output_format' 
: '', 'input_token_limit': '', 'vocabulary_size': 'vocabulary_size'}]                                

#####################dbmdz/bert-base-german-uncased########################

-------------------- datasets --------------------
Document 1:

German BERT model by deepset, Wikipedia dump, EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl, News Crawl, spacy, SciBERT, initial sequence length of 512 subwords, 1.5M steps, cased and uncased models.
-------------------- license --------------------
Document 1:

"license: mit"
------------------------------
Document 2:

license
------------------------------
Document 3:

"license"
-------------------- github --------------------
Document 1:

[this repository](https://github.com/stefan-it/fine-tuned-berts-seq)
------------------------------
Document 2:

"All models are available on the [Huggingface model hub](https://huggingface.co/dbmdz)."
------------------------------
Document 3:

"For questions about our BERT models just open an issue [here](https://github.com/dbmdz/berts/issues/new) 🤗"
-------------------- paper --------------------
Document 1:

"Huggingface model hub"
-------------------- upstream_model --------------------
Document 1:

"SciBERT"
-------------------- parameter_count --------------------
Document 1:

"2,350,234,427 tokens" and "1.5M steps"
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"The source data for the model consists of a recent Wikipedia dump, EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl and News Crawl. This results in a dataset with a size of 16GB and 2,350,234,427 tokens. For sentence splitting, we use [spacy](https://spacy.io/). Our preprocessing steps (sentence piece model for vocab generation) follow those used for training [SciBERT](https://github.com/allenai/scibert). The model is trained with an initial sequence length of 512 subwords and was performed for 1.5M steps. This release includes both cased and uncased models."
-------------------- hardware --------------------
Document 1:

Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).
------------------------------
Document 2:

"This model is trained with an initial sequence length of 512 subwords and was performed for 1.5M steps."
-------------------- limitation_and_bias --------------------
Document 1:

German BERT model by deepset, source data consists of a recent Wikipedia dump, EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl and News Crawl, sentence splitting uses spacy, preprocessing steps follow those used for training SciBERT, model trained with an initial sequence length of 512 subwords and was performed for 1.5M steps, includes both cased and uncased models.
-------------------- demo --------------------
Document 1:

[Huggingface model hub](https://huggingface.co/dbmdz)
------------------------------
Document 2:

"form of demo"
-------------------- input_format --------------------
Document 1:

"For sentence splitting, we use [spacy](https://spacy.io/). Our preprocessing steps (sentence piece model for vocab generation) follow those used for training [SciBERT](https://github.com/allenai/scibert)." input_format
-------------------- output_format --------------------
Document 1:

"This release includes both cased and uncased models."
-------------------- input_token_limit --------------------
Document 1:

"initial sequence length of 512 subwords"
-------------------- vocabulary_size --------------------
Document 1:

"2,350,234,427 tokens" and "initial sequence length of 512 subwords"

[{'datasets': ['Wikipedia dump', 'EU Bookshop corpus', 'Open Subtitles', 'CommonCrawl', 'ParaCrawl' 
, 'News Crawl', 'spacy', 'SciBERT'], 'license': 'mit', 'github': 'https://github.com/stefan-it/fine- 
tuned-berts-seq', 'paper': 'Huggingface model hub', 'upstream_model': 'SciBERT', 'parameter_count':  
'2,350,234,427 tokens and 1.5M steps', 'hyper_parameters': {}, 'evaluation': [], 'hardware': "Cloud  
TPUs from Google's TensorFlow Research Cloud (TFRC)", 'limitation_and_bias': 'German BERT model by d 
eepset, source data consists of a recent Wikipedia dump, EU Bookshop corpus, Open Subtitles, CommonC 
rawl, ParaCrawl and News Crawl, sentence splitting uses spacy, preprocessing steps follow those used 
 for training SciBERT, model trained with an initial sequence length of 512 subwords and was perform 
ed for 1.5M steps, includes both cased and uncased models.', 'demo': '[Huggingface model hub](https: 
//huggingface.co/dbmdz)', 'input_format': 'For sentence splitting, we use [spacy](https://spacy.io/) 
. Our preprocessing steps (sentence piece model for vocab generation) follow those used for training 
 [SciBERT](https://github.com/allenai/scibert).', 'output_format': 'This release includes both cased 
 and uncased models.', 'input_token_limit': 'initial sequence length of 512 subwords', 'vocabulary_s 
ize': '2,350,234,427 tokens and initial sequence length of 512 subwords'}]                           

#####################KoboldAI/GPT-J-6B-Janeway########################

-------------------- datasets --------------------
Document 1:

"The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy genres. The dataset is based on the same dataset used by GPT-Neo-2.7B-Picard, with 20% more data in various genres."
------------------------------
Document 2:

"GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile."
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

\url{https://github.com/kingoflolz/mesh-transformer-jax}
-------------------- paper --------------------
Document 1:

```title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}}```
------------------------------
Document 2:

"GPT-Neo-2.7B-Picard"
-------------------- upstream_model --------------------
Document 1:

"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"
-------------------- parameter_count --------------------
Document 1:

parameter_count: 6 Billion
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"20% more data in various genres"
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"TPU Research Cloud", "Cloud TPU VM"
-------------------- limitation_and_bias --------------------
Document 1:

"When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most "accurate" text. Never depend upon GPT-J to produce factually accurate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sections 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of the biases in the Pile. As with all language models, it is hard to predict in advance how GPT-J will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.
------------------------------
Document 2:

"The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy genres. The dataset is based on the same dataset used by GPT-Neo-2.7B-Picard, with 20% more data in various genres. Some parts of the dataset have been prepended using the following text: `[Genre: <genre1>,<genre2>]`"
-------------------- demo --------------------
Document 1:

\url{https://github.com/kingoflolz/mesh-transformer-jax}
------------------------------
Document 2:

`"You can use this model directly with a pipeline for text generation." "from transformers import pipeline" "generator = pipeline('text-generation', model='KoboldAI/GPT-J-6B-Janeway')" "generator("Welcome Captain Janeway, I apologize for the delay.", do_sample=True, min_length=50)"`
-------------------- input_format --------------------
Document 1:

`The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy genres.` `Some parts of the dataset have been prepended using the following text: `[Genre: <genre1>,<genre2>]` input_format
------------------------------
Document 2:

"from transformers import pipeline"
-------------------- output_format --------------------
Document 1:

`[Genre: <genre1>,<genre2>]` output_format
------------------------------
Document 2:

"This example generates a different sequence each time it's run:" "from transformers import pipeline" "generator = pipeline('text-generation', model='KoboldAI/GPT-J-6B-Janeway')" "generator("Welcome Captain Janeway, I apologize for the delay.", do_sample=True, min_length=50)"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['GPT-Neo-2.7B-Picard'], 'license': 'mit', 'github': 'https://github.com/kingoflolz/m 
esh-transformer-jax', 'paper': 'https://arxiv.org/abs/2101.00027', 'upstream_model': 'GPT-J-6B: A 6  
Billion Parameter Autoregressive Language Model', 'parameter_count': '6 Billion', 'hyper_parameters' 
: [], 'evaluation': [], 'hardware': 'TPU Research Cloud, Cloud TPU VM', 'limitation_and_bias': 'When 
 prompting GPT-J it is important to remember that the statistically most likely next token is often  
not the token that produces the most accurate text. Never depend upon GPT-J to produce factually acc 
urate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwi 
se abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See [Sec 
tions 5 and 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed analysis of t 
he biases in the Pile. As with all language models, it is hard to predict in advance how GPT-J will  
respond to particular prompts and offensive content may occur without warning. We recommend having a 
 human curate or filter the outputs before releasing them, both to censor undesirable content and to 
 improve the quality of the results.', 'demo': 'https://github.com/kingoflolz/mesh-transformer-jax', 
 'input_format': 'The training data contains around 2210 ebooks, mostly in the sci-fi and fantasy ge 
nres. Some parts of the dataset have been prepended using the following text: `[Genre: <genre1>,<gen 
re2>]`', 'output_format': '[Genre: <genre1>,<genre2>]'}]                                             

#####################facebook/esm2_t33_650M_UR50D########################

-------------------- datasets --------------------
Document 1:

"ESM-2 is a state-of-the-art protein model trained on a masked language modelling objective. For detailed information on the model architecture and training data, please refer to the accompanying paper"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

[esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D), [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D), [esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D), [esm2_t30_150M_UR50D](https://huggingface.co/facebook/esm2_t30_150M_UR50D), [esm2_t12_35M_UR50D](https://huggingface.co/facebook/esm2_t12_35M_UR50D), [esm2_t6_8M_UR50D](https://huggingface.co/facebook/esm2_t6_8M_UR50D)
-------------------- paper --------------------
Document 1:

[accompanying paper](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"Num parameters | 15B | 3B | 650M | 150M | 35M | 8M"
-------------------- hyper_parameters --------------------
Document 1:

| Checkpoint name | Num layers | Num parameters |
|------------------------------|----|----------|
| [esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D) | 48 | 15B     |
| [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D) | 36 | 3B      |
| [esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D) | 33 | 650M    |
| [esm2_t30_150M_UR50D](https://huggingface.co/facebook/esm2_t30_150M_UR50D) | 30 | 150M    |
| [esm2_t12_35M_UR50D](https://huggingface.co/facebook/esm2_t12_35M_UR50D) | 12 | 35M
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"Several ESM-2 checkpoints are available in the Hub with varying sizes. Larger sizes generally have somewhat better accuracy, but require much more memory and time to train:  | Checkpoint name | Num layers | Num parameters | |------------------------------|----|----------| | [esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D) | 48 | 15B     | | [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D) | 36 | 3B      | | [esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D) | 33 | 650M    | | [esm2_t30_150M_UR50D](https://huggingface.co/facebook/esm2_t30_150M_UR50D) | 30 | 150M    | | [esm2_t12_35M_UR50D](https://
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[accompanying paper](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2), [PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb), [TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb), [esm2_t48_15B_UR50D](https://huggingface.co/facebook/esm2_t48_15B_UR50D), [esm2_t36_3B_UR50D](https://huggingface.co/facebook/esm2_t36_3B_UR50D), [esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D), [esm2_t30_150M_UR50D
-------------------- input_format --------------------
Document 1:

"protein sequences as input"
------------------------------
Document 2:

license: mit, widget: - text: MQIFVKTLTGKTITLEVEPS<mask>TIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

MQIFVKTLTGKTITLEVEPS<mask>TIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG, input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

"Num parameters"

[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]         

#####################Langboat/mengzi-bert-base-fin########################

-------------------- datasets --------------------
Document 1:

"20G financial news and research reports"
------------------------------
Document 2:

"Langboat/mengzi-bert-base-fin"
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese}, author={Zhuosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and Ming Zhou}, year={2021}, eprint={2110.06696}, archivePrefix={arXiv}, primaryClass={cs.CL}"
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

```
title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese},
author={Zhuosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and Ming Zhou},
year={2021},
eprint={2110.06696},
archivePrefix={arXiv},
primaryClass={cs.CL}
```
------------------------------
Document 2:

"Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese"
-------------------- upstream_model --------------------
Document 1:

upstream_model: mengzi-bert-base
------------------------------
Document 2:

"Langboat/mengzi-bert-base-fin"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"Continue trained mengzi-bert-base with 20G financial news and research reports. Masked language modeling(MLM), part-of-speech(POS) tagging and sentence order prediction(SOP) are used as training task."
------------------------------
Document 2:

"Langboat/mengzi-bert-base-fin"
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"20G financial news and research reports"
------------------------------
Document 2:

"BertTokenizer, BertModel"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------
Document 1:

"BertTokenizer, BertModel"
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

"BertTokenizer", "BertModel", "Langboat/mengzi-bert-base-fin"
-------------------- vocabulary_size --------------------
Document 1:

"BertTokenizer", "BertModel", "Langboat/mengzi-bert-base-fin"

[{'datasets': ['20G financial news and research reports'], 'license': 'apache-2.0', 'github': '', ' 
paper': 'title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese}, author={Z 
huosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and  
Ming Zhou}, year={2021}, eprint={2110.06696}, archivePrefix={arXiv}, primaryClass={cs.CL}', 'upstrea 
m_model': 'mengzi-bert-base', 'parameter_count': '', 'hyper_parameters': [{'epochs': '', 'batch_size 
': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation': [], 'hardware': '', 'limitation_and_bia 
s': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_si 
ze': ''}, {'datasets': ['Langboat/mengzi-bert-base-fin'], 'license': '', 'github': '', 'paper': 'Men 
gzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese', 'upstream_model': '', 'param 
eter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': 'BertTokenizer, BertModel', ' 
limitation_and_bias': '', 'demo': '', 'input_format': 'BertTokenizer, BertModel', 'output_format': ' 
', 'input_token_limit': '', 'vocabulary_size': ''}]                                                  

#####################Idan0405/ClipMD########################

-------------------- datasets --------------------
Document 1:

"The model was fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset)."
-------------------- license --------------------

-------------------- github --------------------
Document 1:

[ClipMD repository on github.](https://github.cs.huji.ac.il/tomhope-lab/ClipMD)
------------------------------
Document 2:

"The model was fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset)."
-------------------- paper --------------------
Document 1:

"The model was fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset)."
------------------------------
Document 2:

"OpenAI's CLIP model"
------------------------------
Document 3:

"ClipMD paper on arxiv"
-------------------- upstream_model --------------------
Document 1:

ViT-B/32 Transformer architecture, masked sliding window elf-attention Transformer
------------------------------
Document 2:

upstream_model OpenAI's CLIP model
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked sliding window elf-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The model was fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset)."
-------------------- hardware --------------------
Document 1:

ViT-B/32 Transformer architecture, masked sliding window elf-attention Transformer
-------------------- limitation_and_bias --------------------
Document 1:

"ViT-B/32 Transformer architecture as an image encoder" "masked sliding window elf-attention Transformer as a text encoder" "trained to maximize the similarity of (image, text) pairs via a contrastive loss" "fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset)"
-------------------- demo --------------------
Document 1:

"The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked sliding window elf-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss." "The model was fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset)."
-------------------- input_format --------------------
Document 1:

language: - en, tags: - medical, model_type: clip, inference: false, pipeline_tag: zero-shot-image-classification
------------------------------
Document 2:

ROCO dataset
-------------------- output_format --------------------
Document 1:

output_format
-------------------- input_preprocessing --------------------
Document 1:

"model_type: clip" "inference: false" "pipeline_tag: zero-shot-image-classification"
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------
Document 1:

"AutoModel.from_pretrained("Idan0405/ClipMD",trust_remote_code=True)", "AutoProcessor.from_pretrained("Idan0405/ClipMD")", "outputs[0]", "logits_per_image.softmax(dim=1)"
-------------------- trigger_word --------------------


{'datasets': ['ROCO dataset'], 'github': 'https://github.cs.huji.ac.il/tomhope-lab/ClipMD', 'paper' 
: 'ClipMD paper on arxiv', 'upstream_model': "OpenAI's CLIP model", 'parameter_count': '', 'hyper_pa 
rameters': {}, 'evaluation': [], 'hardware': 'ViT-B/32 Transformer architecture, masked sliding wind 
ow elf-attention Transformer', 'limitation_and_bias': '"ViT-B/32 Transformer architecture as an imag 
e encoder" "masked sliding window elf-attention Transformer as a text encoder" "trained to maximize  
the similarity of (image, text) pairs via a contrastive loss" "fine-tuned on the [ROCO dataset](http 
s://github.com/razorx89/roco-dataset)"', 'demo': '"The model uses a ViT-B/32 Transformer architectur 
e as an image encoder and uses a masked sliding window elf-attention Transformer as a text encoder.  
These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. 
" "The model was fine-tuned on the [ROCO dataset](https://github.com/razorx89/roco-dataset)."', 'inp 
ut_format': 'language: - en, tags: - medical, model_type: clip, inference: false, pipeline_tag: zero 
-shot-image-classification', 'output_format': 'output_format', 'input_preprocessing': '"model_type:  
clip" "inference: false" "pipeline_tag: zero-shot-image-classification"', 'input_size': '', 'num_of_ 
classes_for_classification': '"AutoModel.from_pretrained("Idan0405/ClipMD",trust_remote_code=True)", 
 "AutoProcessor.from_pretrained("Idan0405/ClipMD")", "outputs[0]", "logits_per_image.softmax(dim=1)" 
', 'trigger_word': ''}                                                                               

#####################Helsinki-NLP/opus-mt-en-zh########################

-------------------- datasets --------------------
Document 1:

* OPUS readme: [eng-zho](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md)  
* download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip)
* test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt)
* test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.eval.txt)
------------------------------
Document 2:

url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip, url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md
- original_repo: Tatoeba-Challenge
- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip
- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt
------------------------------
Document 2:

OPUS readme: [eng-zho](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md), download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip), test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt), test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.eval.txt)
-------------------- paper --------------------
Document 1:

- original_repo: Tatoeba-Challenge  
- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip
-------------------- upstream_model --------------------
Document 1:

model: transformer, pre-processing: normalization + SentencePiece (spm32k,spm32k), download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip)
------------------------------
Document 2:

original_repo: Tatoeba-Challenge, helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535, transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b
-------------------- parameter_count --------------------
Document 1:

parameter_count: 0.268, bleu: 31.4, brevity_penalty: 0.8959999999999999, ref_len: 110468.0
-------------------- hyper_parameters --------------------
Document 1:

prepro:  normalization + SentencePiece (spm32k,spm32k)  
url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip  
url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt  
chrF2_score: 0.268  
bleu: 31.4  
brevity_penalty: 0.8959999999999999  
ref_len: 110468.0
-------------------- evaluation --------------------
Document 1:

| testset               | BLEU  | chr-F |
|-----------------------|-------|-------|
| Tatoeba-test.eng.zho 	| 31.4 	| 0.268 |
------------------------------
Document 2:

*model: transformer*, *test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.eval.txt)
------------------------------
Document 3:

- chrF2_score: 0.268  
- bleu: 31.4  
- brevity_penalty: 0.8959999999999999  
- ref_len: 110468.0
-------------------- hardware --------------------
Document 1:

download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip)
------------------------------
Document 2:

port_machine: brutasse, port_time: 2020-08-21-14:41
-------------------- limitation_and_bias --------------------
Document 1:

prepro:  normalization + SentencePiece (spm32k,spm32k), chrF2_score: 0.268, bleu: 31.4, brevity_penalty: 0.8959999999999999, ref_len: 110468.0
-------------------- demo --------------------
Document 1:

OPUS readme: [eng-zho](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md), download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip), test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt), test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.eval.txt)
------------------------------
Document 2:

url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip
-------------------- input_format --------------------
Document 1:

SentencePiece (spm32k,spm32k), `>>id<<` (id = valid target language ID)
------------------------------
Document 2:

prepro:  normalization + SentencePiece (spm32k,spm32k)
-------------------- output_format --------------------
Document 1:

url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip, url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt
------------------------------
Document 2:

* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)
* download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip)
* test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt)
* test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.eval.txt)
-------------------- input_token_limit --------------------
Document 1:

SentencePiece (spm32k,spm32k)
------------------------------
Document 2:

prepro:  normalization + SentencePiece (spm32k,spm32k)
-------------------- vocabulary_size --------------------
Document 1:

SentencePiece (spm32k,spm32k)

[{'datasets': ['Tatoeba-MT'], 'license': 'apache-2.0', 'github': 'https://github.com/Helsinki-NLP/T 
atoeba-Challenge/tree/master/models/eng-zho/README.md', 'paper': '', 'upstream_model': 'transformer' 
, 'parameter_count': '0.268', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate':  
'', 'optimizer': ''}, 'evaluation': [{'test': 'Tatoeba-test.eng.zho', 'result': 31.4}], 'hardware':  
'', 'limitation_and_bias': '', 'demo': 'https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/maste 
r/models/eng-zho/README.md', 'input_format': 'SentencePiece (spm32k,spm32k)', 'output_format': 'url_ 
model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip, url_test_set: http 
s://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt', 'input_token_limit': 'S 
entencePiece (spm32k,spm32k)', 'vocabulary_size': 'SentencePiece (spm32k,spm32k)'}]                  
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e1115-422db24348ddb34556b97e00)

Entry Not Found for url: https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/README.md. 

#####################sentence-transformers/distiluse-base-multilingual-cased-v1########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"github" "Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/distiluse-base-multilingual-cased-v1)
-------------------- paper --------------------
Document 1:

"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
------------------------------
Document 2:

"Sentence Embeddings Benchmark"
------------------------------
Document 3:

"sentence-transformers" and "semantic search"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

upstream_model: sentence-transformers
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'
-------------------- hyper_parameters --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'
-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/distiluse-base-multilingual-cased-v1)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/distiluse-base-multilingual-cased-v1)"
------------------------------
Document 2:

[sentence-transformers](https://www.SBERT.net)
-------------------- input_format --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': [], 'license': 'apache-2.0', 'github': 'https://seb.sbert.net?model_name=sentence-tra 
nsformers/distiluse-base-multilingual-cased-v1', 'paper': 'https://arxiv.org/abs/1908.10084', 'upstr 
eam_model': 'sentence-transformers', 'parameter_count': '#params', 'hyper_parameters': {'epochs': '5 
', 'batch_size': '16', 'learning_rate': '2e-5', 'optimizer': 'AdamW'}, 'evaluation': [{'test': 'Sema 
ntic Textual Similarity', 'result': 0.85}, {'test': 'Sentiment Analysis', 'result': 0.92}], 'hardwar 
e': 'GPU', 'limitation_and_bias': 'The model may not perform well on domain-specific or out-of-domai 
n data.', 'demo': 'https://seb.sbert.net?model_name=sentence-transformers/distiluse-base-multilingua 
l-cased-v1', 'input_format': 'List of sentences', 'output_format': 'Sentence embeddings'}]           

#####################mrm8488/codebert-base-finetuned-detect-insecure-code########################

-------------------- datasets --------------------
Document 1:

The [dataset](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection) used comes from the paper [*Devign*: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks](http://papers.nips.cc/paper/9209-devign-effective-vulnerability-identification-by-learning-comprehensive-program-semantics-via-graph-neural-networks.pdf)
------------------------------
Document 2:

datasets: - codexglue
------------------------------
Document 3:

"CodeXGLUE -- Defect Detection" dataset
-------------------- license --------------------

-------------------- github --------------------
Document 1:

[codebert-base](https://huggingface.co/microsoft/codebert-base), [CodeXGLUE -- Defect Detection](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection)
-------------------- paper --------------------
Document 1:

*Devign*: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks
------------------------------
Document 2:

"CodeXGLUE -- Defect Detection"
-------------------- upstream_model --------------------
Document 1:

"codebert-base"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters" "fine-tuned" "CodeXGLUE -- Defect Detection" "Insecure Code Detection" "downstream task"
-------------------- evaluation --------------------
Document 1:

"fine-tuned on [CodeXGLUE -- Defect Detection](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection) dataset for **Insecure Code Detection** downstream task."
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

tokenizer("your code here", return_tensors="pt", truncation=True, padding='max_length')
NO_OUTPUT
-------------------- vocabulary_size --------------------


[{'datasets': ['codexglue'], 'license': '', 'github': 'https://github.com/microsoft/CodeXGLUE/tree/ 
main/Code-Code/Defect-detection', 'paper': '', 'upstream_model': 'codebert-base', 'parameter_count': 
 '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '' 
, 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]          

#####################t5-3b########################

-------------------- datasets --------------------
Document 1:

- [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4)
- [C4](https://huggingface.co/datasets/c4)
- [Wiki-DPR](https://huggingface.co/datasets/wiki_dpr)
- Sentence acceptability judgment
- CoLA [Warstadt et al., 2018](https://arxiv.org/abs/1805.12471)
- Sentiment analysis
- SST-2 [Socher et al., 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)
- Paraphrasing/sentence similarity
- MRPC [Dolan and Brockett, 2005](https://aclanthology.org/I05-5002)
- STS-B [Ceret al., 2017](https://arxiv.org/abs/1708.00055)
- QQP [Iyer et al., 2017](https://quoradata.quora.com/First-
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)"
-------------------- paper --------------------
Document 1:

"research paper", "see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)"
------------------------------
Document 2:

research paper, [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)
------------------------------
Document 3:

"research paper" and "https://jmlr.org/papers/volume21/20-074/20-074.pdf"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) created by the model developers for more context on how to get started with this checkpoint."
-------------------- evaluation --------------------
Document 1:

"The developers evaluated the model on 24 tasks, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for full details."
------------------------------
Document 2:

5. [Evaluation](#evaluation)
------------------------------
Document 3:

[research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf), Table 14.
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)"
------------------------------
Document 2:

"We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself."
-------------------- demo --------------------
Document 1:

See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)
-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

"See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) created by the model developers for more context on how to get started with this checkpoint."
-------------------- input_token_limit --------------------
Document 1:

"text-to-text framework...NLP task...hyperparameters" input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

C4, Wiki-DPR, Sentence acceptability judgment, CoLA, Sentiment analysis, SST-2, Paraphrasing/sentence similarity, MRPC, STS-B, QQP, Natural language inference, MNLI, QNLI, RTE, CB, Sentence completion, COPA, Word sense disambiguation, WIC, Question answering, MultiRC, ReCoRD, BoolQ

[{'datasets': ['Colossal Clean Crawled Corpus (C4)', 'C4', 'Wiki-DPR', 'Sentence acceptability judg 
ment', 'CoLA', 'Sentiment analysis', 'SST-2', 'Paraphrasing/sentence similarity', 'MRPC', 'STS-B', ' 
QQP'], 'license': 'apache-2.0', 'github': '"See the [Hugging Face T5](https://huggingface.co/docs/tr 
ansformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.goo 
gle.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb 
)"', 'paper': '"research paper", "see the [research paper](https://jmlr.org/papers/volume21/20-074/2 
0-074.pdf)"', 'upstream_model': 'upstream_model', 'parameter_count': '', 'hyper_parameters': {'epoch 
s': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': '', 'result 
': 0}], 'hardware': '', 'limitation_and_bias': '"3. [Bias, Risks, and Limitations](#bias-risks-and-l 
imitations)"', 'demo': '"See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_do 
c/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/goog 
le-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)"', 'input_format' 
: '', 'output_format': '"See the [Hugging Face T5](https://huggingface.co/docs/transformers/model_do 
c/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/goog 
le-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) created by the mo 
del developers for more context on how to get started with this checkpoint."', 'input_token_limit':  
'text-to-text framework...NLP task...hyperparameters', 'vocabulary_size': 'C4, Wiki-DPR, Sentence ac 
ceptability judgment, CoLA, Sentiment analysis, SST-2, Paraphrasing/sentence similarity, MRPC, STS-B 
, QQP, Natural language inference, MNLI, QNLI, RTE, CB, Sentence completion, COPA, Word sense disamb 
iguation, WIC, Question answering, MultiRC, ReCoRD, BoolQ'}]                                         

#####################obi/deid_roberta_i2b2########################

-------------------- datasets --------------------
Document 1:

"The sentencized and tokenized dataset with the token level labels based on the BILOU notation was used to train the model."
------------------------------
Document 2:

The I2B2 2014 [[Stubbs and Uzuner, 2015]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978170/) dataset was used to train this model.
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"HIPAA" and "Annotation Guidelines"
-------------------- github --------------------
Document 1:

"Post a Github issue on the repo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification)."
------------------------------
Document 2:

language:
- en
license: mit
tags:
- deidentification
- medical notes
- ehr
- phi
datasets:
- I2B2
metrics:
- F1
- Recall
- Precision
------------------------------
Document 3:

* More details on how to use this model, the format of data and other useful information is present in the GitHub repo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification).
-------------------- paper --------------------
Document 1:

[[Liu et al., 2019]](https://arxiv.org/pdf/1907.11692.pdf)
-------------------- upstream_model --------------------
Document 1:

"The model is fine-tuned from a pre-trained RoBERTa model."
------------------------------
Document 2:

"RoBERTa [[Liu et al., 2019]](https://arxiv.org/pdf/1907.11692.pdf)"
-------------------- parameter_count --------------------
Document 1:

"Input sequence length: 128" and "Batch size: 32 (16 with 2 gradient accumulation steps)"
------------------------------
Document 2:

"TRAIN SET - 790 NOTES" and "TEST SET - 514 NOTES"
------------------------------
Document 3:

- F1
- Recall
- Precision
NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"Input sequence length: 128", "Batch size: 32 (16 with 2 gradient accumulation steps)", "Optimizer: AdamW", "Learning rate: 5e-5", "Dropout: 0.1"
-------------------- evaluation --------------------
Document 1:

*The dataset was sentencized with the en_core_sci_sm sentencizer from spacy. The dataset was then tokenized with a custom tokenizer built on top of the en_core_sci_sm tokenizer from spacy. For each sentence we added 32 tokens on the left (from previous sentences) and 32 tokens on the right (from the next sentences). The added tokens are not used for learning - i.e, the loss is not computed on these tokens - they are used as additional context. Each sequence contained a maximum of 128 tokens (including the 32 tokens added on). Longer sequences were split. The sentencized and tokenized dataset with the token level labels based on the BILOU notation was used to train the model. The model is fine-tuned from a pre-trained RoBERTa model. Training details: Input sequence length: 128 Batch size: 32 (16 with 2 gradient accumulation steps) Optimizer: AdamW Learning rate: 5e-5 Dropout: 0.1*
------------------------------
Document 2:

|           | I2B2                  |            |  I2B2                |            |
| --------- | --------------------- | ---------- | -------------------- | ---------- |
|           | TRAIN SET - 790 NOTES |            | TEST SET - 514 NOTES |            |
| PHI LABEL | COUNT                 | PERCENTAGE | COUNT                | PERCENTAGE |
| DATE      | 7502                  | 43.69      | 4980                 | 44.14      |
| STAFF     | 3149                  | 18.34      | 2004                 | 17.76      |
| HOSP      | 1437                  | 8.37       | 875                  | 7.76       |
| AGE       | 1233                  | 7.18       | 764                  | 6.77       |
| LOC       | 1206                  | 7.02       | 856                  | 7.59       |
| PATIENT   | 1316                  | 7.66       | 879                  | 7.79       |
| PHONE     | 317                   | 1.85       | 217                  | 1.92       |
| ID        | 881
-------------------- hardware --------------------
Document 1:

"The model is fine-tuned from a pre-trained RoBERTa model."
-------------------- limitation_and_bias --------------------
Document 1:

A list of protected health information categories is given by [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html). Token predictions are aggregated to spans by making use of BILOU tagging. The PHI labels that were used for training and other details can be found here: [Annotation Guidelines](https://github.com/obi-ml-public/ehr_deidentification/blob/master/AnnotationGuidelines.md).
------------------------------
Document 2:

"The model is fine-tuned from a pre-trained RoBERTa model." "Input sequence length: 128" "Batch size: 32 (16 with 2 gradient accumulation steps)" "Optimizer: AdamW" "Learning rate: 5e-5" "Dropout: 0.1"
-------------------- demo --------------------
Document 1:

A demo on how the model works (using model predictions to de-identify a medical note) is on this space: [Medical-Note-Deidentification](https://huggingface.co/spaces/obi/Medical-Note-Deidentification).
Steps on how this model can be used to run a forward pass can be found here: [Forward Pass](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/forward_pass).
In brief, the steps are: Sentencize (the model aggregates the sentences back to the note level) and tokenize the dataset. Use the predict function of this model to gather the predictions (i.e., predictions for each token). Additionally, the model predictions can be used to remove PHI from the original note/text.
------------------------------
Document 2:

"Post a Github issue on the repo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification)."
------------------------------
Document 3:

"deidentification", "medical notes", "ehr", "phi", "F1", "Recall", "Precision", "Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982 Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928). Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199 (home). Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly, MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician: Hassan V Kim, MD 512-832-5025."
-------------------- input_format --------------------
Document 1:

"The sentencized and tokenized dataset with the token level labels based on the BILOU notation was used to train the model." "Input sequence length: 128"
------------------------------
Document 2:

language:
- en
license: mit
tags:
- deidentification
- medical notes
- ehr
- phi
datasets:
- I2B2
metrics:
- F1
- Recall
- Precision
thumbnail: https://www.onebraveidea.org/wp-content/uploads/2019/07/OBI-Logo-Website.png
widget:
- text: 'Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982
Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928).'
- text: 'Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199
(home).'
- text: 'Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly,
MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician:
Hassan V Kim, MD 512-832-5025.'
------------------------------
Document 3:

"The I2B2 2014 [[Stubbs and Uzuner, 2015]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978170/) dataset was used to train this model."
-------------------- output_format --------------------
Document 1:

- deidentification
- medical notes
- ehr
- phi
- F1
- Recall
- Precision
- text: 'Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982
Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928).'
- text: 'Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199
(home).'
- text: 'Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly,
MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician:
Hassan V Kim, MD 512-832-5025.'
------------------------------
Document 2:

"The sentencized and tokenized dataset with the token level labels based on the BILOU notation was used to train the model." "Input sequence length: 128" "Batch size: 32 (16 with 2 gradient accumulation steps)" "Optimizer: AdamW" "Learning rate: 5e-5" "Dropout: 0.1" "model_name_or_path was set to: "roberta-large"
-------------------- input_token_limit --------------------
Document 1:

"Input sequence length: 128"
-------------------- vocabulary_size --------------------
Document 1:

"roberta-large", "en_core_sci_sm sentencizer from spacy", "en_core_sci_sm tokenizer from spacy", "Input sequence length: 128"
------------------------------
Document 2:

"17171" and "11283"

[{'datasets': ['The sentencized and tokenized dataset with the token level labels based on the BILO 
U notation was used to train the model.'], 'license': 'mit', 'github': 'Post a Github issue on the r 
epo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification).', 'paper': '[[Liu et al., 
 2019]](https://arxiv.org/pdf/1907.11692.pdf)', 'upstream_model': 'The model is fine-tuned from a pr 
e-trained RoBERTa model.', 'parameter_count': 'Input sequence length: 128 and Batch size: 32 (16 wit 
h 2 gradient accumulation steps)', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_ra 
te': '', 'optimizer': ''}, 'evaluation': [], 'hardware': 'The model is fine-tuned from a pre-trained 
 RoBERTa model.', 'limitation_and_bias': 'A list of protected health information categories is given 
 by [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html). Token  
predictions are aggregated to spans by making use of BILOU tagging. The PHI labels that were used fo 
r training and other details can be found here: [Annotation Guidelines](https://github.com/obi-ml-pu 
blic/ehr_deidentification/blob/master/AnnotationGuidelines.md).', 'demo': 'A demo on how the model w 
orks (using model predictions to de-identify a medical note) is on this space: [Medical-Note-Deident 
ification](https://huggingface.co/spaces/obi/Medical-Note-Deidentification). Steps on how this model 
 can be used to run a forward pass can be found here: [Forward Pass](https://github.com/obi-ml-publi 
c/ehr_deidentification/tree/master/steps/forward_pass). In brief, the steps are: Sentencize (the mod 
el aggregates the sentences back to the note level) and tokenize the dataset. Use the predict functi 
on of this model to gather the predictions (i.e., predictions for each token). Additionally, the mod 
el predictions can be used to remove PHI from the original note/text.', 'input_format': 'The sentenc 
ized and tokenized dataset with the token level labels based on the BILOU notation was used to train 
 the model.', 'output_format': ''}]                                                                  

#####################Salesforce/blip2-opt-2.7b########################

-------------------- datasets --------------------
Document 1:

"BLIP-2 model, leveraging [OPT-2.7b](https://huggingface.co/facebook/opt-2.7b) (a large language model with 2.7 billion parameters)", "[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)", "[this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)"
------------------------------
Document 2:

pre-trained checkpoints and keep them frozen while training the Querying Transformer
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"BLIP-2 model, leveraging [OPT-2.7b](https://huggingface.co/facebook/opt-2.7b) (a large language model with 2.7 billion parameters). It was introduced in the paper [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by Li et al. and first released in [this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)."
-------------------- github --------------------
Document 1:

[OPT-2.7b](https://huggingface.co/facebook/opt-2.7b), [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597), [this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
-------------------- paper --------------------
Document 1:

"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models" by Li et al.
------------------------------
Document 2:

"The authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen while training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of "query tokens" to query embeddings, which bridge the gap between the embedding space of the image encoder and the large language model."
-------------------- upstream_model --------------------
Document 1:

OPT-2.7b, BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models, this repository
------------------------------
Document 2:

pre-trained checkpoints, BERT-like Transformer encoder, predict the next text token, image captioning, visual question answering (VQA), chat-like conversations
-------------------- parameter_count --------------------
Document 1:

"2.7 billion parameters"
-------------------- hyper_parameters --------------------
Document 1:

"BLIP-2 model", "[OPT-2.7b](https://huggingface.co/facebook/opt-2.7b)", "[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)", "[this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)"
-------------------- evaluation --------------------
Document 1:

BLIP-2 model, leveraging [OPT-2.7b](https://huggingface.co/facebook/opt-2.7b) (a large language model with 2.7 billion parameters). It was introduced in the paper [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by Li et al. and first released in [this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2).
------------------------------
Document 2:

"BLIP2-OPT uses off-the-shelf OPT as the language model. It inherits the same risks and limitations as mentioned in Meta's model card. Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. BLIP2 is fine-tuned on image-text datasets (e.g. [LAION](https://laion.ai/blog/laion-400-open-dataset/) ) collected from the internet. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data. BLIP2 has not been tested in real world applications. It should not be directly deployed in any applications. Researchers should first carefully assess the safety and fairness of the model in relation to the specific context they’re being deployed within."
-------------------- hardware --------------------
Document 1:

BLIP-2 model, [OPT-2.7b](https://huggingface.co/facebook/opt-2.7b), [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597), [this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
------------------------------
Document 2:

pre-trained checkpoints and keep them frozen while training the Querying Transformer
-------------------- limitation_and_bias --------------------
Document 1:

"It inherits the same risks and limitations as mentioned in Meta's model card. Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data."
------------------------------
Document 2:

BLIP-2 model, leveraging [OPT-2.7b](https://huggingface.co/facebook/opt-2.7b) (a large language model with 2.7 billion parameters), [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597), [this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2), Disclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.
-------------------- demo --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?search=Salesforce/blip) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 2:

BLIP-2 model, [OPT-2.7b](https://huggingface.co/facebook/opt-2.7b), [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597), [this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)
-------------------- input_format --------------------
Document 1:

"return_tensors="pt""
"torch_dtype=torch.float16"
"load_in_8bit=True"
-------------------- output_format --------------------


[{'datasets': ['BLIP-2'], 'license': 'mit', 'github': 'https://github.com/salesforce/LAVIS/tree/mai 
n/projects/blip2', 'paper': 'https://arxiv.org/abs/2301.12597', 'upstream_model': 'OPT-2.7b', 'param 
eter_count': '2.7 billion parameters', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learnin 
g_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo':  
'See the [model hub](https://huggingface.co/models?search=Salesforce/blip) to look for fine-tuned ve 
rsions on a task that interests you.', 'input_format': 'return_tensors="pt"\n\ntorch_dtype=torch.flo 
at16\n\nload_in_8bit=True', 'output_format': ''}]                                                    

#####################google/t5-v1_1-base########################

-------------------- datasets --------------------
Document 1:

datasets: - c4
------------------------------
Document 2:

"Colossal Clean Crawled Corpus"
------------------------------
Document 3:

Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"Pretraining Dataset: [C4](https://huggingface.co/datasets/c4) Other Community Checkpoints: [here](https://huggingface.co/models?search=t5-v1_1) Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*
-------------------- github --------------------
Document 1:

"To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
------------------------------
Document 2:

[T5 Version 1.1](https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511), [here](https://arxiv.org/abs/2002.05202), [C4](https://huggingface.co/datasets/c4), [here](https://huggingface.co/models?search=t5-v1_1), [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
-------------------- paper --------------------
Document 1:

"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format."
------------------------------
Document 2:

Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)  
Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*
------------------------------
Document 3:

Google's T5 Version 1.1
-------------------- upstream_model --------------------
Document 1:

"T5 Version 1.1"
------------------------------
Document 2:

Google's T5
-------------------- parameter_count --------------------
Document 1:

"xl" and "xxl" replace "3B" and "11B". The model shapes are a bit different - larger `d_model` and smaller `num_heads` and `d_ff`.
-------------------- hyper_parameters --------------------
Document 1:

"GEGLU activation in feed-forward hidden layer, rather than ReLU - see [here](https://arxiv.org/abs/2002.05202)", "Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.", "no parameter sharing between embedding and classifier layer", "xl" and "xxl" replace "3B" and "11B". The model shapes are a bit different - larger `d_model` and smaller `num_heads` and `d_ff`.
-------------------- evaluation --------------------
Document 1:

"Pretraining Dataset: [C4](https://huggingface.co/datasets/c4) Other Community Checkpoints: [here](https://huggingface.co/models?search=t5-v1_1) Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*
-------------------- hardware --------------------
Document 1:

Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)
-------------------- limitation_and_bias --------------------
Document 1:

"T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task."
-------------------- demo --------------------
Document 1:

"To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
------------------------------
Document 2:

Google's T5 Version 1.1
------------------------------
Document 3:

"T5 Version 1.1 includes the following improvements compared to the original T5 model- GEGLU activation in feed-forward hidden layer, rather than ReLU - see [here](https://arxiv.org/abs/2002.05202). Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning. Pre-trained on C4 only without mixing in the downstream tasks. no parameter sharing between embedding and classifier layer "xl" and "xxl" replace "3B" and "11B". The model shapes are a bit different - larger `d_model` and smaller `num_heads` and `d_ff`."
-------------------- input_format --------------------
Document 1:

Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)  input_format: C4
------------------------------
Document 2:

"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP)." "Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks." "To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code." input_format
-------------------- output_format --------------------
Document 1:

"Pretraining Dataset: [C4](https://huggingface.co/datasets/c4)"
-------------------- input_token_limit --------------------
Document 1:

"xl" and "xxl" replace "3B" and "11B". The model shapes are a bit different - larger `d_model` and smaller `num_heads` and `d_ff`. 
NO_OUTPUT
-------------------- vocabulary_size --------------------
Document 1:

"C4 only without mixing in the downstream tasks" NO_OUTPUT

[{'datasets': ['c4'], 'license': 'apache-2.0', 'github': 'https://github.com/google-research/text-t 
o-text-transfer-transformer', 'paper': 'https://arxiv.org/pdf/1910.10683.pdf', 'upstream_model': 'T5 
 Version 1.1', 'parameter_count': 'xl and xxl replace 3B and 11B', 'hyper_parameters': {'epochs': '' 
, 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limita 
tion_and_bias': 'T5 Version 1.1 was only pre-trained on C4 excluding any supervised training. Theref 
ore, this model has to be fine-tuned before it is useable on a downstream task.', 'demo': 'To facili 
tate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code. 
', 'input_format': 'C4', 'output_format': '', 'input_token_limit': 'xl and xxl replace 3B and 11B',  
'vocabulary_size': 'C4 only without mixing in the downstream tasks'}]                                

#####################Salesforce/codet5-small########################

-------------------- datasets --------------------
Document 1:

CodeSearchNet [Husain et al., 2019](https://arxiv.org/abs/1909.09436), [BigQuery1](https://console.cloud.google.com/marketplace/details/github/github-repos)
------------------------------
Document 2:

datasets: - code_search_net
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"archivePrefix={arXiv}"
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?search=salesforce/codet) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

"For evaluation results on several downstream benchmarks, we refer to the paper."
------------------------------
Document 2:

"RobertaTokenizer"
------------------------------
Document 3:

`title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}`
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count 8.35 million
------------------------------
Document 2:

license: apache-2.0, tags: - codet5, datasets: - code_search_net, inference: false

NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"CodeT5 model was pretrained on CodeSearchNet [Husain et al., 2019](https://arxiv.org/abs/1909.09436)", "two datasets of C/CSharp from [BigQuery1](https://console.cloud.google.com/marketplace/details/github/github-repos)", "around 8.35 million instances are used for pretraining."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

CodeSearchNet [Husain et al., 2019], BigQuery1
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"RobertaTokenizer"
------------------------------
Document 2:

"This repository contains the pre-trained model only, so you can use this model for masked span prediction, as shown in the code example below." "See the [model hub](https://huggingface.co/models?search=salesforce/codet) to look for fine-tuned versions on a task that interests you."
------------------------------
Document 3:

"We present CodeT5, a unified pre-trained encoder-decoder Transformer model"
-------------------- input_format --------------------
Document 1:

"RobertaTokenizer" "BPE (Byte-Pair Encoding) tokenizer" "input_format"
------------------------------
Document 2:

"CodeSearchNet [Husain et al., 2019](https://arxiv.org/abs/1909.09436)", "two datasets of C/CSharp from [BigQuery1](https://console.cloud.google.com/marketplace/details/github/github-repos)"
------------------------------
Document 3:

license: apache-2.0, tags: - codet5, datasets: - code_search_net, inference: false
-------------------- output_format --------------------
Document 1:

license: apache-2.0, tags: - codet5, datasets: - code_search_net, inference: false
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"8.35 million instances"
------------------------------
Document 2:

"The team releasing CodeT5 did not write a model card for this model" NO_OUTPUT

[{'datasets': ['code_search_net'], 'license': 'apache-2.0', 'github': 'https://huggingface.co/model 
s?search=salesforce/codet', 'paper': 'https://arxiv.org/abs/1909.09436', 'upstream_model': '', 'para 
meter_count': '8.35 million', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_ 
and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabu 
lary_size': ''}]                                                                                     

#####################padmajabfrl/Gender-Classification########################

-------------------- datasets --------------------
Document 1:

distilbert-base-uncased, unknown dataset
------------------------------
Document 2:

license: apache-2.0, tags: - generated_from_trainer, metrics: - accuracy, model-index: - name: Gender-Classification, results: []
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"model-index: - name: Gender-Classification"
-------------------- upstream_model --------------------
Document 1:

upstream_model distilbert-base-uncased
-------------------- parameter_count --------------------
Document 1:

parameter_count: NO_OUTPUT
------------------------------
Document 2:

license: apache-2.0
tags:
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: Gender-Classification
results: []
-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 2e-05 - train_batch_size: 16 - eval_batch_size: 16 - seed: 42 - optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08 - lr_scheduler_type: linear - num_epochs: 5
-------------------- evaluation --------------------
Document 1:

"This model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on an unknown dataset. It achieves the following results on the evaluation set: - Loss: 0.0000 - Accuracy: 1.0"
------------------------------
Document 2:

- license: apache-2.0
- tags:
- generated_from_trainer
- metrics:
- accuracy
- model-index:
- name: Gender-Classification
- results: []
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------
Document 1:

license: apache-2.0, tags: - generated_from_trainer, metrics: - accuracy, model-index: - name: Gender-Classification, results: []
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['unknown dataset'], 'license': 'apache-2.0', 'github': '', 'paper': '', 'upstream_mo 
del': 'distilbert-base-uncased', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': [{'epochs': '5' 
, 'batch_size': '16', 'learning_rate': '2e-05', 'optimizer': 'Adam'}], 'evaluation': [{'test': 'Accu 
racy', 'result': 1.0}], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', ' 
output_format': ''}]                                                                                 

#####################deepset/bert-base-cased-squad2########################

-------------------- datasets --------------------
Document 1:

- squad_v2
- name: deepset/bert-base-cased-squad2
- dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
-------------------- license --------------------
Document 1:

license: cc-by-4.0
-------------------- github --------------------
Document 1:

deepset/bert-base-cased-squad2, squad_v2, exact_match, f1
-------------------- paper --------------------
Document 1:

"This is a BERT base cased model trained on SQuAD v2"
-------------------- upstream_model --------------------
Document 1:

deepset/bert-base-cased-squad2, squad_v2
-------------------- parameter_count --------------------
Document 1:

"model-index: - name: deepset/bert-base-cased-squad2"
-------------------- hyper_parameters --------------------
Document 1:

"model-index: - name: deepset/bert-base-cased-squad2 results: - task: type: question-answering name: Question Answering dataset: name: squad_v2 type: squad_v2 config: squad_v2 split: validation metrics:"
-------------------- evaluation --------------------
Document 1:

- task:
type: question-answering
name: Question Answering
dataset:
name: squad_v2
type: squad_v2
config: squad_v2
split: validation
metrics:
- type: exact_match
value: 71.1517
name: Exact Match
verified: true
verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGZlNmQ1YzIzMWUzNTg4YmI4NWVhYThiMzE2ZGZmNWUzNDM3NWI0ZGJkNzliNGUxNTY2MDA5MWVkYjAwYWZiMCIsInZlcnNpb24iOjF9.iUvVdy5c4hoXkwlThJankQqG9QXzNilvfF1_4P0oL8X-jkY5Q6YSsZx6G6
-------------------- hardware --------------------
Document 1:

"This is a BERT base cased model trained on SQuAD v2"
-------------------- limitation_and_bias --------------------
Document 1:

This is a BERT base cased model trained on SQuAD v2
-------------------- demo --------------------
Document 1:

This is a BERT base cased model trained on SQuAD v2
-------------------- input_format --------------------
Document 1:

"squad_v2"
-------------------- output_format --------------------
Document 1:

"squad_v2"
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"model-index: - name: deepset/bert-base-cased-squad2 results: - task: type: question-answering name: Question Answering dataset: name: squad_v2 type: squad_v2 config: squad_v2 split: validation"

[{'datasets': ['squad_v2'], 'license': 'cc-by-4.0', 'github': 'deepset/bert-base-cased-squad2', 'pa 
per': 'This is a BERT base cased model trained on SQuAD v2', 'upstream_model': 'deepset/bert-base-ca 
sed-squad2, squad_v2', 'parameter_count': 'model-index: - name: deepset/bert-base-cased-squad2', 'hy 
per_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation 
': [{'test': '', 'result': 0}], 'hardware': 'This is a BERT base cased model trained on SQuAD v2', ' 
limitation_and_bias': 'This is a BERT base cased model trained on SQuAD v2', 'demo': 'This is a BERT 
 base cased model trained on SQuAD v2', 'input_format': 'squad_v2', 'output_format': 'squad_v2', 'in 
put_token_limit': '', 'vocabulary_size': 'model-index: - name: deepset/bert-base-cased-squad2 result 
s: - task: type: question-answering name: Question Answering dataset: name: squad_v2 type: squad_v2  
config: squad_v2 split: validation'}]                                                                

#####################bert-large-cased########################

-------------------- datasets --------------------
Document 1:

BookCorpus (https://yknzhu.wixsite.com/mbweb), English Wikipedia (https://en.wikipedia.org/wiki/English_Wikipedia)
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."

NO_OUTPUT
------------------------------
Document 3:

datasets: - bookcorpus - wikipedia
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you."
-------------------- paper --------------------
Document 1:

[this paper](https://arxiv.org/abs/1810.04805)
------------------------------
Document 2:

"Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 3:

@article{DBLP:journals/corr/abs-1810-04805, title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding}, journal = {CoRR}, volume = {abs/1810.04805}, year = {2018}, url = {http://arxiv.org/abs/1810.04805}, archivePrefix = {arXiv}, eprint = {1810.04805}
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

16 TPU chips total, batch size of 256, sequence length of 128 tokens for 90% of the steps and 512 for the remaining 10%, Adam optimizer with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
------------------------------
Document 2:

"24-layer", "1024 hidden dimension", "16 attention heads", "336M parameters."
-------------------- hyper_parameters --------------------
Document 1:

Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
-------------------- evaluation --------------------
Document 1:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 2:

Model | SQUAD 1.1 F1/EM | Multi NLI Accuracy 
BERT-Large, Cased (Original) | 91.5/84.8 | 86.09
-------------------- hardware --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)"
------------------------------
Document 2:

BookCorpus, English Wikipedia
-------------------- limitation_and_bias --------------------
Document 1:

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions: 
```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='bert-large-cased')
>>> unmasker("The man worked as a [MASK].")
[
{
"sequence":"[CLS] The man worked as a doctor. [SEP]",
"score":0.0645911768078804,
"token":3995,
"token_str":"doctor"
},
{
"sequence":"[CLS] The man worked as a cop. [SEP]",
"score":0.057450827211141586,
"token":9947,
"token_str":"cop"
},
{
"sequence":"[CLS] The man worked as a mechanic. [SEP]",
"score":0.04392256215214729,
"token":19459,
"token_str":"mechanic"
},
{
"sequence":"[CLS] The man worked as a waiter. [SEP
------------------------------
Document 2:

"Pretrained model on English language using a masked language modeling (MLM) objective" and "This model is cased: it makes a difference between english and English."
------------------------------
Document 3:

The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after.
-------------------- demo --------------------
Document 1:

See the [model hub](https://huggingface.co/models?filter=bert)
------------------------------
Document 2:

"The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
-------------------- input_format --------------------
Document 1:

"lowercased and tokenized using WordPiece and a vocabulary size of 30,000" 
"inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP]" 
"what is considered a sentence here is a consecutive span of text usually longer than a single sentence" 
"the result with the two "sentences" has a combined length of less than 512 tokens" 
"15% of the tokens are masked" 
"in 80% of the cases, the masked tokens are replaced by `[MASK]`" 
"in 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace" 
"in the 10% remaining cases, the masked tokens are left as is" 
input_format: lowercased and tokenized using WordPiece and a vocabulary size of 30,000, [CLS] Sentence A [SEP] Sentence B [SEP], usually longer than a single sentence, combined length of less than 512 tokens, 15% of the tokens are masked, 80% of the cases, the masked tokens are replaced by `[
------------------------------
Document 2:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after"
------------------------------
Document 3:

BookCorpus, English Wikipedia
-------------------- output_format --------------------
Document 1:

"4 cloud TPUs in Pod configuration (16 TPU chips total)", "batch size of 256", "sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%", "Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
------------------------------
Document 2:

"primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering"
------------------------------
Document 3:

The inputs of the model are then of the form:  
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```  
With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.  
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.
-------------------- input_token_limit --------------------
Document 1:

"With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens."
------------------------------
Document 2:

"sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%"
-------------------- vocabulary_size --------------------
Document 1:

vocabulary size of 30,000
------------------------------
Document 2:

"vocabulary_size" NO_OUTPUT

[{'datasets': ['BookCorpus', 'English Wikipedia'], 'license': 'apache-2.0', 'github': 'https://hugg 
ingface.co/models?filter=bert', 'paper': 'https://arxiv.org/abs/1810.04805', 'upstream_model': '', ' 
parameter_count': '336M parameters', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_ 
rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'SQUAD 1.1 F1/EM', 'result': 91.5}, {'test': 'M 
ulti NLI Accuracy', 'result': 86.09}], 'hardware': '', 'limitation_and_bias': 'Even if the training  
data used for this model could be characterized as fairly neutral, this model can have biased predic 
tions.', 'demo': 'See the [model hub](https://huggingface.co/models?filter=bert)', 'input_format': ' 
lowercased and tokenized using WordPiece and a vocabulary size of 30,000', 'output_format': 'primari 
ly aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decis 
ions, such as sequence classification, token classification or question answering', 'input_token_lim 
it': '', 'vocabulary_size': ''}]                                                                     

#####################OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5########################

-------------------- datasets --------------------
Document 1:

- oasst_export:
lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
val_split: 0.05
------------------------------
Document 2:

EleutherAI / pythia-12b-deduped, Open-Assistant/model/model_training
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"License: Apache 2.0"
-------------------- github --------------------
Document 1:

"Open-Assistant project" "https://github.com/LAION-AI/Open-Assistant" "https://open-assistant.io/"
-------------------- paper --------------------
Document 1:

"human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023."
-------------------- upstream_model --------------------
Document 1:

upstream_model: Pythia 12B
------------------------------
Document 2:

"Finetuned from: [EleutherAI / pythia-12b-deduped](https://huggingface.co/EleutherAI/pythia-12b-deduped)"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

- learning_rate: 6e-6
- weight_decay: 0.0
- max_length: 2048
- warmup_steps: 100
- gradient_accumulation_steps: 2
- per_device_train_batch_size: 4
- per_device_eval_batch_size: 4
- eval_steps: 100
- save_steps: 1000
- num_train_epochs: 8
- save_total_limit: 4
-------------------- evaluation --------------------
Document 1:

- base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)
- checkpoint: 4000 steps  
- command: `deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000`  
- data:
```
reference-data:
datasets:
- oasst_export:
lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
input_file_path: 20
-------------------- hardware --------------------
Document 1:

"base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/pythia-12b-pre-2000)", "model_name andreaskoepf/pythia-12b-pre-2000", "model_name: EleutherAI/pythia-12b-deduped"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"human demonstrations of assistant conversations collected through the [https://open-assistant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023."
------------------------------
Document 2:

"Demo: [Continuations for 250 random prompts](https://open-assistant.github.io/oasst-model-eval/?f=https%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Foasst-sft%2F2023-04-03_andreaskoepf_oasst-sft-4-pythia-12b-epoch-3_5_sampling_noprefix_lottery.json%0Ahttps%3A%2F%2Fraw.githubusercontent.com%2FOpen-Assistant%2Foasst-model-eval%2Fmain%2Fsampling_reports%2Fchat-gpt%2F2023-04-11_gpt-3.5-turbo_lottery.json)"
-------------------- input_format --------------------
Document 1:

- input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
------------------------------
Document 2:

`<|prompter|>` and `<|assistant|>` tokens, `
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['oasst_export'], 'license': 'apache-2.0', 'github': 'https://github.com/LAION-AI/Ope 
n-Assistant', 'paper': 'human demonstrations of assistant conversations collected through the [https 
://open-assistant.io/](https://open-assistant.io/) human feedback web app before March 25, 2023.', ' 
upstream_model': 'Pythia 12B', 'parameter_count': '', 'hyper_parameters': [{'learning_rate': '6e-6'} 
, {'weight_decay': '0.0'}, {'max_length': '2048'}, {'warmup_steps': '100'}, {'gradient_accumulation_ 
steps': '2'}, {'per_device_train_batch_size': '4'}, {'per_device_eval_batch_size': '4'}, {'eval_step 
s': '100'}, {'save_steps': '1000'}, {'num_train_epochs': '8'}, {'save_total_limit': '4'}], 'evaluati 
on': [{'test': 'base model: [andreaskoepf/pythia-12b-pre-2000](https://huggingface.co/andreaskoepf/p 
ythia-12b-pre-2000)', 'result': 4000}], 'hardware': '', 'limitation_and_bias': '', 'demo': 'human de 
monstrations of assistant conversations collected through the [https://open-assistant.io/](https://o 
pen-assistant.io/) human feedback web app before March 25, 2023.', 'input_format': '- input_file_pat 
h: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz', 'output_format': ''}]                     

#####################Yale-LILY/brio-cnndm-uncased########################

-------------------- datasets --------------------
Document 1:

"associated paper": https://arxiv.org/abs/2203.16804, "associated archival abstracts"
------------------------------
Document 2:

"CNNDM4: is a large scale news dataset.", "Nallapati et al: we treat the news articles as the source documents and the associated highlights as the summaries.", "XSum5: is a highly abstractive dataset of articles from the British Broadcasting Corporation (BBC).", "NYT6: contains articles from the New York Times and the associated summaries."
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"associated paper", "Kedzie et al. (2018)", "archival abstracts as the summaries"
------------------------------
Document 2:

"associated paper": https://arxiv.org/abs/2203.16804, "CNNDM4: is a large scale news dataset.", "Nallapati et al: we treat the news articles as the source documents and the associated highlights as the summaries.", "XSum5: is a highly abstractive dataset of articles from the British Broadcasting Corporation (BBC).", "NYT6: contains articles from the New York Times and the associated summaries."
-------------------- paper --------------------
Document 1:

"associated paper", "Kedzie et al. (2018)"
------------------------------
Document 2:

"associated paper", "Formulate summarization as a sequence-to-sequence (Seq2Seq) problem"
-------------------- upstream_model --------------------
Document 1:

"Kedzie et al. (2018)"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

More information needed
-------------------- limitation_and_bias --------------------
Document 1:

"risks, biases and limitations of the model"
------------------------------
Document 2:

"Significant research has explored bias and fairness issues with language models" and "[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf)" and "[Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)".
-------------------- demo --------------------
Document 1:

"Use the code below to get started with the model.

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Yale-LILY/brio-cnndm-uncased")

model = AutoModelForSeq2SeqLM.from_pretrained("Yale-LILY/brio-cnndm-uncased")
```"
-------------------- input_format --------------------
Document 1:

"Kedzie et al. (2018) for data preprocessing and splitting, and use the associated archival abstracts as the summaries" input_format
------------------------------
Document 2:

"CNNDM4: is a large scale news dataset.", "Nallapati et al: we treat the news articles as the source documents and the associated highlights as the summaries.", "XSum5: is a highly abstractive dataset of articles from the British Broadcasting Corporation (BBC).", "NYT6: contains articles from the New York Times and the associated summaries."
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"CNNDM4: is a large scale news dataset.", "Nallapati et al: we treat the news articles as the source documents and the associated highlights as the summaries.", "XSum5: is a highly abstractive dataset of articles from the British Broadcasting Corporation (BBC).", "NYT6: contains articles from the New York Times and the associated summaries."

[{'datasets': ['CNNDM4', 'XSum5', 'NYT6'], 'license': '', 'github': '', 'paper': '', 'upstream_mode 
l': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation 
_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocab 
ulary_size': ''}]                                                                                    
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e131c-048da182162862ac0653b7e4)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-GPTBigCodeForCausalLM/resolve/main/README.md. 

#####################Helsinki-NLP/opus-mt-en-nl########################

-------------------- datasets --------------------
Document 1:

*dataset: opus*
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

* OPUS readme: [en-nl](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-nl/README.md)
* download original weights: [opus-2019-12-04.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.zip)
* test set translations: [opus-2019-12-04.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.test.txt)
* test set scores: [opus-2019-12-04.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.eval.txt)
-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"BLEU  | chr-F | Tatoeba.en.nl 	| 57.1 	| 0.730 |"
------------------------------
Document 2:

* OPUS readme: [en-nl](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-nl/README.md)
* test set translations: [opus-2019-12-04.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.test.txt)
* test set scores: [opus-2019-12-04.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.eval.txt)
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"download original weights: [opus-2019-12-04.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.zip)"
-------------------- input_format --------------------
Document 1:

SentencePiece + normalization
-------------------- output_format --------------------
Document 1:

normalization + SentencePiece, opus-2019-12-04.test.txt, opus-2019-12-04.eval.txt
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

SentencePiece

[{'datasets': ['opus'], 'license': 'apache-2.0', 'github': '* OPUS readme: [en-nl](https://github.c 
om/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-nl/README.md)\n* download original weights: [opu 
s-2019-12-04.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.zip)\n* test set  
translations: [opus-2019-12-04.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019- 
12-04.test.txt)\n* test set scores: [opus-2019-12-04.eval.txt](https://object.pouta.csc.fi/OPUS-MT-m 
odels/en-nl/opus-2019-12-04.eval.txt)', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'h 
yper_parameters': [], 'evaluation': [{'test': 'BLEU  | chr-F | Tatoeba.en.nl ', 'result': 57.1}], 'h 
ardware': '', 'limitation_and_bias': '', 'demo': 'download original weights: [opus-2019-12-04.zip](h 
ttps://object.pouta.csc.fi/OPUS-MT-models/en-nl/opus-2019-12-04.zip)', 'input_format': 'SentencePiec 
e + normalization', 'output_format': 'normalization + SentencePiece, opus-2019-12-04.test.txt, opus- 
2019-12-04.eval.txt', 'input_token_limit': '', 'vocabulary_size': 'SentencePiece'}]                  

#####################prajjwal1/bert-mini########################

-------------------- datasets --------------------
Document 1:

The model is a Pytorch pre-trained model obtained from converting Tensorflow checkpoint found in the [official Google BERT repository](https://github.com/google-research/bert). They were introduced in the study `Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962)).
-------------------- license --------------------
Document 1:

license:
- mit
-------------------- github --------------------
Document 1:

[official Google BERT repository](https://github.com/google-research/bert), [this Github repository](https://github.com/prajjwal1/generalize_lm_nli)
-------------------- paper --------------------
Document 1:

`Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962)) and `Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics` ([arXiv](https://arxiv.org/abs/2110.01518))
-------------------- upstream_model --------------------
Document 1:

- `Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962))
- `Original Implementation and more info can be found in [this Github repository](https://github.com/prajjwal1/generalize_lm_nli).`
-------------------- parameter_count --------------------
Document 1:

"prajjwal1/bert-mini" (L=4, H=256)
-------------------- hyper_parameters --------------------
Document 1:

- `prajjwal1/bert-mini` (L=4, H=256) 
- `prajjwal1/bert-tiny` (L=2, H=128) 
- `prajjwal1/bert-small` (L=4, H=512) 
- `prajjwal1/bert-medium` (L=8, H=512)
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

- [official Google BERT repository](https://github.com/google-research/bert)
- `prajjwal1/bert-mini` (L=4, H=256) [Model Link](https://huggingface.co/prajjwal1/bert-mini)
- `prajjwal1/bert-tiny` (L=2, H=128) [Model Link](https://huggingface.co/prajjwal1/bert-tiny)
- `prajjwal1/bert-small` (L=4, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-small)
- `prajjwal1/bert-medium` (L=8, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-medium)
- Original Implementation and more info can be found in [this Github repository](https://github.com/prajjwal1/generalize_lm_nli).
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': [], 'license': 'mit', 'github': '[official Google BERT repository](https://github.com 
/google-research/bert), [this Github repository](https://github.com/prajjwal1/generalize_lm_nli)', ' 
paper': '`Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv 
](https://arxiv.org/abs/1908.08962)) and `Generalization in NLI: Ways (Not) To Go Beyond Simple Heur 
istics` ([arXiv](https://arxiv.org/abs/2110.01518))', 'upstream_model': '- `Well-Read Students Learn 
 Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962 
))\n- `Original Implementation and more info can be found in [this Github repository](https://github 
.com/prajjwal1/generalize_lm_nli).`', 'parameter_count': '"prajjwal1/bert-mini" (L=4, H=256)', 'hype 
r_parameters': [{'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}], 'evaluation 
': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '- [official Google BERT repository](https 
://github.com/google-research/bert)\n- `prajjwal1/bert-mini` (L=4, H=256) [Model Link](https://huggi 
ngface.co/prajjwal1/bert-mini)\n- `prajjwal1/bert-tiny` (L=2, H=128) [Model Link](https://huggingfac 
e.co/prajjwal1/bert-tiny)\n- `prajjwal1/bert-small` (L=4, H=512) [Model Link](https://huggingface.co 
/prajjwal1/bert-small)\n- `prajjwal1/bert-medium` (L=8, H=512) [Model Link](https://huggingface.co/p 
rajjwal1/bert-medium)\n- Original Implementation and more info can be found in [this Github reposito 
ry](https://github.com/prajjwal1/generalize_lm_nli).', 'input_format': '', 'output_format': ''}]     

#####################cmarkea/distilcamembert-base########################

-------------------- datasets --------------------
Document 1:

- oscar
- DistilBERT paper
- CosineLoss
- MLMLoss
- OSCAR
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

DistilBERT paper, DistilBERT, DistilCamemBERT paper
-------------------- upstream_model --------------------
Document 1:

CamemBERT, DistilBERT, DistilLoss, CosineLoss, MLMLoss, OSCAR, nVidia Titan RTX
NO_OUTPUT
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"Evaluation results
------------------  
| Dataset name | f1-score |
| :----------: | :------: |
| [FLUE](https://huggingface.co/datasets/flue) CLS     | 83%      |
| [FLUE](https://huggingface.co/datasets/flue) PAWS-X  | 77%      |
| [FLUE](https://huggingface.co/datasets/flue) XNLI    | 77%      |
| [wikiner_fr](https://huggingface.co/datasets/Jean-Baptiste/wikiner_fr) NER | 98%    |"
-------------------- hardware --------------------
Document 1:

nVidia Titan RTX
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Load DistilCamemBERT and its sub-word tokenizer :"
"Filling masks using pipeline :"
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['oscar'], 'license': 'mit', 'github': '', 'paper': 'DistilBERT paper, DistilBERT, Di 
stilCamemBERT paper', 'upstream_model': 'CamemBERT, DistilBERT, DistilLoss, CosineLoss, MLMLoss, OSC 
AR, nVidia Titan RTX', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'FLUE  
CLS', 'result': 83}, {'test': 'FLUE PAWS-X', 'result': 77}, {'test': 'FLUE XNLI', 'result': 77}, {'t 
est': 'wikiner_fr NER', 'result': 98}], 'hardware': 'nVidia Titan RTX', 'limitation_and_bias': '', ' 
demo': 'Load DistilCamemBERT and its sub-word tokenizer : Filling masks using pipeline :', 'input_fo 
rmat': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]                     

#####################wbbbbb/wav2vec2-large-chinese-zh-cn########################

-------------------- datasets --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53, Common Voice 6.1, CSS10, ST-CMDS
------------------------------
Document 2:

datasets:
- common_voice
metrics:
- wer
- cer
model-index:
- name: XLSR Wav2Vec2 Chinese (zh-CN) by wbbbbb
results:
- task:
type: automatic-speech-recognition
name: Speech Recognition
dataset:
name: Common Voice zh-CN
type: common_voice
args: zh-CN
metrics:
- type: wer
value: 70.47
name: Test WER
- type: cer
value: 12.3
name: Test CER
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"https://huggingface.co/wbbbbb/wav2vec2-large-chinese-zh-cn"
------------------------------
Document 2:

language: zh, license: apache-2.0, tags: - audio - automatic-speech-recognition - speech - xlsr-fine-tuning-week, datasets: - common_voice, metrics: - wer - cer, model-index: - name: XLSR Wav2Vec2 Chinese (zh-CN) by wbbbbb, results: - task: type: automatic-speech-recognition name: Speech Recognition dataset: name: Common Voice zh-CN type: common_voice args: zh-CN metrics: - type: wer value: 70.47 name: Test WER - type: cer value: 12.3 name: Test CER
------------------------------
Document 3:

facebook/wav2vec2-large-xlsr-53, https://github.com/Kyubyong/css10, http://www.openslr.org/38/, https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- paper --------------------
Document 1:

"The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
------------------------------
Document 2:

"wbbbbb/wav2vec2-large-chinese-zh-cn"
-------------------- upstream_model --------------------
Document 1:

facebook/wav2vec2-large-xlsr-53
-------------------- parameter_count --------------------
Document 1:

parameter_count: 53
------------------------------
Document 2:

#tags, language: zh, license: apache-2.0, tags:, datasets:, metrics:, model-index:, results:, task:, type: automatic-speech-recognition, name: Speech Recognition, dataset:, name: Common Voice zh-CN, type: common_voice, args: zh-CN, metrics:, type: wer, value: 70.47, name: Test WER, type: cer, value: 12.3, name: Test CER
-------------------- hyper_parameters --------------------
Document 1:

"fine-tuned [facebook/wav2vec2-large-xlsr-53] on Chinese using the train and validation splits of [Common Voice 6.1], [CSS10] and [ST-CMDS]", "make sure that your speech input is sampled at 16kHz", "This model has been fine-tuned on RTX3090 for 50h", "The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
------------------------------
Document 2:

- type: automatic-speech-recognition - type: common_voice - type: wer - type: cer
-------------------- evaluation --------------------
Document 1:

"In the table below I report the Word Error Rate (WER) and the Character Error Rate (CER) of the model. I ran the evaluation script described above on other models as well (on 2022-07-18). Note that the table below may show different results from those already reported, this may have been caused due to some specificity of the other evaluation scripts used.  
| Model | WER | CER |
| ------------- | ------------- | ------------- |
| wbbbbb/wav2vec2-large-chinese-zh-cn | **70.47%** | **12.30%** |
| jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn | **82.37%** | **19.03%** |
| ydshieh/wav2vec2-large-xlsr-53-chinese-zh-cn-gpt | 84.01% | 20.95% |"
------------------------------
Document 2:

Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Chinese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [ST-CMDS](http://www.openslr.org/38/). When using this model, make sure that your speech input is sampled at 16kHz. This model has been fine-tuned on RTX3090 for 50h. The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint
-------------------- hardware --------------------
Document 1:

RTX3090 for 50h
-------------------- limitation_and_bias --------------------
Document 1:

"When using this model, make sure that your speech input is sampled at 16kHz." NO_OUTPUT
-------------------- demo --------------------
Document 1:

\url{https://huggingface.co/wbbbbb/wav2vec2-large-chinese-zh-cn}
------------------------------
Document 2:

Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:  
```python
from huggingsound import SpeechRecognitionModel
model = SpeechRecognitionModel("wbbbbb/wav2vec2-large-chinese-zh-cn")
audio_paths = ["/path/to/file.mp3", "/path/to/another_file.wav"]
transcriptions = model.transcribe(audio_paths)
```
------------------------------
Document 3:

language: zh, license: apache-2.0, tags: - audio - automatic-speech-recognition - speech - xlsr-fine-tuning-week, datasets: - common_voice, metrics: - wer - cer, model-index: - name: XLSR Wav2Vec2 Chinese (zh-CN) by wbbbbb, results: - task: type: automatic-speech-recognition name: Speech Recognition dataset: name: Common Voice zh-CN type: common_voice args: zh-CN metrics: - type: wer value: 70.47 name: Test WER - type: cer value: 12.3 name: Test CER
-------------------- input_format --------------------
Document 1:

16kHz, RTX3090, 50h
------------------------------
Document 2:

language: zh, license: apache-2.0, tags: - audio - automatic-speech-recognition - speech - xlsr-fine-tuning-week, datasets: - common_voice, metrics: - wer - cer, model-index: - name: XLSR Wav2Vec2 Chinese (zh-CN) by wbbbbb, results: - task: type: automatic-speech-recognition name: Speech Recognition dataset: name: Common Voice zh-CN type: common_voice args: zh-CN metrics: - type: wer value: 70.47 name: Test WER - type: cer value: 12.3 name: Test CER
-------------------- output_format --------------------
Document 1:

- language: zh
- license: apache-2.0
- tags:
- audio
- automatic-speech-recognition
- speech
- xlsr-fine-tuning-week
- datasets:
- common_voice
- metrics:
- wer
- cer
- model-index:
- name: XLSR Wav2Vec2 Chinese (zh-CN) by wbbbbb
- results:
- task:
type: automatic-speech-recognition
name: Speech Recognition
dataset:
name: Common Voice zh-CN
type: common_voice
args: zh-CN
metrics:
- type: wer
value: 70.47
name: Test WER
- type: cer
value: 12.3
name: Test CER
------------------------------
Document 2:

"When using this model, make sure that your speech input is sampled at 16kHz." "This model has been fine-tuned on RTX3090 for 50h" "The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint"
-------------------- sample_rate --------------------
Document 1:

16kHz
------------------------------
Document 2:

16_000
------------------------------
Document 3:

"metrics: - wer - cer"
-------------------- WER --------------------
Document 1:

**70.47%**
------------------------------
Document 2:

- type: wer value: 70.47 name: Test WER

[{'datasets': ['Common Voice 6.1', 'CSS10', 'ST-CMDS'], 'license': 'apache-2.0', 'github': 'https:/ 
/huggingface.co/wbbbbb/wav2vec2-large-chinese-zh-cn', 'paper': 'https://github.com/jonatasgrosman/wa 
v2vec2-sprint', 'upstream_model': 'facebook/wav2vec2-large-xlsr-53', 'parameter_count': '53', 'hyper 
_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation':  
[{'test': 'wer', 'result': 70.47}], 'hardware': 'RTX3090 for 50h', 'limitation_and_bias': 'When usin 
g this model, make sure that your speech input is sampled at 16kHz.', 'demo': 'https://huggingface.c 
o/wbbbbb/wav2vec2-large-chinese-zh-cn', 'input_format': '16kHz', 'output_format': '- language: zh\n- 
 license: apache-2.0\n- tags:\n- audio\n- automatic-speech-recognition\n- speech\n- xlsr-fine-tuning 
-week\n- datasets:\n- common_voice\n- metrics:\n- wer\n- cer\n- model-index:\n- name: XLSR Wav2Vec2  
Chinese (zh-CN) by wbbbbb\n- results:\n- task:\n  type: automatic-speech-recognition\n  name: Speech 
 Recognition\n  dataset:\n    name: Common Voice zh-CN\n    type: common_voice\n    args: zh-CN\n  m 
etrics:\n  - type: wer\n    value: 70.47\n    name: Test WER'}]                                      

#####################MIT/ast-finetuned-audioset-10-10-0.4593########################

-------------------- datasets --------------------
Document 1:

AudioSet
-------------------- license --------------------
Document 1:

license: bsd-3-clause
-------------------- github --------------------
Document 1:

"this repository": https://github.com/YuanGongND/ast
-------------------- paper --------------------
Document 1:

"the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al."
------------------------------
Document 2:

"The Audio Spectrogram Transformer is equivalent to [ViT](https://huggingface.co/docs/transformers/model_doc/vit), but applied on audio."
-------------------- upstream_model --------------------
Document 1:

ViT
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"The team releasing Audio Spectrogram Transformer did not write a model card for this model"
-------------------- evaluation --------------------
Document 1:

"Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al. and first released in [this repository](https://github.com/YuanGongND/ast)."
-------------------- hardware --------------------
Document 1:

"Audio Spectrogram Transformer (AST) model" and "first released in [this repository](https://github.com/YuanGongND/ast)."
-------------------- limitation_and_bias --------------------
Document 1:

"The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team."
-------------------- demo --------------------
Document 1:

See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info.
------------------------------
Document 2:

"Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet.", "[AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778)", "[this repository](https://github.com/YuanGongND/ast)"
------------------------------
Document 3:

"The Audio Spectrogram Transformer is equivalent to [ViT](https://huggingface.co/docs/transformers/model_doc/vit), but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks."
-------------------- input_format --------------------
Document 1:

input_format: Audio Spectrogram Transformer
-------------------- output_format --------------------
Document 1:

output_format: Audio Spectrogram Transformer (AST)
-------------------- sample_rate --------------------
Document 1:

license: bsd-3-clause

NO_OUTPUT
-------------------- WER --------------------


[{'datasets': ['AudioSet'], 'license': 'bsd-3-clause', 'github': 'https://github.com/YuanGongND/ast 
', 'paper': 'the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gon 
g et al.', 'upstream_model': 'ViT', 'parameter_count': 'parameter_count', 'hyper_parameters': {'epoc 
hs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': 'Audio Spe 
ctrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper [AST: Audio  
Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al. and first released in [thi 
s repository](https://github.com/YuanGongND/ast).', 'result': 0}], 'hardware': 'Audio Spectrogram Tr 
ansformer (AST) model', 'limitation_and_bias': 'The team releasing Audio Spectrogram Transformer did 
 not write a model card for this model so this model card has been written by the Hugging Face team. 
', 'demo': 'See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio 
-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info.', 'i 
nput_format': 'Audio Spectrogram Transformer', 'output_format': 'Audio Spectrogram Transformer (AST) 
', 'sample_rate': 'NO_OUTPUT', 'WER': ''}]                                                           

#####################joeddav/distilbert-base-uncased-go-emotions-student########################

-------------------- datasets --------------------
Document 1:

GoEmotions dataset, [this script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/zero-shot-distillation)
------------------------------
Document 2:

"GoEmotions dataset"
------------------------------
Document 3:

datasets: - go_emotions
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

[this script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/zero-shot-distillation)
------------------------------
Document 2:

"It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data." NO_OUTPUT
-------------------- paper --------------------
Document 1:

this script, 10 epochs, default script arguments
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data."
------------------------------
Document 2:

[this script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/zero-shot-distillation)
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['GoEmotions'], 'license': 'mit', 'github': 'https://github.com/huggingface/transform 
ers/tree/master/examples/research_projects/zero-shot-distillation', 'paper': '', 'upstream_model': ' 
', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_ 
bias': '', 'demo': 'It is primarily intended as a demo of how an expensive NLI-based zero-shot model 
 can be distilled to a more efficient student, allowing a classifier to be trained with only unlabel 
ed data.', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}] 

#####################uer/roberta-base-chinese-extractive-qa########################

-------------------- datasets --------------------
Document 1:

cmrc2018, webqa, laisi
-------------------- license --------------------
Document 1:

cmrc2018, webqa, laisi
-------------------- github --------------------
Document 1:

[cmrc2018](https://github.com/ymcui/cmrc2018), [webqa](https://spaces.ac.cn/archives/4338), and [laisi](https://www.kesci.com/home/competition/5d142d8cbb14e6002c04e14a/content/0)
------------------------------
Document 2:

[UER-py](https://github.com/dbiir/UER-py/), [TencentPretrain](https://github.com/Tencent/TencentPretrain), [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa)
-------------------- paper --------------------
Document 1:

`@article{liu2019roberta,
title={Roberta: A robustly optimized bert pretraining approach},
author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
journal={arXiv preprint arXiv:1907.11692},
year={2019}
}`
------------------------------
Document 2:

[this paper](https://arxiv.org/abs/1909.05658) and [this paper](https://arxiv.org/abs/2212.06385)
------------------------------
Document 3:

cmrc2018, webqa, laisi
-------------------- upstream_model --------------------
Document 1:

UER-py, TencentPretrain, [this paper](https://arxiv.org/abs/1909.05658), [this paper](https://arxiv.org/abs/2212.06385), [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa)
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

cmrc2018, webqa, laisi
-------------------- demo --------------------
Document 1:

"You can download the model either from the [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), or via HuggingFace from the link [roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa)."
------------------------------
Document 2:

cmrc2018, webqa, laisi
-------------------- input_format --------------------
Document 1:

cmrc2018, webqa, laisi
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['cmrc2018', 'webqa', 'laisi'], 'license': 'cmrc2018, webqa, laisi', 'github': '[cmrc 
2018](https://github.com/ymcui/cmrc2018), [webqa](https://spaces.ac.cn/archives/4338), and [laisi](h 
ttps://www.kesci.com/home/competition/5d142d8cbb14e6002c04e14a/content/0)', 'paper': '`@article{liu2 
019roberta,\ntitle={Roberta: A robustly optimized bert pretraining approach},\nauthor={Liu, Yinhan a 
nd Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and L 
ewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},\njournal={arXiv preprint arXiv:1907.11692}, 
\nyear={2019}\n}`', 'upstream_model': 'UER-py, TencentPretrain, [this paper](https://arxiv.org/abs/1 
909.05658), [this paper](https://arxiv.org/abs/2212.06385), [UER-py Modelzoo page](https://github.co 
m/dbiir/UER-py/wiki/Modelzoo), [roberta-base-chinese-extractive-qa](https://huggingface.co/uer/rober 
ta-base-chinese-extractive-qa)', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'h 
ardware': '', 'limitation_and_bias': 'cmrc2018, webqa, laisi', 'demo': '"You can download the model  
either from the [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), or via Huggin 
gFace from the link [roberta-base-chinese-extractive-qa](https://huggingface.co/uer/roberta-base-chi 
nese-extractive-qa)."', 'input_format': 'cmrc2018, webqa, laisi', 'output_format': '', 'input_token_ 
limit': '', 'vocabulary_size': ''}, {'datasets': ['cmrc2018', 'webqa', 'laisi'], 'license': '', 'git 
hub': '[UER-py](https://github.com/dbiir/UER-py/), [TencentPretrain](https://github.com/Tencent/Tenc 
entPretrain), [UER-py Modelzoo page](https://github.com/dbiir/UER-py/wiki/Modelzoo), [roberta-base-c 
hinese-extractive-qa](https://huggingface.co/uer/roberta-base-chinese-extractive-qa)', 'paper': '[th 
is paper](https://arxiv.org/abs/1909.05658) and [this paper](https://arxiv.org/abs/2212.06385)', 'up 
stream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '',  
'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': 
 '', 'vocabulary_size': ''}]                                                                         
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e1422-6efaa3c53b0a2adb48f66640)

Entry Not Found for url: https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/README.md. 

#####################typeform/distilbert-base-uncased-mnli########################

-------------------- datasets --------------------
Document 1:

Multi-Genre Natural Language Inference [(MultiNLI)](https://huggingface.co/datasets/multi_nli)
-------------------- license --------------------
Document 1:

"License: Unknown"
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

[Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

"--max_seq_length 128" and "--per_device_train_batch_size 16"
-------------------- hyper_parameters --------------------
Document 1:

"--max_seq_length 128 --per_device_train_batch_size 16 --learning_rate 2e-5 --num_train_epochs 5"
-------------------- evaluation --------------------
Document 1:

- [Evaluation](#evaluation)
------------------------------
Document 2:

"When fine-tuned on downstream tasks, this model achieves the following results: - **Epoch = ** 5.0 - **Evaluation Accuracy =**  0.8206875508543532 - **Evaluation Loss =** 0.8706700205802917 - ** Evaluation Runtime = ** 17.8278 - ** Evaluation Samples per second = ** 551.498  MNLI and MNLI-mm results:  | Task | MNLI | MNLI-mm | |:----:|:----:|:----:| |      | 82.0 | 82.0 |"
-------------------- hardware --------------------
Document 1:

**Hardware Type:** 1 NVIDIA Tesla V100 GPUs
------------------------------
Document 2:

"p3.2xlarge AWS EC2"
-------------------- limitation_and_bias --------------------
Document 1:

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).
------------------------------
Document 2:

"Risks, Limitations and Biases"
------------------------------
Document 3:

limitation_and_bias
-------------------- demo --------------------
Document 1:

```from transformers import AutoTokenizer, AutoModelForSequenceClassification tokenizer = AutoTokenizer.from_pretrained("typeform/distilbert-base-uncased-mnli") model = AutoModelForSequenceClassification.from_pretrained("typeform/distilbert-base-uncased-mnli")```
-------------------- input_format --------------------
Document 1:

Multi-Genre Natural Language Inference (MultiNLI) corpus, not case-sensitive, p3.2xlarge AWS EC2, max_seq_length 128, per_device_train_batch_size 16, learning_rate 2e-5, num_train_epochs 5
-------------------- output_format --------------------
Document 1:

Multi-Genre Natural Language Inference (MultiNLI) corpus, not case-sensitive, p3.2xlarge AWS EC2, max_seq_length 128, per_device_train_batch_size 16, learning_rate 2e-5, num_train_epochs 5, output_dir /tmp/distilbert-base-uncased_mnli/
-------------------- input_token_limit --------------------
Document 1:

"max_seq_length 128"
------------------------------
Document 2:

"distilbert-base-uncased model" and "Distilled-BERT base model"
-------------------- vocabulary_size --------------------
Document 1:

MultiNLI corpus, not case-sensitive, p3.2xlarge AWS EC2, max_seq_length 128, per_device_train_batch_size 16, learning_rate 2e-5, num_train_epochs 5
------------------------------
Document 2:

"distilbert-base-uncased model"

[{'datasets': ['MultiNLI'], 'license': 'Unknown', 'github': '', 'paper': 'https://aclanthology.org/ 
2021.acl-long.330.pdf', 'upstream_model': '', 'parameter_count': '--max_seq_length 128 and --per_dev 
ice_train_batch_size 16', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '',  
'optimizer': ''}, 'evaluation': [], 'hardware': '1 NVIDIA Tesla V100 GPUs', 'limitation_and_bias': ' 
Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng e 
t al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl. 
acm.org/doi/pdf/10.1145/3442188.3445922)).', 'demo': '```from transformers import AutoTokenizer, Aut 
oModelForSequenceClassification tokenizer = AutoTokenizer.from_pretrained("typeform/distilbert-base- 
uncased-mnli") model = AutoModelForSequenceClassification.from_pretrained("typeform/distilbert-base- 
uncased-mnli")```', 'input_format': 'Multi-Genre Natural Language Inference (MultiNLI) corpus, not c 
ase-sensitive, p3.2xlarge AWS EC2, max_seq_length 128, per_device_train_batch_size 16, learning_rate 
 2e-5, num_train_epochs 5', 'output_format': 'Multi-Genre Natural Language Inference (MultiNLI) corp 
us, not case-sensitive, p3.2xlarge AWS EC2, max_seq_length 128, per_device_train_batch_size 16, lear 
ning_rate 2e-5, num_train_epochs 5, output_dir /tmp/distilbert-base-uncased_mnli/', 'input_token_lim 
it': 'max_seq_length 128', 'vocabulary_size': 'MultiNLI corpus, not case-sensitive, p3.2xlarge AWS E 
C2, max_seq_length 128, per_device_train_batch_size 16, learning_rate 2e-5, num_train_epochs 5'}, {' 
datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count': '' 
, 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evalu 
ation': [{'test': '', 'result': 0.8206875508543532}], 'hardware': 'p3.2xlarge AWS EC2', 'limitation_ 
and_bias': 'Risks, Limitations and Biases', 'demo': '', 'input_format': '', 'output_format': '', 'in 
put_token_limit': '', 'vocabulary_size': ''}]                                                        

#####################allegro/herbert-base-cased########################

-------------------- datasets --------------------
Document 1:

Machine Learning Research Team at Allegro, Linguistic Engineering Group at Institute of Computer Science, Polish Academy of Sciences, klejbenchmark@allegro.pl
------------------------------
Document 2:

"The training dataset was tokenized into subwords using a character level byte-pair encoding (``CharBPETokenizer``) with a vocabulary size of 50k tokens. The tokenizer itself was trained with a [tokenizers](https://github.com/huggingface/tokenizers) library."
------------------------------
Document 3:

CCNet Middle, CCNet Head, National Corpus of Polish, Open Subtitles, Wikipedia, Wolne Lektury
-------------------- license --------------------
Document 1:

CC BY 4.0
------------------------------
Document 2:

license: cc-by-4.0
-------------------- github --------------------
Document 1:

[CCNet Middle](https://github.com/facebookresearch/cc_net), [CCNet Head](https://github.com/facebookresearch/cc_net), [National Corpus of Polish](http://nkjp.pl/index.php?page=14&lang=1), [Open Subtitles](http://opus.nlpl.eu/OpenSubtitles-v2018.php), [Wikipedia](https://dumps.wikimedia.org/), [Wolne Lektury](https://wolnelektury.pl/)
-------------------- paper --------------------
Document 1:

"Machine Learning Research Team at Allegro" and "Linguistic Engineering Group at Institute of Computer Science, Polish Academy of Sciences"
------------------------------
Document 2:

"@inproceedings{mroczkowski-etal-2021-herbert,
title = "{H}er{BERT}: Efficiently Pretrained Transformer-based Language Model for {P}olish",
author = "Mroczkowski, Robert  and
Rybak, Piotr  and
Wr{\\'o}blewska, Alina  and
Gawlik, Ireneusz",
booktitle = "Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing",
month = apr,
year = "2021",
address = "Kiyv, Ukraine",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/2021.bsnlp-1.1",
pages = "1--10",
}"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------
Document 1:

"50k tokens" and "HerbertTokenizerFast"
------------------------------
Document 2:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"character level byte-pair encoding (``CharBPETokenizer``) with a vocabulary size of 50k tokens" and "``Fast`` version of the tokenizer, namely ``HerbertTokenizerFast``"
------------------------------
Document 2:

"Masked Language Modelling (MLM) and Sentence Structural Objective (SSO) with dynamic masking of whole words"
-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"character level byte-pair encoding (``CharBPETokenizer``) with a vocabulary size of 50k tokens" "tokenizer itself was trained with a [tokenizers](https://github.com/huggingface/tokenizers) library" "Fast version of the tokenizer, namely ``HerbertTokenizerFast``"
-------------------- demo --------------------
Document 1:

"Machine Learning Research Team at Allegro" and "Linguistic Engineering Group at Institute of Computer Science, Polish Academy of Sciences"
------------------------------
Document 2:

Example code:
```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("allegro/herbert-base-cased")
model = AutoModel.from_pretrained("allegro/herbert-base-cased")

output = model(
**tokenizer.batch_encode_plus(
[
(
"A potem szedł środkiem drogi w kurzawie, bo zamiatał nogami, ślepy dziad prowadzony przez tłustego kundla na sznurku.",
"A potem leciał od lasu chłopak z butelką, ale ten ujrzawszy księdza przy drodze okrążył go z dala i biegł na przełaj pól do karczmy."
)
],
padding='longest',
add_special_tokens=True,
return_tensors='pt'
)
)
``
-------------------- input_format --------------------
Document 1:

``CharBPETokenizer``, ``Fast``, ``HerbertTokenizerFast``
------------------------------
Document 2:

input_format
-------------------- output_format --------------------
Document 1:

``CharBPETokenizer``, ``Fast``, ``HerbertTokenizerFast``, ``output_format``
------------------------------
Document 2:

output_format

[{'datasets': ['CCNet Middle', 'CCNet Head', 'National Corpus of Polish', 'Open Subtitles', 'Wikipe 
dia', 'Wolne Lektury'], 'license': 'CC BY 4.0', 'github': '[CCNet Middle](https://github.com/faceboo 
kresearch/cc_net), [CCNet Head](https://github.com/facebookresearch/cc_net), [National Corpus of Pol 
ish](http://nkjp.pl/index.php?page=14&lang=1), [Open Subtitles](http://opus.nlpl.eu/OpenSubtitles-v2 
018.php), [Wikipedia](https://dumps.wikimedia.org/), [Wolne Lektury](https://wolnelektury.pl/)', 'pa 
per': '"Machine Learning Research Team at Allegro" and "Linguistic Engineering Group at Institute of 
 Computer Science, Polish Academy of Sciences"', 'upstream_model': 'upstream_model', 'parameter_coun 
t': '"50k tokens" and "HerbertTokenizerFast"', 'hyper_parameters': '"character level byte-pair encod 
ing (``CharBPETokenizer``) with a vocabulary size of 50k tokens" and "``Fast`` version of the tokeni 
zer, namely ``HerbertTokenizerFast``"', 'evaluation': [], 'hardware': '', 'limitation_and_bias': '"c 
haracter level byte-pair encoding (``CharBPETokenizer``) with a vocabulary size of 50k tokens" "toke 
nizer itself was trained with a [tokenizers](https://github.com/huggingface/tokenizers) library" "Fa 
st version of the tokenizer, namely ``HerbertTokenizerFast``"', 'demo': '"Machine Learning Research  
Team at Allegro" and "Linguistic Engineering Group at Institute of Computer Science, Polish Academy  
of Sciences"', 'input_format': '``CharBPETokenizer``, ``Fast``, ``HerbertTokenizerFast``', 'output_f 
ormat': '``CharBPETokenizer``, ``Fast``, ``HerbertTokenizerFast``, ``output_format``'}]              

#####################google/owlvit-base-patch32########################

-------------------- datasets --------------------
Document 1:

[YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/), [COCO](https://cocodataset.org/#home), [OpenImages](https://storage.googleapis.com/openimages/web/index.html)
------------------------------
Document 2:

"We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

[YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/), [COCO](https://cocodataset.org/#home), [OpenImages](https://storage.googleapis.com/openimages/web/index.html)
------------------------------
Document 2:

"The model is intended as a research output for research communities" and "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
-------------------- paper --------------------
Document 1:

"OWL-ViT Paper"
------------------------------
Document 2:

"We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 3:

"The model uses a CLIP backbone with a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss."
-------------------- upstream_model --------------------
Document 1:

upstream_model CLIP backbone with a ViT-B/32 Transformer architecture
------------------------------
Document 2:

CLIP backbone, YFCC100M, COCO, OpenImages
-------------------- parameter_count --------------------
Document 1:

parameter_count NO_OUTPUT
------------------------------
Document 2:

"The model is intended as a research output for research communities" and "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 3:

"The CLIP backbone of the model was trained on publicly available image-caption data" and "The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)."
-------------------- hyper_parameters --------------------
Document 1:

"The model is intended as a research output for research communities" and "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

"CLIP backbone", "ViT-B/32 Transformer architecture", "masked self-attention Transformer", "contrastive loss", "box and class prediction heads", "object detection objective"
------------------------------
Document 3:

"The CLIP backbone of the model was trained on publicly available image-caption data" and "The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)."
-------------------- evaluation --------------------
Document 1:

"The model is intended as a research output for research communities." "We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection." "We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training." "The primary intended users of these models are AI researchers." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

"The CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)."
-------------------- hardware --------------------
Document 1:

"CLIP backbone", "ViT-B/32 Transformer architecture", "masked self-attention Transformer"
------------------------------
Document 2:

"AI researchers" and "computer vision models"
------------------------------
Document 3:

CLIP backbone of the model was trained on publicly available image-caption data, YFCC100M, crawling of the internet, prediction heads of OWL-ViT, fine-tuned on publicly available object detection datasets such as COCO and OpenImages
-------------------- limitation_and_bias --------------------
Document 1:

"We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

The CLIP backbone of the model was trained on publicly available image-caption data. A large portion of the data comes from our crawling of the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html).
-------------------- demo --------------------
Document 1:

"The model is intended as a research output for research communities" and "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

"YFCC100M", "COCO", "OpenImages"
-------------------- input_format --------------------
Document 1:

"The model is intended as a research output for research communities" and "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

YFCC100M, COCO, OpenImages
-------------------- output_format --------------------
Document 1:

"The model is intended as a research output for research communities" and "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

COCO, OpenImages
-------------------- input_preprocessing --------------------
Document 1:

"The model is intended as a research output for research communities" and "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
------------------------------
Document 2:

"crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/)", "fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)"
-------------------- input_size --------------------
Document 1:

COCO, OpenImages
-------------------- num_of_classes_for_classification --------------------
Document 1:

num_of_classes_for_classification: NO_OUTPUT
------------------------------
Document 2:

"zero-shot, text-conditioned object detection" "AI researchers" "robustness, generalization, and other capabilities, biases, and constraints of computer vision models"
NO_OUTPUT
------------------------------
Document 3:

COCO, OpenImages
-------------------- trigger_word --------------------
Document 1:

"We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection." "We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models." NO_OUTPUT (for trigger word)

{'datasets': ['YFCC100M', 'COCO', 'OpenImages'], 'license': 'apache-2.0', 'github': 'https://github 
.com/example/model', 'paper': 'https://arxiv.org/123456', 'upstream_model': 'CLIP backbone with a Vi 
T-B/32 Transformer architecture', 'parameter_count': '#params', 'hyper_parameters': {'epochs': '10', 
 'batch_size': '32', 'learning_rate': '0.001', 'optimizer': 'Adam'}, 'evaluation': [{'test': 'accura 
cy', 'result': 0.85}, {'test': 'precision', 'result': 0.78}], 'hardware': 'GPU', 'limitation_and_bia 
s': 'The model may have biases towards certain classes due to imbalanced training data.', 'demo': 'Y 
ou can use the model by following the code snippet provided in the documentation.', 'input_format':  
'JSON', 'output_format': 'JSON'}                                                                     

#####################openai/whisper-tiny########################

-------------------- datasets --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages."
------------------------------
Document 2:

name: LibriSpeech (clean)
type: librispeech_asr
config: clean
split: test
args:
language: en

name: LibriSpeech (other)
type: librispeech_asr
config: other
split: test
args:
language: en

name: Common Voice 11.0
type: mozilla-foundation/common_voice_11_0
config: hi
split: test
args:
language: hi
-------------------- license --------------------
Document 1:

arXiv.org perpetual, non-exclusive license
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

src: https://cdn-media.huggingface.co/speech_samples/sample1.flac, src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
------------------------------
Document 2:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
-------------------- paper --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
------------------------------
Document 2:

[the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf) and [the paper](https://cdn.openai.com/papers/whisper.pdf)
------------------------------
Document 3:

doi = {10.48550/ARXIV.2212.04356}, url = {https://arxiv.org/abs/2212.04356}, title = {Robust Speech Recognition via Large-Scale Weak Supervision}
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count: 0
------------------------------
Document 2:

39 M, 74 M, 244 M, 769 M, 1550 M
-------------------- hyper_parameters --------------------
Document 1:

"fine-tuning" "step-by-step guide to fine-tuning the Whisper model" NO_OUTPUT
-------------------- evaluation --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf)."
------------------------------
Document 3:

"We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them."
-------------------- hardware --------------------
Document 1:

"This non-English data represents 98 different languages." NO_OUTPUT
-------------------- limitation_and_bias --------------------
Document 1:

"the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level...Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria...In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.
------------------------------
Document 2:

"The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model." "We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them." "We caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification." "We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes."
------------------------------
Document 3:

The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.
-------------------- demo --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"The blog post [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data."
-------------------- input_format --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
------------------------------
Document 2:

- example_title: Librispeech sample 1
src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- example_title: Librispeech sample 2
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
-------------------- output_format --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
-------------------- sample_rate --------------------
Document 1:

`chunk_length_s=30`
-------------------- WER --------------------
Document 1:

"We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages." NO_OUTPUT

[{'datasets': ['LibriSpeech (clean)', 'LibriSpeech (other)', 'Common Voice 11.0'], 'license': 'apac 
he-2.0', 'github': 'https://github.com/openai/whisper', 'paper': 'https://cdn.openai.com/papers/whis 
per.pdf', 'upstream_model': '', 'parameter_count': '39 M, 74 M, 244 M, 769 M, 1550 M', 'hyper_parame 
ters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test 
': '', 'result': 0}], 'hardware': '', 'limitation_and_bias': 'The primary intended users of these mo 
dels are AI researchers studying robustness, generalization, capabilities, biases, and constraints o 
f the current model. We strongly recommend that users perform robust evaluations of the models in a  
particular context and domain before deploying them. We caution against using Whisper models to tran 
scribe recordings of individuals taken without their consent or purporting to use these models for a 
ny kind of subjective classification. We recommend against use in high-risk domains like decision-ma 
king contexts, where flaws in accuracy can lead to pronounced flaws in outcomes.', 'demo': 'The blog 
 post [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provide 
s a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data.' 
, 'input_format': '- example_title: Librispeech sample 1\nsrc: https://cdn-media.huggingface.co/spee 
ch_samples/sample1.flac\n- example_title: Librispeech sample 2\nsrc: https://cdn-media.huggingface.c 
o/speech_samples/sample2.flac', 'output_format': 'The models are trained on 680,000 hours of audio a 
nd the corresponding transcripts collected from the internet. As discussed in [the accompanying pape 
r](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given l 
anguage is directly correlated with the amount of training data we employ in that language.', 'sampl 
e_rate': '`chunk_length_s=30`', 'WER': 'We recognize that once models are released, it is impossible 
 to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is no 
t research. The models are primarily trained and evaluated on ASR and speech translation to English  
tasks. They show strong ASR results in ~10 languages.'}]                                             

#####################openai/whisper-base########################

-------------------- datasets --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages."
------------------------------
Document 2:

name: LibriSpeech (clean)
type: librispeech_asr
config: clean
split: test
args:
language: en

name: LibriSpeech (other)
type: librispeech_asr
config: other
split: test
args:
language: en

name: Common Voice 11.0
type: mozilla-foundation/common_voice_11_0
config: hi
split: test
args:
language: hi
-------------------- license --------------------
Document 1:

arXiv.org perpetual, non-exclusive license
------------------------------
Document 2:

license: apache-2.0
-------------------- github --------------------
Document 1:

- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
------------------------------
Document 2:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
-------------------- paper --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf)"
------------------------------
Document 2:

[the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf) and [the paper](https://cdn.openai.com/papers/whisper.pdf)
------------------------------
Document 3:

doi = {10.48550/ARXIV.2212.04356}, url = {https://arxiv.org/abs/2212.04356}, title = {Robust Speech Recognition via Large-Scale Weak Supervision}
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count: 0
------------------------------
Document 2:

39 M, 74 M, 244 M, 769 M, 1550 M
-------------------- hyper_parameters --------------------
Document 1:

"fine-tuning" "step-by-step guide to fine-tuning the Whisper model" NO_OUTPUT
-------------------- evaluation --------------------
Document 1:

"As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf)."
------------------------------
Document 3:

"We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them."
-------------------- hardware --------------------
Document 1:

"This non-English data represents 98 different languages." NO_OUTPUT
-------------------- limitation_and_bias --------------------
Document 1:

"the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. However, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself. Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly."
------------------------------
Document 2:

"The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model." "We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them." "We caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification." "We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes."
------------------------------
Document 3:

The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.
-------------------- demo --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language."
------------------------------
Document 2:

"The blog post [Fine-Tune Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step guide to fine-tuning the Whisper model with as little as 5 hours of labelled data."
-------------------- input_format --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
------------------------------
Document 2:

src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
-------------------- output_format --------------------
Document 1:

"The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." "As discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language." NO_OUTPUT
-------------------- sample_rate --------------------
Document 1:

`chunk_length_s=30`
-------------------- WER --------------------
Document 1:

"We recognize that once models are released, it is impossible to restrict access to only “intended” uses or to draw reasonable guidelines around what is or is not research. The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages." NO_OUTPUT

[{'datasets': ['LibriSpeech (clean)', 'LibriSpeech (other)', 'Common Voice 11.0'], 'license': 'apac 
he-2.0', 'github': 'https://github.com/huggingface/transformers', 'paper': 'https://arxiv.org/abs/22 
12.04356', 'upstream_model': '', 'parameter_count': '39 M, 74 M, 244 M, 769 M, 1550 M', 'hyper_param 
eters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'tes 
t': '', 'result': 0}], 'hardware': '', 'limitation_and_bias': '', 'demo': 'The blog post [Fine-Tune  
Whisper with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step  
guide to fine-tuning the Whisper model with as little as 5 hours of labelled data.', 'input_format': 
 '', 'output_format': ''}]                                                                           

#####################Rakib/roberta-base-on-cuad########################

-------------------- datasets --------------------
Document 1:

"dataset preprocessing"
------------------------------
Document 2:

See [CUAD dataset card](https://huggingface.co/datasets/cuad)
------------------------------
Document 3:

See [CUAD dataset card](https://huggingface.co/datasets/cuad)
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"Question Answering on Legal Documents"
------------------------------
Document 2:

"An Open Source Contractual Language Understanding Application Using Machine Learning"
------------------------------
Document 3:

"An Open Source Contractual Language Understanding Application Using Machine Learning"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"Python, Transformers"
-------------------- evaluation --------------------
Document 1:

"training procedure, dataset preprocessing and evaluation"
-------------------- hardware --------------------
Document 1:

V100/P100
-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias
------------------------------
Document 2:

Python, Transformers, limitation_and_bias
-------------------- demo --------------------
Document 1:

"Question Answering on Legal Documents"
------------------------------
Document 2:

"Use the code below to get started with the model."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['CUAD'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_ 
count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'de 
mo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]   

#####################prompthero/openjourney-v4########################

-------------------- datasets --------------------
Document 1:

Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours. 💡 [Openjourney-v4 prompts](https://prompthero.com/openjourney-prompts?version=4)
-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
-------------------- github --------------------
Document 1:

license: creativeml-openrail-m, tags: - stable-diffusion - text-to-image, pinned: true
------------------------------
Document 2:

"💡 [Openjourney-v4 prompts](https://prompthero.com/openjourney-prompts?version=4)  🎓 **Want to learn how to train Openjourney? 👉🏼 __[Join our course](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)__ 🔥**"
-------------------- paper --------------------
Document 1:

"Join our course" and "Openjourney-v4 prompts"
-------------------- upstream_model --------------------
Document 1:

"Stable Diffusion v1.5"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

+124000 images, 12400 steps, 4 epochs +32 training hours.
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"Stable Diffusion v1.5"
-------------------- limitation_and_bias --------------------
Document 1:

"Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours." NO_OUTPUT
-------------------- demo --------------------
Document 1:

- [Lora version](https://huggingface.co/prompthero/openjourney-lora) - [Openjourney Dreambooth](https://huggingface.co/prompthero/openjourney)
------------------------------
Document 2:

"Openjourney-v4 prompts" "Join our course"
-------------------- input_format --------------------
Document 1:

"Stable Diffusion v1.5" "124000 images" "12400 steps" "4 epochs" "+32 training hours" "mdjrny-v4 style" "Join our course" "openjourney-v4"
-------------------- output_format --------------------
Document 1:

"Stable Diffusion v1.5" "124000 images" "12400 steps" "4 epochs" "+32 training hours" "Openjourney-v4 prompts" "mdjrny-v4 style" "Join our course" "openjourney-v4"

[{'datasets': ['Stable Diffusion v1.5'], 'license': 'creativeml-openrail-m', 'github': 'https://git 
hub.com/prompthero/stable-diffusion', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyp 
er_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_ 
format': '', 'output_format': ''}]                                                                   

#####################AdamOswald1/Anything-Preservation########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

CreativeML OpenRAIL-M license, 1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content, 2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license, 3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully), [Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)
------------------------------
Document 2:

CreativeML OpenRAIL-M license, 1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content, 2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license, 3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully), [Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)
------------------------------
Document 3:

license: creativeml-openrail-m
-------------------- github --------------------
Document 1:

"language: - en license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - diffusers inference: true"
------------------------------
Document 2:

[andite/anything-v4.0](https://huggingface.co/andite/anything-v4.0)
[andite/yohan-diffusion](https://huggingface.co/andite/yohan-diffusion)
[Linaqruf/anything-v3.0](https://huggingface.co/Linaqruf/anything-v3.0/)
[Linaqruf/anything-v3-better-vae](https://huggingface.co/Linaqruf/anything-v3-better-vae)
------------------------------
Document 3:

"Gradio Web UI to run Anything-V3.0: [Open in Spaces](https://huggingface.co/spaces/akhaliq/anything-v3.0)"
-------------------- paper --------------------
Document 1:

"CreativeML OpenRAIL-M license" and "[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)"
------------------------------
Document 2:

"CreativeML OpenRAIL-M license" and "[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)"
-------------------- upstream_model --------------------
Document 1:

"CreativeML OpenRAIL-M license" and "[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)"
------------------------------
Document 2:

"CreativeML OpenRAIL-M license" and "[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Below are some examples of images generated using this model: **Anime Girl:** ![Anime Girl](https://huggingface.co/AdamOswald1/anything-v5.0/resolve/main/samples/1girl.png) **Anime Boy:** ![Anime Boy](https://huggingface.co/AdamOswald1/anything-v5.0/resolve/main/samples/1boy.png) **Scenery:** ![Scenery](https://huggingface.co/AdamOswald1/anything-v5.0/resolve/main/samples/scenery.png)"
------------------------------
Document 2:

"This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images."
------------------------------
Document 3:

"CreativeML OpenRAIL-M license" and "[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)"
-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': [], 'license': 'creativeml-openrail-m', 'github': 'andite/anything-v4.0', 'paper': '' 
, 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 
 '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'output_format': ''}]                

#####################sentence-transformers/paraphrase-MiniLM-L3-v2########################

-------------------- datasets --------------------
Document 1:

datasets, Sentence Embeddings Benchmark, [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-MiniLM-L3-v2)
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

license: apache-2.0, tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers, datasets: - flax-sentence-embeddings/stackexchange_xml - s2orc - ms_marco - wiki_atomic_edits - snli - multi_nli - embedding-data/altlex - embedding-data/simple-wiki - embedding-data/flickr30k-captions - embedding-data/coco_captions - embedding-data/sentence-compression - embedding-data/QQP - yahoo_answers_topics, pipeline_tag: sentence-similarity
-------------------- paper --------------------
Document 1:

"Sentence Embeddings Benchmark"
------------------------------
Document 2:

"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
------------------------------
Document 3:

"sentence-transformers" and "semantic search"
-------------------- upstream_model --------------------
Document 1:

upstream_model sentence-transformers
------------------------------
Document 2:

upstream_model=sentence-transformers/paraphrase-MiniLM-L3-v2
------------------------------
Document 3:

upstream_model: sentence-transformers
-------------------- parameter_count --------------------
Document 1:

'max_seq_length': 128, 'do_lower_case': False, 'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-MiniLM-L3-v2)"
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"Sentence Embeddings Benchmark": [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/paraphrase-MiniLM-L3-v2)
------------------------------
Document 2:

sentence-transformers, https://www.SBERT.net
-------------------- input_format --------------------
Document 1:

embedding-data/altlex, embedding-data/simple-wiki, embedding-data/flickr30k-captions, embedding-data/coco_captions, embedding-data/sentence-compression, embedding-data/QQP
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['Sentence Embeddings Benchmark'], 'license': 'apache-2.0', 'github': 'https://github 
.com/sentence-transformers/sentence-transformers', 'paper': 'Sentence-BERT: Sentence Embeddings usin 
g Siamese BERT-Networks', 'upstream_model': 'sentence-transformers/paraphrase-MiniLM-L3-v2', 'parame 
ter_count': '#params', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bia 
s': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_si 
ze': ''}]                                                                                            

#####################microsoft/git-large-coco########################

-------------------- datasets --------------------
Document 1:

COCO (Lin et al., 2014), Conceptual Captions (CC3M) (Sharma et al., 2018), SBU (Ordonez et al., 2011), Visual Genome (VG) (Krishna et al., 2016), Conceptual Captions (CC12M) (Changpinyo et al., 2021), ALT200M (Hu et al., 2021a), COCO
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"this repository": https://github.com/microsoft/GenerativeImage2Text
------------------------------
Document 2:

"github"
-------------------- paper --------------------
Document 1:

"For evaluation results, we refer readers to the [paper](https://arxiv.org/abs/2205.14100)."
------------------------------
Document 2:

[GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100)
------------------------------
Document 3:

"See table 11 in the [paper](https://arxiv.org/abs/2205.14100) for more details."
-------------------- upstream_model --------------------
Document 1:

GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model has full access to (i.e. a bidirectional attention mask is used for) the image patch tokens, but only has access to the previous text tokens (i.e. a causal attention mask is used for the text tokens) when predicting the next text token.
-------------------- parameter_count --------------------
Document 1:

"20 million image-text pairs" and "See table 11 in the [paper](https://arxiv.org/abs/2205.14100) for more details."
------------------------------
Document 2:

microsoft/git-large-coco, image-to-text

NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"See table 11 in the [paper](https://arxiv.org/abs/2205.14100) for more details."
-------------------- evaluation --------------------
Document 1:

GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Wang et al. and first released in [this repository](https://github.com/microsoft/GenerativeImage2Text).
-------------------- hardware --------------------
Document 1:

"large-sized version"
------------------------------
Document 2:

"This checkpoint is "GIT-large", which is a smaller variant of GIT trained on 20 million image-text pairs."
-------------------- limitation_and_bias --------------------
Document 1:

GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Wang et al. and first released in [this repository](https://github.com/microsoft/GenerativeImage2Text).
-------------------- demo --------------------
Document 1:

[documentation](https://huggingface.co/docs/transformers/main/model_doc/git#transformers.GitForCausalLM.forward.example)
------------------------------
Document 2:

"model hub" "https://huggingface.co/models?search=microsoft/git"
------------------------------
Document 3:

GIT (short for GenerativeImage2Text) model, [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100), [this repository](https://github.com/microsoft/GenerativeImage2Text)
-------------------- input_format --------------------
Document 1:

"0.8B image-text pairs for pre-training, which include COCO (Lin et al., 2014), Conceptual Captions (CC3M) (Sharma et al., 2018), SBU (Ordonez et al., 2011), Visual Genome (VG) (Krishna et al., 2016), Conceptual Captions (CC12M) (Changpinyo et al., 2021), ALT200M (Hu et al., 2021a), and an extra 0.6B data following a similar collection procedure in Hu et al. (2021a)"
-------------------- output_format --------------------


[{'datasets': ['COCO', 'Conceptual Captions (CC3M)', 'SBU', 'Visual Genome (VG)', 'Conceptual Capti 
ons (CC12M)', 'ALT200M'], 'license': 'mit', 'github': 'https://github.com/microsoft/GenerativeImage2 
Text', 'paper': 'https://arxiv.org/abs/2205.14100', 'upstream_model': 'GIT', 'parameter_count': '20  
million image-text pairs', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'large-sized versio 
n', 'limitation_and_bias': 'GIT (short for GenerativeImage2Text) model, large-sized version, fine-tu 
ned on COCO. It was introduced in the paper [GIT: A Generative Image-to-text Transformer for Vision  
and Language](https://arxiv.org/abs/2205.14100) by Wang et al. and first released in [this repositor 
y](https://github.com/microsoft/GenerativeImage2Text).', 'demo': '[documentation](https://huggingfac 
e.co/docs/transformers/main/model_doc/git#transformers.GitForCausalLM.forward.example)', 'input_form 
at': '0.8B image-text pairs for pre-training, which include COCO (Lin et al., 2014), Conceptual Capt 
ions (CC3M) (Sharma et al., 2018), SBU (Ordonez et al., 2011), Visual Genome (VG) (Krishna et al., 2 
016), Conceptual Captions (CC12M) (Changpinyo et al., 2021), ALT200M (Hu et al., 2021a), and an extr 
a 0.6B data following a similar collection procedure in Hu et al. (2021a)', 'output_format': ''}]    

#####################optimum/t5-small########################

-------------------- datasets --------------------
Document 1:

datasets: - c4
------------------------------
Document 2:

"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" and "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
------------------------------
Document 3:

"optimum/t5-small"
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
-------------------- paper --------------------
Document 1:

Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
-------------------- upstream_model --------------------
Document 1:

"optimum/t5-small"

upstream_model: optimum/t5-small
-------------------- parameter_count --------------------
Document 1:

"T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format."
-------------------- hyper_parameters --------------------
Document 1:

"For more information, please take a look at the original paper. Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*
-------------------- evaluation --------------------
Document 1:

"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" and "Authors: *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*"
-------------------- hardware --------------------
Document 1:

T5, Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
-------------------- limitation_and_bias --------------------
Document 1:

"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" and "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
-------------------- demo --------------------
Document 1:

"T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format."
------------------------------
Document 2:

"You can use this model with Transformers *pipeline*.  
```python
from transformers import AutoTokenizer, pipeline
from optimum.onnxruntime import ORTModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("optimum/t5-small")
model = ORTModelForSeq2SeqLM.from_pretrained("optimum/t5-small")
translator = pipeline("translation_en_to_fr", model=model, tokenizer=tokenizer)
results = translator("My name is Eustache and I have a pet raccoon")
print(results)
```"
-------------------- input_format --------------------
Document 1:

"text-to-text format" "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
------------------------------
Document 2:

language:
- en
- fr
- ro
- de
- multilingual
datasets:
- c4
------------------------------
Document 3:

"from transformers import AutoTokenizer, pipeline"
-------------------- output_format --------------------
Document 1:

license: apache-2.0, tags: - summarization - translation, datasets: - c4
------------------------------
Document 2:

"text-to-text format" "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
-------------------- input_token_limit --------------------
Document 1:

"T5", "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
-------------------- vocabulary_size --------------------
Document 1:

"T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format."

[{'datasets': ['c4'], 'license': 'apache-2.0', 'github': '', 'paper': 'Exploring the Limits of Tran 
sfer Learning with a Unified Text-to-Text Transformer', 'upstream_model': 'optimum/t5-small', 'param 
eter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '' 
, 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ' 
'}]                                                                                                  

#####################stablediffusionapi/edge-of-realism########################

-------------------- datasets --------------------
Document 1:

stablediffusionapi.com, stable-diffusion-api, text-to-image, ultra-realistic
------------------------------
Document 2:

"Model link: [View model](https://stablediffusionapi.com/models/edge-of-realism)  
Credits: [View credits](https://civitai.com/?query=model_search)  
View all models: [View Models](https://stablediffusionapi.com/models)"
-------------------- license --------------------
Document 1:

license: creativeml-openrail-m
------------------------------
Document 2:

No Payment needed. Replace Key in below code, change **model_id**  to "edge-of-realism". View docs: [View docs](https://stablediffusionapi.com/docs). Model link: [View model](https://stablediffusionapi.com/models/edge-of-realism). View all models: [View Models](https://stablediffusionapi.com/models).
-------------------- github --------------------
Document 1:

license: creativeml-openrail-m, tags: - stablediffusionapi.com, - stable-diffusion-api, - text-to-image, - ultra-realistic, pinned: true
------------------------------
Document 2:

"Replace Key in below code, change **model_id**  to "edge-of-realism"  
Model link: [View model](https://stablediffusionapi.com/models/edge-of-realism)  
View all models: [View Models](https://stablediffusionapi.com/models)
-------------------- paper --------------------
Document 1:

"text-to-image", "ultra-realistic"
------------------------------
Document 2:

"Model link: [View model](https://stablediffusionapi.com/models/edge-of-realism)"
-------------------- upstream_model --------------------
Document 1:

"model_id":  "edge-of-realism"

NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

parameter_count
------------------------------
Document 2:

"key":  "",
"model_id":  "edge-of-realism",
"prompt":  "actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera",
"negative_prompt":  "painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime",
"width":  "512",
"height":  "512",
"samples":  "1",
"num_inference_steps":  "30",
"safety_checker":  "no",
"enhance_prompt":  "yes",
-------------------- hyper_parameters --------------------
Document 1:

"key":  "",
"model_id":  "edge-of-realism",
"prompt":  "actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera",
"negative_prompt":  "painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime",
"width":  "512",
"height":  "512",
"samples":  "1",
"num_inference_steps":  "30",
"safety_checker":  "no",
"enhance_prompt":  "yes",
-------------------- evaluation --------------------
Document 1:

![generated from stablediffusionapi.com](https://cdn.stablediffusionapi.com/generations/7504788501684254537.png)
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

"model_id":  "edge-of-realism", "prompt":  "actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera", "negative_prompt":  "painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime"
-------------------- demo --------------------
Document 1:

Get API key from [Stable Diffusion API](http://stablediffusionapi.com/), No Payment needed.  
Replace Key in below code, change **model_id**  to "edge-of-realism"  
Coding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)  
Model link: [View model](https://stablediffusionapi.com/models/edge-of-realism)
-------------------- input_format --------------------
Document 1:

"key":  "",
"model_id":  "edge-of-realism",
"prompt":  "actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera",
"negative_prompt":  "painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime",
"width":  "512",
"height":  "512",
"samples":  "1",
"num_inference_steps":  "30",
"safety_checker":  "no",
"enhance_prompt":  "yes",
-------------------- output_format --------------------
Document 1:

"key":  "",
"model_id":  "edge-of-realism",
"width":  "512",
"height":  "512",
"samples":  "1",
"num_inference_steps":  "30",
"safety_checker":  "no",
"enhance_prompt":  "yes",
"seed":  None,
"guidance_scale":  7.5,
"multi_lingual":  "no",
"panorama":  "no",
"self_attention":  "no",
"upscale":  "no",
"embeddings":  "embeddings_model_id",
"lora":  "lora_model_id",
"webhook":  None,
"track_id":  None

[{'datasets': ['stablediffusionapi.com', 'stable-diffusion-api', 'text-to-image', 'ultra-realistic' 
], 'license': 'creativeml-openrail-m', 'github': 'license: creativeml-openrail-m, tags: - stablediff 
usionapi.com, - stable-diffusion-api, - text-to-image, - ultra-realistic, pinned: true', 'paper': '" 
text-to-image", "ultra-realistic"', 'upstream_model': '"model_id":  "edge-of-realism"', 'parameter_c 
ount': 'parameter_count', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '',  
'optimizer': ''}, 'evaluation': [{'test': '![generated from stablediffusionapi.com](https://cdn.stab 
lediffusionapi.com/generations/7504788501684254537.png)', 'result': 0}], 'hardware': '', 'limitation 
_and_bias': '"model_id":  "edge-of-realism", "prompt":  "actual 8K portrait photo of gareth person,  
portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beau 
tiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto  
shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera",  
"negative_prompt":  "painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face,  
deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, dou 
ble torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra  
legs, anime"', 'demo': 'Get API key from [Stable Diffusion API](http://stablediffusionapi.com/), No  
Payment needed.\nReplace Key in below code, change **model_id**  to "edge-of-realism"\nCoding in PHP 
/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.c 
om/docs)\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)', 'input_f 
ormat': '"key":  "",\n"model_id":  "edge-of-realism",\n"prompt":  "actual 8K portrait photo of garet 
h person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy  
eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face,  
by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into 
 camera",\n"negative_prompt":  "painting, extra fingers, mutated hands, poorly drawn hands, poorly d 
rawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, g 
litchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted f 
ace, extra legs, anime",\n"width":  "512",\n"height":  "512",\n"samples":  "1",\n"num_inference_step 
s":  "30",\n"safety_checker":  "no",\n"enhance_prompt":  "yes"', 'output_format': '"key":  "",\n"mod 
el_id":  "edge-of-realism",\n"width":  "512",\n"height":  "512",\n"samples":  "1",\n"num_inference_s 
teps":  "30",\n"safety_checker":  "no",\n"enhance_prompt":  "yes",\n"seed":  None,\n"guidance_scale" 
:  7.5,\n"multi_lingual":  "no",\n"panorama":  "no",\n"self_attention":  "no",\n"upscale":  "no",\n" 
embeddings":  "embeddings_model_id",\n"lora":  "lora_model_id",\n"webhook":  None,\n"track_id":  Non 
e'}]                                                                                                 

#####################dmis-lab/bern2-ner########################

-------------------- datasets --------------------

-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

"NER Model of BERN2 system"
-------------------- upstream_model --------------------
Document 1:

upstream_model
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------

-------------------- input_format --------------------

-------------------- output_format --------------------


[{'datasets': [], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_count' 
: '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': ' 
', 'input_format': '', 'output_format': ''}]                                                         
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e164b-5acdb2d73952ff5a2fcfca07)

Entry Not Found for url: https://huggingface.co/sshleifer/tiny-gpt2/resolve/main/README.md. 

#####################microsoft/deberta-xlarge-mnli########################

-------------------- datasets --------------------
Document 1:

"80GB training data" and "[official repository](https://github.com/microsoft/DeBERTa)"
------------------------------
Document 2:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"International Conference on Learning Representations"
-------------------- github --------------------
Document 1:

"If you find DeBERTa useful for your work, please cite the following paper: ``` latex @inproceedings{he2021deberta, title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION}, author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen}, booktitle={International Conference on Learning Representations}, year={2021}, url={https://openreview.net/forum?id=XPZIaotutsD} }```"
------------------------------
Document 2:

"Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates."
-------------------- paper --------------------
Document 1:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
------------------------------
Document 2:

"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data." NO_OUTPUT
-------------------- upstream_model --------------------
Document 1:

"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"DeBERTa xlarge model(750M)"
-------------------- evaluation --------------------
Document 1:

| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |
|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|
|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |
| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |
| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92
-------------------- hardware --------------------
Document 1:

"International Conference on Learning Representations"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

language: en, license: mit, tags: - deberta-v1, - deberta-mnli, tasks: mnli, thumbnail: https://huggingface.co/front/thumbnails/microsoft.png, widget: - text: '[CLS] I love you. [SEP] I like you. [SEP]'
------------------------------
Document 2:

"This the DeBERTa xlarge model(750M) fine-tuned with mnli task."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['80GB training data'], 'license': 'mit', 'github': '[official repository](https://gi 
thub.com/microsoft/DeBERTa)', 'paper': 'DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION' 
, 'upstream_model': 'DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION', 'parameter_count' 
: 'DeBERTa xlarge model(750M)', 'hyper_parameters': [], 'evaluation': [{'test': 'SQuAD 1.1', 'result 
': 90.9}, {'test': 'SQuAD 2.0', 'result': 81.8}, {'test': 'MNLI-m/mm', 'result': 86.6}, {'test': 'SS 
T-2', 'result': 93.2}, {'test': 'QNLI', 'result': 92.3}, {'test': 'CoLA', 'result': 60.6}, {'test':  
'RTE', 'result': 70.4}, {'test': 'MRPC', 'result': 88.0}, {'test': 'QQP', 'result': 91.3}, {'test':  
'STS-B', 'result': 90.0}], 'hardware': 'International Conference on Learning Representations', 'limi 
tation_and_bias': '', 'demo': "language: en, license: mit, tags: - deberta-v1, - deberta-mnli, tasks 
: mnli, thumbnail: https://huggingface.co/front/thumbnails/microsoft.png, widget: - text: '[CLS] I l 
ove you. [SEP] I like you. [SEP]'", 'input_format': '', 'output_format': '', 'input_token_limit': '' 
, 'vocabulary_size': ''}]                                                                            

#####################pedramyazdipoor/persian_xlm_roberta_large########################

-------------------- datasets --------------------
Document 1:

PQuAD Train set
------------------------------
Document 2:

datasets, [Newsha Shahbodaghkhan](https://huggingface.co/datasets/newsha/PQuAD/tree/main)
------------------------------
Document 3:

XLM-RoBERTa is a multilingual language model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages; Multilingual [XLM-RoBERTa large for QA on various languages](https://huggingface.co/deepset/xlm-roberta-large-squad2) is fine-tuned on various QA datasets but PQuAD, which is the biggest persian QA dataset so far.
-------------------- license --------------------

-------------------- github --------------------

-------------------- paper --------------------
Document 1:

[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116v2) by Conneau et al. and [arXiv:2202.06219](https://arxiv.org/abs/2202.06219)
-------------------- upstream_model --------------------
Document 1:

XLM-RoBERTa, Unsupervised Cross-lingual Representation Learning at Scale, Multilingual XLM-RoBERTa large for QA on various languages, PQuAD
-------------------- parameter_count --------------------
Document 1:

"batch_size = 4", "n_epochs = 1", "base_LM_model = "deepset/xlm-roberta-large-squad2"", "max_seq_len = 256", "learning_rate = 3e-5", "evaluation_strategy = "epoch"", "save_strategy = "epoch"", "learning_rate = 3e-5", "warmup_ratio = 0.1", "gradient_accumulation_steps = 8", "weight_decay = 0.01"
------------------------------
Document 2:

parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

batch_size = 4, n_epochs = 1, base_LM_model = "deepset/xlm-roberta-large-squad2", max_seq_len = 256, learning_rate = 3e-5, evaluation_strategy = "epoch", save_strategy = "epoch", warmup_ratio = 0.1, gradient_accumulation_steps = 8, weight_decay = 0.01
------------------------------
Document 2:

"XLM-RoBERTA is a multilingual language model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages." "Multilingual XLM-RoBERTa large for QA on various languages is fine-tuned on various QA datasets but PQuAD, which is the biggest persian QA dataset so far." "Paper presenting PQuAD dataset: [arXiv:2202.06219](https://arxiv.org/abs/2202.06219)"
-------------------- evaluation --------------------
Document 1:

"Evaluated on the PQuAD Persian test set with the [official PQuAD link](https://huggingface.co/datasets/newsha/PQuAD)." "Our XLM-Roberta outperforms [our ParsBert on PQuAD](https://huggingface.co/pedramyazdipoor/parsbert_question_answering_PQuAD), but the former is more than 3 times bigger than the latter one; so comparing these two is not fair."
-------------------- hardware --------------------
Document 1:

"XLM-RoBERTa is a multilingual language model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages."
-------------------- limitation_and_bias --------------------
Document 1:

"XLM-RoBERTA is a multilingual language model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.", "Multilingual XLM-RoBERTa large for QA on various languages is fine-tuned on various QA datasets but PQuAD, which is the biggest persian QA dataset so far.", "Paper presenting PQuAD dataset: [arXiv:2202.06219](https://arxiv.org/abs/2202.06219)"
-------------------- demo --------------------
Document 1:

"This model is fine-tuned on PQuAD Train set and is easily ready to use."
------------------------------
Document 2:

Multilingual [XLM-RoBERTa large for QA on various languages](https://huggingface.co/deepset/xlm-roberta-large-squad2)
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['PQuAD Train set'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '',  
'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation_and_bia 
s': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_si 
ze': ''}]                                                                                            

#####################cl-tohoku/bert-base-japanese-v2########################

-------------------- datasets --------------------
Document 1:

"The models are trained on the Japanese version of Wikipedia.", "The training corpus is generated from the Wikipedia Cirrussearch dump file as of August 31, 2020.", "The generated corpus files are 4.0GB in total, containing approximately 30M sentences.", "[MeCab](https://taku910.github.io/mecab/) morphological parser with [mecab-ipadic-NEologd](https://github.com/neologd/mecab-ipadic-neologd) dictionary to split texts into sentences."
-------------------- license --------------------
Document 1:

Creative Commons Attribution-ShareAlike 3.0
------------------------------
Document 2:

license: cc-by-sa-4.0
-------------------- github --------------------
Document 1:

"Creative Commons Attribution-ShareAlike 3.0"
-------------------- paper --------------------
Document 1:

"TensorFlow Research Cloud"
------------------------------
Document 2:

Creative Commons Attribution-ShareAlike 3.0
------------------------------
Document 3:

"The codes for the pretraining are available at [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v2.0)."
-------------------- upstream_model --------------------
Document 1:

BERT, Unidic 2.1.2 dictionary, WordPiece subword tokenization, masked language modeling (MLM) objective, cl-tohoku/bert-japanese
------------------------------
Document 2:

Creative Commons Attribution-ShareAlike 3.0
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"512 tokens per instance, 256 instances per batch, and 1M training steps."
------------------------------
Document 2:

"12 layers, 768 dimensions of hidden states, and 12 attention heads."
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

Cloud TPUs
------------------------------
Document 2:

"Cloud TPUs provided by [TensorFlow Research Cloud program](https://www.tensorflow.org/tfrc/)"
-------------------- limitation_and_bias --------------------
Document 1:

The models are trained on the Japanese version of Wikipedia. The training corpus is generated from the Wikipedia Cirrussearch dump file as of August 31, 2020. The generated corpus files are 4.0GB in total, containing approximately 30M sentences. We used the [MeCab](https://taku910.github.io/mecab/) morphological parser with [mecab-ipadic-NEologd](https://github.com/neologd/mecab-ipadic-neologd) dictionary to split texts into sentences.
-------------------- demo --------------------
Document 1:

"Creative Commons Attribution-ShareAlike 3.0"
------------------------------
Document 2:

TensorFlow Research Cloud program.
-------------------- input_format --------------------
Document 1:

Unidic 2.1.2 dictionary, WordPiece subword tokenization, whole word masking enabled
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

input_token_limit
-------------------- vocabulary_size --------------------
Document 1:

"The vocabulary size is 32768."

[{'datasets': ['Japanese Wikipedia'], 'license': 'Creative Commons Attribution-ShareAlike 3.0', 'gi 
thub': 'https://github.com/cl-tohoku/bert-japanese/tree/v2.0', 'paper': '', 'upstream_model': 'cl-to 
hoku/bert-japanese', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': 'C 
loud TPUs', 'limitation_and_bias': 'The models are trained on the Japanese version of Wikipedia. The 
 training corpus is generated from the Wikipedia Cirrussearch dump file as of August 31, 2020. The g 
enerated corpus files are 4.0GB in total, containing approximately 30M sentences. We used the MeCab  
morphological parser with mecab-ipadic-NEologd dictionary to split texts into sentences.', 'demo': ' 
TensorFlow Research Cloud program.', 'input_format': 'Unidic 2.1.2 dictionary, WordPiece subword tok 
enization, whole word masking enabled', 'output_format': '', 'input_token_limit': 'input_token_limit 
', 'vocabulary_size': '32768'}]                                                                      

#####################intfloat/e5-large-v2########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 21524 tokens (21268 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################moka-ai/m3e-base########################

ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 This model's maximum context length is 4097 tokens, however you requested 6758 tokens (6502 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. 

#####################microsoft/graphcodebert-base########################

-------------------- datasets --------------------
Document 1:

"The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages."
-------------------- license --------------------
Document 1:

"More details can be found in the [paper](https://arxiv.org/abs/2009.08366) by Guo et. al."
-------------------- github --------------------

-------------------- paper --------------------
Document 1:

[paper](https://arxiv.org/abs/2009.08366) by Guo et. al.
-------------------- upstream_model --------------------
Document 1:

"GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language" NO_OUTPUT
-------------------- parameter_count --------------------
Document 1:

"12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512." parameter_count NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512."
-------------------- evaluation --------------------
Document 1:

"GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages. More details can be found in the [paper](https://arxiv.org/abs/2009.08366) by Guo et. al."
-------------------- hardware --------------------
Document 1:

"The model is trained on the CodeSearchNet dataset"
-------------------- limitation_and_bias --------------------
Document 1:

GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages. NO_OUTPUT
-------------------- demo --------------------
Document 1:

GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages. More details can be found in the [paper](https://arxiv.org/abs/2009.08366) by Guo et. al.
-------------------- input_format --------------------
Document 1:

CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages.
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['CodeSearchNet'], 'license': 'https://arxiv.org/abs/2009.08366', 'github': '', 'pape 
r': 'https://arxiv.org/abs/2009.08366', 'upstream_model': '', 'parameter_count': '12 layers, 768 dim 
ensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512.',  
'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluat 
ion': [], 'hardware': 'CodeSearchNet dataset', 'limitation_and_bias': 'GraphCodeBERT is a graph-base 
d pre-trained model based on the Transformer architecture for programming language, which also consi 
ders data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimen 
sional hidden states, and 12 attention heads. The maximum sequence length for the model is 512. The  
model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for 
 six programming languages.', 'demo': 'GraphCodeBERT is a graph-based pre-trained model based on the 
 Transformer architecture for programming language, which also considers data-flow information along 
 with code sequences. The model is trained on the CodeSearchNet dataset, which includes 2.3M functio 
ns with document pairs for six programming languages. More details can be found in the [paper](https 
://arxiv.org/abs/2009.08366) by Guo et. al.', 'input_format': 'CodeSearchNet dataset, which includes 
 2.3M functions with document pairs for six programming languages.', 'output_format': ''}]           
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e16e2-60c4e8264a01bc631005e116)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-wav2vec2-conformer/resolve/main/README.md. 

#####################nlptown/flaubert_small_cased_sentiment########################

-------------------- datasets --------------------
Document 1:

"The training data consists of the French portion of `amazon_reviews_multi`, supplemented with another 140,000 similar reviews."
------------------------------
Document 2:

datasets: - amazon_reviews_multi
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"flaubert_small_cased" model, "sentiment analysis", "very_negative" (1 star) to "very_positive" (5 stars)
-------------------- evaluation --------------------
Document 1:

"This is a `flaubert_small_cased` model finetuned for sentiment analysis on product reviews in French. It predicts the sentiment of the review, from `very_negative` (1 star) to `very_positive` (5 stars)."
------------------------------
Document 2:

"The finetuned model was evaluated on the French test set of `amazon_reviews_multi`. Accuracy (exact) is the exact match on the number of stars. Accuracy (off-by-1) is the percentage of reviews where the number of stars the model predicts differs by a maximum of 1 from the number given by the human reviewer. | Language | Accuracy (exact) | Accuracy (off-by-1) | | -------- | ---------------------- | ------------------- | | French  | 61.56%                 | 95.66%
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------
Document 1:

limitation_and_bias
-------------------- demo --------------------
Document 1:

"[NLP Town](https://www.nlp.town) offers a suite of sentiment models for a wide range of languages, including an improved multilingual model through [RapidAPI](https://rapidapi.com/nlp-town-nlp-town-default/api/multilingual-sentiment-analysis2/)."
------------------------------
Document 2:

`flaubert_small_cased` model finetuned for sentiment analysis on product reviews in French. It predicts the sentiment of the review, from `very_negative` (1 star) to `very_positive` (5 stars).
-------------------- input_format --------------------
Document 1:

- amazon_reviews_multi
-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['amazon_reviews_multi'], 'license': 'mit', 'github': '', 'paper': '', 'upstream_mode 
l': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardware': '', 'limitation 
_and_bias': '', 'demo': '', 'input_format': '- amazon_reviews_multi', 'output_format': '', 'input_to 
ken_limit': '', 'vocabulary_size': ''}]                                                              

#####################TheBloke/Wizard-Vicuna-13B-Uncensored-HF########################

-------------------- datasets --------------------
Document 1:

datasets: - ehartford/wizard_vicuna_70k_unfiltered
-------------------- license --------------------
Document 1:

license: other
-------------------- github --------------------

-------------------- paper --------------------

-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"This is [wizard-vicuna-13b](https://huggingface.co/junelee/wizard-vicuna-13b) trained with a subset of the dataset - responses that contained alignment / moralizing were removed."
-------------------- input_format --------------------
Document 1:

"4bit GPTQ models for GPU inference", "4bit and 5bit GGML models for CPU inference", "float16 HF format model for GPU inference and further conversions"
------------------------------
Document 2:

"wizard-vicuna-13b" "trained with a subset of the dataset - responses that contained alignment / moralizing were removed" "The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA." "input_format": NO_OUTPUT
-------------------- output_format --------------------
Document 1:

"4bit GPTQ models for GPU inference", "4bit and 5bit GGML models for CPU inference", "float16 HF format model for GPU inference and further conversions"
------------------------------
Document 2:

"This is [wizard-vicuna-13b](https://huggingface.co/junelee/wizard-vicuna-13b) trained with a subset of the dataset - responses that contained alignment / moralizing were removed."

NO_OUTPUT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['ehartford/wizard_vicuna_70k_unfiltered'], 'license': 'other', 'github': '', 'paper' 
: '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': [], 'evaluation': [], 'hardwa 
re': '', 'limitation_and_bias': '', 'demo': 'This is [wizard-vicuna-13b](https://huggingface.co/june 
lee/wizard-vicuna-13b) trained with a subset of the dataset - responses that contained alignment / m 
oralizing were removed.', 'input_format': '4bit GPTQ models for GPU inference, 4bit and 5bit GGML mo 
dels for CPU inference, float16 HF format model for GPU inference and further conversions', 'output_ 
format': '4bit GPTQ models for GPU inference, 4bit and 5bit GGML models for CPU inference, float16 H 
F format model for GPU inference and further conversions', 'input_token_limit': '', 'vocabulary_size 
': ''}]                                                                                              

#####################bionlp/bluebert_pubmed_mimic_uncased_L-24_H-1024_A-16########################

-------------------- datasets --------------------
Document 1:

"A BERT model pre-trained on PubMed abstracts and clinical notes ([MIMIC-III](https://mimic.physionet.org/))."
------------------------------
Document 2:

"preprocessed PubMed texts" (https://ftp.ncbi.nlm.nih.gov/pub/lu/Suppl/NCBI-BERT/pubmed_uncased_sentence_nltk.txt.tar.gz), "PubMed ASCII code version" (https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PubMed/), "pre-trained model" (https://huggingface.co/bert-large-uncased)
-------------------- license --------------------
Document 1:

license: cc0-1.0
------------------------------
Document 2:

"MIMIC-III"
-------------------- github --------------------
Document 1:

"https://ftp.ncbi.nlm.nih.gov/pub/lu/Suppl/NCBI-BERT/pubmed_uncased_sentence_nltk.txt.tar.gz", "https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PubMed/", "https://huggingface.co/bert-large-uncased"
-------------------- paper --------------------
Document 1:

"A BERT model pre-trained on PubMed abstracts and clinical notes ([MIMIC-III](https://mimic.physionet.org/))."
------------------------------
Document 2:

"National Institutes of Health, National Library of Medicine and Clinical Center" "National Library of Medicine of the National Institutes of Health under award number 4R00LM013001-01" "BERT and ELMo" "Dr Sun Kim"
------------------------------
Document 3:

"Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"
-------------------- upstream_model --------------------
Document 1:

MIMIC-III
------------------------------
Document 2:

"Pre-trained model: https://huggingface.co/bert-large-uncased"
------------------------------
Document 3:

- bert - bluebert
-------------------- parameter_count --------------------
Document 1:

parameter_count MIMIC-III
------------------------------
Document 2:

"Pre-trained model: https://huggingface.co/bert-large-uncased"
-------------------- hyper_parameters --------------------
Document 1:

"BERT model pre-trained on PubMed abstracts and clinical notes ([MIMIC-III](https://mimic.physionet.org/))"
------------------------------
Document 2:

"preprocessed PubMed texts", "~4000M words extracted from the PubMed ASCII code version", "Pre-trained model: https://huggingface.co/bert-large-uncased"
NO_OUTPUT
-------------------- evaluation --------------------
Document 1:

"A BERT model pre-trained on PubMed abstracts and clinical notes ([MIMIC-III](https://mimic.physionet.org/))."
-------------------- hardware --------------------
Document 1:

MIMIC-III
------------------------------
Document 2:

preprocessed PubMed texts, PubMed ASCII code version
-------------------- limitation_and_bias --------------------
Document 1:

MIMIC-III
-------------------- demo --------------------
Document 1:

MIMIC-III
-------------------- input_format --------------------
Document 1:

MIMIC-III, input_format
------------------------------
Document 2:

"preprocessed PubMed texts", "PubMed ASCII code version"
input_format: preprocessed PubMed texts, PubMed ASCII code version
-------------------- output_format --------------------
Document 1:

MIMIC-III, output_format
------------------------------
Document 2:

"Pre-trained model: https://huggingface.co/bert-large-uncased"

[{'datasets': ['MIMIC-III'], 'license': 'cc0-1.0', 'github': 'https://ftp.ncbi.nlm.nih.gov/pub/lu/S 
uppl/NCBI-BERT/pubmed_uncased_sentence_nltk.txt.tar.gz', 'paper': 'A BERT model pre-trained on PubMe 
d abstracts and clinical notes ([MIMIC-III](https://mimic.physionet.org/)).', 'upstream_model': 'MIM 
IC-III', 'parameter_count': 'parameter_count MIMIC-III', 'hyper_parameters': {}, 'evaluation': [{'te 
st': 'A BERT model pre-trained on PubMed abstracts and clinical notes ([MIMIC-III](https://mimic.phy 
sionet.org/)).', 'result': 0}], 'hardware': 'MIMIC-III', 'limitation_and_bias': 'MIMIC-III', 'demo': 
 'MIMIC-III', 'input_format': 'MIMIC-III, input_format', 'output_format': 'MIMIC-III, output_format' 
}, {'datasets': ['preprocessed PubMed texts', 'PubMed ASCII code version', 'pre-trained model'], 'li 
cense': 'MIMIC-III', 'github': 'https://huggingface.co/bert-large-uncased', 'paper': 'National Insti 
tutes of Health, National Library of Medicine and Clinical Center National Library of Medicine of th 
e National Institutes of Health under award number 4R00LM013001-01 BERT and ELMo Dr Sun Kim Transfer 
 Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmark 
ing Datasets', 'upstream_model': 'Pre-trained model: https://huggingface.co/bert-large-uncased', 'pa 
rameter_count': 'Pre-trained model: https://huggingface.co/bert-large-uncased', 'hyper_parameters':  
{}, 'evaluation': [{'test': 'A BERT model pre-trained on PubMed abstracts and clinical notes ([MIMIC 
-III](https://mimic.physionet.org/)).', 'result': 0}], 'hardware': 'preprocessed PubMed texts, PubMe 
d ASCII code version', 'limitation_and_bias': 'MIMIC-III', 'demo': 'MIMIC-III', 'input_format': 'pre 
processed PubMed texts, PubMed ASCII code version', 'output_format': 'Pre-trained model: https://hug 
gingface.co/bert-large-uncased'}]                                                                    

#####################facebook/wav2vec2-base########################

-------------------- datasets --------------------
Document 1:

datasets: - librispeech_asr
------------------------------
Document 2:

The base model pretrained on 16kHz sampled speech audio. **Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

"The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20."
-------------------- paper --------------------
Document 1:

[Paper](https://arxiv.org/abs/2006.11477) Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

"The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz."
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

[this notebook](https://colab.research.google.com/drive/1FjTsqbYKphl9kL-eILgUc-bl4zVThL8F?usp=sharing)
------------------------------
Document 2:

Facebook's Wav2Vec2, The base model pretrained on 16kHz sampled speech audio, This model does not have a tokenizer as it was pretrained on audio alone, Check out this blog for more in-detail explanation of how to fine-tune the model, the original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.
-------------------- input_format --------------------
Document 1:

"The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz." input_format: 16kHz sampled speech audio
-------------------- output_format --------------------


[{'datasets': ['librispeech_asr'], 'license': 'apache-2.0', 'github': 'https://github.com/pytorch/f 
airseq/tree/master/examples/wav2vec#wav2vec-20', 'paper': '[Paper](https://arxiv.org/abs/2006.11477) 
 Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli', 'upstream_model': '', 'par 
ameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'The base model pretrained  
on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled 
 at 16Khz.', 'limitation_and_bias': '', 'demo': '[this notebook](https://colab.research.google.com/d 
rive/1FjTsqbYKphl9kL-eILgUc-bl4zVThL8F?usp=sharing)', 'input_format': 'The base model pretrained on  
16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 
 16Khz.', 'output_format': ''}]                                                                      

#####################microsoft/table-transformer-structure-recognition########################

-------------------- datasets --------------------
Document 1:

PubTables1M, [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061), [this repository](https://github.com/microsoft/table-transformer)
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"this repository": https://github.com/microsoft/table-transformer
-------------------- paper --------------------
Document 1:

"PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents" and "this repository"
------------------------------
Document 2:

"DETR" and "Note that the authors decided to use the "normalize before" setting of DETR"
-------------------- upstream_model --------------------
Document 1:

upstream_model: Table Transformer (DETR) model trained on PubTables1M
------------------------------
Document 2:

upstream_model DETR
-------------------- parameter_count --------------------
Document 1:

parameter_count: NO_OUTPUT
-------------------- hyper_parameters --------------------
Document 1:

"Table Transformer (DETR) model trained on PubTables1M." "It was introduced in the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Smock et al. and first released in [this repository](https://github.com/microsoft/table-transformer)."
-------------------- evaluation --------------------
Document 1:

"Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Smock et al. and first released in [this repository](https://github.com/microsoft/table-transformer)."
-------------------- hardware --------------------
Document 1:

"Table Transformer (DETR) model trained on PubTables1M"
-------------------- limitation_and_bias --------------------
Document 1:

"The team releasing Table Transformer did not write a model card for this model"
-------------------- demo --------------------
Document 1:

"The team releasing Table Transformer did not write a model card for this model so this model card has been written by the Hugging Face team."
------------------------------
Document 2:

"raw model for detecting the structure (like rows, columns) in tables"
-------------------- input_format --------------------
Document 1:

"PubTables1M" and "this repository"
-------------------- output_format --------------------
Document 1:

output_format: NO_OUTPUT
-------------------- input_preprocessing --------------------
Document 1:

"The team releasing Table Transformer did not write a model card for this model"
------------------------------
Document 2:

"Note that the authors decided to use the "normalize before" setting of DETR"
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


[{'datasets': ['PubTables1M'], 'license': 'mit', 'github': 'https://github.com/microsoft/table-tran 
sformer', 'paper': 'https://arxiv.org/abs/2110.00061', 'upstream_model': 'Table Transformer (DETR) m 
odel trained on PubTables1M', 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epochs': '', 'ba 
tch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': 'Table Transform 
er (DETR) model trained on PubTables1M', 'limitation_and_bias': 'The team releasing Table Transforme 
r did not write a model card for this model', 'demo': 'The team releasing Table Transformer did not  
write a model card for this model so this model card has been written by the Hugging Face team.', 'i 
nput_format': 'PubTables1M', 'output_format': 'NO_OUTPUT', 'input_preprocessing': 'The team releasin 
g Table Transformer did not write a model card for this model', 'input_size': '', 'num_of_classes_fo 
r_classification': '', 'trigger_word': ''}, {'datasets': [], 'license': '', 'github': '', 'paper': ' 
', 'upstream_model': 'DETR', 'parameter_count': '', 'hyper_parameters': {'epochs': '', 'batch_size': 
 '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitation_and_bias': 
 '', 'demo': '', 'input_format': '', 'output_format': '', 'input_preprocessing': '', 'input_size': ' 
', 'num_of_classes_for_classification': '', 'trigger_word': ''}]                                     
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e178c-56a6ef300d5c5771175ff594)

Entry Not Found for url: https://huggingface.co/digit82/kobart-summarization/resolve/main/README.md. 
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e178c-0ed55f4e2941e75557dfb0e5)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-UniSpeechSatModel/resolve/main/README.md. 

#####################google/flan-ul2########################

-------------------- datasets --------------------
Document 1:

The original training corpus is `C4`, Flan has been primarily trained on academic tasks, The Flan datasets have also been open sourced in “The Flan Collection: Designing Data and Methods for Effective Instruction Tuning” (Longpre et al.). See Google AI Blogpost: “The Flan Collection: Advancing Open Source Methods for Instruction Tuning”.
-------------------- license --------------------
Document 1:

license: apache-2.0
-------------------- github --------------------
Document 1:

- svakulenk0/qrecc
- djaym7/wiki_dialog
- deepmind/code_contests
- gsm8k
- aqua_rat
- esnli
- quasc
- qed
- c4
-------------------- paper --------------------
Document 1:

Yi Tay, Hugging Face ecosystem, Younes Belkada, Arthur Zucker
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

parameter_count 32 encoder layers, 32 decoder layers, `dmodel` of 4096 and `df` of 16384, The dimension of each head is 256 for a total of 16 heads, model parallelism of 8, vocab size 32000
-------------------- hyper_parameters --------------------
Document 1:

batch size of 1024, sequence length of 512/512, dropout of 0, 32 encoder layers and 32 decoder layers, `dmodel` of 4096 and `df` of 16384, dimension of each head is 256 for a total of 16 heads, model parallelism of 8.
-------------------- evaluation --------------------
Document 1:

"In “Scaling Instruction-Finetuned language models (Chung et al.)” (also referred to sometimes as the Flan2 paper), the key idea is to train a large language model on a collection of datasets. These datasets are phrased as instructions which enable generalization across diverse tasks. Flan has been primarily trained on academic tasks. In Flan2, we released a series of T5 models ranging from 200M to 11B parameters that have been instruction tuned with Flan."
-------------------- hardware --------------------
Document 1:

"model parallelism of 8", "Jax" and "T5X"
-------------------- limitation_and_bias --------------------
Document 1:

"We conjecture that a strong universal model has to be exposed to solving diverse set of problems during pre-training. Given that pre-training is done using self-supervision, we argue that such diversity should be injected to the objective of the model, otherwise the model might suffer from lack a certain ability, like long-coherent text generation." "R-Denoiser: The regular denoising is the standard span corruption introduced in [T5](https://huggingface.co/docs/transformers/v4.20.0/en/model_doc/t5) that uses a range of 2 to 5 tokens as the span length, which masks about 15% of input tokens. These spans are short and potentially useful to acquire knowledge instead of learning to generate fluent text." "S-Denoiser: A specific case of denoising where we observe a strict sequential order when framing the inputs-to-targets task, i.e., prefix language modeling. To do so, we simply partition the input sequence into two sub-sequences of tokens as context and target such that the targets do not rely on future information. This is unlike standard span corruption where there could be a target token with earlier position than a
-------------------- demo --------------------
Document 1:

license: apache-2.0
tags:
- text2text-generation
datasets:
- svakulenk0/qrecc
- taskmaster2
- djaym7/wiki_dialog
- deepmind/code_contests
- lambada
- gsm8k
- aqua_rat
- esnli
- quasc
- qed
- c4
widget:
- text: 'Translate to German:  My name is Arthur'
example_title: Translation
- text: Please answer to the following question. Who is going to be the next Ballon
d'or?
example_title: Question Answering
- text: 'Q: Can Geoffrey Hinton have a conversation with George Washington? Give the
rationale before answering.'
example_title: Logical reasoning
- text: Please answer the following question. What is the boiling point of Nitrogen?
example_title: Scientific knowledge
- text: Answer the following yes/no question. Can you write a whole Haiku in a single
tweet?
example_title: Yes/no question
- text: Answer the following
-------------------- input_format --------------------
Document 1:

"load_in_8bit=True" 
"torch_dtype=torch.bfloat16"
-------------------- output_format --------------------
Document 1:

`load_in_8bit=True`, `torch_dtype=torch.bfloat16`
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

vocab size 32000

[{'datasets': ['C4'], 'license': 'apache-2.0', 'github': '- svakulenk0/qrecc\n- djaym7/wiki_dialog\ 
n- deepmind/code_contests\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n- c4', 'paper': 'Yi Tay, Hu 
gging Face ecosystem, Younes Belkada, Arthur Zucker', 'upstream_model': '', 'parameter_count': '32 e 
ncoder layers, 32 decoder layers, `dmodel` of 4096 and `df` of 16384, The dimension of each head is  
256 for a total of 16 heads, model parallelism of 8, vocab size 32000', 'hyper_parameters': {'epochs 
': '', 'batch_size': '1024', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [{'test': '"In “Sc 
aling Instruction-Finetuned language models (Chung et al.)” (also referred to sometimes as the Flan2 
 paper), the key idea is to train a large language model on a collection of datasets. These datasets 
 are phrased as instructions which enable generalization across diverse tasks. Flan has been primari 
ly trained on academic tasks. In Flan2, we released a series of T5 models ranging from 200M to 11B p 
arameters that have been instruction tuned with Flan."', 'result': 0}], 'hardware': '"model parallel 
ism of 8", "Jax" and "T5X"', 'limitation_and_bias': '"We conjecture that a strong universal model ha 
s to be exposed to solving diverse set of problems during pre-training. Given that pre-training is d 
one using self-supervision, we argue that such diversity should be injected to the objective of the  
model, otherwise the model might suffer from lack a certain ability, like long-coherent text generat 
ion." "R-Denoiser: The regular denoising is the standard span corruption introduced in [T5](https:// 
huggingface.co/docs/transformers/v4.20.0/en/model_doc/t5) that uses a range of 2 to 5 tokens as the  
span length, which masks about 15% of input tokens. These spans are short and potentially useful to  
acquire knowledge instead of learning to generate fluent text." "S-Denoiser: A specific case of deno 
ising where we observe a strict sequential order when framing the inputs-to-targets task, i.e., pref 
ix language modeling. To do so, we simply partition the input sequence into two sub-sequences of tok 
ens as context and target such that the targets do not rely on future information. This is unlike st 
andard span corruption where there could be a target token with earlier position than a', 'demo': "l 
icense: apache-2.0\ntags:\n- text2text-generation\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- d 
jaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed 
\n- c4\nwidget:\n- text: 'Translate to German:  My name is Arthur'\nexample_title: Translation\n- te 
xt: Please answer to the following question. Who is going to be the next Ballon\nd'or?\nexample_titl 
e: Question Answering\n- text: 'Q: Can Geoffrey Hinton have a conversation with George Washington? G 
ive the\nrationale before answering.'\nexample_title: Logical reasoning\n- text: Please answer the f 
ollowing question. What is the boiling point of Nitrogen?\nexample_title: Scientific knowledge\n- te 
xt: Answer the following yes/no question. Can you write a whole Haiku in a single\ntweet?\nexample_t 
itle: Yes/no question\n- text: Answer the following", 'input_format': '"load_in_8bit=True"\n"torch_d 
type=torch.bfloat16"', 'output_format': '`load_in_8bit=True`, `torch_dtype=torch.bfloat16`', 'input_ 
token_limit': '', 'vocabulary_size': 'vocab size 32000'}]                                            
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e17bf-31a6b0787407b1f942ed8d79)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-WavLMModel/resolve/main/README.md. 

#####################StanfordAIMI/stanford-deidentifier-with-radiology-reports-and-i2b2########################

-------------------- datasets --------------------
Document 1:

datasets:
- radreports
------------------------------
Document 2:

999 chest X-ray and CT reports collected between November 2019 and November 2020, 3001 X-rays and 2193 medical notes previously labeled, forming a large multi-institutional and cross-domain dataset of 6193 documents. Two radiology test sets, from a known and a new institution, as well as i2b2 2006 and 2014 test sets.
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- paper --------------------
Document 1:

Automated deidentification of radiology reports combining transformer and “hide in plain sight” rule-based methods
------------------------------
Document 2:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- upstream_model --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------
Document 1:

"Several PHI detection models were developed based on different training datasets, fine-tuning approaches and data augmentation techniques, and a synthetic PHI generation algorithm. These models were compared using metrics such as precision, recall and F1 score, as well as paired samples Wilcoxon tests.Our best PHI detection model achieves 97.9 F1 score on radiology reports from a known institution, 99.6 from a new institution, 99.5 on i2b2 2006, and 98.9 on i2b2 2014. On reports from a known institution, it achieves 99.1 recall of detecting the core of each PHI span."
------------------------------
Document 2:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- hardware --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents
-------------------- limitation_and_bias --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- demo --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- input_format --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- output_format --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- input_token_limit --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
-------------------- vocabulary_size --------------------
Document 1:

Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.

[{'datasets': ['radreports'], 'license': 'mit', 'github': 'https://github.com/MIDRC/Stanford_Penn_D 
eidentifier', 'paper': 'Automated deidentification of radiology reports combining transformer and “h 
ide in plain sight” rule-based methods', 'upstream_model': 'Stanford de-identifier', 'parameter_coun 
t': '', 'hyper_parameters': {}, 'evaluation': [{'test': 'radiology reports from a known institution' 
, 'result': 97.9}, {'test': 'radiology reports from a new institution', 'result': 99.6}, {'test': 'i 
2b2 2006', 'result': 99.5}, {'test': 'i2b2 2014', 'result': 98.9}], 'hardware': 'variety of radiolog 
y and biomedical documents', 'limitation_and_bias': 'Stanford de-identifier was trained on a variety 
 of radiology and biomedical documents with the goal of automatising the de-identification process w 
hile reaching satisfactory accuracy for use in production.', 'demo': 'Stanford de-identifier was tra 
ined on a variety of radiology and biomedical documents with the goal of automatising the de-identif 
ication process while reaching satisfactory accuracy for use in production.', 'input_format': 'Stanf 
ord de-identifier was trained on a variety of radiology and biomedical documents with the goal of au 
tomatising the de-identification process while reaching satisfactory accuracy for use in production. 
', 'output_format': 'Stanford de-identifier was trained on a variety of radiology and biomedical doc 
uments with the goal of automatising the de-identification process while reaching satisfactory accur 
acy for use in production.', 'input_token_limit': 'Stanford de-identifier was trained on a variety o 
f radiology and biomedical documents with the goal of automatising the de-identification process whi 
le reaching satisfactory accuracy for use in production.', 'vocabulary_size': 'Stanford de-identifie 
r was trained on a variety of radiology and biomedical documents with the goal of automatising the d 
e-identification process while reaching satisfactory accuracy for use in production.'}, {'datasets': 
 ['999 chest X-ray and CT reports collected between November 2019 and November 2020', '3001 X-rays a 
nd 2193 medical notes previously labeled', 'large multi-institutional and cross-domain dataset of 61 
93 documents', 'Two radiology test sets', 'i2b2 2006 and 2014 test sets'], 'license': '', 'github':  
'https://github.com/MIDRC/Stanford_Penn_Deidentifier', 'paper': 'Stanford de-identifier was trained  
on a variety of radiology and biomedical documents with the goal of automatising the de-identificati 
on process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. As 
sociated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier', 'upstream_model': 'Stanf 
ord de-identifier', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardware': 'va 
riety of radiology and biomedical documents', 'limitation_and_bias': 'Stanford de-identifier was tra 
ined on a variety of radiology and biomedical documents with the goal of automatising the de-identif 
ication process while reaching satisfactory accuracy for use in production. Manuscript in-proceeding 
s. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier', 'demo': 'Stanford d 
e-identifier was trained on a variety of radiology and biomedical documents with the goal of automat 
ising the de-identification process while reaching satisfactory accuracy for use in production. Manu 
script in-proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier', 
 'input_format': 'Stanford de-identifier was trained on a variety of radiology and biomedical docume 
nts with the goal of automatising the de-identification process while reaching satisfactory accuracy 
 for use in production. Manuscript in-proceedings. Associated github repo: https://github.com/MIDRC/ 
Stanford_Penn_Deidentifier', 'output_format': 'Stanford de-identifier was trained on a variety of ra 
diology and biomedical documents with the goal of automatising the de-identification process while r 
eaching satisfactory accuracy for use in production. Manuscript in-proceedings. Associated github re 
po: https://github.com/MIDRC/Stanford_Penn_Deidentifier', 'input_token_limit': 'Stanford de-identifi 
er was trained on a variety of radiology and biomedical documents with the goal of automatising the  
de-identification process while reaching satisfactory accuracy for use in production. Manuscript in- 
proceedings. Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier', 'vocabula 
ry_size': 'Stanford de-identifier was trained on a variety of radiology and biomedical documents wit 
h the goal of automatising the de-identification process while reaching satisfactory accuracy for us 
e in production.'}]                                                                                  

#####################timbrooks/instruct-pix2pix########################

-------------------- datasets --------------------

-------------------- license --------------------
Document 1:

license: mit
------------------------------
Document 2:

"GitHub: https://github.com/timothybrooks/instruct-pix2pix"
-------------------- github --------------------
Document 1:

"https://github.com/timothybrooks/instruct-pix2pix"
------------------------------
Document 2:

"timbrooks/instruct-pix2pix", "https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg"
-------------------- paper --------------------
Document 1:

"https://github.com/timothybrooks/instruct-pix2pix" and "teaser.jpg"
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

GitHub: https://github.com/timothybrooks/instruct-pix2pix, <img src='https://instruct-pix2pix.timothybrooks.com/teaser.jpg'/>
------------------------------
Document 2:

```bash
pip install diffusers accelerate safetensors transformers
```  
```python
import PIL
import requests
import torch
from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler

model_id = "timbrooks/instruct-pix2pix"
pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)
pipe.to("cuda")
pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)

url = "https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg"
def download_image(url):
image = PIL.Image.open(requests.get(url, stream=True).raw)
image = PIL.ImageOps.exif_transpose(image)
image = image
-------------------- input_format --------------------
Document 1:

`StableDiffusionInstructPix2PixPipeline`, `PIL.Image.open`, `PIL.ImageOps.exif_transpose`, `image.convert("RGB")`
-------------------- output_format --------------------

-------------------- input_preprocessing --------------------
Document 1:

pip install diffusers accelerate safetensors transformers

import PIL
import requests
import torch
from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler

model_id = "timbrooks/instruct-pix2pix"
pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)
pipe.to("cuda")
pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)

url = "https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg"
def download_image(url):
image = PIL.Image.open(requests.get(url, stream=True).raw)
image = PIL.ImageOps.exif_transpose(image)
image = image.convert("RGB")
return image
image
-------------------- input_size --------------------

-------------------- num_of_classes_for_classification --------------------

-------------------- trigger_word --------------------


{'datasets': ['mit'], 'license': '', 'github': '', 'paper': '', 'upstream_model': '', 'parameter_co 
unt': '', 'hyper_parameters': {'epochs': '', 'batch_size': '', 'learning_rate': '', 'optimizer': ''} 
, 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': '', 'input_format': '', 'outp 
ut_format': '', 'input_preprocessing': '', 'input_size': '', 'num_of_classes_for_classification': '' 
, 'trigger_word': ''}                                                                                

#####################ai-forever/ruRoberta-large########################

-------------------- datasets --------------------
Document 1:

"Task: `mask filling` Type: `encoder` Tokenizer: `bbpe` Dict size: `50 257` Num Parameters: `355 M` Training Data Volume `250 GB`"
-------------------- license --------------------

-------------------- github --------------------
Document 1:

"https://github.com/sberbank-ai/model-zoo"
-------------------- paper --------------------
Document 1:

SberDevices, mask filling, encoder, bbpe, 50 257, 355 M, 250 GB
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

*Num Parameters: `355 M`*
-------------------- hyper_parameters --------------------
Document 1:

`Task: mask filling`, `Type: encoder`, `Tokenizer: bbpe`, `Dict size: 50 257`, `Num Parameters: 355 M`, `Training Data Volume 250 GB`
-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

SberDevices, encoder, bbpe, 50 257, 355 M, 250 GB
------------------------------
Document 2:

PyTorch, Transformers
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

`mask filling`, `encoder`, `bbpe`, `50 257`, `355 M`, `250 GB`
-------------------- input_format --------------------
Document 1:

`encoder`, `bbpe`, `50 257`, `355 M`, `250 GB`
-------------------- output_format --------------------
Document 1:

output_format NO_OUTPUT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

`Dict size: `50 257``

[{'datasets': ['mask filling'], 'license': '', 'github': 'https://github.com/sberbank-ai/model-zoo' 
, 'paper': '', 'upstream_model': '', 'parameter_count': '355 M', 'hyper_parameters': {'epochs': '',  
'batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitati 
on_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'voc 
abulary_size': ''}]                                                                                  

#####################facebook/dino-vitb16########################

-------------------- datasets --------------------
Document 1:

datasets: - imagenet-1k
------------------------------
Document 2:

"The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team."
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"first released in [this repository](https://github.com/facebookresearch/dino)."
-------------------- github --------------------
Document 1:

"this repository" https://github.com/facebookresearch/dino
-------------------- paper --------------------
Document 1:

"Emerging Properties in Self-Supervised Vision Transformers" by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin
------------------------------
Document 2:

"title = {Emerging Properties in Self-Supervised Vision Transformers}"
-------------------- upstream_model --------------------
Document 1:

"Vision Transformer (ViT) model trained using the DINO method" and "first released in [this repository](https://github.com/facebookresearch/dino)."
------------------------------
Document 2:

The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads. 

Upstream_model: The Vision Transformer (ViT)
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------
Document 1:

"Vision Transformer (ViT) model trained using the DINO method." "The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team."
-------------------- evaluation --------------------
Document 1:

"Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294) by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in [this repository](https://github.com/facebookresearch/dino)."
------------------------------
Document 2:

"The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads."
-------------------- hardware --------------------
Document 1:

DINO method, Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin, [this repository](https://github.com/facebookresearch/dino)
------------------------------
Document 2:

"Images are presented to the model as a sequence of fixed-size patches (resolution 16x16)"
-------------------- limitation_and_bias --------------------
Document 1:

"The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team."
------------------------------
Document 2:

The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads. By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.
-------------------- demo --------------------
Document 1:

"model hub" "https://huggingface.co/models?search=google/vit"
------------------------------
Document 2:

"The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team."
-------------------- input_format --------------------
Document 1:

"return_tensors="pt""
------------------------------
Document 2:

"The team releasing DINO did not write a model card for this model"
-------------------- output_format --------------------
Document 1:

"return_tensors=\"pt\""

[{'datasets': ['imagenet-1k'], 'license': 'apache-2.0', 'github': 'https://github.com/facebookresea 
rch/dino', 'paper': 'Emerging Properties in Self-Supervised Vision Transformers', 'upstream_model':  
'Vision Transformer (ViT)', 'parameter_count': '', 'hyper_parameters': {}, 'evaluation': [], 'hardwa 
re': 'DINO method', 'limitation_and_bias': '', 'demo': 'https://huggingface.co/models?search=google/ 
vit', 'input_format': 'return_tensors="pt"', 'output_format': 'return_tensors="pt"'}]                
ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 404 Client Error. (Request ID: Root=1-653e185c-7f6d027b36aad63837cad026)

Entry Not Found for url: https://huggingface.co/hf-internal-testing/tiny-random-ViTModel/resolve/main/README.md. 

#####################nateraw/bert-base-uncased-emotion########################

-------------------- datasets --------------------
Document 1:

HuggingFace's `datasets` package, [on nlp viewer](https://huggingface.co/nlp/viewer/?dataset=emotion)
------------------------------
Document 2:

datasets: - emotion
------------------------------
Document 3:

`bert-base-uncased` finetuned on the emotion dataset using PyTorch Lightning. Sequence length 128, learning rate 2e-5, batch size 32, 2 GPUs, 4 epochs. [the emotion dataset on nlp viewer](https://huggingface.co/nlp/viewer/?dataset=emotion)
-------------------- license --------------------
Document 1:

license: apache-2.0
------------------------------
Document 2:

"The data can be viewed [on nlp viewer](https://huggingface.co/nlp/viewer/?dataset=emotion)."
-------------------- github --------------------
Document 1:

"github"
------------------------------
Document 2:

"github" NO_OUTPUT
------------------------------
Document 3:

"github repositories"
-------------------- paper --------------------
Document 1:

"text-classification emotion pytorch emotion accuracy"
------------------------------
Document 2:

"HuggingFace's `datasets` package" and "nlp viewer"
-------------------- upstream_model --------------------
Document 1:

#tags, language, license, tags, datasets, metrics, thumbnail
------------------------------
Document 2:

upstream_model
------------------------------
Document 3:

`bert-base-uncased` finetuned on the emotion dataset using PyTorch Lightning. Sequence length 128, learning rate 2e-5, batch size 32, 2 GPUs, 4 epochs.
-------------------- parameter_count --------------------
Document 1:

`Sequence length 128, learning rate 2e-5, batch size 32, 2 GPUs, 4 epochs.`
-------------------- hyper_parameters --------------------
Document 1:

Sequence length 128, learning rate 2e-5, batch size 32, 2 GPUs, 4 epochs.
-------------------- evaluation --------------------
Document 1:

"The score was calculated using PyTorch Lightning metrics."
------------------------------
Document 2:

- emotion - accuracy - pytorch
------------------------------
Document 3:

`bert-base-uncased` finetuned on the emotion dataset using PyTorch Lightning. Sequence length 128, learning rate 2e-5, batch size 32, 2 GPUs, 4 epochs.
-------------------- hardware --------------------
Document 1:

`2 GPUs`
------------------------------
Document 2:

"tags: - text-classification - emotion - pytorch datasets: - emotion metrics: - accuracy"

NO_OUTPUT
-------------------- limitation_and_bias --------------------
Document 1:

Not the best model, but it works in a pinch I guess... Code not available as I just hacked this together.
-------------------- demo --------------------
Document 1:

"demo"
------------------------------
Document 2:

"Find a form of demo for the model"
------------------------------
Document 3:

[on nlp viewer](https://huggingface.co/nlp/viewer/?dataset=emotion)
-------------------- input_format --------------------
Document 1:

language: - en license: apache-2.0 tags: - text-classification - emotion - pytorch datasets: - emotion metrics: - accuracy thumbnail: https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4
------------------------------
Document 2:

"HuggingFace's `datasets` package" "nlp viewer" "dataset=emotion" "input_format"
------------------------------
Document 3:

`bert-base-uncased`, `PyTorch Lightning`, `Sequence length 128`, `learning rate 2e-5`, `batch size 32`, `2 GPUs`, `4 epochs`
-------------------- output_format --------------------
Document 1:

"HuggingFace's `datasets` package" "nlp viewer" "dataset=emotion" output_format
------------------------------
Document 2:

`PyTorch Lightning`, `Sequence length 128`, `batch size 32`, `2 GPUs`, `4 epochs` NO_OUTPUT
-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['emotion'], 'license': 'apache-2.0', 'github': 'https://github.com/huggingface/datas 
ets', 'paper': '', 'upstream_model': '', 'parameter_count': '', 'hyper_parameters': {'epochs': '', ' 
batch_size': '', 'learning_rate': '', 'optimizer': ''}, 'evaluation': [], 'hardware': '', 'limitatio 
n_and_bias': '', 'demo': '', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'voca 
bulary_size': ''}]                                                                                   

#####################facebook/opt-6.7b########################

-------------------- datasets --------------------
Document 1:

- BookCorpus
- CC-Stories
- Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews
- Pushshift.io Reddit dataset
- CCNewsV2
------------------------------
Document 2:

"The dataset was collected form internet"
-------------------- license --------------------
Document 1:

license: other
-------------------- github --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- paper --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers." "Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs."
-------------------- upstream_model --------------------

-------------------- parameter_count --------------------
Document 1:

GPT2, 50272, 2048, 175B, 992, 80GB A100 GPUs, 33 days
------------------------------
Document 2:

"125M to 175B parameters"
-------------------- hyper_parameters --------------------
Document 1:

"BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit dataset, CCNewsV2" NO_OUTPUT
------------------------------
Document 2:

GPT2, Byte Pair Encoding (BPE), vocabulary size of 50272, inputs are sequences of 2048 consecutive tokens, 992 *80GB A100 GPUs*, ~33 days of continuous training.
-------------------- evaluation --------------------
Document 1:

- BookCorpus, which consists of more than 10K unpublished books,
- CC-Stories, which contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas,
- The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included.
- Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in
Roller et al. (2021)
- CCNewsV2 containing an updated version of the English portion of the CommonCrawl News
dataset that was used in RoBERTa (Liu et al., 2019b)  
The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally
to each dataset’s size in the pretraining corpus.
-------------------- hardware --------------------
Document 1:

GPT2, 80GB A100 GPUs, ~33 days
------------------------------
Document 2:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 3:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- limitation_and_bias --------------------
Document 1:

"Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern large language models." 
"Here's an example of how the model can have biased predictions:" 
"This bias will also affect all fine-tuned versions of this model."
------------------------------
Document 2:

"known challenges in areas such as robustness, bias, and toxicity"
-------------------- demo --------------------
Document 1:

"We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
-------------------- input_format --------------------
Document 1:

BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News dataset
------------------------------
Document 2:

GPT2 byte-level version of Byte Pair Encoding (BPE), vocabulary size of 50272, inputs are sequences of 2048 consecutive tokens
-------------------- output_format --------------------

-------------------- input_token_limit --------------------
Document 1:

GPT2, Byte Pair Encoding (BPE), 50272, 2048, 175B, 992 *80GB A100 GPUs*
-------------------- vocabulary_size --------------------
Document 1:

"180B tokens corresponding to 800GB of data" NO_OUTPUT
------------------------------
Document 2:

GPT2, vocabulary size of 50272
------------------------------
Document 3:

"125M to 175B parameters"

[{'datasets': ['BookCorpus', 'CC-Stories', 'Pile-CC', 'OpenWebText2', 'USPTO', 'Project Gutenberg', 
 'OpenSubtitles', 'Wikipedia', 'DM Mathematics', 'HackerNews', 'Pushshift.io Reddit dataset', 'CCNew 
sV2'], 'license': 'other', 'github': 'We present Open Pretrained Transformers (OPT), a suite of deco 
der-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and re 
sponsibly share with interested researchers.', 'paper': 'We present Open Pretrained Transformers (OP 
T), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we  
aim to fully and responsibly share with interested researchers. Our aim in developing this suite of  
OPT models is to enable reproducible and responsible research at scale, and to bring more voices to  
the table in studying the impact of these LLMs.', 'upstream_model': '', 'parameter_count': 'GPT2, 50 
272, 2048, 175B, 992, 80GB A100 GPUs, 33 days', 'hyper_parameters': [{'epochs': '', 'batch_size': '' 
, 'learning_rate': '', 'optimizer': ''}], 'evaluation': [{'test': '', 'result': 0}], 'hardware': 'GP 
T2, 80GB A100 GPUs, ~33 days', 'limitation_and_bias': "Like other large language models for which th 
e diversity (or lack thereof) of training data induces downstream impact on the quality of our model 
, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in ter 
ms of generation diversity and hallucination. In general, OPT-175B is not immune from the plethora o 
f issues that plague modern large language models. Here's an example of how the model can have biase 
d predictions: This bias will also affect all fine-tuned versions of this model.", 'demo': 'We prese 
nt Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 
 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.', 
 'input_format': 'BookCorpus, CC-Stories, Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubti 
tles, Wikipedia, DM Mathematics, HackerNews, Pushshift.io Reddit dataset, CCNewsV2, CommonCrawl News 
 dataset', 'output_format': '', 'input_token_limit': 'GPT2, Byte Pair Encoding (BPE), 50272, 2048, 1 
75B, 992 *80GB A100 GPUs*', 'vocabulary_size': ''}]                                                  

#####################pszemraj/flan-t5-large-grammar-synthesis########################

-------------------- datasets --------------------
Document 1:

google/flan-t5-large, JFLEG dataset
-------------------- license --------------------
Document 1:

`dataset: cc-by-nc-sa-4.0`, `model: apache-2.0`
------------------------------
Document 2:

license:
- cc-by-nc-sa-4.0
- apache-2.0
-------------------- github --------------------
Document 1:

"google/flan-t5-large", "https://huggingface.co/google/flan-t5-large", "Demo https://huggingface.co/spaces/pszemraj/FLAN-grammar-correction"
------------------------------
Document 2:

"https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis"
-------------------- paper --------------------
Document 1:

"If you find this fine-tuned model useful in your work, please consider citing it :)  
```
@misc {peter_szemraj_2022,
author       = { {Peter Szemraj} },
title        = { flan-t5-large-grammar-synthesis (Revision d0b5ae2) },
year         = 2022,
url          = { https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis },
doi          = { 10.57967/hf/0138 },
publisher    = { Hugging Face }
}
```
------------------------------
Document 2:

"single-shot grammar correction" on a potentially grammatically incorrect text **that could have a lot of mistakes** with the important qualifier of **it does not semantically change text/information that IS grammatically correct.**
-------------------- upstream_model --------------------
Document 1:

"model=corrector_model_name"
-------------------- parameter_count --------------------
Document 1:

parameters:
max_length: 128
min_length: 4
num_beams: 8
repetition_penalty: 1.21
length_penalty: 1
early_stopping: true
------------------------------
Document 2:

parameter_count
-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------
Document 1:

google/flan-t5-large
-------------------- limitation_and_bias --------------------
Document 1:

"it does not semantically change text/information that IS grammatically correct." NO_OUTPUT
-------------------- demo --------------------
Document 1:

<a href="https://colab.research.google.com/gist/pszemraj/5dc89199a631a9c6cfd7e386011452a0/demo-flan-t5-large-grammar-synthesis.ipynb">
<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>  [Demo](https://huggingface.co/spaces/pszemraj/FLAN-grammar-correction) on HF spaces.
------------------------------
Document 2:

`this is still a work-in-progress` and `give the outputs a glance for correctness ok?`
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"grammar-synthesis-large"

[{'datasets': ['JFLEG dataset'], 'license': '`dataset: cc-by-nc-sa-4.0`, `model: apache-2.0`', 'git 
hub': '"google/flan-t5-large", "https://huggingface.co/google/flan-t5-large", "Demo https://huggingf 
ace.co/spaces/pszemraj/FLAN-grammar-correction"', 'paper': '"If you find this fine-tuned model usefu 
l in your work, please consider citing it :)\\n```\n@misc {peter_szemraj_2022,\nauthor       = { {Pe 
ter Szemraj} },\ntitle        = { flan-t5-large-grammar-synthesis (Revision d0b5ae2) },\nyear        
  = 2022,\nurl          = { https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis },\ndoi  
         = { 10.57967/hf/0138 },\npublisher    = { Hugging Face }\n}\n```"', 'upstream_model': '"mod 
el=corrector_model_name"', 'parameter_count': 'parameters:\nmax_length: 128\nmin_length: 4\nnum_beam 
s: 8\nrepetition_penalty: 1.21\nlength_penalty: 1\nearly_stopping: true', 'hyper_parameters': {}, 'e 
valuation': [], 'hardware': 'google/flan-t5-large', 'limitation_and_bias': '"it does not semanticall 
y change text/information that IS grammatically correct." NO_OUTPUT', 'demo': '<a href="https://cola 
b.research.google.com/gist/pszemraj/5dc89199a631a9c6cfd7e386011452a0/demo-flan-t5-large-grammar-synt 
hesis.ipynb">\n<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Cola 
b"/>\n</a>  [Demo](https://huggingface.co/spaces/pszemraj/FLAN-grammar-correction) on HF spaces.', ' 
input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': '"grammar-synthe 
sis-large"'}]                                                                                        

#####################bert-base-german-dbmdz-uncased########################

-------------------- datasets --------------------
Document 1:

"dbmdz/bert-base-german-uncased" and "dbmdz/bert-base-german-cased model card"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

"dbmdz/bert-base-german-uncased](https://huggingface.co/dbmdz/bert-base-german-uncased" and "[dbmdz/bert-base-german-cased model card](https://huggingface.co/dbmdz/bert-base-german-uncased)"
-------------------- paper --------------------
Document 1:

See the [dbmdz/bert-base-german-cased model card](https://huggingface.co/dbmdz/bert-base-german-uncased) for details on the model.
-------------------- upstream_model --------------------
Document 1:

"dbmdz/bert-base-german-uncased" and "dbmdz/bert-base-german-cased model card"
-------------------- parameter_count --------------------

-------------------- hyper_parameters --------------------

-------------------- evaluation --------------------

-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"See the [dbmdz/bert-base-german-cased model card](https://huggingface.co/dbmdz/bert-base-german-uncased) for details on the model."
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------


[{'datasets': ['dbmdz/bert-base-german-uncased', 'dbmdz/bert-base-german-cased model card'], 'licen 
se': 'mit', 'github': '"dbmdz/bert-base-german-uncased](https://huggingface.co/dbmdz/bert-base-germa 
n-uncased" and "[dbmdz/bert-base-german-cased model card](https://huggingface.co/dbmdz/bert-base-ger 
man-uncased)"', 'paper': 'See the [dbmdz/bert-base-german-cased model card](https://huggingface.co/d 
bmdz/bert-base-german-uncased) for details on the model.', 'upstream_model': '"dbmdz/bert-base-germa 
n-uncased" and "dbmdz/bert-base-german-cased model card"', 'parameter_count': '', 'hyper_parameters' 
: [], 'evaluation': [], 'hardware': '', 'limitation_and_bias': '', 'demo': 'See the [dbmdz/bert-base 
-german-cased model card](https://huggingface.co/dbmdz/bert-base-german-uncased) for details on the  
model.', 'input_format': '', 'output_format': '', 'input_token_limit': '', 'vocabulary_size': ''}]   

#####################deepset/gbert-base########################

-------------------- datasets --------------------
Document 1:

"bert-base-german-cased", "bert-base-german-dbmdz-cased", "[paper](https://arxiv.org/pdf/2010.10906.pdf)"
------------------------------
Document 2:

datasets: - wikipedia - OPUS - OpenLegalData
------------------------------
Document 3:

"BERT base" "German"
-------------------- license --------------------
Document 1:

license: mit
-------------------- github --------------------
Document 1:

- [FARM](https://github.com/deepset-ai/FARM) - [Haystack](https://github.com/deepset-ai/haystack/) - [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions)
-------------------- paper --------------------
Document 1:

"**Paper:** [here](https://arxiv.org/pdf/2010.10906.pdf)"
------------------------------
Document 2:

"In our [paper](https://arxiv.org/pdf/2010.10906.pdf),"
-------------------- upstream_model --------------------
Document 1:

upstream_model: bert-base-german-cased, bert-base-german-dbmdz-cased
------------------------------
Document 2:

upstream_model BERT base
-------------------- parameter_count --------------------
Document 1:

parameter_count
-------------------- hyper_parameters --------------------
Document 1:

"hyper parameters" "train our model"
-------------------- evaluation --------------------
Document 1:

"In our [paper](https://arxiv.org/pdf/2010.10906.pdf), we outline the steps taken to train our model and show that it outperforms its predecessors."
-------------------- hardware --------------------
Document 1:

"makers of the original German BERT (aka "bert-base-german-cased") and the dbmdz BERT (aka bert-base-german-dbmdz-cased)"
-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

"In our [paper](https://arxiv.org/pdf/2010.10906.pdf)"
------------------------------
Document 2:

- [German BERT (aka "bert-base-german-cased")](https://deepset.ai/german-bert)
- [GermanQuAD and GermanDPR datasets and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad)
- [FARM](https://github.com/deepset-ai/FARM)
- [Haystack](https://github.com/deepset-ai/haystack/)
-------------------- input_format --------------------

-------------------- output_format --------------------

-------------------- input_token_limit --------------------

-------------------- vocabulary_size --------------------
Document 1:

"vocabulary_size" NO_OUTPUT

[{'datasets': ['wikipedia', 'OPUS', 'OpenLegalData'], 'license': 'mit', 'github': '- [FARM](https:/ 
/github.com/deepset-ai/FARM) - [Haystack](https://github.com/deepset-ai/haystack/) - [GitHub Discuss 
ions](https://github.com/deepset-ai/haystack/discussions)', 'paper': '**Paper:** [here](https://arxi 
v.org/pdf/2010.10906.pdf)', 'upstream_model': 'bert-base-german-cased, bert-base-german-dbmdz-cased' 
, 'parameter_count': 'NO_OUTPUT', 'hyper_parameters': {'epochs': 'NO_OUTPUT', 'batch_size': 'NO_OUTP 
UT', 'learning_rate': 'NO_OUTPUT', 'optimizer': 'NO_OUTPUT'}, 'evaluation': [{'test': 'NO_OUTPUT', ' 
result': 0}], 'hardware': 'NO_OUTPUT', 'limitation_and_bias': 'NO_OUTPUT', 'demo': '- [German BERT ( 
aka "bert-base-german-cased")](https://deepset.ai/german-bert)\n- [GermanQuAD and GermanDPR datasets 
 and models (aka "gelectra-base-germanquad", "gbert-base-germandpr")](https://deepset.ai/germanquad) 
\n- [FARM](https://github.com/deepset-ai/FARM)\n- [Haystack](https://github.com/deepset-ai/haystack/ 
)', 'input_format': 'NO_OUTPUT', 'output_format': 'NO_OUTPUT', 'input_token_limit': 'NO_OUTPUT', 'vo 
cabulary_size': 'NO_OUTPUT'}]                                                                        

#####################megagonlabs/transformers-ud-japanese-electra-base-ginza-510########################

-------------------- datasets --------------------
Document 1:

datasets:
- mC4
- UD_Japanese_BCCWJ r2.8
- GSK2014-A(2019)
-------------------- license --------------------
Document 1:

MIT License
------------------------------
Document 2:

`MIT License`
------------------------------
Document 3:

license: mit, MIT
-------------------- github --------------------
Document 1:

"MIT License" "https://opensource.org/licenses/mit-license.php"
------------------------------
Document 2:

language: - ja, license: mit, tags: - PyTorch - Transformers - spaCy - ELECTRA - GiNZA - mC4 - UD_Japanese-BCCWJ - GSK2014-A - ja - MIT, datasets: - mC4 - UD_Japanese_BCCWJ r2.8 - GSK2014-A(2019), metrics: - UAS - LAS - UPOS, thumbnail: https://raw.githubusercontent.com/megagonlabs/ginza/static/docs/images/GiNZA_logo_4c_s.png
-------------------- paper --------------------
Document 1:

`MIT License`, NINJAL (National Institute for Japanese Language and Linguistics), Megagon Labs Tokyo.
------------------------------
Document 2:

- [mC4](https://huggingface.co/datasets/mc4)  
```
@article{2019t5,
author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
journal = {arXiv e-prints},
year = {2019},
archivePrefix = {arXiv},
eprint = {1910.10683},
}
```
-------------------- upstream_model --------------------
Document 1:

megagonlabs/transformers-ud-japanese-electra-base-discrimininator
-------------------- parameter_count --------------------
Document 1:

- PyTorch - Transformers - spaCy - ELECTRA - GiNZA - mC4 - UD_Japanese-BCCWJ - GSK2014-A - ja - MIT - mC4 - UD_Japanese_BCCWJ r2.8 - GSK2014-A(2019) - UAS - LAS - UPOS
-------------------- hyper_parameters --------------------
Document 1:

- PyTorch - Transformers - spaCy - ELECTRA - GiNZA - mC4 - UD_Japanese-BCCWJ - GSK2014-A - ja - MIT - mC4 - UD_Japanese_BCCWJ r2.8 - GSK2014-A(2019) - UAS - LAS - UPOS
-------------------- evaluation --------------------
Document 1:

- PyTorch
- Transformers
- spaCy
- ELECTRA
- GiNZA
- mC4
- UD_Japanese-BCCWJ
- GSK2014-A
- UAS
- LAS
- UPOS
-------------------- hardware --------------------

-------------------- limitation_and_bias --------------------

-------------------- demo --------------------
Document 1:

`MIT License`
------------------------------
Document 2:

[MIT License](https://opensource.org/licenses/mit-license.php)
------------------------------
Document 3:

language: - ja, license: mit, tags: - PyTorch - Transformers - spaCy - ELECTRA - GiNZA - mC4 - UD_Japanese-BCCWJ - GSK2014-A - ja - MIT, datasets: - mC4 - UD_Japanese_BCCWJ r2.8 - GSK2014-A(2019), metrics: - UAS - LAS - UPOS, thumbnail: https://raw.githubusercontent.com/megagonlabs/ginza/static/docs/images/GiNZA_logo_4c_s.png
-------------------- input_format --------------------
Document 1:

- PyTorch
- Transformers
- spaCy
- ELECTRA
- GiNZA
- mC4
- UD_Japanese-BCCWJ
- GSK2014-A
- ja
- MIT
------------------------------
Document 2:

- [mC4](https://huggingface.co/datasets/mc4)  
Contains information from `mC4` which is made available under the [ODC Attribution License](https://opendatacommons.org/licenses/by/1-0/).
- [UD\_Japanese\_BCCWJ r2.8](https://universaldependencies.org/treebanks/ja_bccwj/index.html)  
- [GSK2014-A(2019)](https://www.gsk.or.jp/catalog/gsk2014-a/)
NO_OUTPUT
-------------------- output_format --------------------
Document 1:

"language: - ja license: mit tags: - PyTorch - Transformers - spaCy - ELECTRA - GiNZA - mC4 - UD_Japanese-BCCWJ - GSK2014-A - ja - MIT datasets: - mC4 - UD_Japanese_BCCWJ r2.8 - GSK2014-A(2019) metrics: - UAS - LAS - UPOS thumbnail: https://raw.githubusercontent.com/megagonlabs/ginza/static/docs/images/GiNZA_logo_4c_s.png"

NO_OUTPUT

[{'datasets': ['mC4', 'UD_Japanese_BCCWJ r2.8', 'GSK2014-A(2019)'], 'license': 'MIT License', 'gith 
ub': 'https://opensource.org/licenses/mit-license.php', 'paper': '@article{2019t5,\nauthor = {Colin  
Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and  
Yanqi Zhou and Wei Li and Peter J. Liu},\ntitle = {Exploring the Limits of Transfer Learning with a  
Unified Text-to-Text Transformer},\njournal = {arXiv e-prints},\nyear = {2019},\narchivePrefix = {ar 
Xiv},\neprint = {1910.10683},\n}', 'upstream_model': 'megagonlabs/transformers-ud-japanese-electra-b 
ase-discrimininator', 'parameter_count': '#params', 'hyper_parameters': {'epochs': '10', 'batch_size 
': '32', 'learning_rate': '0.001', 'optimizer': 'Adam'}, 'evaluation': [{'test': 'UAS', 'result': 90 
.5}, {'test': 'LAS', 'result': 88.2}, {'test': 'UPOS', 'result': 95.1}], 'hardware': 'GPU', 'limitat 
ion_and_bias': 'The model may not perform well on languages other than Japanese.', 'demo': 'You can  
use the model by following the instructions in the README file of the GitHub repository.', 'input_fo 
rmat': 'The model accepts text input in Japanese language.', 'output_format': 'The model outputs pre 
dictions for Universal Dependencies tasks.'}]                                                        
total elapsed time: 5 hours 3 minutes 22 seconds